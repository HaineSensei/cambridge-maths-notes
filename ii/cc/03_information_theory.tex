\subsection{Shannon's first coding theorem}
\begin{definition}
    A \emph{source} is a sequence of random variables \( X_1, X_2, \dots \) taking values in \( \mathcal A \).
\end{definition}
\begin{example}
    The \emph{Bernoulli} (or \emph{memoryless}) source is a source where the \( X_i \) are independent and identically distributed according to a Bernoulli distribution.
\end{example}
\begin{definition}
    A source \( X_1, X_2, \dots \) is \emph{reliably encodable} at rate \( r \) if there exist subsets \( A_n \subseteq \mathcal A^n \) such that
    \begin{enumerate}
        \item \( \lim \frac{\log \abs{A_n}}{n} = r \);
        \item \( \lim \prob{(X_1, \dots, X_n) \in A_n} = 1 \).
    \end{enumerate}
\end{definition}
\begin{definition}
    The \emph{information rate} \( H \) of a source is the infimum of all reliable encoding rates.
\end{definition}
\begin{example}
    \( 0 \leq H \leq \log\abs{\mathcal A} \), with both bounds attainable.
    The proof is left as an exercise.
\end{example}
Shannon's first coding theorem computes the information rate of certain sources, including Bernoulli sources.

Recall from IA Probability that a probability space is a tuple \( (\Omega, \mathcal F, \mathbb P) \), and a discrete random variable is a function \( X \colon \Omega \to \mathcal A \).
The probability mass function is the function \( p_X \colon \mathcal A \to [0,1] \) given by \( p_X(x) = \prob{X = x} \).
We can consider the function \( p(X) \colon \Omega \to [0,1] \) defined by the composition \( p_X \circ X \), which assigns \( p(X)(\omega) = \prob{X = X(\omega)} \); hence, \( p(X) \) is also a random variable.
