\subsection{?}
\begin{definition}
    A \emph{source} is a sequence of random variables \( X_1, X_2, \dots \) taking values in \( \mathcal A \).
\end{definition}
\begin{example}
    The \emph{Bernoulli} (or \emph{memoryless}) source is a source where the \( X_i \) are independent and identically distributed according to a Bernoulli distribution.
\end{example}
\begin{definition}
    A source \( X_1, X_2, \dots \) is \emph{reliably encodable} at rate \( r \) if there exist subsets \( A_n \subseteq \mathcal A^n \) such that
    \begin{enumerate}
        \item \( \lim \frac{\log \abs{A_n}}{n} = r \);
        \item \( \lim \prob{(X_1, \dots, X_n) \in A_n} = 1 \).
    \end{enumerate}
\end{definition}
\begin{definition}
    The \emph{information rate} \( H \) of a source is the infimum of all reliable encoding rates.
\end{definition}
\begin{example}
    \( 0 \leq H \leq \log\abs{\mathcal A} \), with both bounds attainable.
    The proof is left as an exercise.
\end{example}
Shannon's first coding theorem computes the information rate of certain sources, including Bernoulli sources.

Recall from IA Probability that a probability space is a tuple \( (\Omega, \mathcal F, \mathbb P) \), and a discrete random variable is a function \( X \colon \Omega \to \mathcal A \).
The probability mass function is the function \( p_X \colon \mathcal A \to [0,1] \) given by \( p_X(x) = \prob{X = x} \).
We can consider the function \( p(X) \colon \Omega \to [0,1] \) defined by the composition \( p_X \circ X \), which assigns \( p(X)(\omega) = \prob{X = X(\omega)} \); hence, \( p(X) \) is also a random variable.

Similarly, given a source \( X_1, X_2, \dots \) of random variables with values in \( \mathcal A \), then the probability mass function of any tuple \( X^{(n)} = (X_1, \dots, X_n) \) is \( p_{X^{n}}(x_1, \dots, x_n) = \prob{X_1 = x_1, \dots, X_n = x_n} \).
As \( p_{X^{(n)}} \colon \mathcal A^n \to [0,1] \), and \( X^{(n)} \colon \Omega \to \mathcal A^n \), we can consider \( p(X^{(n)}) = p_{X^{(n)}} \circ X^{(n)} \) defined by \( \omega \mapsto p_{X^{(n)}}(X^{(n)}(\omega)) \).
\begin{example}
    Let \( \mathcal A = \qty{A, B, C} \).
    Suppose
    \[ X^{(2)} = \begin{cases}
        AB & \text{with probability } 0.3 \\
        AC & \text{with probability } 0.1 \\
        BC & \text{with probability } 0.1 \\
        BA & \text{with probability } 0.2 \\
        CA & \text{with probability } 0.25 \\
        CB & \text{with probability } 0.05
    \end{cases} \]
    Then, \( p_{X^{(2)}}(AB) = 0.3 \), and so on.
    Hence,
    \[ p(X^{(2)}) = \begin{cases}
        0.3 & \text{with probability } 0.3 \\
        0.1 & \text{with probability } 0.2 \\
        0.2 & \text{with probability } 0.2 \\
        0.25 & \text{with probability } 0.25 \\
        0.05 & \text{with probability } 0.05
    \end{cases} \]
\end{example}
We say that a source \( X_1,X_2, \dots \) converges in probability to a random variable \( L \) if for all \( \varepsilon > 0 \), \( \lim_{n \to \infty} \prob{\abs{X_n - L} > \varepsilon} = 0 \).
We write \( X_n \xrightarrow{\mathbb P} L \).
The weak law of large numbers states that if \( X_1, X_2, \dots \) is a sequence of independent identically distributed real-valued random variables with finite expectation \( \expect{X_1} \), then \( \frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{\mathbb P} \expect{X} \).
\begin{example}
    Let \( X_1, X_2, \dots \) be a Bernoulli source.
    Then \( p(X_1), p(X_2), \dots \) are independent and identically distributed random variables, and \( p(X_1, \dots, X_n) = p(X_1) \dots p(X_n) \).
    Note that by the weak law of large numbers,
    \[ -\frac{1}{n} \log p(X_1, \dots, X_n) = -\frac{1}{n} \sum_{i=1}^n \log p(X_i) \xrightarrow{\mathbb P} \expect{-\log p(X_1)} = H(X_1) \]
\end{example}
\begin{lemma}
    The information rate of a Bernoulli source \( X_1, X_2, \dots \) is at most the expected word length of an optimal code \( c \colon \mathcal A \to \qty{0,1}^\star \) for \( X_1 \).
\end{lemma}
\begin{proof}
    Let \( \ell_1, \ell_2, \dots \) be the codeword lengths when we encode \( X_1, X_2, \dots \) using \( c \).
    Let \( \varepsilon > 0 \).
    Let
    \[ A_n = \qty{x \in \mathcal A^n \mid c^\star(x) \text{ has length less than } n \qty(\expect{\ell_1} + \varepsilon)} \]
    Then,
    \[ \prob{(X_1, \dots, X_n) \in A_n} = \prob{\sum_{i=1}^n \ell_i \leq n \qty(\expect{\ell_1} + \varepsilon)} = \prob{\abs{\frac{1}{n} \sum_{i=1}^n \ell_i - \expect{\ell_i}} < \varepsilon} \to 1 \]
    Now, \( c \) is decipherable so \( c^\star \) is injective.
    Hence, \( \abs{A_n} \leq 2^{n\qty(\expect{\ell_1} + \varepsilon)} \).
    Making \( A_n \) larger if necessary, we can assume \( \abs{A_n} = \floor{2^{n\qty(\expect{\ell_1} + \varepsilon)}} \).
    Taking logarithms, \( \frac{\log A_n}{n} \to \expect{\ell_1} + \varepsilon \).
    So \( X_1, X_2, \dots \) is reliably encodable at rate \( r = \expect{\ell_1} + \varepsilon \) for all \( \varepsilon > 0 \).
    Hence the information rate is at most \( \expect{\ell_1} \).
\end{proof}
\begin{corollary}
    A Bernoulli source has information rate less than \( H(X_1) + 1 \).
\end{corollary}
\begin{proof}
    Combine the previous lemma with the noiseless coding theorem.
\end{proof}

\subsection{Asymptotic equipartition property}
Suppose we encode \( X_1, X_2, \dots \) in blocks of size \( N \).
Let \( Y_1 = (X_1, \dots, X_N), Y_2 = (X_{N+1}, \dots, X_{2N}) \) and so on, such that \( Y_1, Y_2, \dots \) take values in \( \mathcal A^N \).
One can show that if the source \( X_1, X_2, \dots \) has information rate \( H \), then \( Y_1, Y_2, \dots \) has information rate \( NH \).
\begin{proposition}
    The information rate \( H \) of a Bernoulli source is at most \( H(X_1) \).
\end{proposition}
\begin{proof}
    Apply the previous corollary to the \( Y_i \) to obtain
    \[ NH < H(Y_1) + 1 = H(X_1, \dots, X_N) + 1 = NH(X_1) + 1 \implies H < H(X_1) + \frac{1}{N} \]
    as required.
\end{proof}
\begin{definition}
    A source \( X_1, X_2, \dots \) satisfies the \emph{asymptotic equipartition property} if there exists a constant \( H \geq 0 \) such that
    \[ -\frac{1}{n} \log p(X_1, \dots, X_n) \xrightarrow{\mathbb P} H \]
\end{definition}
\begin{example}
    Suppose we toss a biased coin with probability \( p \) of obtaining a head.
    Let \( X_1, X_2, \dots \) be the results of independent coin tosses.
    If we toss the coin \( N \) times, we expect \( pN \) heads and \( (1-p)N \) tails.
    The probability of any particular sequence of \( pN \) heads and \( (1-p)N \) tails is
    \[ p^{pN} (1-p)^{(1-p)N} = 2^{N (p \log p + (1-p) \log(1-p))} = 2^{-NH(X)} \]
    Not every sequence of tosses is of this form, but there is only a small probability of `atypical sequences'.
    With high probability, it is a `typical sequence' which has a probability close to \( 2^{-NH(X)} \).
\end{example}
\begin{lemma}
    The asymptotic equipartition property for a source \( X_1, X_2, \dots \) is equivalent to the property that for all \( \varepsilon > 0 \), there exists \( n \in \mathbb N \) such that for all \( n \geq n_0 \), there exists a `typical set' \( T_n \subseteq \mathcal A^n \) such that
    \begin{enumerate}
        \item \( \prob{(X_1, \dots, X_n) \in T_n} > 1 - \varepsilon \);
        \item \( 2^{-n(H+\varepsilon)} \leq p(x_1, \dots, x_n) \leq 2^{-n(H-\varepsilon)} \) for all \( (x_1, \dots, x_n) \in T_n \).
    \end{enumerate}
\end{lemma}
\begin{proof}[Proof sketch]
    First, we show that the asymptotic equipartition property implies the alternative definition.
    We define
    \[ T_n = \qty{(x_1, \dots, x_n) \mid \abs{-\frac{1}{n} \log p(x_1, \dots, x_n) - H} \leq \varepsilon} = \qty{(x_1, \dots, x_n) \mid \text{condition (ii) holds}} \]
    For the converse,
    \[ \prob{\abs{\frac{1}{n}\log p(x_1, \dots, x_n) - H} < \varepsilon} \geq \prob{T_n} \to 1 \]
\end{proof}

\subsection{Shannon's first coding theorem}
\begin{theorem}
    Let \( X_1, X_2, \dots \) be a source satisfying the asymptotic equipartition property with constant \( H \).
    Then this source has information rate \( H \).
\end{theorem}
\begin{proof}
    Let \( \varepsilon > 0 \), and let \( T_n \subseteq \mathcal A^n \) be typical sets.
    Then, for all \( n \geq n_0(\varepsilon) \), for all \( (x_1, \dots, x_n) \in T_n \) we have \( p(x_1, \dots, x_n) \geq 2^{-n(H + \varepsilon)} \).
    Therefore, \( 1 \geq \prob{T_n} \geq 2^{-n(H + \varepsilon)} \cdot \abs{T_n} \), giving \( \frac{1}{n} \log \abs{T_n} \leq H + \varepsilon \).
    Taking \( A_n = T_n \) in the definition of reliable encoding shows that the source is reliably encodable at rate \( H + \varepsilon \).

    Conversely, if \( H = 0 \) the proof concludes, so we may assume \( H > 0 \).
    Let \( 0 < \varepsilon < \frac{H}{2} \), and suppose that the source is reliably encodable at rate \( H - 2\varepsilon \) with sets \( A_n \subseteq \mathcal A^n \).
    Let \( T_n \subseteq \mathcal A^n \) be typical sets.
    Then, for all \( (x_1, \dots, x_n) \in T_n \), \( p(x_1, \dots, x_n) \leq 2^{-n(H - \varepsilon)} \), so \( \prob{A_n \cap T_n} \leq 2^{-n(H - \varepsilon)} \abs{A_n} \), giving
    \[ \frac{1}{n} \log \prob{A_n \cap T_n} \leq -(H - \varepsilon) + \frac{1}{n} \log \abs{A_n} \to -(H - \varepsilon) + (H - 2 \varepsilon) = -\varepsilon \]
    Then, \( \log \prob{A_n \cap T_n} \to -\infty \), so \( \prob{A_n \cap T_n} \to 0 \).
    But \( \prob{T_n} \leq \prob{A_n \cap T_n} + \prob{\mathcal A^n \setminus A_n} \to 0 + 0 \), contradicting typicality.
    So we cannot reliably encode at rate \( H - \varepsilon \), so the information rate is at least \( H \).
\end{proof}
\begin{corollary}
    A Bernoulli source \( X_1, X_2, \dots \) has information rate \( H(X_1) \).
\end{corollary}
\begin{proof}
    In a previous example we showed that for a Bernoulli source, \( -\frac{1}{n} \log(X_1, \dots, X_n) \xrightarrow{\mathbb P} H(X_1) \).
    So the asymptotic equipartition property holds with \( H = H(X_1) \), giving the result by Shannon's first coding theorem.
\end{proof}
\begin{remark}
    The asymptotic equipartition property is useful for noiseless coding.
    We can encode the typical sequences using a block code, and encode the atypical sequences arbitrarily.

    Many sources, which are not necessarily Bernoulli, also satisfy the property.
    Under suitable hypotheses, the sequence \( \frac{1}{n} H(X_1, \dots, X_n) \) is decreasing, and the asymptotic equipartition property is satisfied with constant \( H = \lim_{n \to \infty} \frac{1}{n} H(X_1, \dots, X_n) \).
\end{remark}

\subsection{Capacity}
Consider a communication channel with input alphabet \( \mathcal A \) and output alphabet \( \mathcal B \).
Recall the following definitions.
A \emph{code} of length \( n \) is a subset \( C \subseteq \mathcal A^n \).
The \emph{error rate} is \( \hat e(C) = \max_{c \in C} \prob{\text{error} \mid c \text{ sent}} \).
The \emph{information rate} is \( \rho(C) = \frac{\log \abs{C}}{n} \).
A channel can \emph{transmit reliably at rate \( R \)} if there exist codes \( C_1, C_2, \dots \) where \( C_n \) has length \( n \) such that \( \lim_{n \to \infty} \rho(C_n) = R \) and \( \lim_{n \to \infty} \hat e(C_n) = 0 \).
The \emph{(operational) capacity} of a channel is the supremum of all rates at which it can transmit reliably.

Suppose we are given a source with information rate \( r \) bits per second that emits symbols at a rate of \( s \) symbols per second.
Suppose we also have a channel with capacity \( R \) bits per transmission that transmits symbols at a rate of \( S \) transmissions per second.
Usually, information theorists take \( S = s = 1 \).
We will show that reliable encoding and transmission is possible if and only if \( rs \leq RS \).

We will now compute the capacity of the binary symmetric channel with error probability \( p \).
\begin{proposition}
    A binary symmetric channel with error probability \( p < \frac{1}{4} \) has nonzero capacity.
\end{proposition}
\begin{proof}
    Let \( \delta \) be such that \( 2p < \delta < \frac{1}{2} \).
    We claim that we can reliably transmit at rate \( R = 1 - H(\delta) > 0 \).
    Let \( C_n \) be a code of length \( n \), and suppose it has minimum distance \( \floor{n\delta} \) of maximal size.
    Then, by the GSV bound,
    \[ \abs{C_n} = A(n, \floor{n\delta}) \geq 2^{-n(1-H(\delta))} = 2^{nR} \]
    Replacing \( C_n \) with a subcode if necessary, we can assume \( \abs{C_n} = \floor{2^{nR}} \), with minimum distance at least \( \floor{n\delta} \).
    Using minimum distance decoding,
    \[ \hat e(C_n) \leq \prob{\text{in \( n \) uses, the channel makes at least } \floor{\frac{\floor{n\delta}-1}{2}} \text{ errors}} \leq \prob{\text{in \( n \) uses, the channel makes at least } \floor{\frac{n\delta - 1}{2}} \text{ errors}} \]
    Let \( \varepsilon > 0 \) be such that \( p + \varepsilon < \frac{\delta}{2} \).
    Then, for \( n \) sufficiently large, \( \frac{n\delta - 1}{2} = n\qty(\frac{\delta}{2} - \frac{1}{2n}) > n(p + \varepsilon) \).
    Hence, \( \hat e(C_n) \leq \prob{\text{in \( n \) uses, the channel makes at least } n(p+\varepsilon) \text{ errors}} \).
    We show that this value converges to zero as \( n \to \infty \) using the next lemma.
\end{proof}
