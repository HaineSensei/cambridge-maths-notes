\subsection{Norms}
\begin{definition}
	A \emph{norm} on a real vector space is a map \( \norm{\,\cdot\,}_V \colon V \to \mathbb R \) such that
	\begin{enumerate}
		\item \( \norm{\lambda v} = \abs{\lambda} \cdot \norm{v} \);
		\item \( \norm{u + v} \leq \norm{u} + \norm{v} \);
		\item \( \norm{v} = 0 \) if and only if \( v = 0 \).
	\end{enumerate}
\end{definition}
\begin{definition}
	Let \( (E, \mathcal E, \mu) \) be a measure space.
	We define \( L^p(E,\mathcal E,\mu) = L^p(\mu) = L^p \) for the space of measurable functions \( f \colon E \to \mathbb R \) such that \( \norm{f}_p \) is finite, where
	\[ \norm{f}_p = \begin{cases}
		\qty(\int_E \abs{f(x)}^p \dd{\mu(x)})^{\frac{1}{p}} & 1 \leq p < \infty \\
		\esssup \abs{f} = \inf \qty{\lambda > 0 \mid \abs{f} \leq \lambda \text{ almost everywhere}} & p = \infty
	\end{cases} \]
\end{definition}
We must check that \( \norm{\,\cdot\,}_p \) as defined is a norm.
Clearly (i) holds for all \( 1 \leq p \leq \infty \).
Property (ii) holds for \( p = 1 \) and \( p = \infty \), and we will prove later that this holds for other values of \( p \).
The last property does not hold: \( f = 0 \) implies \( \norm{f}_p = 0 \), but \( \norm{f}_p = 0 \) implies only that \( \abs{f}^p = 0 \) almost everywhere, so \( f \) is zero almost everywhere on \( E \).
Therefore, to rigorously define the norm, we must construct the quotient space \( \mathcal L^p \) of functions that coincide almost everywhere.
We write \( [f] \) for the equivalence class of functions that are equal almost everywhere.
The functional \( \norm{\,\cdot\,}_p \) is then a norm on \( \mathcal L^p \).
\begin{proposition}[Chebyshev's inequality, Markov's inequality]
	Let \( f \colon E \to \mathbb R \) be nonnegative and measurable.
	Then for all \( \lambda > 0 \),
	\[ \mu(\qty{x \in E \mid f(x) \geq \lambda}) = \mu(f \geq \lambda) \leq \frac{\mu(f)}{\lambda} \]
\end{proposition}
\begin{proof}
	Integrate the inequality \( \lambda \mathbbm 1_{\qty{f \geq \lambda}} \leq f \), which holds on \( E \).
\end{proof}
\begin{definition}
	Let \( I \subseteq R \) be an interval.
	Then we say a map \( c \colon I \to \mathbb R \) is \emph{convex} if for all \( x, y \in I \) and \( t \in [0,1] \), we have \( c(tx + (1-t)y) \leq tc(x) + (1-t)c(y) \).
	Equivalently, for all \( x < t < y \) and \( x, y \in I \), we have \( \frac{c(t) - c(x)}{t-x} \leq \frac{c(y) - c(t)}{y-t} \).
\end{definition}
Since a convex function is continuous on the interior of the interval, it is Borel measurable.
\begin{lemma}
	Let \( I \subseteq R \) be an interval, and let \( m \in I^\circ \).
	If \( c \) is convex on \( I \), there exist \( a, b \) such that \( c(x) \geq ax + b \), and \( c(m) = am + b \).
\end{lemma}
\begin{proof}
	Define \( a = \sup \qty{\frac{c(m) - c(x)}{m - x} \mid x < m, x \in I} \).
	This exists in \( \mathbb R \) by the second definition of convexity.
	Let \( y \in I \), and \( y > m \).
	Then \( a \leq \frac{c(y) - c(m)}{y - m} \), so \( c(y) \geq ay - am + c(m) = ay + b \) where we define \( b = c(m) - am \).
	Similarly, for \( y < m \), by definition of the supremum, \( \frac{c(m) - c(y)}{m - y} \leq a \), we have \( c(y) \geq ay + b \).
\end{proof}
\begin{theorem}[Jensen's inequality]
	Let \( X \) be a random variable taking values in an interval \( I \subseteq \mathbb R \), such that \( \expect{\abs{X}} < \infty \).
	Let \( c \colon I \to \mathbb R \) be a convex function.
	Then \( c(\expect{X}) \leq \expect{c(X)} \).
\end{theorem}
Note that the integral \( \expect{c(X)} \) is defined as \( \expect{c^+(X)} - \expect{c^-(X)} \), and this is well-defined and takes values in \( (-\infty, \infty] \).
\begin{proof}
	Define \( m = \expect{X} = \int_I z \dd{\mu_X(z)} \).
	If \( m \not\in I^\circ \), \( X \) must equal \( m \) almost surely, and then the result follows.
	Now let \( m \in I^\circ \).
	Applying the previous lemma, we find \( a, b \) such that \( c^-(X) \leq \abs{a} \cdot \abs{X} + \abs{b} \).
	Hence, \( \expect{c^-(X)} \leq \abs{a} \expect{\abs{X}} + \abs{b} < \infty \), and \( \expect{c(X)} = \expect{c^+(X)} - \expect{c^-(X)} \) is well-defined in \( (-\infty,\infty] \).
	Integrating the inequality from the lemma, and using linearity of the integral,
	\[ \expect{c(X)} \geq a \expect{X} + b = am + b = c(m) = c(\expect{X}) \]
\end{proof}
\begin{remark}
	If \( 1 \leq p < q < \infty \), \( c(x) = \abs{x}^{\frac{q}{p}} \) is a convex function.
	If \( X \) is a bounded random variable (so lies in \( L^\infty(\mathbb P) \)), we then have
	\[ \norm{X}_p = \expect{\abs{X^p}}^{\frac{1}{p}} = c(\expect{\abs{X}^p})^{\frac{1}{q}} \leq \expect{c(\abs{X}^p)}^{\frac{1}{q}} = \norm{X}_q \]
	Using the monotone convergence theorem, this extends to all \( X \in L^q(\mathbb P) \) when \( \norm{X}_q \) is finite.
	In particular, \( L^q(\mathbb P) \subseteq L^p(\mathbb P) \) for all \( 1 \leq p \leq q \leq \infty \).
\end{remark}
\begin{theorem}[H\"older's inequality]
	Let \( f, g \) be measurable functions on \( (E,\mathcal E,\mu) \).
	If \( p, q \) are \emph{conjugate}, so \( \frac{1}{p} + \frac{1}{q} = 1 \) and \( 1 \leq p, q \leq \infty \), we have
	\[ \mu(\abs{fg}) = \int_E \abs{f(x)g(x)} \dd{\mu} \leq \norm{f}_p \cdot \norm{g}_q \]
\end{theorem}
\begin{remark}
	For \( p = q = 2 \), this is exactly the Cauchy--Schwarz inequality on \( L^2 \).
\end{remark}
\begin{proof}
	The cases \( p = 1 \) or \( p = \infty \) are obvious.
	We can assume \( f \in L^p \) and \( g \in L^q \) without loss of generality since the right hand side would otherwise be infinite.
	We can also assume \( f \) is not equal to zero almost everywhere, otherwise this reduces to \( 0 \leq 0 \).
	Hence, \( \norm{f}_p > 0 \).
	Then, we can divide both sides by \( \norm{f}_p \) and then assume \( \norm{f}_p = 1 \).
	\[ \mu(\abs{fg}) = \int_E \abs{g} \frac{1}{\abs{f}^{p-1}} \abs{f}^p \mathbbm 1_{\qty{\abs{f} > 0}} \dd{\mu} \]
	Note that we can set \( \abs{f}^p \dd{\mu} = \dd{\mathbb P} \), and since \( L^q(\mathbb P) \subseteq L^1(\mathbb P) \),
	\[ \int_E \abs{g} \frac{1}{\abs{f}^{p-1}} \abs{f}^p \mathbbm 1_{\qty{\abs{f} > 0}} \dd{\mu} \leq \qty( \int \abs{g}^q \frac{1}{\abs{f}^{q(p-1)}} \underbrace{\abs{f}^p \dd{\mu}}_{\dd{\mathbb P}})^{\frac{1}{q}} = \qty(\int_E \abs{g}^q \dd{\mu})^{\frac{1}{q}} \]
\end{proof}
\begin{theorem}[Minkowski's inequality]
	Let \( f, g \colon (E, \mathcal E, \mu) \to \mathbb R \) be measurable functions.
	Then for all \( 1 \leq p \leq \infty \), we have \( \norm{f + g}_p \leq \norm{f}_p + \norm{g}_p \).
\end{theorem}
\begin{proof}
	The results for \( p = 1, \infty \) are clear.
	Suppose \( 1 < p < \infty \).
	We can assume without loss of generality that \( f, g \in L^p \).
	We can integrate the pointwise inequality \( \abs{f + g}^p \leq 2^p (\abs{f}^p + \abs{g}^p) \) to deduce that \( \norm{f + g}_p^p \leq 2^p (\norm{f}_p^p + \norm{g}_p^p) < \infty \) so \( f + g \in L^p \).
	We assume that \( 0 < \norm{f+g}_p \), otherwise the result is trivial.
	Now, using H\"older's inequality with \( q \) conjugate to \( p \),
	\begin{align*}
		\norm{f + g}_p^p &= \int_E \abs{f + g}^{p-1} \abs{f + g} \dd{\mu} \\
		&\leq \int_E \abs{f + g}^{p-1} \abs{f} \dd{\mu} + \int_E \abs{f + g}^{p-1} \abs{g} \dd{\mu} \\
		&\leq \qty(\int_E \abs{f + g}^{q(p-1)} \dd{\mu})^{\frac{1}{q}} \qty(\norm{f}_p + \norm{g}_p) \\
		&\leq \qty(\int_E \abs{f + g}^p \dd{\mu})^{\frac{1}{q}} \qty(\norm{f}_p + \norm{g}_p) \\
		&\leq \norm{f+g}_p^{\frac{p}{q}} \qty(\norm{f}_p + \norm{g}_p)
	\end{align*}
	Dividing both sides by \( \norm{f+g}_p^{\frac{p}{q}} \), we obtain \( \norm{f+g}_p \leq \norm{f}_p + \norm{g}_p \).
\end{proof}
So the \( L^p \) spaces are indeed normed spaces.

\subsection{Banach spaces}
\begin{definition}
	A \emph{Banach space} is a complete normed vector space.
\end{definition}
\begin{theorem}[\( \mathcal L^p \) is a Banach space]
	Let \( 1 \leq p \leq \infty \), and let \( f_n \in L^p \) be a Cauchy sequence, so for all \( \varepsilon > 0 \) there exists \( N \) such that for all \( m, n \geq N \), we have \( \norm{f_m - f_n}_p < \varepsilon \).
	Then there exists a function \( f \in L^p \) such that \( f_n \to f \) in \( L^p \), so \( \norm{f_n - f}_p \to 0 \) as \( n \to \infty \).
\end{theorem}
\begin{proof}
	For this proof, we assume \( p < \infty \); the other case is already proven in IB Analysis and Topology.
	Since \( f_n \) is Cauchy, using \( \varepsilon = 2^{-k} \) we extract a subsequence \( f_{N_k} \) of \( L^p \) functions such that
	\[ S = \sum_{k=1}^\infty \norm{f_{N_{k+1}} - f_{N_k}}_p \leq \sum_{k=1}^\infty 2^{-k} < \infty \]
	By Minkowski's inequality, for any \( K \), we have
	\[ \norm{\sum_{k=1}^K \abs{f_{N_{k+1}} - f_{N_k}}}_p \leq \sum_{k=1}^K \norm{f_{N_{k+1}} - f_{N_k}}_p \leq S < \infty \]
	By the monotone convergence theorem applied to \( \abs{\sum_{k=1}^K \abs{f_{N_{k+1}} - f_{N_k}}}^p \) which increases to \( \abs{\sum_{k=1}^\infty \abs{f_{N_{k+1}} - f_{N_k}}}^p \), we find
	\[ \norm{\sum_{k=1}^\infty \abs{f_{N_{k+1}} - f_{N_k}}}_p \leq S < \infty \]
	Since the integral is finite, we see that \( \sum_{k=1}^\infty \abs{f_{N_{k+1}} - f_{N_k}} \) is finite almost everywhere.
	Then \( \sum_{k=1}^K (f_{N_{k+1}}(x) - f_{N_k}(x)) = f_{N_{k+1}}(x) - f_{N_1}(x) \) converges in the real line for all \( x \) in a set \( A \) that has full measure, so \( \mu(A^c) = 0 \).
	In particular, \( f_{N_k}(x) \) is a Cauchy sequence of reals, so by completeness of the real line, we can define the limit
	\[ f(x) = \begin{cases}
		\lim_{k \to \infty} f_{N_k}(x) & x \in A \\
		0 & x \in A^c
	\end{cases} \]
	so \( f_{N_k} \to f \) as \( k \to \infty \) almost everywhere.
	Now, by Fatou's lemma,
	\[ \norm{f_n - f}_p^p = \mu(\abs{f_n - f}^p) = \mu(\lim_k \abs{f_n - f_{N_k}}^p) \leq \liminf_k \mu(\abs{f_n - f_{N_k}}^p) \]
	Since the \( f_n \) are Cauchy,
	\[ \norm{f}_p \leq \underbrace{\norm{f - f_N}_p}_{\leq \varepsilon} + \underbrace{\norm{f_N}_p}_{< \infty} < \infty \]
	so \( f \in L^p \), and \( \norm{f_n - f}_p^p \leq \varepsilon^p \) for \( n, N_k \geq N \), so \( f_n \to f \) in \( L^p \).
\end{proof}
\begin{remark}
	If \( V \) is any of the spaces \( C([a,b]) \), \( \qty{f \text{ simple}} \), \( \qty{f \text{ a linear combination of indicators of intervals}} \), then \( V \) is dense in \( L^1(\mu) \) where \( \mu \) is the Lebesgue measure on \( \mathcal B([a,b]) \).
	So the completion \( \overline{(V,\norm{\,\cdot\,})} \) is exactly \( L^1(\mu) \).
\end{remark}

\subsection{Hilbert spaces}
\begin{definition}
	A symmetric bilinear form \( \inner{\cdot, \cdot} \colon V \times V \to \mathbb R \) on a real vector space \( V \) is called an \emph{inner product} if \( \inner{v,v} \geq 0 \) and \( \inner{v,v} = 0 \) implies \( v = 0 \).
	In this case, we can define a norm \( \norm{v} = \sqrt{\inner{v,v}} \).
	If \( (V,\inner{\cdot,\cdot}) \) is complete, we say that it is a \emph{Hilbert space}.
\end{definition}
\begin{corollary}
	The space \( \mathcal L^2 \) is a Hilbert space for the inner product \( \inner{f,g} = \int_E fg \dd{\mu} \).
\end{corollary}
\begin{example}
	An analog of the Pythagorean theorem holds.
	Let \( f, g \in L^2 \), then \( \norm{f + g}_2^2 = \norm{f}_2^2 + 2\inner{f,g} + \norm{g}_2^2 \).
	We say \( f \) is \emph{orthogonal} to \( g \) if \( \inner{f,g} = 0 \).
	\( f \) and \( g \) are orthogonal if and only if \( \norm{f + g}_2^2 = \norm{f}_2^2 + \norm{g}_2^2 \).
	For centred (mean zero) random variables \( X, Y \), we have \( \inner{X,Y} = \expect{XY} = \expect{(X - \expect{X})(Y - \expect{Y})} = \Cov{X,Y} \) which vanishes when \( X \) and \( Y \) are orthogonal.
\end{example}
\begin{example}
	The parallelogram identity holds: \( \norm{f+g}_2^2 + \norm{f-g}_2^2 = 2 \qty(\norm{f}_2^2 + \norm{g}_2^2) \)
\end{example}
\begin{definition}
	Let \( V \subseteq L^2(\mu) \).
	We define its \emph{orthogonal complement} to be \( V^\perp = \qty{f \in L^2(\mu) \mid \forall g \in V,\, \inner{f,g} = 0} \).
	We say that a subset \( V \) of \( \mathcal L^2 \) is \emph{closed} if any sequence \( f_n \in V \) that converges in \( \mathcal L^2 \), its limit \( f \) coincides almost everywhere with some \( v \in V \).
\end{definition}
\begin{theorem}
	Let \( V \) be a closed linear subspace of \( \mathcal L^2(\mu) \).
	Then for all \( f \in \mathcal L^2 \), there exists an orthogonal decomposition \( f = v + u \) where \( v \in V \) and \( u \in V^\perp \) such that \( \norm{f - v}_2 \leq \norm{f - g}_2 \) for all \( g \in V \) with equality only if \( v = g \) almost everywhere.
	We call \( v \) the \emph{projection} of \( f \) onto \( V \).
\end{theorem}
\begin{proof}
	In this proof, we set \( p = 2 \) for all norms.
	We define \( d(f,V) = \inf_{g \in V} \norm{g - f} \), and let \( g_n \in V \) be a sequence of functions such that \( \norm{g_n - f} \) converges to \( d(f,V) \).
	By the parallelogram law,
	\begin{align*}
		2\norm{f - g_n}^2 + 2\norm{f - g_m}^2 &= \norm{2f - (g_n + g_m)}^2 + \norm{g_n - g_m}^2 \\
		&= 4 \norm{f - \underbrace{\frac{g_n + g_m}{2}}_{\in V}}^2 + \norm{g_n - g_m}^2 \\
		&\geq 4 d(f,V)^2 + \norm{g_n - g_m}^2
	\end{align*}
	Taking the limit superior as \( n, m \to \infty \), \( \limsup_{m,n} \norm{g_n - g_m}^2 \leq 4d(f,V) - 4d(f,V) = 0 \).
	So the sequence \( g_n \) is Cauchy in \( L^2 \), so by completeness, it converges to some \( v \in L^2 \).
	Since \( V \) is closed, \( v \in V \).
	In particular, \( d(f,V) = \inf_{g \in V} \norm{g - f} = \norm{v - f} \).

	Note that \( d(f,V)^2 \leq F(t) = \norm{f - (v+th)}^2 \) where \( t \in \mathbb R \) and \( h \in V \).
	So we obtain the first-order condition \( F'(0) = 2\inner{f - v, h} = 0 \) for all \( h \).
	Defining \( f - v = u \), we have \( f = u + v \) and \( u \in V^\perp \) since \( h \) was arbitrary.

	For uniqueness, suppose \( f = w + z \) with \( w \in V \) and \( z \in V^\perp \).
	Then \( v - w + u - z = f - f = 0 \), so taking norms, \( 0 = \norm{v - w + u - z}^2 = \norm{v - w}^2 + \norm{u - z}^2 \) so \( v = w \) and \( u - z \) (almost everywhere) by orthogonality.
\end{proof}

\subsection{Convergence in probability and uniform integrability}
\begin{theorem}[bounded convergence]
	Let \( X_n \) be random variables on \( (\Omega, \mathcal F, \mathbb P) \) such that \( \abs{X_n} \leq C < \infty \) and they converge in probability to \( X \).
	Then \( X_n \to X \) in \( L^1(\mathbb P) \).
\end{theorem}
\begin{proof}
	We know that \( X_{n_k} \to X \) almost surely along a subsequence \( n_k \).
	So \( \abs{X} = \lim_k \abs{X_{n_k}} \leq C < \infty \) almost surely.
	Then
	\begin{align*}
		\expect{\abs{X_n - X}} &= \expect{\abs{X_n - X} \qty(\mathbbm 1_{\qty{\abs{X_n - X} > \frac \varepsilon 2}} + \mathbbm 1_{\qty{\abs{X_n - x} \leq \frac \varepsilon 2}})} \\
		&\leq 2 C \prob{\abs{X_n - X} \geq \frac{\varepsilon}{2}} + \frac \varepsilon 2 \\
		&< \varepsilon
	\end{align*}
	for sufficiently large \( n \).
\end{proof}
If \( X \in L^1(\mathbb P) \), then as \( \delta \to 0 \),
\[ I_X(\delta) = \sup \qty{ \expect{\abs{X} \mathbbm 1_A} \mid \prob{A} \leq \delta} \downarrow 0 \]
Suppose this does not hold.
Then there exists \( \varepsilon > 0 \) and a sequence of events \( A_n \in \mathcal F \) such that \( \prob{A_n} \leq 2^{-n} \) but \( \expect{\abs{X} \mathbbm 1_{A_n}} \geq \varepsilon \).
Since \( \sum_n \prob{A_n} < \infty \), by the first Borel--Cantelli lemma, we have \( \prob{\bigcap_n \bigcup_{m \geq n} A_m} = 0 \).
But \( \expect{\abs{X} \mathbbm 1_{A_n}} \leq \expect{\abs{X} \mathbbm 1_{\bigcup_{m \geq n} A_m}} \).
Note that \( \mathbbm 1_{\bigcup_{m \geq n} A_m} \to \mathbbm 1_{\bigcap_n \bigcup_{m \geq n} A_n} \), so \( \expect{\abs{X} \mathbbm 1_{\bigcup_{m \geq n} A_m}} \to \expect{\abs{X} \mathbbm 1_{\bigcap_n \bigcup_{m \geq n}}} \) by the dominated convergence theorem, but this is equal to zero, giving a contradiction.
\begin{definition}
	For a collection \( \mathcal X \subseteq L^1(\mathbb P) \) of random variables, we say \( \mathcal X \) is \emph{uniformly integrable} if it is bounded in \( L^1(\mathbb P) \), and
	\[ I_{\mathcal X}(\delta) = \sup \qty{ \expect{\abs{X}\mathbbm 1_A} \mid \prob{A} \leq \delta, X \in \mathcal X} \downarrow 0 \]
\end{definition}
\begin{remark}
	Note that \( X_n = n\mathbbm 1_{\qty[0,\frac{1}{n}]} \) for the Lebesgue measure \( \mu \) on \( [0,1] \) is bounded in \( L^1(\mathbb P) \) but not uniformly integrable.
	If \( \mathcal X \) is bounded in \( L^p(\mathbb P) \) for \( p > 1 \), then by H\"older's inequality,
	\[ \expect{\abs{X}\mathbbm 1_A} \leq \underbrace{\norm{X}_p}_{\text{bounded}} \cdot \underbrace{\prob{A}^{\frac 1 q}}_{\leq \delta^{\frac 1 q} \to 0} \]
\end{remark}
\begin{lemma}
	\( \mathcal X \subseteq L^1(\mathbb P) \) is uniformly integrable if and only if \( \sup_{X \in \mathcal X} \expect{\abs{X} \mathbbm 1_{\qty{\abs{X} > K}}} \to 0 \) as \( K \to \infty \).
\end{lemma}
\begin{proof}
	Let \( \mathcal X \) be uniformly integrable.
	Applying Markov's inequality, as \( K \to \infty \),
	\[ \prob{\abs{X} > K} \leq \frac{\expect{\abs{X}}}{K} = \frac{\expect{\abs{X}\mathbbm 1_{\Omega}}}{K} \leq \frac{I_{\mathcal X}(1)}{K} \to 0 \]
	Using the uniform integrability property using \( A = \qty{\abs{X} > K} \), we obtain the required limit.
	Conversely, we have
	\[ \expect{\abs{X}} = \expect{\abs{X}\qty(\mathbbm 1_{\qty{\abs{X} \leq K}} + \mathbbm 1_{\qty{\abs{X} > K}})} \leq K + \frac{\varepsilon}{2} \]
	for sufficiently large \( K \).
	So \( \mathcal X \) is bounded in \( L^1(\mathbb P) \) as required.
	Then for \( A \) such that \( \prob{A} \leq \delta \),
	\[ \expect{\abs{X}\mathbbm 1_A\qty(\mathbbm 1_{\qty{\abs{X} \leq K}} + \mathbbm 1_{\qty{\abs{X} > K}})} \leq K\prob{A} + \expect{\abs{X}\mathbbm 1_{\qty{\abs{X} > K}}} \leq K\delta + \frac{\varepsilon}{2} < \varepsilon \]
	for sufficiently small \( \delta \).
\end{proof}
\begin{theorem}
	Let \( X_n, X \) be random variables on \( (\Omega, \mathcal F, \mathbb P) \).
	Then the following are equivalent.
	\begin{enumerate}
		\item \( X_n, X \in L^1(\mathbb P) \) and \( X_n \to X \) in \( L^1(\mathbb P) \).
		\item \( \qty{X_n \mid n \in \mathbb N} \) is uniformly integrable, and \( X_n \to X \) in probability.
	\end{enumerate}
\end{theorem}
\begin{proof}
	\emph{(i) implies (ii).}
	Using Markov's inequality,
	\[ \prob{\abs{X_n - X} > \varepsilon} \leq \frac{\expect{\abs{X_n - X}}}{\varepsilon} \to 0 \]
	so \( X_n \to X \) in probability.
	Since any finite collection is uniformly integrable, so are \( X \) along with \( X_1, \dots, X_N \) for each \( N \).
	For the indices larger than \( N \), we have
	\[ \expect{\abs{X_n} \mathbbm 1_A} \leq \expect{\abs{X_n - X} \mathbbm 1_A} + \expect{\abs{X} \mathbbm 1_A} \leq \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \]
	for sufficiently large \( N \) and sufficiently small \( \delta \), so all \( X_n \) are uniformly integrable.

	\emph{(ii) implies (i).}
	Along a subsequence, \( X_n \to X \) almost surely.
	So
	\[ \expect{\abs{X}} = \expect{\liminf_k \abs{X_{n_k}}} \leq \liminf_k \expect{\abs{X_{n_k}}} \leq I_{\mathcal X}(1) < \infty \]
	almost surely, so \( X \in L^1(\mathbb P) \).
	Next, we define random variables \( g(X_n) = X_n^K = \max(-K, \min(K, X_n)) \) and \( g(X) = X^K = \max(-K, \min(K, X)) \), where \( g \) is continuous.
	Then for some \( \varepsilon' > 0 \),
	\[ \prob{\abs{g(X_n) - g(X)} > \varepsilon} \leq \prob{\abs{X_n - X} > \varepsilon'} \to 0 \]
	as \( n \to \infty \), since \( X_n \to X \) in probability and \( g \) is continuous.
	Then by bounded convergence, \( X_n^K \to X^K \) in \( L^1 \), and so
	\begin{align*}
		\expect{\abs{X_n - X}} &\leq \expect{\abs{X_n - X_n^K}} + \expect{\abs{X_n^K - X^K}} + \expect{\abs{X^K - X}} \\
		&= \expect{\abs{X_n} \mathbbm 1_{\qty{\abs{X_n} > k}}} + \expect{\abs{X_n^K - X^K}} + \expect{\abs{X}\mathbbm 1_{\qty{\abs{X} > K}}} \\
		&< \varepsilon
	\end{align*}
	by choosing sufficiently large \( K \) and \( n \).
\end{proof}
