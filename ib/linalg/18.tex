\subsection{Changing basis}
Let \( \phi \colon V \times V \to \mathbb F \) be a bilinear form.
Let \( V \) be a finite-dimensional \( F \)-vector space.
Let \( B \) be a basis of \( V \) and let \( [\phi]_B = [\phi]_{BB} \) be the matrix with entries \( \phi(e_i, e_j) \).
\begin{lemma}
	Let \( \phi \) be a bilinear form \( V \times V \to F \).
	Then if \( B, B' \) are bases for \( V \), and \( P = [I]_{B', B} \) we have
	\[
		[\phi]_{B'} = P^\transpose \phi_B P
	\]
\end{lemma}
\begin{proof}
	This is a special case of the general change of basis formula.
\end{proof}
\begin{definition}
	Let \( A, B \in M_n(F) \) be square matrices.
	We say that \( A, B \) are \textit{congruent} if there exists \( P \in M_n(F) \) such that \( A = P^\transpose B P \).
\end{definition}
\begin{remark}
	Congruence is an equivalence relation.
\end{remark}
\begin{definition}
	A bilinear form \( \phi \) on \( V \) is \textit{symmetric} if, for all \( u, v \in V \), we have
	\[
		\phi(u,v) = \phi(v,u)
	\]
\end{definition}
\begin{remark}
	If \( A \) is a square matrix, we say \( A \) is symmetric if \( A = A^\transpose \).
	Equivalently, \( A_{ij} = A_{ji} \) for all \( i, j \).
	So \( \phi \) is symmetric if and only if \( [\phi]_B \) is symmetric for any basis \( B \).
	Note further that to represent \( \phi \) by a diagonal matrix in some basis \( B \), it must necessarily be symmetric, since
	\[
		P^\transpose A P = D \implies D = D^\transpose = \qty(P^\transpose A P)^\transpose = P^\transpose A^\transpose P \implies A = A^\transpose
	\]
\end{remark}

\subsection{Quadratic forms}
\begin{definition}
	A map \( Q \colon V \to F \) is a \textit{quadratic form} if there exists a bilinear form \( \phi \colon V \times V \to F \) such that, for all \( u \in V \),
	\[
		Q(u) = \phi(u,u)
	\]
	So a quadratic form is the restriction of a bilinear form to the diagonal.
\end{definition}
\begin{remark}
	Let \( B = (e_i) \) be a basis of \( V \).
	Let \( A = [\phi]_B = (\phi(e_i, e_j)) = (a_{ij}) \).
	Then, for \( u = \sum_i x_i e_i \in V \),
	\[
		Q(u) = \phi(u,u) = \phi\qty(\sum_i x_i e_i, \sum_j x_j e_j) = \sum_i \sum_j x_i x_j \phi(e_i, e_j) = \sum_i \sum_j x_i x_j a_{ij}
	\]
	We can check that this is equal to
	\[
		Q(u) = x^\transpose A x
	\]
	where \( [u]_B = x \).
	Note further that
	\[
		x^\transpose A x = \sum_i \sum_j a_{ij} x_i x_j = \sum_i \sum_j a_{ji} x_i x_j = \sum_i \sum_j \frac{a_{ij} + a_{ji}}{2} x_i x_j = x^\transpose \qty(\underbrace{\frac{A + A^\transpose}{2}}_{\mathclap{\text{symmetric}}}) x
	\]
	So we can always express the quadratic form as a symmetric matrix in any basis.
\end{remark}
\begin{proposition}
	If \( Q \colon V \to F \) is a quadratic form, then there exists a unique symmetric bilinear form \( \phi \colon V \times V \to F \) such that \( Q(u) = \phi(u,u) \).
\end{proposition}
\begin{proof}
	Let \( \psi \) be a bilinear form on \( V \) such that for all \( u \in V \), we have \( Q(u) = \psi(u,u) \).
	Then, let
	\[
		\phi(u,v) = \frac{1}{2}\qty[\psi(u,v) + \psi(v,u)]
	\]
	Certainly \( \phi \) is a bilinear form and symmetric.
	Further, \( \phi(u,u) = \psi(u,u) = Q(u) \).
	So there exists a symmetric bilinear form \( \phi \) such that \( Q(u) = \phi(u,u) \), so it suffices to prove uniqueness.
	Let \( \phi \) be a symmetric bilinear form such that for all \( u \in V \) we have \( Q(u) = \phi(u,u) \).
	Then, we can find
	\[
		Q(u + v) = \phi(u + v, u + v) = \phi(u,u) + \phi(v,v) + 2\phi(u,v)
	\]
	Thus \( \phi(u,v) \) is defined uniquely by \( Q \), since
	\[
		2 \phi(u,v) = Q(u+v) - Q(u) - Q(v)
	\]
	So \( \phi \) is unique (when \( 2 \) is invertible in \( F \)).
	This identity for \( \phi(u,v) \) is known as the polarisation identity.
\end{proof}

\subsection{Diagonalisation of symmetric bilinear forms}
\begin{theorem}
	Let \( \phi \colon V \times V \to F \) be a symmetric bilinear form, where \( V \) is finite-dimensional.
	Then there exists a basis \( B \) of \( V \) such that \( [\phi]_B \) is diagonal.
\end{theorem}
\begin{proof}
	By induction on the dimension, suppose the theorem holds for all dimensions less than \( n \) for \( n \geq 2 \).
	If \( \phi(u,u) = 0 \) for all \( u \in V \), then \( \phi = 0 \) by the polarisation identity, which is diagonal.
	Otherwise \( \phi(e_1, e_1) \neq 0 \) for some \( e_1 \in V \).
	Let
	\[
		U = \qty(\genset{e_1})^\perp = \qty{v \in V \colon \phi(e_1, v) = 0}
	\]
	This is a vector subspace of \( V \), which is in particular
	\[
		\ker \qty{ \phi(e_1, \wildcard) \colon V \to F }
	\]
	By the rank-nullity theorem, \( \dim U = n - 1 \).
	We now claim that \( U + \genset{e_1} \) is a direct sum.
	Indeed, for \( v = \genset{e_1} \cap U \), we have \( v = \lambda e_1 \) and \( \phi(e_1, v) = 0 \).
	Hence \( \lambda = 0 \), since by assumption \( \phi(e_1, e_1) \neq 0 \).
	So we find a basis \( B' = (e_2, \dots, e_n) \) of \( U \), which we extend by \( e_1 \) to \( B = (e_1, e_2, \dots, e_n) \).
	Since \( U \oplus \genset{e_1} \) has dimension \( n \), this is a basis of \( V \).
	Under this basis, we find
	\[
		[\phi]_B = \begin{pmatrix}
			\phi(e_1, e_1) & 0                          \\
			0              & \qty[\eval{\phi}_{U}]_{B'}
		\end{pmatrix}
	\]
	because
	\[
		\phi(e_1, e_j) = \phi(e_j, e_1) = 0
	\]
	for all \( j \geq 2 \).
	By the inductive hypothesis we can take a basis \( B' \) such that the restricted \( \phi \) to be diagonal, so \( [\phi]_B \) is diagonal in this basis.
\end{proof}
\begin{example}
	Let \( V = \mathbb R^3 \) and choose the canonical basis \( (e_i) \).
	Let
	\[
		Q(x_1, x_2, x_3) = x_1^2 + x_2^2 + 2x_3^2 + 2x_1 x_2 + 2x_1 x_3 - 2x_2 x_3
	\]
	Then, if \( Q(x_1, x_2, x_3) = x^\transpose A x \), we have
	\[
		A = \begin{pmatrix}
			1 & 1  & 1  \\
			1 & 1  & -1 \\
			1 & -1 & 2
		\end{pmatrix}
	\]
	Note that the off-diagonal terms are halved from their coefficients since in the expansion of \( x^\transpose A x \) they are included twice.
	Then, we can find a basis in which \( A \) is diagonal.
	We could use the above algorithm to find a basis, or complete the square in each component.
	We can write
	\[
		Q(x_1, x_2, x_3) = (x_1 + x_2 + x_3)^2 + x_3^2 - 4 x_2 x_3 = (x_1 + x_2 + x_3)^2 + (x_3 - 2x_2)^2 - (2x_2)^2
	\]
	This yields a new coordinate basis \( x_1', x_2', x_3' \).
	Then \( P^{-1} A P \) is diagonal.
	\( P \) is given by
	\[
		\begin{pmatrix} x_1' \\ x_2' \\ x_3' \end{pmatrix} = \underbrace{\begin{pmatrix} 1 & 1 & 1 \\ 0 & -2 & 1 \\ 0 & -2 & 0 \end{pmatrix}}_{P^{-1}} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix}
	\]
\end{example}
