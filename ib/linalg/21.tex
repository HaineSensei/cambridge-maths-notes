\subsection{Cauchy-Schwarz inequality}
\begin{lemma}
	For an inner product space,
	\[
		\abs{\inner{u,v}} \leq \norm{a} \cdot \norm{b}
	\]
\end{lemma}
\begin{proof}
	Let \( t \in F \).
	Then,
	\[
		0 \leq \norm{t u - v} = \inner{tu - v, tu - v} = t \overline t \inner{u,u} - u \inner{u,v} - \overline t \inner{v,u} + \norm{v}^2
	\]
	Since the inner product is Hermitian,
	\[
		0 \leq \abs{t}^2 \norm{u}^2 + \norm{v}^2 - 2 \Re(t \inner{u,v})
	\]
	By choosing
	\[
		t = \frac{\overline{\inner{u,v}}}{\norm{u}^2}
	\]
	we have
	\[
		0 \leq \frac{\abs{\inner{u,v}}^2}{\norm{u}^2} + \norm{v}^2 - 2 \Re(\frac{\abs{\inner{u,v}}^2}{\norm{u}^2})
	\]
	Since the term under the real part operator is real, the result holds.
\end{proof}
\noindent Note that equality implies collinearity in the Cauchy-Schwarz inequality.
\begin{corollary}[triangle inequality]
	In an inner product space,
	\[
		\norm{u+v} \leq \norm{u} + \norm{v}
	\]
\end{corollary}
\begin{proof}
	We have
	\[
		\norm{u+v}^2 = \inner{u+v, u+v} = \norm{u^2} + 2 \Re(\inner{u,v}) + \norm{v}^2 \leq \norm{u^2} + \norm{v}^2 + 2 \norm{u} \cdot \norm{v} = (\norm{u} + \norm{v})^2
	\]
\end{proof}
\begin{remark}
	Any inner product induces a norm, but not all norms derive from scalar products.
\end{remark}

\subsection{Orthogonal and orthonormal sets}
\begin{definition}
	A set \( (e_1, \dots, e_k) \) of vectors of \( V \) is said to be \textit{orthogonal} if \( \inner{e_i, e_j} = 0 \) for all \( i \neq j \).
	The set is said to be \textit{orthonormal} if it is orthogonal and \( \norm{e_i} = 1 \) for all \( i \).
	In this case, \( \inner{e_i, e_j} = \delta_{ij} \).
\end{definition}
\begin{lemma}
	If \( (e_1, \dots, e_k) \) are orthogonal and non-zero, then they are linearly independent.
	Further, let \( v \in \genset{\qty{e_i}} \).
	Then,
	\[
		v = \sum_{j=1}^k \lambda_j e_j \implies \lambda_j = \frac{\inner{v, e_j}}{\norm{e_j}^2}
	\]
\end{lemma}
\begin{proof}
	Suppose
	\[
		\sum_{i=1}^k \lambda_i e_i = 0
	\]
	Then,
	\[
		0 = \inner{\sum_{i=1}^k \lambda_i, e_j} \implies \sum_{i=1}^k \lambda_i \inner{e_i, e_j}
	\]
	Thus \( \lambda_j = 0 \) for all \( j \).
	Further, for \( v \) in the span of these vectors,
	\[
		\inner{v, e_j} = \sum_{i=1}^k \lambda_i \inner{e_i, e_j} = \lambda_j \norm{e_j}^2
	\]
\end{proof}

\subsection{Parseval's identity}
\begin{corollary}
	Let \( V \) be a finite-dimensional inner product space over \( F \).
	Let \( (e_1, \dots, e_n) \) be an orthonormal basis.
	Then, for any vectors \( u, v \in V \), we have
	\[
		\inner{u, v} = \sum_{i=1}^n \inner{u, e_i} \overline{\inner{v, e_i}}
	\]
	Hence,
	\[
		\norm{u}^2 = \sum_{i=1}^n \abs{\inner{u,e_i}}^2
	\]
\end{corollary}
\begin{proof}
	By orthonormality,
	\[
		u = \sum_{i=1}^n \inner{u, e_i} e_i;\quad v = \sum_{i=1}^n \inner{v, e_i} e_i
	\]
	Hence, by sesquilinearity,
	\[
		\inner{u,v} = \sum_{i=1}^n \inner{u, e_i} \overline{\inner{v, e_i}}
	\]
	By taking \( u = v \) we find
	\[
		\norm{u}^2 = \inner{u,u} = \sum_{i=1}^n \abs{\inner{u,e_i}}^2
	\]
\end{proof}

\subsection{Gram-Schmidt orthogonalisation process}
\begin{theorem}
	Let \( V \) be an inner product space.
	Let \( (v_i)_{i \in I} \) be a linearly independent family of vectors such that \( I \) is countable.
	Then there exists a family \( (e_i)_{i \in I} \) of orthonormal vectors such that for all \( k \geq 1 \),
	\[
		\genset{v_1, \dots, v_k} = \genset{e_1, \dots, e_k}
	\]
\end{theorem}
\begin{proof}
	This proof is an explicit algorithm to compute the family \( (e_i) \), which will be computed by induction on \( k \).
	For \( k = 1 \), take \( e_1 = \frac{v_1}{\norm{v_1}} \).
	Inductively, suppose \( (e_1, \dots, e_k) \) satisfy the conditions as above.
	Then we will find a valid \( e_{k+1} \).
	We define
	\[
		e_{k+1}' = v_{k+1} - \sum_{i=1}^k \inner{v_{k+1}, e_i} e_i
	\]
	This ensures that the inner product between \( e_{k+1}' \) and any basis vector \( e_j \) is zero, while maintaining the same span.
	Suppose \( e_{k+1}' = 0 \).
	Then, \( v_{k+1} \in \genset{e_1, \dots, e_k} = \genset{v_1, \dots, v_k} \) which contradicts the fact that the family is free.
	Thus,
	\[
		e_{k+1} = \frac{e_{k+1}'}{\norm{e_{k+1}'}}
	\]
	satisfies the requirements.
\end{proof}
\begin{corollary}
	In finite-dimensional inner product spaces, there always exists an orthonormal basis.
	In particular, any orthonormal set of vectors can be extended into an orthonormal basis.
\end{corollary}
\begin{remark}
	Let \( A \in M_n(\mathbb R) \) be a real-valued (or complex-valued) matrix.
	Then, the column vectors of \( A \) are orthogonal if \( A^\transpose A = I \) (or \( A^\transpose \overline A = I \) in the complex-valued case).
\end{remark}

\subsection{Orthogonality of matrices}
\begin{definition}
	A matrix \( A \in M_n(\mathbb R) \) is \textit{orthogonal} if \( A^\transpose A = I \), hence \( A^\transpose = A^{-1} \).
	A matrix \( A \in M_n(\mathbb C) \) is \textit{unitary} if \( A^\transpose \overline A = I \), hence \( A^\dagger = A^{-1} \).
\end{definition}
\begin{proposition}
	Let \( A \) be a square, non-singular, real-valued (or complex-valued) matrix.
	Then \( A \) can be written as \( A = RT \) where \( T \) is upper triangular and \( R \) is orthogonal (or respectively unitary).
\end{proposition}
\begin{proof}
	We apply the Gram-Schmidt process to the column vectors of the matrix.
	This gives us an orthonormal set of vectors, which gives an upper triangular matrix in this new basis.
\end{proof}

\subsection{Orthogonal complement and projection}
\begin{definition}
	Let \( V \) be an inner product space.
	Let \( V_1, V_2 \leq V \).
	Then we say that \( V \) is the \textit{orthogonal direct sum} of \( V_1 \) and \( V_2 \) if \( V = V_1 \oplus V_2 \) and for all vectors \( v_1\in V_1, v_2\in V_2 \) we have \( \inner{v_1, v_2} = 0 \).
	When this holds, we write \( V = V_1 \overset{\perp}{\oplus} V_2 \).
\end{definition}
\begin{remark}
	If for all vectors \( v_1, v_2 \) we have \( \inner{v_1, v_2} = 0 \), then \( v \in V_1 \cap V_2 \implies \norm{v}^2 = 0 \implies v = 0 \).
	Hence the sum is always direct if the subspaces are orthogonal.
\end{remark}
\begin{definition}
	Let \( V \) be an inner product space and let \( W \leq V \).
	We define the \textit{orthogonal} of \( W \) to be
	\[
		W^\perp = \qty{v \in V \colon \forall w \in W, \inner{v,w} = 0}
	\]
\end{definition}
\begin{lemma}
	For any inner product space \( V \) and any subspace \( W \leq V \), we have \( V = W \overset{\perp}{\oplus} W^\perp \).
\end{lemma}
\begin{proof}
	First note that \( W^\perp \leq V \).
	Then, if \( w \in W \), \( w \in W^\perp \), we have
	\[
		\norm{w}^2 = \inner{w, w} = 0
	\]
	since they are orthogonal, so the vector subspaces intersect only in the zero vector.
	Now, we need to show \( V = W + W^\perp \).
	Let \( (e_1, \dots, e_k) \) be an orthonormal basis of \( W \) and extend it into \( (e_1, \dots, e_k, e_{k+1}, \dots, e_n) \) which can be made orthonormal.
	Then, \( (e_{k+1}, \dots, e_n) \) are elements of \( W^\perp \) and form a basis.
\end{proof}
