\subsection{Cayley-Hamilton theorem}
\begin{theorem}
	Let \( V \) be a finite dimensional \( F \)-vector space.
	Let \( \alpha \in L(V) \) with characteristic polynomial \( \chi_\alpha(t) = \det(\alpha - t I) \).
	Then \( \chi_\alpha(\alpha) = 0 \).
\end{theorem}
\noindent Two proofs will provided; one more physical and based on \( F = \mathbb C \) and one more algebraic.
\begin{proof}
	Let \( B = \qty{v_1, \dots, v_n} \) be a basis of \( V \) such that \( [\alpha]_B \) is triangular.
	This can be done when \( F = \mathbb C \).
	Note, if the diagonal entries in this basis are \( a_i \),
	\[
		\chi_\alpha(t) = \prod_{i=1}^n (a_i - t) \implies \chi_\alpha(\alpha) = (\alpha - a_1 I) \dots (\alpha - a_n I)
	\]
	We want to show that this expansion evaluates to zero.
	Let \( U_j = \vecspan \qty{v_1, \dots, v_j} \).
	Let \( v \in V = U_n \).
	We want to compute \( \chi_\alpha(\alpha)(v) \).
	Note, by construction of the triangular matrix.
	\begin{align*}
		\chi_\alpha(\alpha)(v) & = (\alpha - a_1 I) \dots \underbrace{(\alpha - a_n I)(v)}_{\in U_{n-1}}                     \\
		                       & = (\alpha - a_1 I) \dots \underbrace{(\alpha - a_{n-1} I)(\alpha - a_n I)(v)}_{\in U_{n-2}} \\
		                       & = \dots                                                                                     \\
		                       & \in U_0
	\end{align*}
	Hence this evaluates to zero.
\end{proof}
\noindent The following proof works for any field where we can equate coefficients, but is much less intuitive.
\begin{proof}
	We will write
	\[
		\det(t I - \alpha) = (-1)^n \chi_\alpha(t) = t^n + a_{n-1}t^{n-1} + \dots + a_0
	\]
	For any matrix \( B \), we have proven \( B \adj B = (\det B) I \).
	We apply this relation to the matrix \( B = tI - A \).
	We can check that
	\[
		\adj B = \adj(tI - A) = B_{n-1} t^{n-1} + \dots + B_1 t + B_0
	\]
	since adjugate matrices are degree \( (n-1) \) polynomials for each element.
	Then, by applying \( B \adj B = (\det B) I \),
	\[
		(tI - A) [ B_{n-1} t^{n-1} + \dots + B_1 t + B_0 ] = (\det B) I = (t^n + \dots + a_0) I
	\]
	Since this is true for all \( t \), we can equate coefficients.
	This gives
	\begin{align*}
		t^n     & :      & I         & = B_{n-1}            \\
		t^{n-1} & :      & a_{n-1} I & = B_{n-2} - AB_{n-1} \\
		        & \vdots &           & \vdots               \\
		t^0     & :      & a_0 I     & = -A B_1
	\end{align*}
	Then, substituting \( A \) for \( t \) in each relation will give, for example, \( A^n I = A^n B_{n-1} \).
	Computing the sum of all of these identities, we recover the original polynomial in terms of \( A \) instead of in terms of \( t \).
	Many terms will cancel since the sum telescopes, yielding
	\[
		A^n + a_{n-1} A^{n-1} + \dots + a_0 I = 0
	\]
\end{proof}

\subsection{Algebraic and geometric multiplicity}
\begin{definition}
	Let \( V \) be a finite dimensional \( F \)-vector space.
	Let \( \alpha \in L(V) \) and let \( \lambda \) be an eigenvalue of \( \alpha \).
	Then
	\[
		\chi_\alpha(t) = (t-\lambda)^{a_\lambda} q(t)
	\]
	where \( q(t) \) is a polynomial over \( F \) such that \( (t-\lambda) \) does not divide \( q \).
	\( a_\lambda \) is known as the \textit{algebraic multiplicity} of the eigenvalue \( \lambda \).
	We define the \textit{geometric multiplicity} \( g_\lambda \) of \( \lambda \) to be the dimension of the eigenspace associated with \( \lambda \), so \( g_\lambda = \dim \ker (\alpha - \lambda I) \).
\end{definition}
\begin{lemma}
	If \( \lambda \) is an eigenvalue of \( \alpha \in L(V) \), then \( 1 \leq g_\lambda \leq a_\lambda \).
\end{lemma}
\begin{proof}
	We have \( g_\lambda = \dim \ker (\alpha - \lambda I) \).
	There exists a nontrivial vector \( v \in V \) such that \( v \in \ker(\alpha - \lambda I) \) since \( \lambda \) is an eigenvalue.
	Hence \( g_\lambda \geq 1 \).
	We will show that \( g_\lambda \leq a_\lambda \).
	Indeed, let \( v_1, \dots, v_{g_\lambda} \) be a basis of \( V_\lambda \equiv \ker (\alpha - \lambda I) \).
	We complete this into a basis \( B \equiv \qty(v_1, \dots, v_{g_\lambda}, v_{g_\lambda + 1}, \dots, v_n) \) of \( V \).
	Then note that
	\[
		[\alpha]_B = \begin{pmatrix}
			\lambda I_{g_\lambda} & \star \\
			0                     & A_1
		\end{pmatrix}
	\]
	for some matrix \( A_1 \).
	Now,
	\[
		\det (\alpha - tI) = \det \begin{pmatrix}
			(\lambda - t) I_{g_\lambda} & \star     \\
			0                           & A_1 - t I
		\end{pmatrix}
	\]
	By the formula for determinants of block matrices with a zero block on the off diagonal,
	\[
		\det (\alpha - tI) = (\lambda-t)^{g_\lambda} \det(A_1 - t I)
	\]
	Hence \( g_\lambda \leq a_\lambda \) since the determinant is a polynomial that could have more factors of the same form.
\end{proof}
\begin{lemma}
	Let \( V \) be a finite dimensional \( F \)-vector space.
	Let \( \alpha \in L(V) \) and let \( \lambda \) be an eigenvalue of \( \alpha \).
	Let \( c_\lambda \) be the multiplicity of \( \lambda \) as a root of the minimal polynomial of \( \alpha \).
	Then \( 1 \leq c_\lambda \leq a_\lambda \).
\end{lemma}
\begin{proof}
	By the Cayley-Hamilton theorem, \( \chi_\alpha(\alpha) = 0 \).
	Since \( m_\alpha \) is linear, \( m_\alpha \) divides \( \chi_\alpha \).
	Hence \( c_\lambda \leq a_\lambda \).
	Now we show \( c_\lambda \geq 1 \).
	Indeed, \( \lambda \) is an eigenvalue hence there exists a nonzero \( v \in V \) such that \( \alpha(v) = \lambda v \).
	For such an eigenvector, \( \alpha^P(v) = \lambda^P v \) for \( P \in \mathbb N \).
	Hence for \( p \in F[t] \), \( p(\alpha)(v) = [p(\lambda)](v) \).
	Hence \( m_\alpha(\alpha)(v) = [m_\alpha(\lambda)](v) \).
	Since the left hand side is zero, \( m_\alpha(\lambda) = 0 \).
	So \( c_\lambda \geq 1 \).
\end{proof}
\begin{example}
	Let
	\[
		A = \begin{pmatrix}
			1 & 0 & -2 \\
			0 & 1 & 1  \\
			0 & 0 & 2
		\end{pmatrix}
	\]
	The minimal polynomial can be computed by considering the characteristic polynomial
	\[
		\chi_A(t) = (t-1)^2(t-2)
	\]
	So the minimal polynomial is either \( (t-1)^2(t-2) \) or \( (t-1)(t-2) \)
	We check \( (t-1)(t-2) \).
	\( (A - I)(A - 2I) \) can be found to be zero.
	So \( m_A(t) = (t-1)(t-2) \).
	Since this is a product of distinct linear factors, \( A \) is diagonalisable.
\end{example}
\begin{example}
	Let \( A \) be a Jordan block of size \( n \geq 2 \).
	Then \( g_\lambda = 1 \), \( a_\lambda = n \), and \( c_\lambda = n \).
\end{example}

\subsection{Characterisation of diagonalisable complex endomorphisms}
\begin{lemma}
	Let \( F = \mathbb C \).
	Let \( V \) be a finite-dimensional \( \mathbb C \)-vector space.
	Let \( \alpha \) be an endomorphism of \( V \).
	Then the following are equivalent.
	\begin{enumerate}[(i)]
		\item \( \alpha \) is diagonalisable;
		\item for all \( \lambda \) eigenvalues of \( \alpha \), we have \( a_\lambda = g_\lambda \);
		\item for all \( \lambda \) eigenvalues of \( \alpha \), \( c_\lambda = 1 \).
	\end{enumerate}
\end{lemma}
\begin{proof}
	First, the fact that (i) is true if and only if (iii) is true has already been proven.
	Now let us show that (i) is equivalent to (ii).
	Let \( \lambda_1, \dots, \lambda_k \) be the distinct eigenvalues of \( \alpha \).
	We have already found that \( \alpha \) is diagonalisable if and only if \( V = \bigoplus V_{\lambda_i} \).
	The sum was found to be always direct, regardless of diagonalisability.
	We will compute the dimension of \( V \) in two ways;
	\[
		n = \dim V = \deg \chi_\alpha;\quad n = \dim V = \sum_{i=1}^k a_{\lambda_i}
	\]
	since \( \chi_\alpha \) is a product of \( (t-\lambda_i) \) factors as \( F = \mathbb C \).
	Since the sum is direct,
	\[
		\dim \qty(\bigoplus_{i=1}^k V_{\lambda_i}) = \sum_{i=1}^k g_{\lambda_i}
	\]
	\( \alpha \) is diagonalisable if and only if the dimensions are equal, so
	\[
		\sum_{i=1}^k g_{\lambda_i} = \sum_{i=1}^k a_{\lambda_i}
	\]
	Conversely, we have proven that for all eigenvalues \( \lambda_i \), we have \( g_{\lambda_i} \leq a_{\lambda_i} \).
	Hence, \( \sum_{i=1}^k g_{\lambda_i} = \sum_{i=1}^k a_{\lambda_i} \) holds if and only if \( g_{\lambda_i} = a_{\lambda_i} \) for all \( i \).
\end{proof}
