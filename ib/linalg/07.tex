\subsection{Conjugation and similarity}
Consider the following special case of changing basis.
If \( \alpha \colon V \to V \) is linear, \( \alpha \) is called an \textit{endomorphism}.
If \( B = B', C = C' \) then the special case of the change of basis formula is
\[
	[\alpha]_{B',B'} = P^{-1} [\alpha]_{B,B} P
\]
Then, we say square matrices \( A, A' \) are \textit{similar} or \textit{conjugate} if there exists \( P \) such that \( A' = P^{-1} A P \).

\subsection{Elementary operations}
\begin{definition}
	An \textit{elementary column operation} is
	\begin{enumerate}[(i)]
		\item swap columns \( i, j \)
		\item replace column \( i \) by \( \lambda \) multiplied by the column
		\item add \( \lambda \) multiplied by column \( i \) to column \( j \)
	\end{enumerate}
\end{definition}
We define analogously the elementary row operations.
Note that these elementary operations are invertible (for \( \lambda \neq 0 \)).
These operations can be realised through the action of elementary matrices.
For instance, the column swap operation can be realised using
\[
	T_{ij} = \begin{pmatrix}
		I_n & 0 & 0   \\
		0   & A & 0   \\
		0   & 0 & I_m
	\end{pmatrix};\quad A = \begin{pmatrix}
		0 & 0   & 1 \\
		0 & I_k & 0 \\
		1 & 0   & 1
	\end{pmatrix}
\]
To multiply a column by \( \lambda \),
\[
	n_{i,\lambda} = \begin{pmatrix}
		I_n & 0       & 0   \\
		0   & \lambda & 0   \\
		0   & 0       & I_m
	\end{pmatrix}
\]
To add a multiple of a column,
\[
	c_{ij,\lambda} = I + \lambda E_{ij}
\]
where \( E_{ij} \) is the matrix defined by elements \( (e_{ij})_{pq} = \delta_{ip} \delta_{jq} \).
An elementary column (or row) operation can be performed by multiplying \( A \) by the corresponding elementary matrix from the right (on the left for row operations).
This will essentially provide a constructive proof that any \( n \times n \) matrix is equivalent to
\[
	\begin{pmatrix} I_r & 0 \\ 0 & 0 \end{pmatrix}
\]
We will start with a matrix \( A \).
If all entries are zero, we are done.
So we will pick \( a_{ij} = \lambda \neq 0 \), and swap rows \( i,1 \) and columns \( j,0 \).
This ensures that \( a_{11} = \lambda \neq 0 \).
Now we multiply column 1 by \( \frac{1}{\lambda} \).
Finally, we can clear out row 1 and column 1 by subtracting multiples of the first row or column.
Then we can perform similar operations on the \( (n-1)\times(n-1) \) matrix in the bottom right block and inductively finish this process.

\subsection{Gauss' pivot algorithm}
If only row operations are used, we can reach the `row echelon' form of the matrix, a specific case of an upper triangular matrix.
On each row, there are a number of zeroes until there is a one, called the pivot.
First, we assume that \( a_{ij} \neq 0 \).
We swap rows \( i, 1 \).
Then divide the first row by \( \lambda = a_{i1} \) to get a one in the top left.
We can use this one to clear the rest of the first column.
Then, we can repeat on the next column, and iterate.
This is a technique for solving a linear system of equations.

\subsection{Representation of square invertible matrices}
\begin{lemma}
	If \( A \) is an \( n \times n \) square invertible matrix, then we can obtain \( I_n \) using only row elementary operations, or only column elementary operations.
\end{lemma}
\begin{proof}
	We show an algorithm that constructs this \( I_n \).
	This is exactly going to invert the matrix, since the resultant operations can be combined to get the inverse matrix.
	We will show here the proof for column operations.
	We argue by induction on the number of rows.
	Suppose we can make the form
	\[
		\begin{pmatrix} I_k & 0 \\ A & B \end{pmatrix}
	\]
	We want to obtain the same structure with \( k+1 \) rows.
	We claim that there exists \( j > k \) such that \( a_{k+1,j} \neq 0 \).
	Indeed, otherwise we can show that the vector
	\[
		\begin{pmatrix} 0 \\ \vdots \\ 1 \\ \vdots \\ 0 \end{pmatrix} = \delta_{k+1,i}
	\]
	is not in the span of the column vectors of \( A \).
	This contradicts the invertibility of the matrix.
	Now, we will swap columns \( k+1, j \) and divide this column by \( \lambda \).
	We can now use this 1 to clear the rest of the \( k+1 \) row.

	Inductively, we have found \( A E_1 \dots E_n = I_n \) where \( E_n \) are elementary.
	Thus, we can find \( A^{-1} \).
\end{proof}
\begin{proposition}
	Any invertible square matrix is a product of elementary matrices.
\end{proposition}
The proof is exactly the proof of the lemma above.
