\subsection{Statement of the Algorithm}
Consider minimising \(f(x)\) such that \(f \colon \mathbb R^n \to \mathbb R\) is a convex function.
Recall that a local minimum of \(f\) is also the global minimum.
Consider the following `greedy' method:
\begin{itemize}
	\item Start at a point \(\vb x_0\).
	\item Search for close points around \(\vb x_0\) whose values of \(f\) are smaller than \(f(\vb x_0)\).
	      \begin{itemize}
		      \item If such a point exists, let this be \(\vb x_1\).
		            Repeat the algorithm.
		      \item If such a point does not exist, we have found a local minimum, which is the global minimum.
	      \end{itemize}
\end{itemize}
We can find such \(\vb x_1\) points by considering the Taylor series expansion of \(f\) around a point.
\[
	f(\vb x - \varepsilon\grad{f(\vb x)}) \approx f(\vb x) - \varepsilon \grad{f(\vb x)}^\transpose \cdot \grad{f(\vb x)} = f(\vb x) - \varepsilon\norm{\grad{f(\vb x)}}^2 \leq f(\vb x)
\]
Hence \(-\grad{f(\vb x)}\) is called a descending direction.
Although the gradient of the function is the most natural way of decreasing a function, any \(\vb v\) with \(f(\vb x) \cdot \vb v < 0\) is a descending direction.
This gives us the \textit{gradient descent} algorithm.

\begin{algorithm*}[H]
	\SetAlgoLined{}
	\KwResult{Global minimum of \(f(\vb x)\)}
	start at a point \(\vb x_0\)\;
	\(t \leftarrow 0\)\;
	\Repeat{\(\grad{f(\vb x)} = 0\) or \(t\) is large enough}{
		find a descending direction \(\vb v_t\), e.g.\ \(-\grad{f(\vb x)}\)\;
		choose a step size \(\eta_t\)\;
		\(\vb x_{t+1} \leftarrow \vb x_t + \eta_t \vb v_t\)\;
	}
	\caption{Gradient Descent Algorithm}
\end{algorithm*}

\noindent Different choices of \(\vb v_t\) and \(\eta_t\) give rise to many different qualities of algorithm.

\subsection{\(\beta\)-Smoothness Assumption}
Some restrictions must be applied to a function to let us prove that gradient descent works.
\begin{definition}
	A continuously differentiable function \(f \colon \mathbb R^n \to \mathbb R\) is \textit{\(\beta\)-smooth} if \(\grad{f}\) is a \(\beta\)-Lipschitz function:
	\[
		\norm{\grad{f(\vb x) - \grad{f(\vb y)}}} \leq \beta \norm{\vb x - \vb y}
	\]
\end{definition}
\noindent In the following sections, we assume all functions \(f\) are \(\beta\)-smooth.
Further, if \(f\) is twice differentiable (i.e.\ the Hessian exists everywhere), then the \(\beta\)-smoothness assumption is equivalent to
\[
	\laplacian{f(\vb x)} \preceq \beta I
\]
so all eigenvalues of \(\laplacian{f(\vb x)}\) have \(\lambda \leq \beta\).
Also,
\[
	\vb u^\transpose \laplacian{f(\vb x)} \vb u \leq \vb u^\transpose (\beta I) \vb u = \beta \norm{\vb u}^2
\]
\begin{definition}
	The \textit{linear approximation} to \(f\) at \(\vb x\) is
	\[
		f(\vb x) + \grad{f(\vb x)}^\transpose (\vb y - \vb x)
	\]
\end{definition}
\noindent We might assume that the linear approximation is close to the actual function in a small neighbourhood around \(x\).
\begin{claim}
	If \(f\) is \(\beta\)-smooth, then
	\[
		f(\vb y) \leq f(\vb x) + \grad{f(\vb x)}^\transpose (\vb y - \vb x) + \frac{\beta}{2}\norm{\vb y - \vb x}^2
	\]
\end{claim}
\noindent Note that
\[
	f(\vb x) + \grad{f(\vb x)}^\transpose (\vb y - \vb x) \leq f(\vb y)
\]
since \(f\) is convex, so this claim would show that \(f\) really is close to the actual function, deviating by an arbitrarily small amount as we let \(\vb x\) approach \(\vb y\).
\begin{proof}
	By Taylor's theorem,
	\begin{align*}
		f(\vb y) & = f(\vb x) + \grad{f(\vb x)}^\transpose (\vb y - \vb x) + \frac{1}{2}(\vb y - \vb x)^\transpose \laplacian{f(\vb z)}(\vb y - \vb x) \\
		         & \leq f(\vb x) + \grad{f(\vb x)}^\transpose (\vb y - \vb x) + \frac{1}{2}(\vb y - \vb x)^\transpose (\beta I) (\vb y - \vb x)        \\
		         & = f(\vb x) + \grad{f(\vb x)}^\transpose (\vb y - \vb x) + \frac{\beta}{2}\norm{\vb y - \vb x}^2                                     \\
	\end{align*}
\end{proof}
\begin{corollary}
	If we move by a step size of \(\frac{1}{\beta}\), we will descend by at least \(\frac{1}{2\beta}\norm{\grad{f(x)}}^2\).
	\[
		f\qty(\vb x - \frac{1}{\beta}\grad{f(\vb x)}) \leq f(\vb x) - \frac{1}{2\beta}\norm{\grad{f(x)}}^2
	\]
\end{corollary}
\begin{proof}
	Consider
	\[
		f(\vb x) + \grad{f(\vb x)}^\transpose (\vb y - \vb x) + \frac{\beta}{2}\norm{\vb y - \vb x}^2
	\]
	as a function of \(\vb y\), and try to minimise it for a fixed \(\vb x\).
	\[
		\nabla_{\vb y} \qty(f(\vb x) + \grad{f(\vb x)}^\transpose (\vb y - \vb x) + \frac{\beta}{2}\norm{\vb y - \vb x}^2) = \grad{f(\vb x)} + \beta(\vb y - \vb x) = 0
	\]
	Hence,
	\begin{align*}
		\frac{\grad{f(\vb x)}}{\beta} & = \vb x - \vb y                          \\
		\vb y                         & = \vb x - \frac{1}{\beta}\grad{f(\vb x)}
	\end{align*}
	Substituting into the claim above, we have
	\begin{align*}
		f\qty(x - \frac{1}{\beta}\grad{f(\vb x)}) & \leq f(\vb x) + \grad{f(\vb x)}^\transpose \qty(\frac{-1}{\beta}\grad{f(\vb x)}) + \frac{\beta}{2}\norm{\frac{1}{\beta}\grad{f(\vb x)}}^2 \\
		                                          & = f(\vb x) - \frac{1}{\beta}\norm{\grad{f(\vb x)}}^2 + \frac{1}{2\beta}\norm{\grad{f(\vb x)}}^2                                           \\
		                                          & = f(\vb x) - \frac{1}{2\beta}\norm{\grad{f(\vb x)}}^2                                                                                     \\
	\end{align*}
\end{proof}

\begin{claim}[Improved first order condition]
	\[
		f(\vb y) \geq f(\vb x) + \grad{f(\vb x)}^\transpose (\vb y - \vb x) + \frac{1}{2\beta} \norm{\grad{f(\vb x) - \grad{f(\vb y)}}}^2
	\]
\end{claim}
\begin{proof}
	For any \(\vb z\), by the standard first order condition and the corollary above we have
	\[
		f(\vb x) + \grad{f(\vb x)}^\transpose (\vb z - \vb x) \leq f(\vb z) \leq f(\vb y) + \grad{f(\vb y)}^\transpose (\vb z - \vb y) + \frac{\beta}{2}\norm{\vb z - \vb y}^2
	\]
	This then implies
	\[
		f(\vb x) - f(\vb y) \leq \grad{f(\vb x)}^\transpose (\vb x - \vb z) + \grad{f(\vb y)}^\transpose (\vb z - \vb y) + \frac{\beta}{2}\norm{\vb z - \vb y}^2
	\]
	The left hand side is not dependent on \(\vb z\), so by minimising \(\vb z\) we get the best bound for the left hand side.
	We set the gradient of \(\vb z\) to zero.
	\begin{align*}
		-\grad{f(\vb x)} + \grad{f(y)} + \beta (\vb z - \vb y) & = 0 \\
		\implies \vb z = \frac{\grad{f(\vb x) - \grad{f(\vb y)}}}{\beta} + \vb y
	\end{align*}
	Substituting back, we have
	\[
		f(\vb x) - f(\vb y) \leq \grad{f(\vb x)}^\transpose (\vb x - \vb y) - \frac{1}{2\beta} \norm{\grad{f(\vb x) - \grad{f(\vb y)}}}^2
	\]
\end{proof}

\subsection{\(\alpha\)-Strong Convexity Assumption}
In general, a small gradient does not imply that we are close to the optimum value of the function.
We must therefore add an additional assumption in order to justify gradient descent.
\begin{definition}
	A function \(f \colon \mathbb R^n \to \mathbb R\) is called \textit{\(\alpha\)-strongly convex} if
	\[
		f(\vb y) \geq f(\vb x) + \grad{f(\vb x)}^\transpose (\vb y - \vb x) + \frac{\alpha}{2}\norm{\vb y - \vb x}^2
	\]
\end{definition}
\noindent If \(f\) is twice differentiable, then its Hessian satisfies
\[
	\laplacian{f(\vb x)} \succeq \alpha I
\]
for all \(\vb x\).
\begin{claim}
	Let \(f\) be \(\alpha\)-strongly convex.
	Let \(p^\star\) be the optimal cost; i.e.\ the minimum value of \(f\).
	Then for any \(\vb x\) we have
	\[
		p^\star \geq f(\vb x) - \frac{1}{2\alpha}\norm{\grad{f(\vb x)}}^2
	\]
\end{claim}
\begin{remark}
	If \(\norm{\grad{f(\vb x)}} \leq \sqrt{2\alpha\varepsilon}\), then
	\[
		p^\star \leq f(\vb x) \leq p^\star + \varepsilon
	\]
	So a small gradient means we are close to the optimum.
\end{remark}
\begin{proof}
	The \(\alpha\)-strong convexity assumption gives
	\[
		f(\vb y) \geq f(\vb x) + \grad{f(\vb x)}^\transpose (\vb y - \vb x) + \frac{\alpha}{2}\norm{\vb y - \vb x}^2
	\]
	Taking the minimum over \(\vb y\) of both sides, the left hand side becomes \(p^\star\).
	Setting the gradient of the right hand side to zero,
	\begin{align*}
		\grad{f(\vb x)} - \alpha (\vb x - \vb y) & = 0               \\
		\frac{\grad{f(\vb x)}}{\alpha}           & = (\vb x - \vb y) \\
	\end{align*}
	This gives
	\begin{align*}
		p^\star & \geq f(\vb x) + \grad{f(\vb x)}^\transpose \qty(-\frac{\grad{f(\vb x)}}{\alpha}) + \frac{\alpha}{2}\norm{\frac{\grad{f(\vb x)}}{\alpha}}^2 \\
		        & = f(\vb x) - \frac{\norm{\grad{f(\vb x)}}^2}{\alpha} + \frac{\norm{\grad{f(\vb x)}}^2}{2\alpha}                                            \\
		        & = f(\vb x) - \frac{\norm{\grad{f(\vb x)}}^2}{2\alpha}
	\end{align*}
\end{proof}
\begin{claim}
	Let \(\vb x^\star\) be the minimising value, i.e.\ \(f(\vb x^\star) = p^\star\).
	Then
	\[
		\norm{\vb x - \vb x^\star} \leq \frac{2}{\alpha} \norm{\grad(\vb x)}
	\]
	So if a function is strongly convex, we can find a region in which we know the global maximum lies.
\end{claim}
\begin{proof}
	By the Cauchy-Schwarz inequality,
	\begin{align*}
		f(\vb x^\star) & \geq f(\vb x) + \grad{f(\vb x)}^\transpose (\vb x^\star - \vb x) + \frac{\alpha}{2}\norm{\vb x^\star - \vb x}^2  \\
		               & \geq f(\vb x) - \norm{\grad{f(\vb x)}} \norm{\vb x^\star - \vb x} + \frac{\alpha}{2}\norm{\vb x^\star - \vb x}^2
	\end{align*}
	Since \(f(\vb x^\star) \leq f(\vb x)\), we have
	\[
		0 \geq f(\vb x^\star) - f(\vb x) \geq - \norm{\grad{f(\vb x)}} \norm{\vb x^\star - \vb x} + \frac{\alpha}{2}\norm{\vb x^\star - \vb x}^2
	\]
	Hence,
	\begin{align*}
		\norm{\grad{f(\vb x)}} \norm{\vb x^\star - \vb x} & \geq \frac{\alpha}{2}\norm{\vb x^\star - \vb x}^2 \\
		\norm{\grad{f(\vb x)}}                            & \geq \frac{\alpha}{2}\norm{\vb x^\star - \vb x}   \\
	\end{align*}
\end{proof}
