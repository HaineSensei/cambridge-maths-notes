\subsection{Estimators}
Suppose \( X_1, \dots, X_n \) are i.i.d.\ observations with a p.d.f.\ (or p.m.f.) \( f_X(x \mid \theta) \), where \( \theta \) is an unknown parameter in some parameter space \( \Theta \).
Let \( X = (X_1, \dots, X_n) \).
\begin{definition}
	An \textit{estimator} is a statistic, or a function of the data, written \( T(X) = \hat\theta \), which is used to approximate the true value of \( \theta \).
	This does not depend (explicitly) on \( \theta \).
	The distribution of \( T(X) \) is called its \textit{sampling distribution}.
\end{definition}
\begin{example}
	Let \( X_1, \dots, X_n \sim N(0,1) \) be i.i.d.
	Let \( \hat \mu = T(X) = \overline X_n \).
	The sampling distribution is \( T(X) \sim N\qty(\mu, \frac{1}{n}) \).
	Note that this sampling distribution in general depends on the true parameter \( \mu \).
\end{example}
\begin{definition}
	The \textit{bias} of \( \hat \theta \) is
	\[ \mathrm{bias}\qty(\hat \theta) = \esub{\theta}{\hat \theta} - \theta \]
	Note that \( \hat \theta \) is a function only of \( X_1, \dots, X_n \), and the expectation operator \( \mathbb E_\theta \) assumes that the true value of the parameter is \( \theta \).
\end{definition}
\begin{remark}
	In general, the bias is a function of the true parameter \( \theta \), even though it is not explicit in the notation.
\end{remark}
\begin{definition}
	An estimator with zero bias for all \( \theta \) is called an \textit{unbiased estimator}.
\end{definition}
\begin{example}
	The estimator \( \hat \mu \) in the above example is unbiased, since
	\[ \esub{\mu}{\hat \mu} = \exp_\mu(\overline X_n) = \mu \]
	for all \( \mu \in \mathbb R \).
\end{example}
\begin{definition}
	The \textit{mean squared error} of \( \theta \) is defined as
	\[ \mathrm{mse}\qty(\hat \theta) = \esub{\theta}{\qty(\hat \theta - \theta)^2} \]
\end{definition}
\begin{remark}
	Like the bias, the mean squared error is, in general, a function of the true parameter \( \theta \).
\end{remark}
