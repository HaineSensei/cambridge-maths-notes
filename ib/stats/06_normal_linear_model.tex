\subsection{Multivariate normal distribution}
Let \( X = (X_1, \dots, X_n) \) be a vector of random variables.
Then we define
\[ \expect{X} = \begin{pmatrix}
    \expect{X_1} \\
    \vdots \\
    \expect{X_n}
\end{pmatrix};\quad \Var{X} = \qty( \expect{\qty(X_i - \expect{X_i})\qty(X_j - \expect{X_j})} )_{i,j} \]
The familiar linearity results are
\[ \expect{AX+b} = A\expect{X} + b;\quad A \Var{X} A^\transpose \]
where \( A \in \mathbb R^{k \times n}, b \in \mathbb R^k \) are constant.
\begin{definition}
    We say that \( X \) has a \textit{multivariate normal distribution} if, for any fixed \( t \in \mathbb R^n \), we have \( t^\transpose X \sim N(\mu, \sigma^2) \) for some parameters \( \mu, \sigma^2 \).
\end{definition}
\begin{proposition}
    Let \( X \) be multivariate normal.
    Then \( AX+b \) is multivariate normal, where \( A \in \mathbb R^{k \times n}, b \in \mathbb R^k \) are constant.
\end{proposition}
\begin{proof}
    Let \( t \in \mathbb R^k \).
    Then,
    \[ t^\transpose (Ax+b) = \underbrace{(A^\transpose t)^\transpose X}_{\sim N(\mu, \sigma^2)} + t^\transpose b \]
    which is the sum of a normal random variable and a constant.
    So this is \( N(\mu + t^\transpose b, \sigma^2) \).
\end{proof}
\begin{proposition}
    A multivariate normal distribution is fully specified by its mean and covariance matrix.
\end{proposition}
\begin{proof}
    Let \( X_1, X_2 \) be multivariate normal vectors with the same mean \( \mu \) and the same covariance matrix \( \Sigma \).
    We will show that these two random variables have the same moment generating function, and hence the same distribution.
    \[ M_{X_1}(t) = \expect{ e^{1 \cdot t^\transpose X_1} } \]
\end{proof}
