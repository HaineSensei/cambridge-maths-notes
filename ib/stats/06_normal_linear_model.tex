\subsection{Multivariate normal distribution}
Let \( X = (X_1, \dots, X_n) \) be a vector of random variables.
Then we define
\[ \expect{X} = \begin{pmatrix}
    \expect{X_1} \\
    \vdots \\
    \expect{X_n}
\end{pmatrix};\quad \Var{X} = \qty( \expect{\qty(X_i - \expect{X_i})\qty(X_j - \expect{X_j})} )_{i,j} \]
The familiar linearity results are
\[ \expect{AX+b} = A\expect{X} + b;\quad A \Var{X} A^\transpose \]
where \( A \in \mathbb R^{k \times n}, b \in \mathbb R^k \) are constant.
\begin{definition}
    We say that \( X \) has a \textit{multivariate normal distribution} if, for any fixed \( t \in \mathbb R^n \), we have \( t^\transpose X \sim N(\mu, \sigma^2) \) for some parameters \( \mu, \sigma^2 \).
\end{definition}
\begin{proposition}
    Let \( X \) be multivariate normal.
    Then \( AX+b \) is multivariate normal, where \( A \in \mathbb R^{k \times n}, b \in \mathbb R^k \) are constant.
\end{proposition}
\begin{proof}
    Let \( t \in \mathbb R^k \).
    Then,
    \[ t^\transpose (Ax+b) = \underbrace{(A^\transpose t)^\transpose X}_{\sim N(\mu, \sigma^2)} + t^\transpose b \]
    which is the sum of a normal random variable and a constant.
    So this is \( N(\mu + t^\transpose b, \sigma^2) \).
\end{proof}
\begin{proposition}
    A multivariate normal distribution is fully specified by its mean and covariance matrix.
\end{proposition}
\begin{proof}
    Let \( X_1, X_2 \) be multivariate normal vectors with the same mean \( \mu \) and the same covariance matrix \( \Sigma \).
    We will show that these two random variables have the same moment generating function, and hence the same distribution.
    \[ M_{X_1}(t) = \expect{ e^{1 \cdot t^\transpose X_1} } \]
	Note that \( t^\transpose X_1 \) is univariate normal.
	Hence, this is equal to
	\[ M_{X_1}(t) = \exp(1 \cdot \expect{t^\transpose X_1} + \frac{1}{2} \Var{t^\transpose X_1} \cdot 1^2) = \exp(t^\transpose \mu + \frac{1}{2} t^\transpose \Sigma t) \]
	This depends only on \( \mu \) and \( \Sigma \), and we obtain the same moment generating function for \( X_2 \).
\end{proof}

\subsection{Orthogonal projections}
\begin{definition}
	A matrix \( P \in \mathbb R^{n \times n} \) is an \textit{orthogonal projection} onto its column space \( \mathrm{col}(P) \) if, for all \( v \in \mathrm{col}(P) \), we have \( Pv = v \), and for all \( w \in \mathrm{col}(P)^\perp \), we have \( Pw = 0 \).
\end{definition}
\begin{proposition}
	\( P \) is an orthogonal projection if and only if it is idempotent and symmetric.
\end{proposition}
\begin{proof}
	If \( P \) is idempotent and symmetric, let \( v \in \mathrm{col}(P) \), so \( v = Pa \) for some \( a \in \mathbb R^n \).
	Then, \( Pv = PPa = Pa = v \).
	Now, let \( w \in \mathrm{col}(P)^\perp \).
	By definition, \( P^\transpose w = 0 \).
	By symmetry, \( Pw = 0 \).

	Now, suppose \( P \) is an orthogonal projection.
	Any vector \( a \in \mathbb R^n \) can be uniquely written as \( a = v + w \) where \( v \in \mathrm{col}(P) \) and \( w \in \mathrm{col}(P)^\perp \).
	Then \( PPa = PPv + PPw = Pv = P(v+w) = Pa \).
	As this holds for all \( a \), we have that \( P \) is idempotent.
	Let \( u_1, u_2 \in \mathbb R^n \), and note \( (P u_1) \cdot ((I-P) u_2) = 0 \), as \( P u_1 \in \mathrm{col}(P) \) and \( (I-P) u_2 \in \mathrm{col}(P)^\perp \).
	We have \( u_1^\transpose P^\transpose (I-P) u_2 = 0 \).
	Since this holds for all \( u_1, u_2 \), \( P^\transpose (I-P) = 0 \) so \( P^\transpose = P^\transpose P \).
	Note that \( P^\transpose P \) is symmetric, so \( P^\transpose \) is symmetric, and hence \( P \) is symmetric.
\end{proof}
\begin{corollary}
	Let \( P \) be an orthogonal projection matrix.
	Then \( I-P \) is also an orthogonal projection matrix.
\end{corollary}
\begin{proof}
	Clearly, if \( P \) is symmetric, so is \( I-P \), so it suffices to prove idempotence.
	We have \( (I-P)(I-P) = I - 2P + P^2 = I - 2P + P = I - P \) as required.
\end{proof}
\begin{proposition}
	If \( P \) is an orthogonal projection, then \( P = UU^\transpose \) where the columns of \( U \) are an orthonormal basis for the column space of \( P \).
\end{proposition}
\begin{proof}
	First, we show that \( UU^\transpose \) is an orthogonal projection.
	This is clearly symmetric.
	It is idempotent: \( U U^\transpose U U^\transpose = U U^\transpose \) since \( U^\transpose U = I \), as the columns of \( U \) form an orthonormal basis for the column space of \( P \).
	Further, the column space of \( P \) is exactly the column space of \( U U^\transpose \).
\end{proof}
\begin{proposition}
	The rank of an orthogonal projection matrix is equal to its trace.
\end{proposition}
\begin{proof}
	The rank is the dimension of the column space, which is \( \rank P = \rank(U^\transpose U) = \tr(U^\transpose U) = \tr(U U^\transpose) = \tr P \).
\end{proof}
\begin{theorem}
	Let \( X \) be multivariate normal, where \( X \sim N(0,\sigma^2 I) \), and let \( P \) be an orthogonal projection.
	Then
	\begin{enumerate}[(i)]
		\item \( PX \sim N(0,\sigma^2 P) \), and \( (I-P)X \sim N(0,\sigma^2(I-P)) \), and these two random variables are independent;
		\item \( \frac{\norm{PX}^2}{\sigma^2} \sim \chi^2_{\rank P} \).
	\end{enumerate}
\end{theorem}
\begin{proof}
	The vector \( (P, I-P)^\transpose X \) is multivariate normal, since it is a linear function of \( X \).
	This distribution is fully specified by its mean and variance.
	\[ \expect{\begin{pmatrix}
		PX \\
		(I-P)X
	\end{pmatrix}} = \begin{pmatrix}
		P \\
		I-P
	\end{pmatrix} \expect{X} = 0 \]
	Further,
	\[ \Var{\begin{pmatrix}
		PX \\
		(I-P)X
	\end{pmatrix}} = \begin{pmatrix}
		P \\
		I - P
	\end{pmatrix} \sigma^2 I \begin{pmatrix}
		P \\
		I - P
	\end{pmatrix} = \sigma^2 \begin{pmatrix}
		P^2 & P(I-P) \\
		P(I-P) & (I-P)^2
	\end{pmatrix} = \sigma^2 \begin{pmatrix}
		P & 0 \\
		0 & I-P
	\end{pmatrix} \]
	Now we must show that the variables \( PX, (I-P)X \) are independent.
	Let \( Z \sim N(0,\sigma^2 P), Z' \sim N(0,\sigma^2(I-P)) \) be independent.
	Then we can see that \( (Z, Z')^\transpose \) is multivariate normal with
	\[ \mu = 0;\quad \Sigma = \begin{pmatrix}
		P & 0 \\
		0 & I - P
	\end{pmatrix} \]
	Hence \( (PX, (1-P)X)^\transpose \) is equal in distribution to \( (Z, Z')^\transpose \).
	So \( PX \) is independent of \( (I-P)X \).

	We must show that \( \frac{\norm{PX}^2}{\sigma^2} \sim \chi^2_{\rank P} \).
	Note that
	\[ \frac{\norm{PX}^2}{\sigma^2} = \frac{X^\transpose P^\transpose P X}{\sigma^2} = \frac{X^\transpose \qty(U U^\transpose)^\transpose U U^\transpose}{\sigma^2} = \frac{\norm{U^\transpose X}^2}{\sigma^2} \]
	Note, \( U^\transpose X \sim N(0,\sigma^2 U^\transpose U) = N(0,\sigma^2 I_{\rank P}) \).
	So
	\[ \frac{(U^\transpose X)_i}{\sigma} \overset{\text{iid}}{\sim} N(0,1) \]
	for \( i = 1, \dots, \rank P \).
	Hence
	\[ \frac{\norm{PX}^2}{\sigma^2} = \sum_{i=1}^{\rank P} \qty(\frac{(U^\transpose X)_i}{\sigma})^2 \sim \chi^2_{\rank P} \]
\end{proof}
\begin{theorem}
	Let \( X_1, \dots, X_n \overset{\text{iid}}{\sim} N(\mu,\sigma^2) \) for some unknown \( \mu \in \mathbb R \) and \( \sigma^2 > 0 \).
	The maximum likelihood estimators for \( \mu \) and \( \sigma \) are
	\[ \hat \mu = \overline X = \frac{1}{n} \sum_i X_i;\quad \hat \sigma^2 = \frac{S_{xx}}{n} = \frac{\sum_i \qty(X_i - \overline X)^2}{n} \]
	Further,
	\begin{enumerate}[(i)]
		\item \( \overline X \sim N\qty(\mu, \frac{\sigma^2}{n}) \);
		\item \( \frac{S_{xx}}{\sigma^2} \sim \chi^2_{n-1} \);
		\item \( \overline X, S_{xx} \) are independent.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Let \( P \) be the square \( n \times n \) matrix with all entries \( \frac{1}{n} \).
	This is an orthogonal projection matrix, as it is symmetric and idempotent.
	Note that
	\[ PX = \begin{pmatrix}
		\overline X \\
		\vdots \\
		\overline X
	\end{pmatrix} \]
	We will write the observations \( X \) as
	\[ X = \underbrace{\begin{pmatrix}
		\mu \\
		\vdots \\
		\mu
	\end{pmatrix}}_{M} + \varepsilon;\quad \varepsilon \sim N(0,\sigma^2 I) \]
	Note that \( \overline X \) is a function of \( P \varepsilon \), since \( \overline X = (PX)_1 = (PM + P\varepsilon)_1 \).
	Further,
	\[ S_{xx} = \sum_i \frac{\qty(X_i - \overline X)^2}{n} = \norm{X - PX}^2 = \norm{(I-P)X}^2 = \norm{(I-P)\varepsilon}^2 \]
	Hence \( S_{xx} \) is a function of \( (I-P)\varepsilon \).
	Since \( P\varepsilon \) and \( (I-P)\varepsilon \) are independent, \( \overline X \) and \( S_{xx} \) are independent.
	Since \( I-P \) is a projection with rank equal to its trace \( n-1 \), we apply the previous theorem to obtain
	\[ S_{xx} = \norm{(I-P)\varepsilon}^2 \chi^2_{n-1} \]
\end{proof}

\subsection{Linear model}
Suppose we have data in pairs \( (x_1, Y_1), \dots, (x_n, Y_n) \), where \( Y_i \in \mathbb R, x_i \in \mathbb R^p \).
The \( Y_i \) are known as the \textit{response} variables, or the \textit{dependent} variables.
The \( x_{i1}, x_{ip} \) are the \textit{predictors}, or \textit{independent} variables.
We will model the expectation of the response \( Y_i \) as a linear function of the predictors \( (x_{i1}, \dots, x_{ip}) \).
\begin{example}
	Let \( Y_i \) be the number of insurance claims that driver \( i \) makes in a given year, and \( x_{i1}, \dots, x_{ip} \) is a set of variables about the specific driver.
	Predictors include age, the number of years they have held their license, and the number of points on their license, for instance.
\end{example}
We assume that
\[ Y_i = \alpha + \beta_1 x_{i1} + \dots + \beta_p x_{ip} + \varepsilon_i \]
where \( \alpha \in \mathbb R \) is an \textit{intercept}, \( \beta_i \) are the \textit{coefficients}, and \( \varepsilon \) is a \textit{noise vector}, which is a random variable.
The intercept and coefficients are the parameters of interest.
We will often eliminate the intercept by making one of the predictors \( x_{i1} = 1 \) for all \( i \), so \( \beta_1 \) plays the role of the intercept.

Note that we can use a linear model to model nonlinear relationships.
For example, suppose \( Y_i = a + bz_i + cz_i^2 + \varepsilon_i \).
We can rephrase this as a linear model with \( x_i = (1, z_i, z_i^2) \).

The coefficient \( \beta_j \) can be interpreted as the effect on \( Y_i \) of increasing \( x_{ij} \) by one, while keeping all other predictors fixed.
This cannot be interpreted as a causal relationship, unless this is a randomised control experiment.

\subsection{Matrix formulation}
Let
\[ Y = \begin{pmatrix}
	Y_1 \\
	\vdots \\
	Y_n
\end{pmatrix};\quad X = \begin{pmatrix}
	x_{11} & \cdots & x_{1p} \\
	\vdots & \ddots & \vdots \\
	x_{n1} & \cdots & x_{np}
\end{pmatrix};\quad \beta = \begin{pmatrix}
	\beta_1 \\
	\vdots \\
	\beta_p
\end{pmatrix};\quad \varepsilon = \begin{pmatrix}
	\varepsilon_1 \\
	\vdots \\
	\varepsilon_n
\end{pmatrix} \]
We call \( X \) the \textit{design matrix}.
The linear model is that
\[ Y = X\beta + \varepsilon \]
\( X\beta \) is considered fixed.
Since \( \varepsilon \) is random, this makes \( Y \) into a random variable.

\subsection{Assumptions}
We make a number of \textit{moment assumptions} on the noise vector \( \varepsilon \).
This allows us to deduce more results about the linear model.
\begin{enumerate}[(i)]
	\item \( \expect{\varepsilon} = 0 \implies \expect{Y_i} x_i^\transpose \beta \);
	\item \( \Var \varepsilon = \sigma^2 \) (this property is known as homoskedasticity), which is equivalent to both \( \Var \varepsilon_i = \sigma^2 \) and \( \Cov{\varepsilon_i, \varepsilon_j} \).
\end{enumerate}
We will always assume that the design matrix \( X \) has full rank \( p \), or equivalently, that it has linearly independent columns.
Since \( X \in \mathbb R^{n \times p} \), this requires that \( n \geq p \), so we need at least as many samples as we have predictors.

\subsection{Least squares estimation}
\begin{definition}
	The \textit{least squares estimator} \( \hat \beta \) minimises the \textit{residual sum of squares}, which is
	\[ S(\beta) = \norm{Y-X\beta}^2 = \sum_i \qty(Y_i - x_i^\transpose \beta)^2 \]
	The term \( Y_i - x_i^\transpose \beta \) is called the \text{\( i \)th residual}.
\end{definition}
Since \( S(\beta) \) is a positive definite quadratic in \( \beta \), it is minimised at the stationary point.
\[ \eval{\pdv{S(\beta)}{\beta_k}}_{\beta = \hat \beta} = 0 \iff \forall k,\;-2\sum_{i=1}^n x_{ik} \qty(Y_i - \sum_k x_{ij} \hat \beta_j) = 0 \iff X^\transpose X \hat \beta = X^\transpose T \]
As \( X \) has full column rank, \( X^\transpose X \) is invertible.
\[ \hat \beta = (X^\transpose X)^{-1} X^\transpose Y \]
This is notably a linear function of \( Y \), given fixed \( X \).
Note that
\[ \expect{\hat \beta} = (X^\transpose X)^{-1} X^\transpose \expect{Y} = (X^\transpose X)^{-1} X^\transpose X\beta = \beta \]
So \( \hat \beta \) is an unbiased estimator.
Further,
\begin{align*}
	\Var{\hat \beta} &= (X^\transpose X)^{-1} X^\transpose \Var Y \qty[(X^\transpose X)^{-1} X^\transpose]^\transpose \\
	&= (X^\transpose X)^{-1} X^\transpose \sigma^2 I \qty[(X^\transpose X)^{-1} X^\transpose]^\transpose \\
	&= \sigma^2 (X^\transpose X)^{-1}
\end{align*}
\begin{theorem}[Gauss-Markov theorem]
	Let an estimator \( \beta^\star \) of \( \beta \) be unbiased and a linear function of \( Y \), so \( \beta^\star = CY \).
	Then, for any fixed \( t \in \mathbb R^p \), we have
	\[ \Var{t^\transpose \hat \beta} \leq \Var{t^\transpose \beta\star} \]
	where \( \hat \beta \) is the least squares estimator.
	We say that \( \hat \beta \) is the best linear unbiased estimator (BLUE).
\end{theorem}
\begin{remark}
	We can think of \( t \in \mathbb R^p \) as a vector of predictors for a new sample.
	Then \( t^\transpose \hat \beta \) is the prediction for \( \expect{Y_i} \) for this new sample, using the least squares estimator.
	\( t^\transpose \beta^\star \) is the prediction with \( \beta^\star \).
	In both cases, the prediction is unbiased.
\end{remark}
\begin{proof}
	Note that
	\[ \Var{t^\transpose \beta^\star} - \Var{t^\transpose \hat \beta} = t^\transpose \qty[\Var{\beta^\star} - \Var{\hat\beta}] t \]
	To prove that this quantity is always non-negative, we must show that \( \Var{\beta^\star} - \Var{\hat\beta} \) is positive semidefinite.
	Let \( A = C - (X^\transpose X)^{-1} X^\transpose \).
	Note that \( \expect{AY} = \expect{\beta^\star} - \expect{\hat\beta} = 0 \).
	Also, \( \expect{AY} = A \expect{Y} = AX\beta \).
	This holds for all \( \beta \), so \( AX = 0 \).
	Now, since \( X^\transpose X \) is symmetric,
	\begin{align*}
		\Var{\beta^\star} &= \Var{CY} \\
		&= \Var{(A+(X^\transpose X)^{-1} X^\transpose)Y} \\
		&= A+(X^\transpose X)^{-1} X^\transpose \Var Y \qty[A+(X^\transpose X)^{-1} X^\transpose]^\transpose \\
		&= A+(X^\transpose X)^{-1} X^\transpose \sigma^2 I \qty[A+(X^\transpose X)^{-1} X^\transpose]^\transpose \\
		&= \sigma^2 \qty(A A^\transpose + (X^\transpose X)^{-1} + AX(X^\transpose X)^{-1} + (X^\transpose X)^{-1} X^\transpose A^\transpose) \\
		&= \sigma^2 AA^\transpose + \Var{\hat \beta} \\
		\Var{\beta^\star} - \Var{\hat\beta} &= \sigma^2 AA^\transpose
	\end{align*}
	Note that the outer product \( A A^\transpose \) is always positive semidefinite.
\end{proof}

\subsection{Fitted values and residuals}
\begin{definition}
	The \textit{fitted values} are \( \hat Y = X \hat \beta = X(X^\transpose X)^{-1} X^\transpose Y \), where \( P = X (X^\transpose X)^{-1} X^\transpose \) is the \textit{hat matrix}.
	The \textit{residuals} are \( Y - \hat Y = (I-P)Y \).
\end{definition}
\begin{proposition}
	\( P \) is the orthogonal projection onto the column space of the design matrix.
\end{proposition}
\begin{proof}
	If \( v \) is in the column space of \( X \), then \( v = Xb \) for some \( b \).
	Hence
	\[ Pv = X(X^\transpose X)^{-1} X^\transpose X b = Xb = v \]
	If \( w \) is in the orthogonal complement, then
	\[ Pw = X(X^\transpose X)^{-1} \underbrace{X^\transpose w}_{0} = 0 \]
\end{proof}
\begin{corollary}
	The fitted values are an orthogonal projection of the response variables to the column space of the design matrix.
	The residuals are orthogonal to the column space.
\end{corollary}

\subsection{Normal linear model}
The normal linear model is a linear model under the assumption that \( \varepsilon \sim N(0,\sigma^2 I) \), where \( \sigma^2 \) is unknown.
The parameters in the model are now \( (\beta, \sigma^2) \).
The likelihood function in the normal linear model is
\[ L(\beta, \sigma^2) = f_Y(y\mid \beta,\sigma^2) = (2\pi \sigma^2)^{-\frac{n}{2}} \exp{-\frac{1}{2\sigma^2} \sum_i (Y_i - x_i^\transpose \beta)^2} \]
The log-likelihood is
\[ \ell(\beta,\sigma^2) = \text{constant} - \frac{n}{2}\log \sigma^2 - \frac{1}{2\sigma^2} \norm{Y-X\beta}^2 \]
To maximise this as a function of \( \beta \) for any fixed \( \sigma^2 \), we must minimise the residual sum of squares \( S(\beta) = \norm{Y-X\beta}^2 \).
So \( \hat \beta = (X^\transpose X)^{-1} X^\transpose Y \) is the maximum likelihood estimator of \( \beta \).
