\subsection{Definition}
Let \( I \) be a finite or countable set.
All of our random variables will be defined on the same probability space \( (\Omega, \mathcal F, \mathbb P) \).
\begin{definition}
	A stochastic process \( (X_n)_{n \geq 0} \) is called a \textit{Markov chain} if \( \forall n \geq 0 \) and for \( x_1 \dots x_{n+1} \in I \),
	\[
		\prob{X_{n+1} = x_{n+1} \mid X_n = x_n, \dots,X_1 = x_1} = \prob{X_{n+1} = x_{n+1} \mid X_n = x_n}
	\]
\end{definition}
\noindent We can think of \( n \) as a discrete measure of time.
If \( \prob{X_{n+1} = y \mid X_n = x} \) for all \( x, y \) is independent of \( n \), then \( X \) is called time-homogeneous.
Otherwise, \( X \) is called time-inhomogeneous.
In this course, we only study time-homogeneous Markov chains.
If we consider time-homogeneous chains only, we may as well take \( n = 0 \) and we can write
\[
	P(x,y) = \prob{X_1 = y \mid X_0 = x};\quad \forall x,y \in I
\]
\begin{definition}
	A \textit{stochastic matrix} is a matrix where the sum of each row is equal to 1.
\end{definition}
\noindent We call \( P \) the \textit{transition matrix}.
It is a stochastic matrix:
\[
	\sum_{y \in I} P(x,y) = 1
\]
\begin{remark}
	The index set does not need to be \( \mathbb N \); it could alternatively be \( \qty{0,1,\dots,N} \) for \( N \in \mathbb N \).
\end{remark}
\noindent We say that \( X \) is \(\Markov{\lambda, P}\) if \( X_0 \) has distribution \(\lambda\), and P is the transition matrix.
Hence,
\begin{enumerate}[(i)]
	\item \( \prob{X_0 = x_0} = \lambda_{x_0} \)
	\item \( \prob{X_{n+1} = x_{n+1} \mid X_n = x_n, \dots, X_0 = x_0} = P(x_n, x_{n+1}) =: P_{x_n x_{n+1}} \)
\end{enumerate}
We usually draw a diagram of the transition matrix using a graph.
Directed edges between nodes are labelled with their transition probabilities.

\subsection{Sequence Definition}
\begin{theorem}
	The process \( X \) is \( \Markov{\lambda, P} \) if and only if \( \forall n \geq 0 \) and all \( x_0, \dots, x_n \in I \), we have
	\[
		\prob{X_0 = x_0, \dots, X_n = x_n} = \lambda_{x_0} P(x_0, x_1) P(x_1, x_2) \dots P(x_{n-1}, x_n)
	\]
\end{theorem}
\begin{proof}
	If \( X \) is Markov, then we have
	\begin{align*}
		\prob{X_0 = x_0, \dots, X_n = x_n} & = \prob{X_n = x_n \mid X_{n-1} = x_{n-1}, \dots, X_0 = x_0} \\
		&\cdot \prob{X_{n-1} = x_{n-1}, \dots, X_0 = x_0} \\
		                                   & = P(x_{n-1}, x_n) \prob{X_{n-1} = x_{n-1}, \dots, X_0 = x_0}                                           \\
		                                   & = P(x_{n-1}, x_n) \dots P(x_0, x_1) \lambda_{x_0}
	\end{align*}
	as required.
	Conversely, \( \prob{X_0 = x_0} = \lambda_{x_0} \) satisfies (i).
	The transition matrix is given by
	\[
		\prob{X_n = x_n \mid X_0 = x_0, \dots, X_{n-1} = x_{n-1}} = \frac{\lambda_{x_0} P(x_0, x_1) \dots P(x_{n-1}, x_n)}{\lambda_{x_0} P(x_0, x_1) \dots P(x_{n-2}, x_{n-1})} = P(x_{n-1}, x_n)
	\]
	which is exactly the Markov property as required.
\end{proof}

\subsection{Point Masses}
\begin{definition}
	For \( i \in I \), the \( \delta_i \)-mass at \( i \) is defined by
	\[
		\delta_{ij} = \mathbbm 1 (i = j)
	\]
	This is a probability measure that has probability 1 at \( i \) only.
\end{definition}

\subsection{Independence of Sequences}
Recall that discrete random variables \( (X_n) \) are considered independent if for all \( x_1, \dots, x_n \in I \), we have
\[
	\prob{X_1 = x_1, \dots, X_n = x_n} = \prob{X_1 = x_1}\dots\prob{X_n = x_n}
\]
A sequence \( (X_n) \) is independent if for all \( k \), \( i_1 < i_2 < \dots < i_n \) and for all \( x_1, \dots, x_k \), we have
\[
	\prob{X_{i_1} = x_1, \dots, X_{i_k} = x_k} = \prod_{j=1}^n \prob{X_{i_j} = x_j}
\]
Let \( X = (X_n), Y = (Y_n) \) be sequences of discrete random variables.
They are independent if for all \(k,m\), \( i_1 < \dots < i_k \), \( j_1 < \dots < j_m \),
\[
	\prob{X_1 = x_1, \dots, X_{i_k} = x_{i_k}, Y_{j_1} = y_{j_1}, \dots, Y_{j_m}} = \prob{X_1 = x_1, \dots, X_{i_k} = x_{i_k}} \prob{Y_{j_1} = y_{j_1}, \dots, Y_{j_m}}
\]
