\documentclass{article}

\input{../util.tex}

\title{Analysis}
\author{Cambridge University Mathematical Tripos: Part IA}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Limits and Convergence: Reviewing Numbers and Sets}
\subsection{Definition of Limit}
\begin{definition}
    We say that the sequence $a_n \to a$ as $n \to \infty$ if given $\varepsilon > 0$, $\exists N$ such that $\abs{a_n - a} < \varepsilon$ for all $n \geq N$. Note that this $N$ is actually a function of $\varepsilon$; we may need to choose a very large $N$ if the $\varepsilon$ provided is very small, for instance.
\end{definition}
\begin{definition}
    An increasing sequence is a sequence for which $a_n \leq a_{n+1}$, and a decreasing sequence is a sequence for which $a_n \geq a_{n+1}$. Such increasing and decreasing sequences are called monotone.
    A strictly increasing sequence or a strictly decreasing sequence simply strengthens the inequalities to not include the equality case.
\end{definition}

\subsection{Fundamental Axiom of the Real Numbers}
If we have some increasing sequence $a_n \in \mathbb R$, where $\exists A \in \mathbb R$ such that $\forall n \geq 1$, $a_n \leq A$, then $\exists a \in \mathbb R$ such that $a_n \to a$ as $n \to \infty$. This is also known as the `least upper bound' axiom or property. This axiom applies equivalently to decreasing sequences of real numbers bounded below. We can also rephrase the axiom to state that every non-empty set of real numbers that is bounded above has a supremum.
\begin{definition}
    We say that the supremum $\sup S$ of a non-empty, bounded above set $S$ is $K$ if
    \begin{enumerate}[(i)]
        \item $x \leq K$ for all $x \in S$
        \item given $\varepsilon > 0$, $\exists x \in S$ such that $x > K - \varepsilon$
    \end{enumerate}
\end{definition}
Note that the supremum (and hence the infimum) is unique.

\subsection{Properties of Limits}
\begin{lemma}
    The following properties about real sequences hold.
    \begin{enumerate}[(i)]
        \item The limit is unique. That is, if $a_n \to a$ and $a_n \to b$, then $a = b$.
        \item If $a_n \to a$ as $n \to \infty$ and $n_1 < n_2 < \dots$, then $a_{n_j} \to a$ as $j \to \infty$. In other words, subsequences converge to the same limit.
        \item If $a_n = c$ for all $n$, then $a_n \to c$ as $n \to \infty$.
        \item If $a_n \to a$ and $b_n \to b$, then $a_n + b_n \to a + b$.
        \item If $a_n \to a$ and $b_n \to b$, then $a_nb_n \to ab$.
        \item If $a_n \to a$, $a_n \neq 0$ for all $n$, and $a \neq 0$, then $\frac{1}{a_n} \to \frac{1}{a}$.
        \item If $a_n \to a$, and $a_n \leq A$ for all $n$, then $a \leq A$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    We prove the some of these statements here.
    \begin{enumerate}[(i)]
        \item Given $\varepsilon > 0$, $\exists n_1$ such that $\abs{a_n - a} < \varepsilon$ for all $n \geq n_1$, and $\exists n_2$ such that $\abs{a_n - b} < \varepsilon$ for all $n \geq n_2$. So let $N = \max(n_1, n_2)$, so both inequalities hold. Then for all $n \geq N$, using the triangle inequality, $\abs{a - b} \leq \abs{a_n - a} + \abs{a_n - b} < 2\varepsilon$. So $a=b$.
        \item Given $\varepsilon > 0$, $\exists N$ such that $\abs{a_n - a} < \varepsilon$ for all $n \geq N$. Since $n_j \geq j$ (by induction), $\abs{a_{n_j} - a} < \varepsilon$ for all $j \geq N$.
              \setcounter{enumi}{4}
        \item $\abs{a_nb_n - ab} \leq \abs{a_nb_n - a_nb} + \abs{a_nb - ab} = \abs{a_n}\abs{b_n - b} + \abs{b}\abs{a_n - a}$.

              If $a_n \to a$, then given $\varepsilon > 0$, $\exists N_1$ such that $\abs{a_n - a} < \varepsilon$ for all $n \geq N_1$. ($\ast$)

              If $b_n \to b$, then given $\varepsilon > 0$, $\exists N_2$ such that $\abs{b_n - b} < \varepsilon$ for all $n \geq N_2$.

              Using ($\ast$), if $n \geq N_1(1)$ (i.e. $\varepsilon = 1$), $\abs{a_n - a} < 1$, so $\abs{a_n} \leq \abs{a} + 1$.

              Therefore $\abs{a_n b_n - ab} \leq \varepsilon(\abs{a} + 1 + \abs{b})$ for all $n \geq N_3(\varepsilon) = \max\{ N_1(1), N_1(\varepsilon), N_2(\varepsilon) \}$.
    \end{enumerate}
\end{proof}

\subsection{Harmonic Series}
\begin{lemma}
    The sequence $\frac{1}{n}$ tends to zero as $n \to \infty$.
\end{lemma}
\begin{proof}
    We know that $\frac{1}{n}$ is a decreasing sequence, and it is bounded below by zero. Hence it converges to a limit $a$. We will prove now that $a = 0$. $\frac{1}{2n} = \frac{1}{2}\cdot \frac{1}{n}$, and by property (v) above, $\frac{1}{2n}$ tends to $\frac{1}{2}\cdot a$. But $\frac{1}{2n}$ is a subsequence of $\frac{1}{n}$, and so by property (ii) it converges to $a$. So by property (i), $\frac{1}{2} \cdot a = a$ hence $a=0$.
\end{proof}

\subsection{Limits in the Complex Plane}
\begin{remark}
    The definition of the limit of a sequence makes perfect sense for $a_n \in \mathbb C$.
\end{remark}
\begin{definition}
    $a_n \to a$ if given $\varepsilon > 0$, $\exists N$ such that $\forall n \geq N$, $\abs{a_n - a} < \varepsilon$.
\end{definition}
From this definition, it is easy to check that properties (i)--(vi) hold for complex numbers. However, property (vii) makes no sense in the world of the complex numbers since they do not have an ordering.

\section{More on Convergence}
\subsection{The Bolzano-Weierstrass Theorem}
\begin{theorem}
    If $x_n$ is a sequence of real numbers, and there exists some $k$ such that $\abs{x_n} \leq k$ for all $n$, then we can find $n_1 < n_2 < n_3 < n_4 < \dots$ and $x \in \mathbb R$ such that $x_{n_j} \to x$ as $j \to \infty$. In other words, any bounded sequence has a convergent subsequence.
\end{theorem}
\begin{remark}
    This theorem does not state anything about the uniqueness of such a subsequence; indeed, there could exist many subsequences that have possibly different limits. For example, $x_n = (-1)^n$ gives $x_{2n+1} \to -1$ and $x_{2n} \to 1$.
\end{remark}
\begin{proof}
    Let $[a_1, b_1]$ be the range of the sequence, i.e. $[-k, k]$. Then let the midpoint $c_1 = \frac{a_1 + b_1}{2}$. Consider the following alternatives:
    \begin{enumerate}
        \item $x_n \in [a_1, c]$ for infinitely many values of $n$.
        \item $x_n \in [c, b_1]$ for infinitely many values of $n$.
    \end{enumerate}
    Note that cases 1 and 2 could hold at the same time. If case 1 holds, we set $a_2 = a_1$ and $b_2 = c$. If case 1 fails, then case 2 must hold, so we can set $a_2 = c$ and $b_2 = b_1$. We have now constructed a subsequence whose range is half as large as the original sequence, and it contains infinitely many values of $x_n$.

    We can proceed inductively to construct sequences $a_n, b_n$ such that $x_m \in [a_n, b_n]$ for infinitely many values of $m$. This is known as a `bisection method'. By construction, $a_{n-1} \leq a_n \leq b_n \leq b_{n-1}$. Since we are dividing by two each time,
    \[ b_n - a_n = \frac{1}{2}(b_{n-1} - a_{n-1}) \tag{$\ast$} \]
    Note that $a_n$ is a bounded, increasing sequence; and $b_n$ is a bounded, decreasing sequence. By the Fundamental Axiom of the Real Numbers, $a_n$ and $b_n$ converge to limits $a \in [a_1, b_1]$ and $b \in [a_1, b_1]$. Using $(\ast)$, $b-a = \frac{b-a}{2} \implies b = a$.

    Since $x_m \in [a_n, b_n]$ for infinitely many values of $m$, having chosen $n_j$ such that $x_{n_j} \in [a_j, b_j]$, there is $n_{j+1} > n_j$ such that $x_{n_{j+1}} \in [a_{j+1}, b_{j+1}]$. Informally, this works because we have an unlimited supply of such $x$ values. Hence
    \[ a_j \leq x_{n_j} \leq b_j \]
    So this $x_{n_j} \to a$, so we have constructed a convergent subsequence.
\end{proof}

\subsection{Cauchy Sequences}
\begin{definition}
    A sequence $a_n$ is called a Cauchy sequence if given $\varepsilon > 0$ there exists $N > 0$ such that $\abs{a_n - a_m} < \varepsilon$ for all $n, m \geq N$. Informally, the terms of the sequence grow ever closer together such that there are infinitely many consecutive terms within a small region.
\end{definition}
\begin{lemma}
    If a sequence converges, it is a Cauchy sequence.
\end{lemma}
\begin{proof}
    If $a_n \to a$, given $\varepsilon > 0$ then $\exists N$ such that $\forall n \geq N, \abs{a_n - a} < \varepsilon$. Then take $m, n \geq N$, and we have
    \[ \abs{a_n - a_m} \leq \abs{a_n - a} + \abs{a_m - a} < 2\varepsilon \]
\end{proof}
\begin{theorem}
    Every Cauchy sequence converges.
\end{theorem}
\begin{proof}
    First, we note that if $a_n$ is a Cauchy sequence then it is bounded. Let us take $\varepsilon = 1$, so $N = N(1)$ in the Cauchy property. Then
    \[ \abs{a_n - a_m} < 1 \]
    for all $m, n \geq N(1)$. So by the triangle inequality,
    \[ \abs{a_m} \leq \abs{a_m - a_N} + \abs{a_N} < 1 + \abs{a_N} \]
    So the sequence after this point is bounded by $1 + \abs{a_N}$. The remaining terms in the sequence are only finitely many, so we can compute the maximum of all of those terms along with $1+\abs{a_N}$ to produce a bound $k$ for all $n$.

    By the Bolzano-Weierstrass Theorem, this sequence $a_n$ has a convergent subsequence $a_{n_j} \to a$. We want to prove that $a_n \to a$. Given $\varepsilon > 0$, there exists $j_0$ such that $\abs{a_{n_j} - a} < \varepsilon$ for all $j \geq j_0$. Also, $\exists N(\varepsilon)$ such that $\abs{a_m - a_n} < \varepsilon$ for all $m, n \geq N(\varepsilon)$. Combining these, we can take a $j$ such that $n_j \geq \max \{ N(\varepsilon), n_{j_0} \}$. Then, if $n \geq N(\varepsilon)$, using the triangle inequality,
    \[ \abs{a_n - a} \leq \abs{a_n - a_{n_j}} + \abs{a_{n_j} - a} < 2\varepsilon \]
\end{proof}
\noindent Therefore, on $\mathbb R$, a sequence is convergent if and only if it is a Cauchy sequence. This is sometimes referred to as the general principle of convergence, however this is a relatively old-fashioned name. This property is very useful, since we don't need to know what the limit actually is.

\subsection{Series}
Let $a_n$ be a real or complex sequence. We say that $\sum_{j=1}^\infty a_j$ converges to $s$ if the sequence of partial sums $s_N$ converges to $s$ as $N \to \infty$, i.e.
\[ s_N = \sum_{j=1}^N a_j \to s \]
If the sequence of partial sums does not converge, then we say that the series diverges. Note that any problem on series can be turned into a problem on sequences, by considering their partial sums.
\begin{lemma}
    \begin{enumerate}[(i)]
        \item If $\sum_{j=1}^\infty a_j$ and $\sum_{j=1}^\infty b_j$ converge, then so does $\sum_{j=1}^\infty (\lambda a_j + \mu b_j)$, where $\lambda, \mu \in \mathbb C$.
        \item Suppose $\exists N$ such that $a_j = b_j$ for all $j \geq N$. Then either $\sum_{j=1}^\infty a_j$ and $\sum_{j=1}^\infty b_j$ both converge, or they both diverge. In other words, the initial terms do not matter for considering convergence (but the sum will change).
    \end{enumerate}
\end{lemma}
\begin{proof}
    \begin{enumerate}[(i)]
        \item We have
              \begin{align*}
                  s_N            & = \sum_{j=1}^\infty (\lambda a_j + \mu b_j)                 \\
                                 & = \sum_{j=1}^\infty \lambda a_j + \sum_{j=1}^\infty \mu b_j \\
                                 & = \lambda c_N + \mu d_N                                     \\
                  \therefore s_N & \to \lambda c + \mu d
              \end{align*}
        \item For any $n \geq N$, we have
              \begin{align*}
                  s_N & = \sum_{j=1}^n a_j = \sum_{j=1}^{N-1} a_j + \sum_{j=n}^N a_j \\
                  d_N & = \sum_{j=1}^n b_j = \sum_{j=1}^{N-1} b_j + \sum_{j=n}^N b_j \\
              \end{align*}
              Taking the difference, we get
              \[ s_N - d_N = \sum_{j=1}^{N-1} a_j - \sum_{j=1}^{N-1} b_j \]
              which is finite. So $s_N$ converges if and only if $d_N$ also converges.
    \end{enumerate}
\end{proof}

\section{Convergence Tests}
\subsection{Geometric Series}
Let $a_n = x^{n-1}$, where $n \geq 1$. Then
\[ s_n = \sum_{j=1}^n a_j = 1 + x + x^2 + \dots + x^{n-1} \]
Then
\[ s_n = \begin{cases}
        \frac{1 - x^n}{1 - x} & \text{if } x \neq 1 \\
        n                     & \text{if } x = 1
    \end{cases} \]
This can be shown by observing that
\[ x s_n = x + x^2 + \dots + x^n = s_n - 1 + x^n \implies s_n(1-x) = 1-x^n \]
If $\abs{x} < 1$, then $x^n \to 0$ as $x \to \infty$. So $s_n \to \frac{1}{1-x}$. If $x > 1$, then $x^n \to \infty$ and so $s_n \to \infty$. If $x < -1$, $s_n$ oscillates. For completeness, if $x=-1$, $s_n$ oscillates between 0 and 1.

Note that the statement $s_n \to \infty$ means that given $a \in \mathbb R$, $\exists N$ such that $s_n > a$ for all $n \geq N$, and a similar statement holds for negative infinity (swapping the inequality). If $s_n$ does not converge or tend to $\pm \infty$, we say that $s_n$ oscillates.

Thus the geometric series converges if and only if $\abs{x} < 1$. Note that to prove that $x^n \to 0$ if $\abs{x} < 1$, we can consider the caase $0 < x < 1$ and write $1/x = 1 + \delta$ for some positive $\delta$. Then $x^n = \frac{1}{(1 + \delta)^n} \leq \frac{1}{1 + \delta n}$ from the binomial expansion, and this tends to zero as required.

\begin{lemma}
    If $\sum_{j=1}^\infty a_j$ converges, then $\lim_{j \to \infty} a_j = 0$.
\end{lemma}
\begin{proof}
    Given $s_n = \sum_{j=1}^n a_j$, we have $a_n = s_n - s_{n-1}$. If $s_n \to a$, then $a_n \to 0$ since $s_{n-1}$ also tends to $a$.
\end{proof}
\begin{remark}
    The converse is not true. For example, the harmonic series diverges, but the terms approach zero. Consider
    \begin{align*}
        s_{2n} & = s_n + \frac{1}{n+1} + \frac{1}{n+2} + \dots + \frac{1}{2n} \\
               & > s_n + \frac{1}{2n} + \frac{1}{2n} + \dots + \frac{1}{2n}   \\
               & = s_n + \frac{1}{2}
    \end{align*}
    So as $n \to \infty$, if the sequence is convergent then the sequences $s_n$ and $s_{2n}$ tend to the same limit, but they clearly do not.
\end{remark}

\subsection{Comparison Test}
In this section, we will let $a_n \in \mathbb R, a_n \geq 0$. In other words, all series contain only non-negative real terms.
\begin{theorem}
    Suppose $0 \leq b_n \leq a_n$ for all $n$. If $\sum_{j=1}^\infty a_j$ converges, then $\sum_{j=1}^\infty b_j$ converges.
\end{theorem}
\begin{proof}
    Let $s_N$ be the $N$th partial sum over the $a_n$, and let $d_N$ be the $N$th partial sum over the $d_N$. Since $b_n \leq a_n, d_N \leq s_N$. But $s_N \to s$, so $d_N \leq s_N \leq s$. So $d_N$ is an increasing sequence that is bounded above by $s$, so it converges.
\end{proof}
\noindent For example, let us analyse the behaviour of the sum of the sequence $\frac{1}{n^2}$. Note that
\[ \frac{1}{n^2} < \frac{1}{n(n-1)} = \frac{1}{n-1} - \frac{1}{n} \]
for $n \geq 2$. By the comparison test, it is sufficient to show that the series on the right hand side converges, in order to show that the original series converges.
\[ \sum_{j=2}^N a_j = 1 - \frac{1}{N} \to 1 \]
as required. So the original series tends to some value less than or equal to 2.

\subsection{Root Test or Cauchy's Test}
\begin{theorem}
    Suppose we have a sequence of non-negative terms $a_n$. Suppose that $a_n^{1/n} \to a$ as $n \to \infty$. Then if $a < 1$, the series $\sum a_n$ converges. If $a > 1$, the series $\sum a_n$ diverges.
\end{theorem}
\begin{remark}
    Nothing can be said if $a=1$. There is an example later of this fact.
\end{remark}
\begin{proof}
    If $a < 1$, let us choose an $r$ such that $a < r < 1$. By the definition of the limit, $\exists N$ such that $\forall n \geq N$, $a_n^{1/n} < r$. This implies that $a_n < r^n$. The geometric series $\sum r^n$ converges. By comparison, the series $a_n$ converges.

    If $a > 1$, for all $n \geq N$, $a_n^{1/n} > 1$ which implies $a_n > 1$, thus $\sum a_n$ diverges, since $a_n$ does not tend to zero.
\end{proof}

\subsection{Ratio Test or d'Alembert's Test}
\begin{theorem}
    Suppose $a_n > 0$, and $\frac{a_{n+1}}{a_n} \to \ell$. If $\ell < 1$, then the series $\sum a_n$ converges. If $\ell > 1$, then the series $\sum a_n$ diverges.
\end{theorem}
\begin{remark}
    Like before, no conclusion can be drawn if $\ell = 1$.
\end{remark}
\begin{proof}
    Suppose $\ell < 1$. We can choose $\ell < r < 1$, $\exists N$ such that $\forall n \geq N$, $\frac{a_{n+1}}{a_n} < r$. Therefore $a_n < r^{n-N} a_N$. Hence, $a_n < k r^n$ where $k$ is independent of $n$. Applying the comparison test, the series $\sum a_n$ must converge.

    If $\ell > 1$, we can choose $\ell > r > 1$. Then $\exists N$ such that $\forall n \geq N$, $\frac{a_{n+1}}{a_n} > r$. As before, $a_n > r^{n-N} a_N$. But the $r^{n-N}$ diverges, so the original series diverges.
\end{proof}

\section{More Convergence Tests}
\subsection{Examples of Ratio and Root Tests}
Consider $\sum_1^\infty \frac{n}{2^n}$. We have
\[ \frac{a_{n+1}}{a_n} = \frac{(n+1)/2^{n+1}}{n/2^n} \to \frac{1}{2} \]
So we have convergence, by the ratio test. Now, consider $\sum_1^\infty \frac{1}{n}$ and $\sum_1^\infty \frac{1}{n^2}$. In both cases, the ratio test gives limit 1. So the ratio test is inconclusive if the limit is 1. Since $n^{1/n} \to 1$, the root test is also inconclusive when the limit is 1. To check this limit, we can write
\[ n^{1/n} = 1 + \delta_n;\quad \delta_n > 0 \]
\[ n = (1 + \delta_n)^n > \frac{n(n-1)}{2}\delta_n^2 \]
using the binomial expansion.
\[ \implies \delta_n^2 < \frac{2}{n-1} \implies \delta_n \to 0 \]
The root test is a good candidate for series that contain powers of $n$, for example
\[ \sum_1^\infty \left[ \frac{n+1}{3n+5} \right]^n \]
In this instance, for example, we have convergence.

\subsection{Cauchy's Condensation Test}
\begin{theorem}
    Let $a_n$ be a decreasing sequence of positive terms. Then $\sum_1^\infty a_n$ converges if and only if $\sum_1^\infty 2^n a_{2^n}$ converges.
\end{theorem}
\begin{proof}
    First, note that if $a_n$ is decreasing, then
    \[ a_{2^k} \underset{(\ast)}{\leq} a_{2^{k-1} + i} \underset{(\dagger)}{\leq} a_{2^{k-1}};\quad 1 \leq i \leq 2^{k-1};\quad k \geq 1 \]
    Now let us assume that $\sum a_n$ converges to $A \in \mathbb R$. Then, by $(\ast)$,
    \begin{align*}
        2^{n-1} a_{2^n} & = a_{2^n} + a_{2^n} + \dots + a_{2^n}                \\
                        & \leq a_{2^{n-1}+1} + a_{2^{n-1}+2} + \dots + a_{2^n} \\
                        & = \sum_{m=2^{n-1}+1}^{2^n}a_m
    \end{align*}
    Thus,
    \[ \sum_{n=1}^N 2^{n-1}a_{2^n} \leq \sum_{n=1}^N \sum_{m=2^{n-1}+1}^{2^n} a_m = \sum_{n=2}^{2^N} a_m \]
    Therefore,
    \[ \sum_{n=1}^N 2^n a_{2^n} \leq 2 \sum_{n=2}^{2^N} a_m \leq 2(A-a_1) \]
    Thus $\sum_{n=1}^N 2^n a_{2^n}$ converges, since it is increasing and bounded above. For the converse, we will assume that $\sum 2^n a_{2^n}$ converges to $B$. Using $(\dagger)$,
    \begin{align*}
        \sum_{m=2^{n-1}}^{2^n} a_m & = a_{2^n} + a_{2^n} + \dots + a_{2^n}                \\
                                   & \leq a_{2^{n-1}} + a_{2^{n-1}} + \dots + a_{2^{n-1}} \\
                                   & = 2^{n-1}a_{2^{n-1}}
    \end{align*}
    So we have
    \[ \sum_{m=2}^{2^N} a_m = \sum_{n=1}^N \sum_{m=2^{n-1}+1}^{2^n} a_m \leq \sum_{n=1}^N 2^{n-1} a_{2^{n-1}} \leq \frac{1}{2} B \]
    Therefore, $\sum_{m=1}^N a_m$ is a bounded, increasing sequence and hence converges.
\end{proof}
\noindent Let us consider an example of this test. Consider the series definition of the Riemann zeta function
\[ \zeta(k) = \sum_{n=1}^\infty \frac{1}{n^k} \]
For what $k \in \mathbb R, k>0$ does this series converge? This is equivalent to asking if the following series converges.
\[ \sum_{n=1}^\infty 2^n \left[ \frac{1}{2^n} \right]^k = \sum_{n=1}^\infty \left( 2^{1-k} \right)^n \]
Hence it converges if and only if $2^{1-k} < 1 \iff k > 1$.

\subsection{Alternating Series}
An alternating series is a series where the sign on each term switches between positive and negative.
\begin{theorem}[Alternating Series Test]
    If $a_n$ decreases and tends to zero as $u \to \infty$, then the alternating series
    \[ \sum_1^\infty (-1)^{n+1} a_n \]
    converges.
\end{theorem}
\begin{proof}
    Let us consider the partial sum
    \[ s_n = a_1 - a_2 + a_3 - a_4 + \dots + (-1)^{n+1}a_n \]
    In particular,
    \[ s_{2n} = (a_1 - a_2) + (a_3 - a_4) + \dots + (a_{2n-1} - a_{2n}) \]
    Since the sequence is decreasing, each parenthesised block is positive. Then $s_{2n} \geq s_{2n-2}$. We can also write the partial sum as
    \[ s_{2n} = a_1 - (a_2 - a_3) - (a_4 - a_5) - \dots - (a_{2n-2} - a_{2n-1}) - a_{2n} \]
    Each parenthesised block here is negative. So $s_{2n} \leq a_1$. So $s_{2n}$ is increasing and bounded above, so it must converge. Now, note that
    \[ s_{2n+1} = s_{2n} + a_{2n+1} \to s_{2n} \]
    since $a_{2n+1} \to 0$. So $s_{2n+1}$ also converges, in fact to the same limit. Hence $s_n$ converges to this same limit.
\end{proof}

\section{Absolute Convergence}
\subsection{Absolute Convergence}
\begin{definition}
    Let $a_n \in \mathbb C$. Then if $\sum_{n=1}^\infty \abs{a_n}$ converges, then the series is called absolutely convergent.
\end{definition}
\begin{remark}
    Since $\abs{a_n} \geq 0$, we can use the previous tests to check for absolute convergence.
\end{remark}
\begin{theorem}
    Let $a_n \in \mathbb C$. If this series is absolutely convergent, it is convergent.
\end{theorem}
\begin{proof}
    Suppose first that $a_n$ is a sequence of real numbers. Then let
    \[ v_n = \begin{cases}
            a_n & \text{if } a_n \geq 0 \\
            0   & \text{if } a_n < 0
        \end{cases};\quad w_n = \begin{cases}
            0    & \text{if } a_n \geq 0 \\
            -a_n & \text{if } a_n < 0
        \end{cases} \]
    Hence,
    \[ v_n = \frac{\abs{a_n} + a_n}{2};\quad w_n = \frac{\abs{a_n} - a_n}{2} \]
    Clearly, $v_n, w_n \geq 0$, and $a_n = v_n - w_n$, and $\abs{a_n} = v_n + w_n$. If $\sum \abs{a_n}$ converges, then by comparison $\sum v_n$ and $\sum w_n$ also converge, and hence $\sum a_n$ converges. Now, let us consider the case where $a_n$ is complex. Then we can write $a_n = x_n + iy_n$ where $x_n, y_n$ are real sequences. Note that $\abs{x_n}, \abs{y_n} \leq \abs{a_n}$. So by comparison $x_n$ and $y_n$ converge, so $a_n$ converges.
\end{proof}
\noindent Here are some examples.
\begin{enumerate}
    \item The alternating harmonic series $\sum \frac{(-1)^n}{n}$ is convergent, but not absolutely convergent.
    \item $\sum \frac{z^n}{2^n}$ is absolutely convergent when $\abs{z} < \abs{2}$, because it reduces to a real geometric series. If $\abs{z} \geq 2$, then $\abs{a_n} \geq 1$, so we do not have absolute convergence.
\end{enumerate}

\subsection{Conditional Convergence and Rearrangement}
If the series is convergent but not absolutely convergent, it is called \textit{conditionally} convergent. The sum to which a series converges depends on the order in which the terms are added.
\begin{definition}
    Let $\sigma$ be a bijection of the positive integers to itself, then
    \[ a_n' = a_{\sigma(n)} \]
    is a rearrangement of $a_n$.
\end{definition}
\begin{theorem}
    If $\sum_1^\infty a_n$ is absolutely convergent, then every rearrangement of this series converges to the same value.
\end{theorem}
\begin{proof}
    First, let us consider the real case. Let $\sum a_n'$ be a rearrangement of $\sum a_n$. Let $s_n = \sum_1^n a_n$, and $t_n = \sum_1^n a_n'$. Let $s_n$ converge to $s$. Suppose first that $a_n \geq 0$. Then given any $n \in \mathbb N$, we can find some $q \in \mathbb N$ such that $s_q$ contains every term of $t_n$. Since the $a_n \geq 0$,
    \[ t_n \leq s_q \leq s \]
    As $n \to \infty$, the $t_n$ is an increasing sequence bounded above, so it must tend to a limit $t$, where $t \leq s$. Note, however, that this argument is symmetric; we can equally derive that $s \leq t$. Therefore $s = t$.

    Now, let us drop the condition that $a_n \geq 0$. We can now consider $v_n, w_n$ from above:
    \[ v_n = \frac{\abs{a_n} + a_n}{2};\quad w_n = \frac{\abs{a_n} - a_n}{2} \]
    Since $\sum\abs{a_n}$ converges, both $\sum v_n, \sum w_n$ converge. Since all $v_n, w_n \geq 0$, we can deduce that $\sum v_n = \sum v_n'$ and $\sum w_n' = \sum w_n$. The claim follows since $a_n = v_n - w_n$.

    For the case $a_n \in \mathbb C$, we can write $a_n = x_n + iy_n$, noting that $\abs{x_n}, \abs{y_n} \geq \abs{a_n}$. By comparison, the series $\sum x_n, \sum y_n$ are absolutely convergent, and by the previous case, $\sum x_n = \sum x_n'$ and $\sum y_n' = \sum y_n'$. Since $a_n' = x_n' + y_n'$, $\sum a_n = \sum a_n'$ as required.
\end{proof}

\section{Continuity}
\subsection{Definitions}
Let $E \subseteq \mathbb C$ be a non-empty set, and $f \colon E \to \mathbb C$ be any function, and let $a \in E$. Certainly, this includes the case in which $f$ is a real-valued function and $E \subseteq \mathbb R$.
\begin{definition}
    $f$ is continuous at $a$ if for every sequence $z_n \in E$ that converges to $a$, we have $f(z_n) \to f(a)$.
\end{definition}
\noindent We can use an alternative definition:
\begin{definition}[$\varepsilon$-$\delta$ definition]
    $f$ is continuous at $a$ if given $\varepsilon > 0$, $\exists \delta > 0$ such that for every $z \in E$, if $\abs{z - a} < \delta$, then $\abs{f(z) - f(a)} < \varepsilon$.
\end{definition}
\noindent We will immediately prove that both definitions are equivalent. First, let us prove that the $\varepsilon$-$\delta$ definition implies the first definition.
\begin{proof}
    We know that given $\varepsilon > 0, \exists \delta > 0$ such that for all $z \in E$, $\abs{z - a} < \delta$ implies $\abs{f(z) - f(a)} < \varepsilon$. Let $z_n \to a$, then by the definition of the limit of the sequence then there exists $n_0$ such that for all $n \geq n_0$ we have $\abs{z_n - a} < \delta$. But this implies that $\abs{f(z_n) - f(a)} < \varepsilon$, i.e. $f(z_n) \to f(a)$.
\end{proof}
\noindent We now prove the converse, that the first definition implies the second.
\begin{proof}
    We know that for every sequence $z_n \in E$ that converges to $a$, $f(z_n) \to f(a)$. Suppose $f$ is not continuous at $a$, according to the $\varepsilon$-$\delta$ definition. Then there exists some $\varepsilon$ such that for all $\delta > 0$, there exists $z \in E$ such that $\abs{z - a} < \delta$ but $\abs{f(z) - f(a)} \geq \varepsilon$. So, let us construct a sequence of $\delta$ values to substitute into this definition. Let $\delta = 1/n$. Then the $z_n$ given by this $\delta$ is such that $\abs{z_n - a} < 1/n$ and $\abs{f(z_n) - f(a)} \geq \varepsilon$. Clearly, $z_n \to a$, but $f(z_n)$ does not tend to $f(a)$ because the difference between the two is always greater than $\varepsilon$. This is a contradiction, since we assumed that $f$ is continuous by the first definition. So $f$ is continuous by the $\varepsilon$-$\delta$ definition.
\end{proof}

\subsection{Making Continuous Functions}
We can create new continuous functions from old ones by manipulating them in a number of ways.
\begin{proposition}
    Let $g, f \colon E \to \mathbb C$ be continous functions at a point $a \in E$. Then all of the functions
    \begin{itemize}
        \item $f(z) + g(z)$
        \item $f(z)g(z)$
        \item $\lambda f(z)$ for some constant $\lambda$
    \end{itemize}
    are all continuous. In addition, if $f(z) \neq 0$ everywhere in $E$, then $\frac{1}{f}$ is a continuous function at $a$.
\end{proposition}
\begin{proof}
    Using the first definition, this is obvious using the fact that limits of sequences behave analogously.
\end{proof}
\noindent Trivially, the function $f(z) = z$ is continuous. From this, we can derive that every polynomial is continuous at every point in $\mathbb C$. Note that we say that $f$ is continuous on the entire set $E$ if it is continuous at every point $a \in E$.

\subsection{Composition of Continuous Functions}
\begin{theorem}
    Let $f \colon A \to \mathbb C$ and $g \colon B \to \mathbb C$ where $A, B \subseteq \mathbb C$ be two functions that can be composed, i.e. $f(A) \subseteq B$. If $f$ is continuous at $a \in A$ and $g$ is continuous at $f(a) \in B$, then $g \circ f \colon A \to \mathbb C$ is continuous at $a$.
\end{theorem}
\begin{proof}
    Take any sequence $z_n \to a$. By assumption, $f(z_n) \to f(a)$. Now, let us define a new sequence $w_n = f(z_n)$. Then $w_n \in B$ and $w_n \to f(a)$. Thus, $g(f(z_n)) = g(w_n) \to g(f(a))$ by continuity, as required.
\end{proof}
\noindent Consider the function $f\colon \mathbb R \to \mathbb R$ defined by
\[ f(x) = \begin{cases}
        \sin\left( \frac{1}{x} \right) & x \neq 0 \\
        0                              & x = 0
    \end{cases} \]
This is assuming the knowledge of $\sin(x)$ being a continuous function $\mathbb R\to \mathbb R$, which we will prove later. So $f(x)$ is certainly continuous at every point on $\mathbb R$ excluding 0, since it is the composition of two continuous functions. We can prove it is discontinuous at $x=0$ by providing a sequence, for example
\[ \frac{1}{x_n} = \left(2n + \frac{1}{2}\right)\pi \]
Then $x_n \to 0$, and $f(x_n) = 1$. But $f(0) \neq 1$, so it is discontinuous. Let us modify the example as follows.
\[ f(x) = \begin{cases}
        x\sin\left( \frac{1}{x} \right) & x \neq 0 \\
        0                               & x = 0
    \end{cases} \]
We can prove that this sequence is continuous at 0. For an arbitrary sequence $x_n \to 0$, then $\abs{f(x_n)} \leq \abs{x_n}$ because $\abs{\sin x} \leq 1$. So $f(x_n)$ is bounded by $x_n$, which tends to zero, so $f(x_n)$ tends to zero as required. Now for a final example, let
\[ f(x) = \begin{cases}
        1 & x \in \mathbb Q    \\
        0 & x \notin \mathbb Q
    \end{cases} \]
This is discontinuous at every point. If $x \in \mathbb Q$, take a sequence $x_n \to x$ with all $x_n$ irrational, then $f(x_n) = 0$ but $f(x) = 1$. Similarly, if $x \notin \mathbb Q$, take a sequence $x_n \to x$ with all $x_n$ rational, then $f(x_n) = 1$ but $f(x) = 0$.

\section{Limit of a Function}
\subsection{Definition}
Let $f \colon E \subseteq \mathbb C \to \mathbb C$. We would like to define what is meant by $\lim_{z \to a} f(z)$, even when $a \notin E$. Further, if we have a set with an isolated point, for example $E = \{ 0 \} \cup [1, 2]$, it does not make sense to talk about limits tending to 0 since there are no points in $E$ close to 0.
\begin{definition}
    Let $E \subseteq \mathbb C,\, a \in \mathbb C$. $a$ is a limit point of $E$ if for any $\delta > 0$, there exists $z \in E$ such that $0 < \abs{z - a} < \delta$.
\end{definition}
\noindent First, note that $a$ is a limit point if and only if there exists a sequence $z_n \in E$ such that $z_n \to a$, but notably $z_n \neq a$ for all $n$.
\begin{definition}
    Let $f \colon E \subseteq \mathbb C \to \mathbb C$, and let $a \in \mathbb C$ be a limit point of $E$. We say that $f \to \ell$ as $z \to a$, if given $\varepsilon > 0$ there exists $\delta > 0$ such that whenever $0 < \abs{z - a} < \delta$ and $z \in E$, $\abs{f(z) - \ell} < \varepsilon$. Equivalently, $f(z_n) \to \ell$ for every sequence $z_n \in E$, such that $z_n \to a$ but $z_n \neq a$.
\end{definition}
\noindent Therefore if $a \in E$ is a limit point, then $\lim_{z \to a} f(z) = f(a)$ if and only if $f$ is continuous at $a$. If $a \in E$ is isolated (not a limit point) then $f$ at $a$ is trivially continuous, since there are no points near $a$ but $a$ itself.

\subsection{Properties}
The limit of a function has very similar properties when compared to the limit of a sequence.
\begin{enumerate}
    \item It is unique. $f(z) \to A$, $f(z) \to B$ implies $A = B$.
    \item $f(z) \to A$, $g(z) \to B$ implies
          \begin{enumerate}
              \item $f(z) + g(z) \to A + B$
              \item $f(z)\cdot g(z) \to AB$
              \item If $B \neq 0$, $\frac{f(z)}{g(z)} \to \frac{A}{B}$
          \end{enumerate}
\end{enumerate}

\subsection{Intermediate Value Theorem}
\begin{theorem}
    Let $f \colon [a, b] \to \mathbb R$ be a continuous function where $f(a) \neq f(b)$. Then $f$ takes all values in the interval $[f(a), f(b)]$.
\end{theorem}
\begin{proof}
    Without loss of generality, let us assume $f(a) < f(b)$. Let us take an $\eta$ such that $f(a) < \eta < f(b)$. We want to prove that there exists some value $c \in [a, b]$ with $f(c) = \eta$. Let $s$ be the set of points defined by
    \[ s = \{ x \in [a, b] \colon f(x) < \eta \} \]
    $a \in s$ therefore the set $s$ is non-empty. The set is also clearly bounded above by $b$. So there is a supremum of this set, say $\sup s = c$ where $c \leq b$. This point $c$ can be visualised as the last point at which $y=f(x)$ crosses the line $y=c$. We intend to show that the function at this rightmost point is $\eta$.

    By the definition of the supremum, given $n$ there exists $x_n \in s$ such that $c - \frac{1}{n} < x_n \leq c$. So the sequence $x_n$ tends to $c$. We know that $f(x_n) < \eta$ for all $x_n$ by definition of the set $s$. By the continuity of $f$, $f(x_n) \to f(c)$. Thus,
    \begin{equation}
        f(c) \leq \eta \tag{$\ast$}
    \end{equation}
    Now, let us consider the fact that $c \neq b$. If $c = b$, then $f(b) \leq \eta$ which is a contradiction since $\eta < f(b)$. So for a large $n$, we can ensure that $c + \frac{1}{n} \in [a,b]$. So by continuity of the function, $f(c + \frac{1}{n}) \to f(c)$. But since $c + \frac{1}{n} > c$, then necessarily $f(c + \frac{1}{n}) \geq \eta$ because $c$ is the supremum of $s$. Thus
    \[ f(c) \geq \eta \]
    Combining this with $(\ast)$ we get $f(c) = \eta$.
\end{proof}
\noindent This theorem is very useful for finding zeroes and fixed points. For example, we can prove the existence of the $N$th root of a positive real number $y$. Let
\[ f(x) = x^N \]
Then $f$ is certainly continuous on the interval $[0, 1+y]$, since
\[ 0 = f(0) < y < (1+y)^N = f(1 + y) \]
By the intermediate value theorem, there exists a point $c \in (0, 1+y)$ such that $f(c) = c^N = y$. So $c$ is a positive $N$th root of $y$. We can also prove the uniqueness of such a point. Suppose $d^N = y$ with $d>0$ and $d \neq c$. Without loss of generality, suppose $d < c$. Then $d^N < c^N$ so $d^N \neq y$, which is a contradiction.

\section{Bounds and Inverses}
\subsection{Bounds of a Continuous Function}
\begin{theorem}
    Let $f \colon [a, b] \to \mathbb R$ be continuous. Then the function is bounded, i.e. there exists $k \in \mathbb R$ such that $\abs{f(x)} \leq k$ for every point $x \in [a, b]$.
\end{theorem}
\begin{proof}
    Suppose that such a function $f$ is not bounded. Then in particular, given any integer $n \geq 1$, there exists $x_n \in [a, b]$ such that $\abs{f(x_n)} > n$. By the Bolzano-Weierstrass theorem, the sequence $x_n$, which is bounded by $a \leq x_n \leq b$, has a convergent subsequence $x_{n_j} \to x$, such that $x \in [a, b]$. Then by continuity of $f$, $f(x_{n_j}) \to f(x)$. But $\abs{f(x_{n_j})} > n_j \to \infty$. This is a contradiction.
\end{proof}
\noindent We can actually improve this statement.
\begin{theorem}
    Suppose $f \colon [a, b] \to \mathbb R$ is a continuous function. Then there exist $x_1, x_2 \in [a, b]$ such that
    \[ f(x_1) \leq f(x) \leq f(x_2) \]
    for all $x \in [a, b]$. In other words, a continuous function on a closed bounded interval is bounded and attains its bounds.
\end{theorem}
\begin{proof}
    Let $A = \{ f(x) \colon x \in [a, b] \}$ be the image of $[a, b]$ under $f$. By the above theorem, $A$ is bounded. It is also non-empty, hence it has a supremum $M = \sup A$ (and analogously an infimum $\inf A$, whose proof is almost identical). Then by the definition of the supremum, given an integer $n \geq 1$ there exists $x_n \in [a, b]$ such that $M - \frac{1}{n} < f(x_n) \leq M$. By the Bolzano-Weierstrass theorem, there exists a convergent subsequence $x_{n_j} \to x \in [a, b]$. Since $f(x_{n_j}) \to M$, then by continuity, $f(x) = M$.
\end{proof}
\noindent Here is an alternative proof of the same theorem.
\begin{proof}
    As before, let $A$ be the image of $f$, and $M$ be the supremum of $A$. Suppose there is no $x_2 \in [a, b]$ such that $f(x_2) = M$. Then let $g(x) = \frac{1}{M - f(x)}$ for $x \in [a, b]$. Since there exists no $x$ such that $M = f(x)$, $g(x)$ is continuous since we are never dividing by zero. So $g$ is bounded. So by the previous theorem, there is some $k > 0$ such that $g(x) \leq k$ for all $x \in [a, b]$. This means that $f(x) \leq M - \frac{1}{k}$ on $[a, b]$ for this $k$, but this cannot happen since $M$ is the supremum.
\end{proof}
\noindent Note that these theorems are certainly false if the interval is not closed: consider the counterexample $(0, 1]$ and the function $x \mapsto x^{-1}$.

\subsection{Inverse Functions}
\begin{definition}
    $f$ is increasing for $x \in [a, b]$ if $f(x_1) \leq f(x_2)$ for all $x_1 \leq x_2 \in [a, b]$. If $f(x_1) < f(x_2)$ then the function is strictly increasing. A function may be called decreasing or strictly decreasing analogously.
\end{definition}
\begin{definition}
    A function $f$ is called monotone if it is either increasing or decreasing.
\end{definition}
\begin{theorem}
    Let $f \colon [a, b] \to \mathbb R$ be continuous and strictly increasing for $x \in [a, b]$. Let $c = f(a)$, $d = f(b)$. Then $f \colon [a, b] \to [c, d]$ is bijective, and the inverse $g := f^{-1} \colon [c, d] \to [a, b]$ is continuous and strictly increasing.
\end{theorem}
\noindent A similar theorem holds for strictly decreasing functions.
\begin{proof}
    Let $c < k < d$. From the intermediate value theorem, there exists $h$ such that $f(h) = k$. This $h$ must be unique since the function is strictly increasing. Then we can define $g(k) = h$, giving us an inverse $g \colon [c, d] \to [a, b]$ for $f$.

    First, note that $g$ is strictly increasing. Indeed, for $y_1 < y_2$ then $y_1 = f(x_1)$, $y_2 = f(x_2)$. This means that if $x_2 \geq x_1$, then since $f$ is increasing $y_2 \leq y_1$ which is a contradiction.

    Now, note that $g$ is continuous. Indeed, given $\varepsilon > 0$, we can let $k_1 = f(h - \varepsilon)$ and $k_2 = f(h + \varepsilon)$. If $f$ is strictly increasing, then $k_1 < k < k_2$. Then $h - \varepsilon < g(y) < h + \varepsilon$. So let $\delta = \min(k_2 - k, k - k_1)$ where $k \in (c, d)$, establishing continuity as claimed.
\end{proof}

\end{document}