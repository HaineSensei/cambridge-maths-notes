\documentclass{article}

\input{../util.tex}

\title{Analysis}
\author{Cambridge University Mathematical Tripos: Part IA}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Limits and Convergence: Reviewing Numbers and Sets}
\subsection{Definition of Limit}
\begin{definition}
    We say that the sequence $a_n \to a$ as $n \to \infty$ if given $\varepsilon > 0$, $\exists N$ such that $\abs{a_n - a} < \varepsilon$ for all $n \geq N$. Note that this $N$ is actually a function of $\varepsilon$; we may need to choose a very large $N$ if the $\varepsilon$ provided is very small, for instance.
\end{definition}
\begin{definition}
    An increasing sequence is a sequence for which $a_n \leq a_{n+1}$, and a decreasing sequence is a sequence for which $a_n \geq a_{n+1}$. Such increasing and decreasing sequences are called monotone.
    A strictly increasing sequence or a strictly decreasing sequence simply strengthens the inequalities to not include the equality case.
\end{definition}

\subsection{Fundamental Axiom of the Real Numbers}
If we have some increasing sequence $a_n \in \mathbb R$, where $\exists A \in \mathbb R$ such that $\forall n \geq 1$, $a_n \leq A$, then $\exists a \in \mathbb R$ such that $a_n \to a$ as $n \to \infty$. This is also known as the `least upper bound' axiom or property. This axiom applies equivalently to decreasing sequences of real numbers bounded below. We can also rephrase the axiom to state that every non-empty set of real numbers that is bounded above has a supremum.
\begin{definition}
    We say that the supremum $\sup S$ of a non-empty, bounded above set $S$ is $K$ if
    \begin{enumerate}[(i)]
        \item $x \leq K$ for all $x \in S$
        \item given $\varepsilon > 0$, $\exists x \in S$ such that $x > K - \varepsilon$
    \end{enumerate}
\end{definition}
Note that the supremum (and hence the infimum) is unique.

\subsection{Properties of Limits}
\begin{lemma}
    The following properties about real sequences hold.
    \begin{enumerate}[(i)]
        \item The limit is unique. That is, if $a_n \to a$ and $a_n \to b$, then $a = b$.
        \item If $a_n \to a$ as $n \to \infty$ and $n_1 < n_2 < \dots$, then $a_{n_j} \to a$ as $j \to \infty$. In other words, subsequences converge to the same limit.
        \item If $a_n = c$ for all $n$, then $a_n \to c$ as $n \to \infty$.
        \item If $a_n \to a$ and $b_n \to b$, then $a_n + b_n \to a + b$.
        \item If $a_n \to a$ and $b_n \to b$, then $a_nb_n \to ab$.
        \item If $a_n \to a$, $a_n \neq 0$ for all $n$, and $a \neq 0$, then $\frac{1}{a_n} \to \frac{1}{a}$.
        \item If $a_n \to a$, and $a_n \leq A$ for all $n$, then $a \leq A$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    We prove the some of these statements here.
    \begin{enumerate}[(i)]
        \item Given $\varepsilon > 0$, $\exists n_1$ such that $\abs{a_n - a} < \varepsilon$ for all $n \geq n_1$, and $\exists n_2$ such that $\abs{a_n - b} < \varepsilon$ for all $n \geq n_2$. So let $N = \max(n_1, n_2)$, so both inequalities hold. Then for all $n \geq N$, using the triangle inequality, $\abs{a - b} \leq \abs{a_n - a} + \abs{a_n - b} < 2\varepsilon$. So $a=b$.
        \item Given $\varepsilon > 0$, $\exists N$ such that $\abs{a_n - a} < \varepsilon$ for all $n \geq N$. Since $n_j \geq j$ (by induction), $\abs{a_{n_j} - a} < \varepsilon$ for all $j \geq N$.
              \setcounter{enumi}{4}
        \item $\abs{a_nb_n - ab} \leq \abs{a_nb_n - a_nb} + \abs{a_nb - ab} = \abs{a_n}\abs{b_n - b} + \abs{b}\abs{a_n - a}$.

              If $a_n \to a$, then given $\varepsilon > 0$, $\exists N_1$ such that $\abs{a_n - a} < \varepsilon$ for all $n \geq N_1$. ($\ast$)

              If $b_n \to b$, then given $\varepsilon > 0$, $\exists N_2$ such that $\abs{b_n - b} < \varepsilon$ for all $n \geq N_2$.

              Using ($\ast$), if $n \geq N_1(1)$ (i.e. $\varepsilon = 1$), $\abs{a_n - a} < 1$, so $\abs{a_n} \leq \abs{a} + 1$.

              Therefore $\abs{a_n b_n - ab} \leq \varepsilon(\abs{a} + 1 + \abs{b})$ for all $n \geq N_3(\varepsilon) = \max\{ N_1(1), N_1(\varepsilon), N_2(\varepsilon) \}$.
    \end{enumerate}
\end{proof}

\subsection{Harmonic Series}
\begin{lemma}
    The sequence $\frac{1}{n}$ tends to zero as $n \to \infty$.
\end{lemma}
\begin{proof}
    We know that $\frac{1}{n}$ is a decreasing sequence, and it is bounded below by zero. Hence it converges to a limit $a$. We will prove now that $a = 0$. $\frac{1}{2n} = \frac{1}{2}\cdot \frac{1}{n}$, and by property (v) above, $\frac{1}{2n}$ tends to $\frac{1}{2}\cdot a$. But $\frac{1}{2n}$ is a subsequence of $\frac{1}{n}$, and so by property (ii) it converges to $a$. So by property (i), $\frac{1}{2} \cdot a = a$ hence $a=0$.
\end{proof}

\subsection{Limits in the Complex Plane}
\begin{remark}
    The definition of the limit of a sequence makes perfect sense for $a_n \in \mathbb C$.
\end{remark}
\begin{definition}
    $a_n \to a$ if given $\varepsilon > 0$, $\exists N$ such that $\forall n \geq N$, $\abs{a_n - a} < \varepsilon$.
\end{definition}
From this definition, it is easy to check that properties (i)--(vi) hold for complex numbers. However, property (vii) makes no sense in the world of the complex numbers since they do not have an ordering.

\section{More on Convergence}
\subsection{The Bolzano-Weierstrass Theorem}
\begin{theorem}
    If $x_n$ is a sequence of real numbers, and there exists some $k$ such that $\abs{x_n} \leq k$ for all $n$, then we can find $n_1 < n_2 < n_3 < n_4 < \dots$ and $x \in \mathbb R$ such that $x_{n_j} \to x$ as $j \to \infty$. In other words, any bounded sequence has a convergent subsequence.
\end{theorem}
\begin{remark}
    This theorem does not state anything about the uniqueness of such a subsequence; indeed, there could exist many subsequences that have possibly different limits. For example, $x_n = (-1)^n$ gives $x_{2n+1} \to -1$ and $x_{2n} \to 1$.
\end{remark}
\begin{proof}
    Let $[a_1, b_1]$ be the range of the sequence, i.e. $[-k, k]$. Then let the midpoint $c_1 = \frac{a_1 + b_1}{2}$. Consider the following alternatives:
    \begin{enumerate}
        \item $x_n \in [a_1, c]$ for infinitely many values of $n$.
        \item $x_n \in [c, b_1]$ for infinitely many values of $n$.
    \end{enumerate}
    Note that cases 1 and 2 could hold at the same time. If case 1 holds, we set $a_2 = a_1$ and $b_2 = c$. If case 1 fails, then case 2 must hold, so we can set $a_2 = c$ and $b_2 = b_1$. We have now constructed a subsequence whose range is half as large as the original sequence, and it contains infinitely many values of $x_n$.

    We can proceed inductively to construct sequences $a_n, b_n$ such that $x_m \in [a_n, b_n]$ for infinitely many values of $m$. This is known as a `bisection method'. By construction, $a_{n-1} \leq a_n \leq b_n \leq b_{n-1}$. Since we are dividing by two each time,
    \[ b_n - a_n = \frac{1}{2}(b_{n-1} - a_{n-1}) \tag{$\ast$} \]
    Note that $a_n$ is a bounded, increasing sequence; and $b_n$ is a bounded, decreasing sequence. By the Fundamental Axiom of the Real Numbers, $a_n$ and $b_n$ converge to limits $a \in [a_1, b_1]$ and $b \in [a_1, b_1]$. Using $(\ast)$, $b-a = \frac{b-a}{2} \implies b = a$.

    Since $x_m \in [a_n, b_n]$ for infinitely many values of $m$, having chosen $n_j$ such that $x_{n_j} \in [a_j, b_j]$, there is $n_{j+1} > n_j$ such that $x_{n_{j+1}} \in [a_{j+1}, b_{j+1}]$. Informally, this works because we have an unlimited supply of such $x$ values. Hence
    \[ a_j \leq x_{n_j} \leq b_j \]
    So this $x_{n_j} \to a$, so we have constructed a convergent subsequence.
\end{proof}

\subsection{Cauchy Sequences}
\begin{definition}
    A sequence $a_n$ is called a Cauchy sequence if given $\varepsilon > 0$ there exists $N > 0$ such that $\abs{a_n - a_m} < \varepsilon$ for all $n, m \geq N$. Informally, the terms of the sequence grow ever closer together such that there are infinitely many consecutive terms within a small region.
\end{definition}
\begin{lemma}
    If a sequence converges, it is a Cauchy sequence.
\end{lemma}
\begin{proof}
    If $a_n \to a$, given $\varepsilon > 0$ then $\exists N$ such that $\forall n \geq N, \abs{a_n - a} < \varepsilon$. Then take $m, n \geq N$, and we have
    \[ \abs{a_n - a_m} \leq \abs{a_n - a} + \abs{a_m - a} < 2\varepsilon \]
\end{proof}
\begin{theorem}
    Every Cauchy sequence converges.
\end{theorem}
\begin{proof}
    First, we note that if $a_n$ is a Cauchy sequence then it is bounded. Let us take $\varepsilon = 1$, so $N = N(1)$ in the Cauchy property. Then
    \[ \abs{a_n - a_m} < 1 \]
    for all $m, n \geq N(1)$. So by the triangle inequality,
    \[ \abs{a_m} \leq \abs{a_m - a_N} + \abs{a_N} < 1 + \abs{a_N} \]
    So the sequence after this point is bounded by $1 + \abs{a_N}$. The remaining terms in the sequence are only finitely many, so we can compute the maximum of all of those terms along with $1+\abs{a_N}$ to produce a bound $k$ for all $n$.

    By the Bolzano-Weierstrass Theorem, this sequence $a_n$ has a convergent subsequence $a_{n_j} \to a$. We want to prove that $a_n \to a$. Given $\varepsilon > 0$, there exists $j_0$ such that $\abs{a_{n_j} - a} < \varepsilon$ for all $j \geq j_0$. Also, $\exists N(\varepsilon)$ such that $\abs{a_m - a_n} < \varepsilon$ for all $m, n \geq N(\varepsilon)$. Combining these, we can take a $j$ such that $n_j \geq \max \{ N(\varepsilon), n_{j_0} \}$. Then, if $n \geq N(\varepsilon)$, using the triangle inequality,
    \[ \abs{a_n - a} \leq \abs{a_n - a_{n_j}} + \abs{a_{n_j} - a} < 2\varepsilon \]
\end{proof}
\noindent Therefore, on $\mathbb R$, a sequence is convergent if and only if it is a Cauchy sequence. This is sometimes referred to as the general principle of convergence, however this is a relatively old-fashioned name. This property is very useful, since we don't need to know what the limit actually is.

\subsection{Series}
Let $a_n$ be a real or complex sequence. We say that $\sum_{j=1}^\infty a_j$ converges to $s$ if the sequence of partial sums $s_N$ converges to $s$ as $N \to \infty$, i.e.
\[ s_N = \sum_{j=1}^N a_j \to s \]
If the sequence of partial sums does not converge, then we say that the series diverges. Note that any problem on series can be turned into a problem on sequences, by considering their partial sums.
\begin{lemma}
    \begin{enumerate}[(i)]
        \item If $\sum_{j=1}^\infty a_j$ and $\sum_{j=1}^\infty b_j$ converge, then so does $\sum_{j=1}^\infty (\lambda a_j + \mu b_j)$, where $\lambda, \mu \in \mathbb C$.
        \item Suppose $\exists N$ such that $a_j = b_j$ for all $j \geq N$. Then either $\sum_{j=1}^\infty a_j$ and $\sum_{j=1}^\infty b_j$ both converge, or they both diverge. In other words, the initial terms do not matter for considering convergence (but the sum will change).
    \end{enumerate}
\end{lemma}
\begin{proof}
    \begin{enumerate}[(i)]
        \item We have
              \begin{align*}
                  s_N            & = \sum_{j=1}^\infty (\lambda a_j + \mu b_j)                 \\
                                 & = \sum_{j=1}^\infty \lambda a_j + \sum_{j=1}^\infty \mu b_j \\
                                 & = \lambda c_N + \mu d_N                                     \\
                  \therefore s_N & \to \lambda c + \mu d
              \end{align*}
        \item For any $n \geq N$, we have
              \begin{align*}
                  s_N & = \sum_{j=1}^n a_j = \sum_{j=1}^{N-1} a_j + \sum_{j=n}^N a_j \\
                  d_N & = \sum_{j=1}^n b_j = \sum_{j=1}^{N-1} b_j + \sum_{j=n}^N b_j \\
              \end{align*}
              Taking the difference, we get
              \[ s_N - d_N = \sum_{j=1}^{N-1} a_j - \sum_{j=1}^{N-1} b_j \]
              which is finite. So $s_N$ converges if and only if $d_N$ also converges.
    \end{enumerate}
\end{proof}

\section{Convergence Tests}
\subsection{Geometric Series}
Let $a_n = x^{n-1}$, where $n \geq 1$. Then
\[ s_n = \sum_{j=1}^n a_j = 1 + x + x^2 + \dots + x^{n-1} \]
Then
\[ s_n = \begin{cases}
        \frac{1 - x^n}{1 - x} & \text{if } x \neq 1 \\
        n                     & \text{if } x = 1
    \end{cases} \]
This can be shown by observing that
\[ x s_n = x + x^2 + \dots + x^n = s_n - 1 + x^n \implies s_n(1-x) = 1-x^n \]
If $\abs{x} < 1$, then $x^n \to 0$ as $x \to \infty$. So $s_n \to \frac{1}{1-x}$. If $x > 1$, then $x^n \to \infty$ and so $s_n \to \infty$. If $x < -1$, $s_n$ oscillates. For completeness, if $x=-1$, $s_n$ oscillates between 0 and 1.

Note that the statement $s_n \to \infty$ means that given $a \in \mathbb R$, $\exists N$ such that $s_n > a$ for all $n \geq N$, and a similar statement holds for negative infinity (swapping the inequality). If $s_n$ does not converge or tend to $\pm \infty$, we say that $s_n$ oscillates.

Thus the geometric series converges if and only if $\abs{x} < 1$. Note that to prove that $x^n \to 0$ if $\abs{x} < 1$, we can consider the caase $0 < x < 1$ and write $1/x = 1 + \delta$ for some positive $\delta$. Then $x^n = \frac{1}{(1 + \delta)^n} \leq \frac{1}{1 + \delta n}$ from the binomial expansion, and this tends to zero as required.

\begin{lemma}
    If $\sum_{j=1}^\infty a_j$ converges, then $\lim_{j \to \infty} a_j = 0$.
\end{lemma}
\begin{proof}
    Given $s_n = \sum_{j=1}^n a_j$, we have $a_n = s_n - s_{n-1}$. If $s_n \to a$, then $a_n \to 0$ since $s_{n-1}$ also tends to $a$.
\end{proof}
\begin{remark}
    The converse is not true. For example, the harmonic series diverges, but the terms approach zero. Consider
    \begin{align*}
        s_{2n} & = s_n + \frac{1}{n+1} + \frac{1}{n+2} + \dots + \frac{1}{2n} \\
               & > s_n + \frac{1}{2n} + \frac{1}{2n} + \dots + \frac{1}{2n}   \\
               & = s_n + \frac{1}{2}
    \end{align*}
    So as $n \to \infty$, if the sequence is convergent then the sequences $s_n$ and $s_{2n}$ tend to the same limit, but they clearly do not.
\end{remark}

\subsection{Comparison Test}
In this section, we will let $a_n \in \mathbb R, a_n \geq 0$. In other words, all series contain only non-negative real terms.
\begin{theorem}
    Suppose $0 \leq b_n \leq a_n$ for all $n$. If $\sum_{j=1}^\infty a_j$ converges, then $\sum_{j=1}^\infty b_j$ converges.
\end{theorem}
\begin{proof}
    Let $s_N$ be the $N$th partial sum over the $a_n$, and let $d_N$ be the $N$th partial sum over the $d_N$. Since $b_n \leq a_n, d_N \leq s_N$. But $s_N \to s$, so $d_N \leq s_N \leq s$. So $d_N$ is an increasing sequence that is bounded above by $s$, so it converges.
\end{proof}
\noindent For example, let us analyse the behaviour of the sum of the sequence $\frac{1}{n^2}$. Note that
\[ \frac{1}{n^2} < \frac{1}{n(n-1)} = \frac{1}{n-1} - \frac{1}{n} \]
for $n \geq 2$. By the comparison test, it is sufficient to show that the series on the right hand side converges, in order to show that the original series converges.
\[ \sum_{j=2}^N a_j = 1 - \frac{1}{N} \to 1 \]
as required. So the original series tends to some value less than or equal to 2.

\subsection{Root Test or Cauchy's Test}
\begin{theorem}
    Suppose we have a sequence of non-negative terms $a_n$. Suppose that $a_n^{1/n} \to a$ as $n \to \infty$. Then if $a < 1$, the series $\sum a_n$ converges. If $a > 1$, the series $\sum a_n$ diverges.
\end{theorem}
\begin{remark}
    Nothing can be said if $a=1$. There is an example later of this fact.
\end{remark}
\begin{proof}
    If $a < 1$, let us choose an $r$ such that $a < r < 1$. By the definition of the limit, $\exists N$ such that $\forall n \geq N$, $a_n^{1/n} < r$. This implies that $a_n < r^n$. The geometric series $\sum r^n$ converges. By comparison, the series $a_n$ converges.

    If $a > 1$, for all $n \geq N$, $a_n^{1/n} > 1$ which implies $a_n > 1$, thus $\sum a_n$ diverges, since $a_n$ does not tend to zero.
\end{proof}

\subsection{Ratio Test or d'Alembert's Test}
\begin{theorem}
    Suppose $a_n > 0$, and $\frac{a_{n+1}}{a_n} \to \ell$. If $\ell < 1$, then the series $\sum a_n$ converges. If $\ell > 1$, then the series $\sum a_n$ diverges.
\end{theorem}
\begin{remark}
    Like before, no conclusion can be drawn if $\ell = 1$.
\end{remark}
\begin{proof}
    Suppose $\ell < 1$. We can choose $\ell < r < 1$, $\exists N$ such that $\forall n \geq N$, $\frac{a_{n+1}}{a_n} < r$. Therefore $a_n < r^{n-N} a_N$. Hence, $a_n < k r^n$ where $k$ is independent of $n$. Applying the comparison test, the series $\sum a_n$ must converge.

    If $\ell > 1$, we can choose $\ell > r > 1$. Then $\exists N$ such that $\forall n \geq N$, $\frac{a_{n+1}}{a_n} > r$. As before, $a_n > r^{n-N} a_N$. But the $r^{n-N}$ diverges, so the original series diverges.
\end{proof}

\section{More Convergence Tests}
\subsection{Examples of Ratio and Root Tests}
Consider $\sum_1^\infty \frac{n}{2^n}$. We have
\[ \frac{a_{n+1}}{a_n} = \frac{(n+1)/2^{n+1}}{n/2^n} \to \frac{1}{2} \]
So we have convergence, by the ratio test. Now, consider $\sum_1^\infty \frac{1}{n}$ and $\sum_1^\infty \frac{1}{n^2}$. In both cases, the ratio test gives limit 1. So the ratio test is inconclusive if the limit is 1. Since $n^{1/n} \to 1$, the root test is also inconclusive when the limit is 1. To check this limit, we can write
\[ n^{1/n} = 1 + \delta_n;\quad \delta_n > 0 \]
\[ n = (1 + \delta_n)^n > \frac{n(n-1)}{2}\delta_n^2 \]
using the binomial expansion.
\[ \implies \delta_n^2 < \frac{2}{n-1} \implies \delta_n \to 0 \]
The root test is a good candidate for series that contain powers of $n$, for example
\[ \sum_1^\infty \left[ \frac{n+1}{3n+5} \right]^n \]
In this instance, for example, we have convergence.

\subsection{Cauchy's Condensation Test}
\begin{theorem}
    Let $a_n$ be a decreasing sequence of positive terms. Then $\sum_1^\infty a_n$ converges if and only if $\sum_1^\infty 2^n a_{2^n}$ converges.
\end{theorem}
\begin{proof}
    First, note that if $a_n$ is decreasing, then
    \[ a_{2^k} \underset{(\ast)}{\leq} a_{2^{k-1} + i} \underset{(\dagger)}{\leq} a_{2^{k-1}};\quad 1 \leq i \leq 2^{k-1};\quad k \geq 1 \]
    Now let us assume that $\sum a_n$ converges to $A \in \mathbb R$. Then, by $(\ast)$,
    \begin{align*}
        2^{n-1} a_{2^n} & = a_{2^n} + a_{2^n} + \dots + a_{2^n}                \\
                        & \leq a_{2^{n-1}+1} + a_{2^{n-1}+2} + \dots + a_{2^n} \\
                        & = \sum_{m=2^{n-1}+1}^{2^n}a_m
    \end{align*}
    Thus,
    \[ \sum_{n=1}^N 2^{n-1}a_{2^n} \leq \sum_{n=1}^N \sum_{m=2^{n-1}+1}^{2^n} a_m = \sum_{n=2}^{2^N} a_m \]
    Therefore,
    \[ \sum_{n=1}^N 2^n a_{2^n} \leq 2 \sum_{n=2}^{2^N} a_m \leq 2(A-a_1) \]
    Thus $\sum_{n=1}^N 2^n a_{2^n}$ converges, since it is increasing and bounded above. For the converse, we will assume that $\sum 2^n a_{2^n}$ converges to $B$. Using $(\dagger)$,
    \begin{align*}
        \sum_{m=2^{n-1}}^{2^n} a_m & = a_{2^n} + a_{2^n} + \dots + a_{2^n}                \\
                                   & \leq a_{2^{n-1}} + a_{2^{n-1}} + \dots + a_{2^{n-1}} \\
                                   & = 2^{n-1}a_{2^{n-1}}
    \end{align*}
    So we have
    \[ \sum_{m=2}^{2^N} a_m = \sum_{n=1}^N \sum_{m=2^{n-1}+1}^{2^n} a_m \leq \sum_{n=1}^N 2^{n-1} a_{2^{n-1}} \leq \frac{1}{2} B \]
    Therefore, $\sum_{m=1}^N a_m$ is a bounded, increasing sequence and hence converges.
\end{proof}
\noindent Let us consider an example of this test. Consider the series definition of the Riemann zeta function
\[ \zeta(k) = \sum_{n=1}^\infty \frac{1}{n^k} \]
For what $k \in \mathbb R, k>0$ does this series converge? This is equivalent to asking if the following series converges.
\[ \sum_{n=1}^\infty 2^n \left[ \frac{1}{2^n} \right]^k = \sum_{n=1}^\infty \left( 2^{1-k} \right)^n \]
Hence it converges if and only if $2^{1-k} < 1 \iff k > 1$.

\subsection{Alternating Series}
An alternating series is a series where the sign on each term switches between positive and negative.
\begin{theorem}[Alternating Series Test]
    If $a_n$ decreases and tends to zero as $u \to \infty$, then the alternating series
    \[ \sum_1^\infty (-1)^{n+1} a_n \]
    converges.
\end{theorem}
\begin{proof}
    Let us consider the partial sum
    \[ s_n = a_1 - a_2 + a_3 - a_4 + \dots + (-1)^{n+1}a_n \]
    In particular,
    \[ s_{2n} = (a_1 - a_2) + (a_3 - a_4) + \dots + (a_{2n-1} - a_{2n}) \]
    Since the sequence is decreasing, each parenthesised block is positive. Then $s_{2n} \geq s_{2n-2}$. We can also write the partial sum as
    \[ s_{2n} = a_1 - (a_2 - a_3) - (a_4 - a_5) - \dots - (a_{2n-2} - a_{2n-1}) - a_{2n} \]
    Each parenthesised block here is negative. So $s_{2n} \leq a_1$. So $s_{2n}$ is increasing and bounded above, so it must converge. Now, note that
    \[ s_{2n+1} = s_{2n} + a_{2n+1} \to s_{2n} \]
    since $a_{2n+1} \to 0$. So $s_{2n+1}$ also converges, in fact to the same limit. Hence $s_n$ converges to this same limit.
\end{proof}

\section{Absolute Convergence}
\subsection{Absolute Convergence}
\begin{definition}
    Let $a_n \in \mathbb C$. Then if $\sum_{n=1}^\infty \abs{a_n}$ converges, then the series is called absolutely convergent.
\end{definition}
\begin{remark}
    Since $\abs{a_n} \geq 0$, we can use the previous tests to check for absolute convergence.
\end{remark}
\begin{theorem}
    Let $a_n \in \mathbb C$. If this series is absolutely convergent, it is convergent.
\end{theorem}
\begin{proof}
    Suppose first that $a_n$ is a sequence of real numbers. Then let
    \[ v_n = \begin{cases}
            a_n & \text{if } a_n \geq 0 \\
            0   & \text{if } a_n < 0
        \end{cases};\quad w_n = \begin{cases}
            0    & \text{if } a_n \geq 0 \\
            -a_n & \text{if } a_n < 0
        \end{cases} \]
    Hence,
    \[ v_n = \frac{\abs{a_n} + a_n}{2};\quad w_n = \frac{\abs{a_n} - a_n}{2} \]
    Clearly, $v_n, w_n \geq 0$, and $a_n = v_n - w_n$, and $\abs{a_n} = v_n + w_n$. If $\sum \abs{a_n}$ converges, then by comparison $\sum v_n$ and $\sum w_n$ also converge, and hence $\sum a_n$ converges. Now, let us consider the case where $a_n$ is complex. Then we can write $a_n = x_n + iy_n$ where $x_n, y_n$ are real sequences. Note that $\abs{x_n}, \abs{y_n} \leq \abs{a_n}$. So by comparison $x_n$ and $y_n$ converge, so $a_n$ converges.
\end{proof}
\noindent Here are some examples.
\begin{enumerate}
    \item The alternating harmonic series $\sum \frac{(-1)^n}{n}$ is convergent, but not absolutely convergent.
    \item $\sum \frac{z^n}{2^n}$ is absolutely convergent when $\abs{z} < \abs{2}$, because it reduces to a real geometric series. If $\abs{z} \geq 2$, then $\abs{a_n} \geq 1$, so we do not have absolute convergence.
\end{enumerate}

\subsection{Conditional Convergence and Rearrangement}
If the series is convergent but not absolutely convergent, it is called \textit{conditionally} convergent. The sum to which a series converges depends on the order in which the terms are added.
\begin{definition}
    Let $\sigma$ be a bijection of the positive integers to itself, then
    \[ a_n' = a_{\sigma(n)} \]
    is a rearrangement of $a_n$.
\end{definition}
\begin{theorem}
    If $\sum_1^\infty a_n$ is absolutely convergent, then every rearrangement of this series converges to the same value.
\end{theorem}
\begin{proof}
    First, let us consider the real case. Let $\sum a_n'$ be a rearrangement of $\sum a_n$. Let $s_n = \sum_1^n a_n$, and $t_n = \sum_1^n a_n'$. Let $s_n$ converge to $s$. Suppose first that $a_n \geq 0$. Then given any $n \in \mathbb N$, we can find some $q \in \mathbb N$ such that $s_q$ contains every term of $t_n$. Since the $a_n \geq 0$,
    \[ t_n \leq s_q \leq s \]
    As $n \to \infty$, the $t_n$ is an increasing sequence bounded above, so it must tend to a limit $t$, where $t \leq s$. Note, however, that this argument is symmetric; we can equally derive that $s \leq t$. Therefore $s = t$.

    Now, let us drop the condition that $a_n \geq 0$. We can now consider $v_n, w_n$ from above:
    \[ v_n = \frac{\abs{a_n} + a_n}{2};\quad w_n = \frac{\abs{a_n} - a_n}{2} \]
    Since $\sum\abs{a_n}$ converges, both $\sum v_n, \sum w_n$ converge. Since all $v_n, w_n \geq 0$, we can deduce that $\sum v_n = \sum v_n'$ and $\sum w_n' = \sum w_n$. The claim follows since $a_n = v_n - w_n$.

    For the case $a_n \in \mathbb C$, we can write $a_n = x_n + iy_n$, noting that $\abs{x_n}, \abs{y_n} \geq \abs{a_n}$. By comparison, the series $\sum x_n, \sum y_n$ are absolutely convergent, and by the previous case, $\sum x_n = \sum x_n'$ and $\sum y_n' = \sum y_n'$. Since $a_n' = x_n' + y_n'$, $\sum a_n = \sum a_n'$ as required.
\end{proof}

\section{Continuity}
\subsection{Definitions}
Let $E \subseteq \mathbb C$ be a non-empty set, and $f \colon E \to \mathbb C$ be any function, and let $a \in E$. Certainly, this includes the case in which $f$ is a real-valued function and $E \subseteq \mathbb R$.
\begin{definition}
    $f$ is continuous at $a$ if for every sequence $z_n \in E$ that converges to $a$, we have $f(z_n) \to f(a)$.
\end{definition}
\noindent We can use an alternative definition:
\begin{definition}[$\varepsilon$-$\delta$ definition]
    $f$ is continuous at $a$ if given $\varepsilon > 0$, $\exists \delta > 0$ such that for every $z \in E$, if $\abs{z - a} < \delta$, then $\abs{f(z) - f(a)} < \varepsilon$.
\end{definition}
\noindent We will immediately prove that both definitions are equivalent. First, let us prove that the $\varepsilon$-$\delta$ definition implies the first definition.
\begin{proof}
    We know that given $\varepsilon > 0, \exists \delta > 0$ such that for all $z \in E$, $\abs{z - a} < \delta$ implies $\abs{f(z) - f(a)} < \varepsilon$. Let $z_n \to a$, then by the definition of the limit of the sequence then there exists $n_0$ such that for all $n \geq n_0$ we have $\abs{z_n - a} < \delta$. But this implies that $\abs{f(z_n) - f(a)} < \varepsilon$, i.e. $f(z_n) \to f(a)$.
\end{proof}
\noindent We now prove the converse, that the first definition implies the second.
\begin{proof}
    We know that for every sequence $z_n \in E$ that converges to $a$, $f(z_n) \to f(a)$. Suppose $f$ is not continuous at $a$, according to the $\varepsilon$-$\delta$ definition. Then there exists some $\varepsilon$ such that for all $\delta > 0$, there exists $z \in E$ such that $\abs{z - a} < \delta$ but $\abs{f(z) - f(a)} \geq \varepsilon$. So, let us construct a sequence of $\delta$ values to substitute into this definition. Let $\delta = 1/n$. Then the $z_n$ given by this $\delta$ is such that $\abs{z_n - a} < 1/n$ and $\abs{f(z_n) - f(a)} \geq \varepsilon$. Clearly, $z_n \to a$, but $f(z_n)$ does not tend to $f(a)$ because the difference between the two is always greater than $\varepsilon$. This is a contradiction, since we assumed that $f$ is continuous by the first definition. So $f$ is continuous by the $\varepsilon$-$\delta$ definition.
\end{proof}

\subsection{Making Continuous Functions}
We can create new continuous functions from old ones by manipulating them in a number of ways.
\begin{proposition}
    Let $g, f \colon E \to \mathbb C$ be continous functions at a point $a \in E$. Then all of the functions
    \begin{itemize}
        \item $f(z) + g(z)$
        \item $f(z)g(z)$
        \item $\lambda f(z)$ for some constant $\lambda$
    \end{itemize}
    are all continuous. In addition, if $f(z) \neq 0$ everywhere in $E$, then $\frac{1}{f}$ is a continuous function at $a$.
\end{proposition}
\begin{proof}
    Using the first definition, this is obvious using the fact that limits of sequences behave analogously.
\end{proof}
\noindent Trivially, the function $f(z) = z$ is continuous. From this, we can derive that every polynomial is continuous at every point in $\mathbb C$. Note that we say that $f$ is continuous on the entire set $E$ if it is continuous at every point $a \in E$.

\subsection{Composition of Continuous Functions}
\begin{theorem}
    Let $f \colon A \to \mathbb C$ and $g \colon B \to \mathbb C$ where $A, B \subseteq \mathbb C$ be two functions that can be composed, i.e. $f(A) \subseteq B$. If $f$ is continuous at $a \in A$ and $g$ is continuous at $f(a) \in B$, then $g \circ f \colon A \to \mathbb C$ is continuous at $a$.
\end{theorem}
\begin{proof}
    Take any sequence $z_n \to a$. By assumption, $f(z_n) \to f(a)$. Now, let us define a new sequence $w_n = f(z_n)$. Then $w_n \in B$ and $w_n \to f(a)$. Thus, $g(f(z_n)) = g(w_n) \to g(f(a))$ by continuity, as required.
\end{proof}
\noindent Consider the function $f\colon \mathbb R \to \mathbb R$ defined by
\[ f(x) = \begin{cases}
        \sin\left( \frac{1}{x} \right) & x \neq 0 \\
        0                              & x = 0
    \end{cases} \]
This is assuming the knowledge of $\sin(x)$ being a continuous function $\mathbb R\to \mathbb R$, which we will prove later. So $f(x)$ is certainly continuous at every point on $\mathbb R$ excluding 0, since it is the composition of two continuous functions. We can prove it is discontinuous at $x=0$ by providing a sequence, for example
\[ \frac{1}{x_n} = \left(2n + \frac{1}{2}\right)\pi \]
Then $x_n \to 0$, and $f(x_n) = 1$. But $f(0) \neq 1$, so it is discontinuous. Let us modify the example as follows.
\[ f(x) = \begin{cases}
        x\sin\left( \frac{1}{x} \right) & x \neq 0 \\
        0                               & x = 0
    \end{cases} \]
We can prove that this sequence is continuous at 0. For an arbitrary sequence $x_n \to 0$, then $\abs{f(x_n)} \leq \abs{x_n}$ because $\abs{\sin x} \leq 1$. So $f(x_n)$ is bounded by $x_n$, which tends to zero, so $f(x_n)$ tends to zero as required. Now for a final example, let
\[ f(x) = \begin{cases}
        1 & x \in \mathbb Q    \\
        0 & x \notin \mathbb Q
    \end{cases} \]
This is discontinuous at every point. If $x \in \mathbb Q$, take a sequence $x_n \to x$ with all $x_n$ irrational, then $f(x_n) = 0$ but $f(x) = 1$. Similarly, if $x \notin \mathbb Q$, take a sequence $x_n \to x$ with all $x_n$ rational, then $f(x_n) = 1$ but $f(x) = 0$.

\section{Limit of a Function}
\subsection{Definition}
Let $f \colon E \subseteq \mathbb C \to \mathbb C$. We would like to define what is meant by $\lim_{z \to a} f(z)$, even when $a \notin E$. Further, if we have a set with an isolated point, for example $E = \{ 0 \} \cup [1, 2]$, it does not make sense to talk about limits tending to 0 since there are no points in $E$ close to 0.
\begin{definition}
    Let $E \subseteq \mathbb C,\, a \in \mathbb C$. $a$ is a limit point of $E$ if for any $\delta > 0$, there exists $z \in E$ such that $0 < \abs{z - a} < \delta$.
\end{definition}
\noindent First, note that $a$ is a limit point if and only if there exists a sequence $z_n \in E$ such that $z_n \to a$, but notably $z_n \neq a$ for all $n$.
\begin{definition}
    Let $f \colon E \subseteq \mathbb C \to \mathbb C$, and let $a \in \mathbb C$ be a limit point of $E$. We say that $f \to \ell$ as $z \to a$, if given $\varepsilon > 0$ there exists $\delta > 0$ such that whenever $0 < \abs{z - a} < \delta$ and $z \in E$, $\abs{f(z) - \ell} < \varepsilon$. Equivalently, $f(z_n) \to \ell$ for every sequence $z_n \in E$, such that $z_n \to a$ but $z_n \neq a$.
\end{definition}
\noindent Therefore if $a \in E$ is a limit point, then $\lim_{z \to a} f(z) = f(a)$ if and only if $f$ is continuous at $a$. If $a \in E$ is isolated (not a limit point) then $f$ at $a$ is trivially continuous, since there are no points near $a$ but $a$ itself.

\subsection{Properties}
The limit of a function has very similar properties when compared to the limit of a sequence.
\begin{enumerate}
    \item It is unique. $f(z) \to A$, $f(z) \to B$ implies $A = B$.
    \item $f(z) \to A$, $g(z) \to B$ implies
          \begin{enumerate}
              \item $f(z) + g(z) \to A + B$
              \item $f(z)\cdot g(z) \to AB$
              \item If $B \neq 0$, $\frac{f(z)}{g(z)} \to \frac{A}{B}$
          \end{enumerate}
\end{enumerate}

\subsection{Intermediate Value Theorem}
\begin{theorem}
    Let $f \colon [a, b] \to \mathbb R$ be a continuous function where $f(a) \neq f(b)$. Then $f$ takes all values in the interval $[f(a), f(b)]$.
\end{theorem}
\begin{proof}
    Without loss of generality, let us assume $f(a) < f(b)$. Let us take an $\eta$ such that $f(a) < \eta < f(b)$. We want to prove that there exists some value $c \in [a, b]$ with $f(c) = \eta$. Let $s$ be the set of points defined by
    \[ s = \{ x \in [a, b] \colon f(x) < \eta \} \]
    $a \in s$ therefore the set $s$ is non-empty. The set is also clearly bounded above by $b$. So there is a supremum of this set, say $\sup s = c$ where $c \leq b$. This point $c$ can be visualised as the last point at which $y=f(x)$ crosses the line $y=c$. We intend to show that the function at this rightmost point is $\eta$.

    By the definition of the supremum, given $n$ there exists $x_n \in s$ such that $c - \frac{1}{n} < x_n \leq c$. So the sequence $x_n$ tends to $c$. We know that $f(x_n) < \eta$ for all $x_n$ by definition of the set $s$. By the continuity of $f$, $f(x_n) \to f(c)$. Thus,
    \begin{equation}
        f(c) \leq \eta \tag{$\ast$}
    \end{equation}
    Now, let us consider the fact that $c \neq b$. If $c = b$, then $f(b) \leq \eta$ which is a contradiction since $\eta < f(b)$. So for a large $n$, we can ensure that $c + \frac{1}{n} \in [a,b]$. So by continuity of the function, $f(c + \frac{1}{n}) \to f(c)$. But since $c + \frac{1}{n} > c$, then necessarily $f(c + \frac{1}{n}) \geq \eta$ because $c$ is the supremum of $s$. Thus
    \[ f(c) \geq \eta \]
    Combining this with $(\ast)$ we get $f(c) = \eta$.
\end{proof}
\noindent This theorem is very useful for finding zeroes and fixed points. For example, we can prove the existence of the $N$th root of a positive real number $y$. Let
\[ f(x) = x^N \]
Then $f$ is certainly continuous on the interval $[0, 1+y]$, since
\[ 0 = f(0) < y < (1+y)^N = f(1 + y) \]
By the intermediate value theorem, there exists a point $c \in (0, 1+y)$ such that $f(c) = c^N = y$. So $c$ is a positive $N$th root of $y$. We can also prove the uniqueness of such a point. Suppose $d^N = y$ with $d>0$ and $d \neq c$. Without loss of generality, suppose $d < c$. Then $d^N < c^N$ so $d^N \neq y$, which is a contradiction.

\section{Bounds and Inverses}
\subsection{Bounds of a Continuous Function}
\begin{theorem}
    Let $f \colon [a, b] \to \mathbb R$ be continuous. Then the function is bounded, i.e. there exists $k \in \mathbb R$ such that $\abs{f(x)} \leq k$ for every point $x \in [a, b]$.
\end{theorem}
\begin{proof}
    Suppose that such a function $f$ is not bounded. Then in particular, given any integer $n \geq 1$, there exists $x_n \in [a, b]$ such that $\abs{f(x_n)} > n$. By the Bolzano-Weierstrass theorem, the sequence $x_n$, which is bounded by $a \leq x_n \leq b$, has a convergent subsequence $x_{n_j} \to x$, such that $x \in [a, b]$. Then by continuity of $f$, $f(x_{n_j}) \to f(x)$. But $\abs{f(x_{n_j})} > n_j \to \infty$. This is a contradiction.
\end{proof}
\noindent We can actually improve this statement.
\begin{theorem}
    Suppose $f \colon [a, b] \to \mathbb R$ is a continuous function. Then there exist $x_1, x_2 \in [a, b]$ such that
    \[ f(x_1) \leq f(x) \leq f(x_2) \]
    for all $x \in [a, b]$. In other words, a continuous function on a closed bounded interval is bounded and attains its bounds.
\end{theorem}
\begin{proof}
    Let $A = \{ f(x) \colon x \in [a, b] \}$ be the image of $[a, b]$ under $f$. By the above theorem, $A$ is bounded. It is also non-empty, hence it has a supremum $M = \sup A$ (and analogously an infimum $\inf A$, whose proof is almost identical). Then by the definition of the supremum, given an integer $n \geq 1$ there exists $x_n \in [a, b]$ such that $M - \frac{1}{n} < f(x_n) \leq M$. By the Bolzano-Weierstrass theorem, there exists a convergent subsequence $x_{n_j} \to x \in [a, b]$. Since $f(x_{n_j}) \to M$, then by continuity, $f(x) = M$.
\end{proof}
\noindent Here is an alternative proof of the same theorem.
\begin{proof}
    As before, let $A$ be the image of $f$, and $M$ be the supremum of $A$. Suppose there is no $x_2 \in [a, b]$ such that $f(x_2) = M$. Then let $g(x) = \frac{1}{M - f(x)}$ for $x \in [a, b]$. Since there exists no $x$ such that $M = f(x)$, $g(x)$ is continuous since we are never dividing by zero. So $g$ is bounded. So by the previous theorem, there is some $k > 0$ such that $g(x) \leq k$ for all $x \in [a, b]$. This means that $f(x) \leq M - \frac{1}{k}$ on $[a, b]$ for this $k$, but this cannot happen since $M$ is the supremum.
\end{proof}
\noindent Note that these theorems are certainly false if the interval is not closed: consider the counterexample $(0, 1]$ and the function $x \mapsto x^{-1}$.

\subsection{Inverse Functions}
\begin{definition}
    $f$ is increasing for $x \in [a, b]$ if $f(x_1) \leq f(x_2)$ for all $x_1 \leq x_2 \in [a, b]$. If $f(x_1) < f(x_2)$ then the function is strictly increasing. A function may be called decreasing or strictly decreasing analogously.
\end{definition}
\begin{definition}
    A function $f$ is called monotone if it is either increasing or decreasing.
\end{definition}
\begin{theorem}
    Let $f \colon [a, b] \to \mathbb R$ be continuous and strictly increasing for $x \in [a, b]$. Let $c = f(a)$, $d = f(b)$. Then $f \colon [a, b] \to [c, d]$ is bijective, and the inverse $g := f^{-1} \colon [c, d] \to [a, b]$ is continuous and strictly increasing.
\end{theorem}
\noindent A similar theorem holds for strictly decreasing functions.
\begin{proof}
    Let $c < k < d$. From the intermediate value theorem, there exists $h$ such that $f(h) = k$. This $h$ must be unique since the function is strictly increasing. Then we can define $g(k) = h$, giving us an inverse $g \colon [c, d] \to [a, b]$ for $f$.

    First, note that $g$ is strictly increasing. Indeed, for $y_1 < y_2$ then $y_1 = f(x_1)$, $y_2 = f(x_2)$. This means that if $x_2 \geq x_1$, then since $f$ is increasing $y_2 \leq y_1$ which is a contradiction.

    Now, note that $g$ is continuous. Indeed, given $\varepsilon > 0$, we can let $k_1 = f(h - \varepsilon)$ and $k_2 = f(h + \varepsilon)$. If $f$ is strictly increasing, then $k_1 < k < k_2$. Then $h - \varepsilon < g(y) < h + \varepsilon$. So let $\delta = \min(k_2 - k, k - k_1)$ where $k \in (c, d)$, establishing continuity as claimed.
\end{proof}

\section{Differentiability}
\subsection{Definitions}
Let $f \colon E \subseteq \mathbb C \to \mathbb C$. Mostly we will take $E$ to be an interval in the real numbers, or a disc in the complex plane.
\begin{definition}
    Let $x \in E$ be a point such that there exists a sequence $x_n \in E$ with $x_n \neq x$, but $x_n \to x$, i.e. $x$ is a limit point. $f$ is said to be differentiable at $x$ with derivative $f'(x)$ if
    \[ \lim_{y \to x} \frac{f(y) - f(x)}{y - x} = f'(x) \]
\end{definition}
\noindent If $f$ is differentiable at each point in $E$, we say that $f$ is differentiable on $E$.
\begin{remark}
    One interpretation of the definition is to write it in the form
    \[ \varepsilon(h) := f(x+h) - f(x) - hf'(x);\quad \lim_{h \to 0} \frac{\varepsilon(h)}{h} = 0 \]
    so $\varepsilon$ is $o(h)$. Hence,
    \[ f(x+h) = f(x) + hf'(x) + \varepsilon(h) \]
    We could have made an alternative definition for differentiability. $f$ is differentiable at $x$ if there exists $A$ and $\varepsilon$ such that
    \[ f(x+h) = f(x) + hA + \varepsilon(h) \text{ where } \lim_{h \to 0} \frac{\varepsilon(h)}{h} = 0 \]
    If such an $A$ exists, then it is unique, since $A$ is the limit
    \[ A = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h} \]
    We could have alternatively written the definition as
    \[ f(x+h) = f(x) + hf'(x) + h\varepsilon_f(h) \text{ where } \lim_{h \to 0} \varepsilon_f(h) = 0 \]
    or perhaps
    \[ f(x) = f(a) + (x-a)f'(a) + (x-a)\varepsilon_f(x) \text{ where } \lim_{x \to a} \varepsilon_f(x) = 0 \]
    Note further that if $f$ is differentiable at $x$, $f$ is certainly continuous at $x$. This follows from the fact that $\varepsilon(h) \to 0$, and hence $f(x+h) \to f(x)$ as $h \to 0$.
\end{remark}
\noindent As an example, let us consider $f(x) = \abs{x}$ for $f \colon \mathbb R \to \mathbb R$. Is the function at the point $x=0$ differentiable? If $x > 0$, we have $f'(x) = 1$, but if $x < 0$, we have $f'(x) = -1$. These results can be checked directly using the definitions above. But we have produced two sequences for $h \to 0$ which give different values, so the derivative is not defined here.

\subsection{Differentiation of Sums and Products}
\begin{proposition}
    \begin{enumerate}[(i)]
        \item If $f(x) = c$ for all $x \in E$, then $f$ is differentiable with $f'(x) = 0$.
        \item If $f$ and $g$ are differentiable at $x$, then so is $f+g$, where $(f+g)'(x) = f'(x) + g'(x)$.
        \item If $f$ and $g$ are differentiable at $x$, then so is $fg$, where $(fg)'(x) = f'(x)g(x) + g'(x)f(x)$.
        \item If $f$ is differentiable at $x$ and $f(x) \neq 0$, then so is $\frac{1}{f}$, where $(\frac{1}{f})'(x) = \frac{-f'(x)}{(f(x))^2}$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}[(i)]
        \item $\lim_{h \to 0} \frac{c-c}{h} = 0$ as required.
        \item Since all relevant limits are well-defined,
              \[ \lim_{h \to 0} \frac{f(x+h) + g(x+h) - f(x) - g(x)}{h} = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} + \lim_{h \to 0} \frac{g(x+h) - g(x)}{h} = f'(x) + g'(x) \]
        \item Let $\phi(x) = f(x)g(x)$. Then, since $f$ is continuous at $x$,
              \begin{align*}
                  \lim_{h \to 0} \frac{\phi(x+h) - \phi(x)}{h} & = \lim_{h \to 0} \frac{f(x+h)g(x+h) - f(x)g(x)}{h}                            \\
                                                               & = \lim_{h \to 0} f(x+h) \frac{g(x+h) - g(x)}{h} + g(x)\frac{f(x+h) - f(x)}{h} \\
                                                               & = \lim_{h \to 0} f(x) \frac{g(x+h) - g(x)}{h} + g(x)\frac{f(x+h) - f(x)}{h}   \\
                                                               & = f(x)g'(x) + g(x)f'(x)
              \end{align*}
        \item Let $\phi(x) = \frac{1}{f(x)}$. Then,
              \begin{align*}
                  \lim_{h \to 0} \frac{\phi(x+h) - \phi(x)}{h} & = \lim_{h \to 0} \frac{\frac{1}{f(x+h)} - \frac{1}{f(x)}}{h} \\
                                                               & = \lim_{h \to 0} \frac{f(x) - f(x+h)}{hf(x)f(x+h)}           \\
                                                               & = \frac{-f'(x)}{f(x)f(x)}                                    \\
              \end{align*}
    \end{enumerate}
\end{proof}
\begin{remark}
    From (iii) and (iv), we can immediately find the quotient rule,
    \[ \left( \frac{f(x)}{g(x)} \right)' = \frac{g(x)f'(x) - f(x)g'(x)}{(g(x))^2} \]
\end{remark}

\section{Properties of the Derivative}
\subsection{Differentiating Polynomial Terms}
As an example of the differentiability properties we saw last lecture, we can find the derivative of $f(x) = x^n$ for $n \in \mathbb Z$, $n > 0$. If $n=1$, clearly $f'(x) = 1$. We can show inductively that $f'(x) = nx^{n-1}$. Indeed,
\begin{align*}
    (x^n)' & = x \cdot (x^{n-1})' + (x)' \cdot x^{n-1} \\
           & = (n-1)x^{n-1} + x^{n-1}                  \\
           & = nx^{n-1}
\end{align*}
We can now take $f(x) = x^{-n}$. Using the reciprocal law,
\begin{align*}
    f'(x) & = \frac{-(x^n)'}{(x^n)^2}  \\
          & = \frac{-nx^{n-1}}{x^{2n}} \\
          & = -nx^{-n-1}
\end{align*}

\subsection{Chain Rule}
\begin{theorem}
    Let $f \colon U \to \mathbb C$ be such that $f(x) \in V$ for all $x \in U$. If $f$ is differentiable at $a \in U$, and $g \colon V \to \mathbb C$ is differentiable at $f(a) \in V$, then $g \circ f$ is differentiable at $a$ with
    \[ gf'(a) = f'(a)g'(f(a)) \]
\end{theorem}
\begin{proof}
    We know that we can write
    \[ f(x) = f(a) + (x-a)f'(a) + \varepsilon_f(x)(x-a) \]
    where $\lim_{x \to a} \varepsilon_f(x) = 0$. Further,
    \[ g(y) = g(b) + (y-b)g'(b) + \varepsilon_g(y)(y-b) \]
    where $\lim_{y \to b} \varepsilon_g(y) = 0$, and $b = f(a)$. We will set $\varepsilon_f(a) = 0$ and $\varepsilon_g(b) = 0$, so they are continuous at $x=a$ and $y=b$, so that everything is well-defined when we begin to compose the functions. Now, $y=f(x)$, so
    \begin{align*}
        g(f(x)) & =  g(b) + (f(x) - b)g'(b) + \varepsilon_g(f(x))(f(x) - b)                                                                                                       \\
                & = g(f(a)) + \left[ (x-a)f'(a) + \varepsilon_f(x)(x-a) \right]\left[ g'(b) + \varepsilon_g(f(x)) \right]                                                         \\
                & = g(f(a)) + (x-a)f'(a)g'(b) + (x-a)\underbrace{\left[ \varepsilon_f(x) g'(b) + \varepsilon_g(f(x)) \left( f'(a) + \varepsilon_f(x) \right) \right]}_{\sigma(x)} \\
    \end{align*}
    Now, we just need to show that $\lim_{x \to a} \sigma(x) = 0$ in order to prove the theorem. Clearly
    \[ \sigma(x) = \underbrace{\varepsilon_f(x)}_{\to 0} g'(b) + \underbrace{\varepsilon_g(f(x))}_{\to 0} \left( f'(a) + \varepsilon_f(x) \right) \]
    Hence $\sigma(x) \to 0$ as required.
\end{proof}

\subsection{Rolle's Theorem}
\begin{theorem}
    Let $f \colon [a,b] \to \mathbb R$ be a continuous function on $[a,b]$ and differentiable on $(a, b)$. If $f(a) = f(b)$, then there exists $c \in (a,b)$ such that $f'(c) = 0$.
\end{theorem}
\begin{proof}
    Let $M$ be the maximum point and $m$ be the minimum point of the function. Recall that in Lecture 8 we proved that any function achieves its bounds. Let $k = f(a)$. If $M=m=k$, then $f$ must be a constant, and clearly $f'(c) = 0$ for every value $c \in (a, b)$. Otherwise, either $M > k$ or $m < k$. Suppose $M > k$ (the proof is very similar if $m < k$). Then there exists some value $c \in (a, b)$ such that $f(c) = M$. We would like to show that $f'(c) = 0$, so let us suppose that $f'(c) \neq 0$. If $f'(c) > 0$, then there are values $d > c$ where $f(d) > f(c)$. Indeed,
    \[ f(h+c) - f(c) = h\left[ f'(c) + \varepsilon(h) \right] \]
    For a small, positive $h$, this value is positive. This contradicts the fact that $M$ is the maximum. Similarly, if $f'(c) < 0$ there are values $d < c$ with $f(d) > f(c)$. Hence $f'(c) = 0$.
\end{proof}

\subsection{Mean Value Theorem}
We can make a small change to Rolle's theorem and obtain the mean value theorem.
\begin{theorem}
    Let $f \colon [a,b] \to \mathbb R$ be a continuous function on $[a,b]$ and differentiable on $(a, b)$. Then there exists $c \in (a, b)$ such that
    \[ f(b) - f(a) = f'(c)(b-a) \]
\end{theorem}
\begin{proof}
    Let $\phi$ be a function defined by $\phi(x) = f(x) - kx$, choosing a $k$ such that $\phi(a) = \phi(b)$. We can find that
    \[ f(b) - bk = f(a) - ak \implies k = \frac{f(b) - f(a)}{b - a} \]
    By Rolle's theorem, there exists $c \in (a,b)$ such that $\phi'(c) = 0$. Now, note that $f'(x) = \phi'(x) + k$, hence there exists $c$ such that $f'(c) = k$.
\end{proof}
\begin{remark}
    We will often rewrite the mean value theorem as follows.
    \[ f(a + h) = f(a) + hf'(a + \theta h) \]
    where $\theta \in (0, 1)$. Note, however, that $\theta$ is a function of $h$, so if we begin to shrink $h$ then $\theta$ may change.
\end{remark}

\section{???}
\subsection{Properties of a Function from its Derivative}
We can deduce certain facts about a function by observing the properties its derivative exhibits. These results are mostly trivial corollaries to the mean value theorem, proven in the last lecture.
\begin{corollary}
    Let $f \colon [a,b] \to \mathbb R$ be continuous, and differentiable on $(a, b)$. Then we have
    \begin{enumerate}[(i)]
        \item If $f'(x) > 0$ for all $x \in (a, b)$, then $f$ is strictly increasing on $[a, b]$;
        \item If $f'(x) \geq 0$ for all $x \in (a, b)$, then $f$ is increasing on $[a, b]$;
        \item If $f'(x) = 0$ for all $x \in (a, b)$, then $f$ is constant on $[a, b]$.
    \end{enumerate}
\end{corollary}
\noindent Part (iii) of this corollary is essentially solving the most simple differential equation; we are showing that the only possible solutions to this equation are the constant functions. Note that similar statements about decreasing functions hold.
\begin{proof}
    \begin{enumerate}[(i)]
        \item We have $f(y) - f(x) = f'(c)(y-x)$ for some $c \in (x, y)$. If $f'(c) > 0$, then $f(y) - f(x) > 0$.
        \item Analogously to before, $f(y) - f(x) = f'(c)(y-x)$ for some $c \in (x, y)$. If $f'(c) \geq 0$, then  $f(y) - f(x) \geq 0$.
        \item By the mean value theorem on $[a, x]$, if $f'(c) = 0$, then $f(x) - f(a) = 0$.
    \end{enumerate}
\end{proof}

\subsection{Inverse Function Theorem}
\begin{theorem}
    Let $f \colon [a, b] \to \mathbb R$ be a continuous function on $[a, b]$ and differentiable on $(a, b)$, with $f'(x) > 0$ everywhere on $(a, b)$. Let $f(a) = c, f(b) = d$. Then the function $f \colon [a,b] \to [c,d]$ is bijective, and $f^{-1} \colon [c,d] \to [a,b]$ is differentiable on $(c, d)$ with
    \[ \left( f^{-1} \right)' (x) = \frac{1}{f'\left(f^{-1}(x)\right)} \]
\end{theorem}
\noindent Note, in lecture 8 it was proven that a continuous strictly increasing function has a continous inverse. This strengthens that claim to include the differentiability property if the original function was differentiable.
\begin{proof}
    We know from lecture 8 that there exists $g \colon [c,d] \to [a,b]$ which is a strictly increasing continuous function, which is the inverse of $f$. We must now show that $g$ is differentiable and that its derivative has the required form as stated in the claim. Now, let $y = f(x)$. Given $k \neq 0$, let $h$ be given by
    \[ y + k = f(x+h) \]
    Alternatively, written in terms of $g$,
    \[ x + h = g(y + k) \]
    So clearly $h \neq 0$. Since $g$ is continuous, if $k \to 0$ then $h \to 0$. Then
    \begin{align*}
        \frac{g(y + k) - g(y)}{k}                           & = \frac{x + h - x}{f(x+h) - y}           \\
                                                            & = \frac{h}{f(x+h) - f(x)}                \\
        \therefore \lim_{k \to 0} \frac{g(y + k) - g(y)}{k} & = \lim_{h \to 0} \frac{h}{f(x+h) - f(x)} \\
                                                            & = \frac{1}{f'(x)}
    \end{align*}
    as required.
\end{proof}

\subsection{Derivative of Rational Powers}
First, let $g(x) = x^{1/q}$ for some positive integer $q$. We can find that $f(x) = x^q$ has the derivative $f'(x) = qx^{q-1}$. By the inverse function theorem, $g'(x) = \frac{1}{q}x^{1/q - 1}$. Now, if $g(x) = x^{p/q}$, where $p$ is an integer and $q$ is a positive integer, then by the chain rule $g'(x) = \frac{p}{q}x^{p/q - 1}$ which matches the expected result.

\subsection{Mean Value Theorem Applied to Limits}
Suppose $f, g \colon [a,b] \to \mathbb R$ are continuous, and differentiable on $(a, b)$. Suppose further that $g(a) \neq g(b)$. The mean value theorem can be applied to both functions, and will give two points $s, t \in (a, b)$ such that
\[ \frac{f(b) - f(a)}{g(b) - g(a)} = \frac{(b-a)f'(s)}{(b-a)g'(t)} = \frac{f'(s)}{g'(t)} \]
This gives us a way to simplify a limit of the form of the left hand side (as $b \to a$) by instead considering the right hand side. We can apply Cauchy's mean value theorem, seen in the next lecture.

\section{Extensions of the Mean Value Theorem}
\subsection{Cauchy's Mean Value Theorem}
\begin{theorem}
    If $f, g \colon [a,b] \to \mathbb R$ are continuous, and differentiable on $(a, b)$, there exists $t \in (a,b)$ such that
    \[ (f(b) - f(a))g'(t) = f'(t)(g(b) - g(a)) \]
\end{theorem}
\noindent We can recover the normal mean value theorem from Cauchy's generalisation by taking $g(x) = x$.
\begin{proof}
    Let
    \[ \phi(x) = \begin{vmatrix}
            1    & 1    & 1    \\
            f(a) & f(x) & f(b) \\
            g(a) & g(x) & g(b)
        \end{vmatrix} \]
    Certainly $\phi(x)$ is continuous on $[a,b]$ and differentiable on $(a, b)$, by using previous results. Also, $\phi(a) = \phi(b) = 0$ by observing the linear dependence of the columns. By Rolle's theorem, there exists $t \in (a, b)$ such that $\phi'(t) = 0$. We can expand $\phi'(t)$ and this will show the required result.
    \[ \phi'(x) = f'(x)g(b) - g'(x)f(b) + f(a)g'(x) - g(a)f'(x) = f'(x) [g(b) - g(a)] + g'(x) [f(a) - f(b)] \]
\end{proof}

\subsection{Example of L'H\^opital's Rule}
The derivation of L'H\^opital's rule is on an example sheet, so in this subsection we will consider only a special case of it, using Cauchy's mean value theorem.
\[ \ell = \lim_{x \to 0} \frac{e^x - 1}{\sin x} \]
We can write
\[ \ell = \lim_{x \to 0} \frac{e^x - e^0}{\sin x - \sin 0} = \frac{e^t}{\cos t} \]
for some $t \in (0, x)$. So as $x \to 0$, $t \to 0$ and hence
\[ \frac{e^t}{\cos t} \to 1 \]

\subsection{Taylor's Theorem}
\begin{theorem}[Taylor's Theorem with Lagrange's Remainder]
    Suppose $f$ and its derivatives up to order $n-1$ are continuous in $[a, a+h]$, and $f^{(n)}$ exists for $x \in (a, a+h)$. Then
    \[ f(a+h) = f(a) + hf'(a) + \frac{h^2}{2!} f''(a) + \dots + \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(a) + \frac{h^n}{n!}f^{(n)}(a + \theta h) \]
    where $\theta \in (0, 1)$.
\end{theorem}
\noindent Note that for $n=1$, this is exactly the mean value theorem, so this can be seen as an $n$th order extension of the mean value theorem. We commonly write $R_n$ for the final error term $\frac{h^n}{n!}f^{(n)}(a + \theta h)$. This is known as Lagrange's form of the remainder.
\begin{proof}
    For $0 \leq t \leq h$, we define
    \[ \phi(t) = f(a+t) - f(a) - tf'(a) - \dots - \frac{t^{n-1}}{(n-1)!}f^{(n-1)}(a) - \frac{t^n}{n!}B \]
    where we choose $B$ suitably such that $\phi(h) = 0$. (Recall that in the proof of the mean value theorem, we used $f(x) - kx$ and picked $k$ suitably such that this allowed the use of Rolle's theorem. This is entirely analogous, but generalised to the $n$th derivative). Note that
    \[ \phi(0) = \phi'(0) = \dots = \phi^{(n-1)}(0) = 0 \]
    We can use Rolle's theorem inductively $n$ times. Since $\phi(0) = \phi(h) = 0$, there is a point $0 < h_1 < h$ such that $\phi'(h_1) = 0$. Since $\phi'(0) = \phi'(h_1) = 0$, there is a point $0 < h_2 < h_1$ such that $\phi''(h_2) = 0$. This continues until we find a point $0 < h_n < h$ such that $\phi^{(n)}(h_n) = 0$. Hence $h_n = \theta h$ for some $0 < \theta < 1$. Now, $\phi^{(n)}(t) = f^{(n)}(a + t) - B$. We can see now that $B = f^{(n)}(a + \theta h)$, which gives the required result.
\end{proof}
\noindent We can prove an alternative version of Taylor's theorem with a different error term.
\begin{theorem}[Taylor's Theorem with Cauchy's Remainder]
    Suppose (equivalently to before) $f$ and its derivatives up to order $n-1$ are continuous in $[a, a+h]$, and $f^{(n)}$ exists for $x \in (a, a+h)$. Then
    \[ f(a+h) = f(a) + hf'(a) + \frac{h^2}{2!} f''(a) + \dots + \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(a) + R_n \]
    where
    \[ R_n = \frac{(1 - \theta)^{n-1}h^nf^{(n)}(a + \theta h)}{(n-1)!} \]
    for $\theta \in (0, 1)$.
\end{theorem}
\begin{proof}
    For simplicity, in this proof we let $a = 0$, although the same argument applies when $a \neq 0$. Let us define
    \[ F(t) = f(h) - f(t) - (h-t)f'(t) - \dots - \frac{(h-t)^{n-1}f^{(n-1)}(t)}{(n-1)!} \]
    for $t \in [0, h]$. Then
    \begin{align*}
        F'(t) & = -f'(t) + f'(t) - (h-t)f''(t) + (h-t)f''(t) - \frac{1}{2} (h-t)^2f'''(t) + \frac{1}{2} (h-t)^2f'''(t) \\
              & - \dots - \frac{(h-t)^{n-1}}{(n-1)!}f^{(n)}(t)                                                         \\
              & = - \frac{(h-t)^{n-1}}{(n-1)!}f^{(n)}(t)
    \end{align*}
    Let
    \[ \phi(t) = F(t) - \left[ \frac{h-t}{h} \right]^p F(0) \]
    where $p \in \mathbb N$ and $1 \leq p \leq n$. Then
    \[ \phi(0) = \phi(h) = 0 \]
    By Rolle's theorem, there exists $\theta \in (0, 1)$ such that
    \[ \phi'(\theta h) = 0 \]
    We can compute $\phi'$ to find
    \[ \phi'(\theta h) = F'(\theta h) + \frac{p(1-\theta)^{p-1}}{h} F(0) = 0 \]
    Substituting everything back into $F$ gives
    \[ 0 = \frac{-h^{n-1}(1-\theta)^{n-1}}{(n-1)!}f^{(n)}(\theta h) + \frac{p(1-\theta)^{p-1}}{h}\left[ f(h) - f(0) - h'(0) - \dots - \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(0) \right] \]
    Hence
    \[ f(h) = f(0) + hf'(0) + \frac{h^2}{2!} f''(0) + \dots + \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(0) + \underbrace{\frac{h^n(1 - \theta)^{n-1}f^{(n)}(\theta h)}{(n-1)!\cdot p(1-\theta)^{p-1}}}_{R_n} \]
    By letting $p = n$, we get Lagrange's remainder. If $p=1$, we get Cauchy's remainder.
\end{proof}

\section{Applications of Remainders in Taylor's Theorem}
\subsection{Bounding Error Terms}
Recall that Lagrange's remainder is
\[ R_n = \frac{h^n}{n!}f^{(n)}(a + \theta h) \]
and Cauchy's remainder is
\[ R_n = \frac{(1 - \theta)^{n-1}h^nf^{(n)}(a + \theta h)}{(n-1)!} \]
and that we can write
\[ f(h) = P_{n-1}(h) + R_n \]
where $P_{n-1}$ is the Taylor polynomial to $(n-1)$th order. To get a Taylor series for a function $f$, we need to prove that the $R_n$ tend to zero as $n \to \infty$. In general, this requires estimates for the $R_n$ and it could take a lot of effort to prove whether this limit is zero or not. Note also that the theorems deducing the remainder terms work equally well in an interval $[a+h, a]$ where $h < 0$.

\subsection{Binomial Series}
\begin{proposition}
    Let
    \[ f(x) = (1 + x)^r \]
    for some $r \in \mathbb Q$. If $\abs{x} < 1$, then
    \[ f(x) = 1 + \binom{r}{1}x + \dots + \binom{r}{n}x^n + \dots \]
    where
    \[ \binom{r}{n} = \frac{r(r-1)\cdots(r-n+1)}{n!} \]
\end{proposition}
\begin{proof}
    Clearly,
    \[ f^{(n)}(x) = r(r-1)\cdots(r-n+1)(1+x)^{r-n} \]
    These coefficients correspond exactly with that of the Taylor polynomial. If $r \in \mathbb N$, then $f^{(r+1)}(x) \equiv 0$, so clearly the $R_n$ are zero as $n \to \infty$. In general, using Lagrange's form of the remainder,
    \[ R_n = \frac{x^n}{n!} f^{(n)}(\theta x) = \binom{r}{n} \frac{x^n}{(1 + \theta x)^{n-r}} \]
    Note that in principle, $\theta$ depends both on $x$ and $n$. For $0 < x < 1$, $(1 + \theta x)^{n - r} > 1$ for $n > r$. Now observe that the series given by
    \[ \sum \binom{r}{n} x^n \]
    is absolutely convergent for $\abs{x} < 1$. Indeed, we can apply the ratio test and find that
    \[ \abs{\frac{a_{n+1}}{a_n}} = \abs{\frac{(r-n)x}{n+1}} \]
    which tends to $\abs{x}$ as $n \to \infty$. In particular therefore, the terms $\binom{r}{n}x^n$ tend to zero for $\abs{x} < 1$. Hence for $n > r$ and $0 < x < 1$, we have
    \[ \abs{R_n} \leq \abs{\binom{r}{n}x^n} \to 0 \]
    So the claim is proven in the range $0 \leq x < 1$. If $x < 0$, then the step when we compare $(1 + \theta x)^{n-r}$ with 1 breaks down. Let us instead use the Cauchy form of the remainder to bypass this step.
    \[ R_n = \frac{(1 - \theta)^{n-1}x^nf^{(n)}(\theta x)}{(n-1)!} = \frac{(1-\theta)^{n-1} r(r-1)\cdots(r-n+1)(1+\theta x)^{r-n} x^n}{(n-1)!} \]
    By regrouping terms, we get
    \[ R_n = \frac{r(r-1)\cdots(r-n+1)}{(n-1)!} \cdot \frac{(1-\theta)^{n-1}}{(1 + \theta x)^{n-r}} x^n = r\binom{r-1}{n-1}x^n (1+\theta x)^{r-1} \left( \underbrace{\frac{1-\theta}{1 + \theta x}}_{<1} \right)^{n-1} \]
    Hence
    \[ \abs{R_n} \leq \abs{r \binom{r-1}{n-1}x^n} (1+\theta x)^{r-1} \]
    This will then tend to zero, after a bit more effort; we can bound the $(1 + \theta x)^{r-1}$ term by the maximum of $1$ and $(1 + x)^{r-1}$, which is independent of $n$, and then the result will follow.
\end{proof}

\subsection{Complex Differentiation}
The complex derivative and the real derivative have the same core properties, for instance linearity, the product rule and the chain rule. However, the complex derivative is significantly more restrictive than the real derivative, since we can approach a point in any number of directions. If we can find a function that is complex differentiable with this restriction, we actually get a whole array of features for free. As an example of this restriction, consider the function $f(z) = \overline{z}$. This function is actually nowhere differentiable. If it were differentiable, then any sequence tending to $z$ would yield the same limit when substituted into the definition of the derivative. Consider first the sequence
\[ z_n = z + \frac{1}{n} \to z \]
Then
\[ \frac{f(z_n) - f(z)}{z_n - z} = \frac{\overline{z} + \frac{1}{n} - \overline{z}}{z + \frac{1}{n} - z} = 1 \]
Now consider the sequence
\[ z_n = z + \frac{i}{n} \to z \]
Then
\[ \frac{f(z_n) - f(z)}{z_n - z} = \frac{\overline{z} - \frac{i}{n} - \overline{z}}{z + \frac{i}{n} - z} = -1 \]
Hence $f(z)$ is nowhere differentiable. On the other hand, the real function $f(x, y) = (x, -y)$ is clearly real differentiable, since it is linear; but in the complex world the function $z \mapsto \overline{z}$ is not linear.

\section{Power Series}
\subsection{Definition}
A power series is a series of the form
\[ \sum_{n=0}^\infty a_n z^n \]
where $z \in \mathbb C$, and the $a_n$ is a given sequence of complex numbers. We can also take a power series of the form
\[ \sum_{n=0}^\infty a_n (z-z_0)^n \]
but for simplicity we will take $z_0 = 0$ in all of the analysis we will conduct on power series.

\subsection{Radius of Convergence}
\begin{lemma}
    If the series
    \[ \sum_{n=0}^\infty a_n z_1^n \]
    converges for some point $z_1$, and $\abs{z} < \abs{z_1}$, then the series
    \[ \sum_{n=0}^\infty a_n z^n \]
    also converges absolutely.
\end{lemma}
\begin{proof}
    Since $\sum_{n=0}^\infty a_n z_1^n$ converges, $a_n z_1^n \to 0$. Thus the sequence $a_n z_1^n$ is bounded by some $k > 0$, i.e. for all $n$, $\abs{a_n z_1^n}<k$. Then
    \[ \abs{a_n z^n} \leq k\abs{\frac{z}{z_1}}^n \]
    Since the geometric series $\sum_0^\infty \abs{\frac{z}{z_1}}^n$ converges, the lemma follows by comparison.
\end{proof}
\noindent Using this lemma, we can find that there exists a radius inside which any given power series converges absolutely. This radius might be zero, and it might be infinite.
\begin{theorem}
    Any power series either
    \begin{enumerate}[(i)]
        \item converges absolutely for all $z$, or
        \item converges absolutely for all $z$ where $\abs{z} < R$ and diverges for all $z$ where $\abs{z} > R$, or
        \item converges for $z = 0$ only.
    \end{enumerate}
\end{theorem}
\noindent The circle $\abs{z} = R$ is called the circle of convergence, and $R$ is called the radius of convergence. Note that this theorem does not make any claim about the behaviour \textit{on} the circle of convergence, just the behaviour inside it.
\begin{proof}
    Let
    \[ S = \left\{ x \in \mathbb R \colon x \geq 0, \sum_0^\infty a_n x^n \text{ converges} \right\} \]
    Clearly, $0 \in S$. By the above lemma, if $x_1 \in S$, then $[0, x_1] \subseteq S$. If $S = [0, \infty)$, then we have case (i) above due to the lemma. If $S \neq [0, \infty)$, there exists a supremum $0 \leq R = \sup S < \infty$. We must now just deal with case (ii), which is $R > 0$. For all $z_1$ with $\abs{z_1} < R$ there exists $R_0$ such that $\abs{z_1} < R_0 < R$, and absolute convergence follows using the lemma. If $\abs{z_1} > R$, there exists $R_0$ such that $\abs{z_1} > R_0 > R$. If the series with $z_1$ converges, then by the lemma the same would be true for $R_0$. But $R_0$ does not converge, so this is a contradiction.
\end{proof}

\begin{lemma}
    If
    \[ \abs{\frac{a_{n+1}}{a_n}} \to \ell \]
    as $n \to \infty$, then $R = \frac{1}{\ell}$.
\end{lemma}
\begin{proof}
    By the ratio test, we have absolute convergence if
    \[ \abs{\frac{a_{n+1}}{a_n} \frac{z^{n+1}}{z^n}} < 1 \]
    So we have absolute convergence if $\abs{z} < \frac{1}{\ell}$ and divergence if $\abs{z} > \frac{1}{\ell}$ as required.
\end{proof}
\begin{lemma}
    If
    \[ \abs{a_n^{1/n}} \to \ell \]
    as $n \to \infty$, then $R = \frac{1}{\ell}$.
\end{lemma}
\noindent This can be shown similarly using the root test.

\subsection{Examples of Radii of Convergence}
\begin{enumerate}
    \item Consider the series $\sum_0^\infty \frac{z^n}{n!}$. Using the ratio test, the series converges absolutely everywhere.
    \item The geometric series $\sum_0^\infty z^n$ gives $R=1$ by the ratio test. In this case, $\abs{z} = 1$ gives divergence.
    \item The series $\sum_0^\infty n!z^n$ has $R=0$, which again can be seen using the ratio test.
    \item Consider $\sum_1^\infty \frac{z^n}{n}$. This also has $R = 1$ by the ratio test. Note that the series diverges for $z=1$ since we get the harmonic series. However, it converges when $z = -1$ by the alternating series test. To work out the behaviour at other points on the circle of convergence, we could consider the series $\sum_1^\infty \frac{z^n}{n}(1-z)$, which converges exactly when the original series does. The partial sums are
          \begin{align*}
              S_N & = \sum_1^N \left[ \frac{z_n - z^{n+1}}{n} \right]            \\
                  & = \sum_1^N \frac{z^n}{n} - \sum_1^N \frac{z^{n+1}}{n}        \\
                  & = \sum_1^N \frac{z^n}{n} - \sum_2^{N+1} \frac{z^n}{n-1}      \\
                  & = z - \frac{z^{N+1}}{N+1} + \sum_2^{N+1} \frac{-z^n}{n(n-1)} \\
          \end{align*}
          If $\abs{z} = 1$, then the term $\frac{z^{N+1}}{N+1}$ will vanish as $N \to \infty$. If $z \neq 1$, the term $\sum_2^{N+1} \frac{-z^n}{n(n-1)}$ converges as $N \to \infty$. So $S_N$ does indeed converge for $\abs{z} = 1$, $z \neq 1$.
    \item Now, consider $\sum_1^\infty \frac{z^n}{n^2}$. This has $R=1$ by the ratio test, but it converges for all $z$ with $\abs{z} = 1$.
    \item If we have $\sum_0^\infty nz^n$, we have $R=1$, but diverges for all $\abs{z} = 1$.
\end{enumerate}
\noindent In conclusion, we cannot determine the behaviour at the boundary in the general case. Inside the radius of convergence, power series will behave as if they were simply polynomials.

\section{Infinite Differentiability of Power Series}
\subsection{Proving Infinite Differentiability}
\begin{theorem}
    Let $f(z) = \sum_0^\infty a_n z^n$ have a radius of convergence $R$. Then $f$ is complex differentiable at all points with $\abs{z} < R$, with
    \[ f'(z) = \sum_1^\infty n a_n z^{n-1} \]
    with the same radius of convergence as the original series.
\end{theorem}
\noindent This proof comprises the entire subsection. This whole subsection is non-examinable, but included for completeness. First, we will state two lemmas.
\begin{lemma}
    If $\sum_0^\infty a_n z^n$ has radius of convergence $R$, then both series
    \[ \sum_1^\infty n a_n z^{n-1} \]
    and
    \[ \sum_2^\infty n(n-1)a_n z^{n-2} \]
    also have radius of convergence $R$.
\end{lemma}
\begin{proof}
    Let $R_0$ be such that $0 < \abs{z} < R_0 < R$. Since $a_0 R_0^n \to 0$, the sequence $a_0 R_0^n$ is bounded. In other words there exists a $k$ such that $\abs{a_n R_0^n} \leq k$ for all $n \geq 0$. Thus,
    \[ \abs{a_n n z^{n-1}} = \frac{n}{\abs{z}}\abs{a_n R_0^n} \abs{\frac{z}{R_0}}^n \leq \frac{kn}{\abs{z}}\abs{\frac{z}{R_0}}^n \]
    But
    \[ \sum n\abs{\frac{z}{R_0}}^n \]
    converges by the ratio test, since the ratio is
    \[ \frac{n+1}{n}\abs{\frac{z}{R_0}}^{n+1} \abs{\frac{R_0}{z}}^n = \frac{n+1}{n}\abs{\frac{z}{R_0}} \to \abs{\frac{z}{R_0}} < 1 \]
    Hence, the original series $\sum_1^\infty n a_n z^{n-1}$ is absolutely bounded above by a convergent series, and therefore is absolutely convergent. So it is known that the radius of convergence of this derivative series is \textit{at least} $R$. Now, if $\abs{z} > R$, the series diverges since $\abs{a_n z^n}$ is unbounded, and hence $\abs{n a_n z^n}$ is also unbounded. The same proof applies to the series for the second derivative.
\end{proof}
\noindent We will need this `second derivative' condition in order to talk about the remainder term after the first derivative, which is related to the second derivative.
\begin{lemma}
    First, for all $2 \leq r \leq n$.
    \[ \binom{n}{r} \leq n(n-1)\binom{n-2}{r-2} \]
    Further, for all $z \in \mathbb C$, $h \in \mathbb C$,
    \[ \abs{(z + h)^n - z^n - nhz^{n-1}} \leq n(n-1)(\abs{z} + \abs{h})^{n-2}\abs{h}^2 \]
\end{lemma}
\begin{proof}
    For the first part, we can expand the definitions to get
    \[ \frac{\binom{n}{r}}{\binom{n-2}{r-2}} = \frac{n(n-1)}{r(r-1)} \leq n(n-1) \]
    as required. For the second part, we can apply the binomial expansion to cancel the other two terms, and we get
    \begin{align*}
        (z + h)^n - z^n - nhz^{n-1}                  & = \left( \sum_{r=0}^n \binom{n}{r} z^{n-r} h^r \right)  - z^n - nhz^{n-1}                                                               \\
                                                     & = \sum_{r=2}^n \binom{n}{r} z^{n-r} h^r                                                                                                 \\
        \therefore \abs{(z + h)^n - z^n - nhz^{n-1}} & = \abs{\sum_{r=2}^n \binom{n}{r} z^{n-r} h^r}                                                                                           \\
                                                     & \leq \sum_{r=2}^n \abs{\binom{n}{r} z^{n-r} h^r}                                                                                        \\
                                                     & = \sum_{r=2}^n \binom{n}{r} \abs{z}^{n-r} \abs{h}^r                                                                                     \\
                                                     & \leq n(n-1) \underbrace{\left[ \sum_{r=2}^n \binom{n-2}{r-2} \abs{z}^{n-r} \abs{h}^{r-2} \right]}_{(\abs{z} + \abs{h})^{n-2}} \abs{h}^2 \\
                                                     & = n(n-1) (\abs{z} + \abs{h})^{n-2} \abs{h}^2                                                                                            \\
    \end{align*}
    as required.
\end{proof}
\noindent Now, we can prove the original theorem.
\begin{proof}
    By the first lemma, we may define $f'(z)$ to be
    \[ f'(z) = \sum_1^\infty n a_n z^{n-1} \]
    We now just need to prove that
    \[ \lim_{h \to 0} I = 0;\quad I = \frac{f(z + h) - f(z) - h f'(z)}{h} \]
    We can substitute the expressions we have found for each power series:
    \begin{align*}
        I       & = \frac{\sum_0^\infty a_n (z+h)^n - \sum_0^\infty a_n z^n - h \sum_1^\infty n a_n z^{n-1}}{h}            \\
                & = \frac{1}{h} \sum_0^\infty \left[ a_n (z+h)^n - a_n z^n - h n a_n z^{n-1} \right]                       \\
                & = \frac{1}{h} \sum_0^\infty a_n \left[ (z+h)^n - z^n - h n z^{n-1} \right]                               \\
        \abs{I} & = \frac{1}{\abs{h}} \abs{\lim_{N \to \infty} \sum_0^N a_n \left[ (z+h)^n - z^n - h n z^{n-1} \right]}    \\
        \intertext{Since the modulus function is continuous,}
        \abs{I} & = \frac{1}{\abs{h}} \lim_{N \to \infty} \abs{\sum_0^N a_n \left[ (z+h)^n - z^n - h n z^{n-1} \right]}    \\
                & \leq \frac{1}{\abs{h}} \lim_{N \to \infty} \sum_0^N \abs{a_n \left[ (z+h)^n - z^n - h n z^{n-1} \right]} \\
                & = \frac{1}{\abs{h}} \sum_0^\infty \abs{a_n} \cdot \abs{(z+h)^n - z^n - h n z^{n-1}}                      \\
        \intertext{By the second part of the second lemma above,}
        \abs{I} & \leq \frac{1}{\abs{h}} \sum_0^\infty \abs{a_n} \cdot n(n-1)(\abs{z} + \abs{h})^{n-2}\abs{h}^2            \\
                & = \abs{h} \sum_0^\infty \abs{a_n} \cdot n(n-1)(\abs{z} + \abs{h})^{n-2}                                  \\
    \end{align*}
    For $\abs{h}$ small enough, $(\abs{z} + \abs{h}) < R$. Therefore, by the first lemma above,
    \[ \sum_0^\infty \abs{a_n} \cdot n(n-1)(\abs{z} + \abs{h})^{n-2} \]
    converges to some $A(h)$. But $A(h)$ is monotonically decreasing, so
    \[ \abs{I} \leq \abs{h} A(h) \leq \abs{h} A(r) \]
    for some $r$ such that $\abs{z} + r < R$. We can now let $h \to 0$, giving
    \[ \abs{I} \to 0 \]
    as required.
\end{proof}

\subsection{Defining Standard Functions}
We can now use this differentiability property to cleanly define the standard exponential, logarithmic and trigonometric functions. Let $e \colon \mathbb C \to \mathbb C$ be defined by
\[ e(z) = \sum_0^\infty \frac{z^n}{n!} \]
We have already seen that it has infinite radius of convergence. Straight from the above theorem, $e$ is infinitely differentiable everywhere, and it is its own derivative. Note that if a function $F \colon \mathbb C \to \mathbb C$ has $F'(z) = 0$ for all $z \in \mathbb C$, then $F$ is constant. Indeed, consider $g(t) = F(tz) = u(t) + iv(t)$ where $t, u, v \in \mathbb R$. Then by the chain rule, $g'(t) = F'(tz)z = 0$ and hence $u'(t) + iv'(t) = 0$, giving $u'(t) = 0$ and $v'(t) = 0$ everywhere. We can now apply the real-valued case, showing that $u$ and $v$ (and hence $F$) are constant everywhere. Now, let $a, b \in \mathbb C$, and consider
\[ F(z) = e(a + b - z)e(z) \]
Then
\[ F'(z) = -e(a+b-z)e(z) + e(a+b-z)e(z) = 0 \]
Hence $e(a + b - z)e(z)$ is constant for all $z$, hence
\[ e(a + b - z)e(z) = e(a + b - 0)e(0) = e(a + b) \]
Since $z$ is arbitrary, we can set $z=b$ to recover the familiar relation
\[ e(a+b-b)e(b) = e(a+b) \implies e(a)e(b) = e(a+b) \]

\section{Exponents, Logarithms and Powers}
\subsection{Exponential and Logarithmic Functions}
Last lecture, we covered the power series form of the exponential function $e \colon \mathbb C \to \mathbb C$. Note that if we input a real number, the output is also real. Hence, $e \colon \mathbb R \to \mathbb R$. This restricted definition of the function has the following properties.
\begin{theorem}
    \begin{enumerate}[(i)]
        \item $e \colon \mathbb R \to \mathbb R$ is everywhere differentiable, and $e'(x) = e(x)$.
        \item $e(x+y) = e(x)e(y)$.
        \item $e(x) > 0$.
        \item $e$ is strictly increasing.
        \item $e(x) \to \infty$ as $x \to \infty$, and $e(x) \to 0$ as $x \to -\infty$.
        \item $e \colon \mathbb R \to (0, \infty)$ is a bijection.
    \end{enumerate}
\end{theorem}
\begin{proof}
    The first two properties follow from the last lecture.
    \begin{enumerate}[(i)]
        \setcounter{enumi}{2}
        \item Clearly, $e(x) > 0$ for all $x \geq 0$ by considering the power series, which contains only positive terms for $x>0$, and also $e(0) = 1$. Also, $e(0) = e(x - x) = e(x)e(-x)$, hence for all negative $x$, $e(x) > 0$.
        \item Since $e'(x) = e(x)$, $e'(x) = e(x) > 0$ everywhere.
        \item By considering partial sums, if $x>0$ we have $e(x) > 1+x$, so if $x \to \infty$, $e(x) \to \infty$. When $x \to -\infty$, $e(x) = \frac{1}{e(x)} \to 0$.
        \item Injectivity follows from being strictly increasing. For surjectivity, we need to show that given any $y \in (0, \infty)$ there exists some $x$ such that $e(x) = y$. Due to property (v) above, we can certainly find real numbers $a$ and $b$ such that $e(a) < y < e(b)$. By the intermediate value theorem, there exists $x \in \mathbb R$ such that $e(x) = y$.
    \end{enumerate}
\end{proof}
\begin{remark}
    We have essentially proven that $e \colon (\mathbb R, +) \to ((0, \infty), \times)$ is a group isomorphism. This is exactly the same as showing that it is a bijection. Since $e$ is a function, there exists an inverse function $\ell \colon ((0, \infty), \times) \to (\mathbb R, +)$.
\end{remark}
\begin{theorem}
    \begin{enumerate}[(i)]
        \item $\ell \colon (0, \infty) \to \mathbb R$ is a bijection, and $\ell(e(x)) = x$ for all $x \in \mathbb R$, and $e(\ell(x)) = x$ for all $x \in (0, \infty)$.
        \item $\ell$ is differentiable and its derivative is $\ell'(t) = \frac{1}{t}$.
        \item $\ell(xy) = \ell(x) + \ell(y)$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}[(i)]
        \item This first propety is obvious from the definition.
        \item By the inverse function theorem, $\ell$ is differentiable everywhere and $\ell'(t) = \frac{1}{t}$ as required.
        \item From IA Groups, if $e$ is an isomorphism, so is its inverse.
    \end{enumerate}
\end{proof}

\subsection{Real Numbered Exponents}
We will now define for $\alpha \in \mathbb R$ and $x > 0$ the function
\[ r_\alpha(x) = e(\alpha \ell(x)) \]
This can be taken as the definition of $x$ raised to the power $\alpha$.
\begin{theorem}
    Suppose $x, y > 0$ and $\alpha, \beta \in \mathbb R$. Then
    \begin{enumerate}[(i)]
        \item $r_\alpha(xy) = r_\alpha(x)r_\alpha(y)$
        \item $r_{\alpha + \beta}(x) = r_\alpha(x) r_\beta(x)$
        \item $r_\alpha(r_\beta(x)) = r_{\alpha\beta}(x)$
        \item $r_1(x) = x$, and $r_0(x) = 1$
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}[(i)]
        \item $r_\alpha(xy) = e(\alpha \ell(xy)) = e(\alpha \ell(x) + \alpha \ell(y)) = e(\alpha \ell(x))e(\alpha\ell(y)) = r_\alpha(x)r_\alpha(y)$
        \item $r_{\alpha + \beta}(x) = e((\alpha + \beta) \ell(x)) = e(\alpha\ell(x) + \beta\ell(x)) = e(\alpha\ell(x))e(\beta\ell(x)) = r_\alpha(x) r_\beta(x)$
        \item $r_\alpha(r_\beta(x)) = e(\alpha \ell[e(\beta \ell(x))]) = e(\alpha \beta \ell(x)) = r_{\alpha\beta}(x)$
        \item $r_1(x) = e(\ell(x)) = x$, and $r_0(x) = e(0 \ell(x)) = e(0) = 1$
    \end{enumerate}
\end{proof}
\noindent Suppose we want to compute $r_n(x)$, where $n \in\mathbb Z$. Then $r_n(x) = r_{1 + \dots + 1}(x) = x \cdots x$, so we have aggreement between $r_n(x)$ and our previous definition of $x^n$. Similarly, since $r_1(x) r_{-1}(x) = 1$, we have $r_{-1}(x) = \frac{1}{x}$. Further, $r_{\frac{1}{q}}(x) = x^\frac{1}{q}$. Therefore, $r_{\frac{p}{q}}(x) = x^{\frac{p}{q}}$. So this definition is simply a more general definition for exponentiation by a real number.

From now, we will let $\exp(x) \equiv e(x)$, $\log(x) \equiv \ell(x)$, and $x^\alpha \equiv r_\alpha(x)$. In fact, $\exp(x) = e^x$ for a suitable number $e$, since $e(x) = e(x \log(e)) = r_x(e) = e^x$ where $e := e(1) = \sum_0^\infty \frac{1}{n!}$.

Finally, we can compute the derivative of $x^\alpha$ using the chain rule.
\[ (x^\alpha)' = \left( e^{\alpha \log x} \right)' = e^{\alpha \log x} \alpha \frac{1}{x} = \alpha x^\alpha x^{-1} = \alpha x^{\alpha - 1} \]
as expected. Further, if $f(x) = a^x$, we can find
\[ f'(x) = \left( e^{x \log a} \right)' = e^{x \log a} \log a = a^x \log a \]

\section{Trigonometric Functions}
\subsection{Power Series Definitions}
We define
\begin{align*}
    \cos z & = 1 - \frac{z^2}{2!} + \frac{z^4}{4!} - \frac{z^6}{6!} + \dots = \sum_0^\infty \frac{(-1)^k z^{2k}}{(2k)!}     \\
    \sin z & = z - \frac{z^3}{3!} + \frac{z^5}{5!} - \frac{z^7}{7!} + \dots = \sum_0^\infty \frac{(-1)^k z^{2k+1}}{(2k+1)!} \\
\end{align*}
Both power series have infinite radius of convergence, by the ratio test (the same proof from the exponential function can be used here). Hence $\cos z$ and $\sin z$ are differentiable, and $\dv{z}\cos z = -\sin z$ and $\dv{z}\sin z = \cos z$ as expected, by termwise differentiation. Further, we can deduce that
\[ e^{iz} = \sum_0^\infty \frac{(iz)^n}{n!} = \sum_0^\infty \frac{(iz)^{2k}}{(2k)!} + \sum_0^\infty \frac{(iz)^{2k+1}}{(2k+1)!} \]
Note that
\[ (iz)^{2k} = (-1)^k z^{2k};\quad (iz)^{2k+1} = i (-1)^k z^{2k+1} \]
Hence,
\[ e^{iz} = \cos z + i \sin z \]
Similarly,
\[ e^{-iz} = \cos z - i \sin z \]
We can then write
\[ \cos z = \frac{1}{2}\qty( e^{iz} + e^{-iz} );\quad \sin z = \frac{1}{2i}\qty( e^{iz} - e^{-iz} ) \]
Many common trigonometric identities follow from this, such as the identity $\cos^2 z + \sin^2 z \equiv 1$. However, we have not deduced the period of the functions. Now, restricted to the real case, $\sin x, \cos x \in \mathbb R$, and the identity $\cos^2 z + \sin^2 z \equiv 1$ gives that $\abs{\sin x} \leq 1$ and $\abs{\cos x} \leq 1$ for all real $x$.

\subsection{Definition of $\pi$}
\begin{proposition}
    There is a smallest positive number $\pi$ such that
    \[ \cos \frac{\pi}{2} = 0 \]
    and we have $\sqrt{2} < \pi < \sqrt{3}$.
\end{proposition}
\begin{proof}
    If $0 < x < 2$,
    \[ \sin x = \qty(x - \frac{x^3}{3!}) + \qty(\frac{x^5}{5!} - \frac{x^7}{7!}) + \cdots \]
    For this range of values, each parenthesised block is positive, so $\sin x > 0$. So in this range,
    \[ \dv{x} \cos x < 0 \]
    Hence, $\cos x$ is a strictly decreasing function on this interval. Now,
    \[ \cos \frac{\sqrt{2}}{2} = \qty(\frac{\sqrt{2}^4}{4!} - \frac{\sqrt{2}^6}{6!}) + \cdots > 0 \]
    since each bracketed block is positive.
    \[ \cos \frac{\sqrt{3}}{2} = 1 - \frac{\sqrt{3}^2}{2!} + \frac{\sqrt{3}^4}{4!} - \qty(\frac{\sqrt{3}^6}{6!} - \frac{\sqrt{3}^8}{8!}) + \cdots < 0 \]
    since all the bracketed terms are positive, and being subtracted from a negative number. By the intermediate value theorem, the existence of such a $\pi$ follows.
\end{proof}
\begin{corollary}
    We have that $\sin \frac{\pi}{2} = 1$.
\end{corollary}
\begin{proof}
    We know that $\cos^2 \frac{\pi}{2} + \sin^2 \frac{\pi}{2} = 1$, and $\sin \frac{\pi}{2} > 0$, so the result follows.
\end{proof}
\begin{theorem}
    The following standard properties about the periodicity of trigonometric functions hold.
    \begin{enumerate}[(i)]
        \item $\sin(z + \frac{\pi}{2}) = \cos z$, and $\cos(z + \frac{\pi}{2}) = -\sin z$
        \item $\sin(z + \pi) = -\sin z$, and $\cos(z + \pi) = -\cos z$
        \item $\sin(z + 2 \pi) = \sin z$, and $\cos(z + 2\pi) = \cos z$
    \end{enumerate}
\end{theorem}
\noindent The proofs are immediate from the angle addition formulae. This then implies that
\[ e^{iz + 2\pi i} = e^{iz} \]
Hence $e^{z}$ is periodic with period $2 \pi i$.

\end{document}