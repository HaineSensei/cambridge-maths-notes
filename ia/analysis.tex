\documentclass{article}

\input{../util.tex}

\title{Analysis}
\author{Cambridge University Mathematical Tripos: Part IA}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Limits and Convergence: Reviewing Numbers and Sets}
\subsection{Definition of Limit}
\begin{definition}
    We say that the sequence $a_n \to a$ as $n \to \infty$ if given $\varepsilon > 0$, $\exists N$ such that $\abs{a_n - a} < \varepsilon$ for all $n \geq N$. Note that this $N$ is actually a function of $\varepsilon$; we may need to choose a very large $N$ if the $\varepsilon$ provided is very small, for instance.
\end{definition}
\begin{definition}
    An increasing sequence is a sequence for which $a_n \leq a_{n+1}$, and a decreasing sequence is a sequence for which $a_n \geq a_{n+1}$. Such increasing and decreasing sequences are called monotone.
    A strictly increasing sequence or a strictly decreasing sequence simply strengthens the inequalities to not include the equality case.
\end{definition}

\subsection{Fundamental Axiom of the Real Numbers}
If we have some increasing sequence $a_n \in \mathbb R$, where $\exists A \in \mathbb R$ such that $\forall n \geq 1$, $a_n \leq A$, then $\exists a \in \mathbb R$ such that $a_n \to a$ as $n \to \infty$. This is also known as the `least upper bound' axiom or property. This axiom applies equivalently to decreasing sequences of real numbers bounded below. We can also rephrase the axiom to state that every non-empty set of real numbers that is bounded above has a supremum.
\begin{definition}
    We say that the supremum $\sup S$ of a non-empty, bounded above set $S$ is $K$ if
    \begin{enumerate}[(i)]
        \item $x \leq K$ for all $x \in S$
        \item given $\varepsilon > 0$, $\exists x \in S$ such that $x > K - \varepsilon$
    \end{enumerate}
\end{definition}
Note that the supremum (and hence the infimum) is unique.

\subsection{Properties of Limits}
\begin{lemma}
    The following properties about real sequences hold.
    \begin{enumerate}[(i)]
        \item The limit is unique. That is, if $a_n \to a$ and $a_n \to b$, then $a = b$.
        \item If $a_n \to a$ as $n \to \infty$ and $n_1 < n_2 < \dots$, then $a_{n_j} \to a$ as $j \to \infty$. In other words, subsequences converge to the same limit.
        \item If $a_n = c$ for all $n$, then $a_n \to c$ as $n \to \infty$.
        \item If $a_n \to a$ and $b_n \to b$, then $a_n + b_n \to a + b$.
        \item If $a_n \to a$ and $b_n \to b$, then $a_nb_n \to ab$.
        \item If $a_n \to a$, $a_n \neq 0$ for all $n$, and $a \neq 0$, then $\frac{1}{a_n} \to \frac{1}{a}$.
        \item If $a_n \to a$, and $a_n \leq A$ for all $n$, then $a \leq A$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    We prove the some of these statements here.
    \begin{enumerate}[(i)]
        \item Given $\varepsilon > 0$, $\exists n_1$ such that $\abs{a_n - a} < \varepsilon$ for all $n \geq n_1$, and $\exists n_2$ such that $\abs{a_n - b} < \varepsilon$ for all $n \geq n_2$. So let $N = \max(n_1, n_2)$, so both inequalities hold. Then for all $n \geq N$, using the triangle inequality, $\abs{a - b} \leq \abs{a_n - a} + \abs{a_n - b} < 2\varepsilon$. So $a=b$.
        \item Given $\varepsilon > 0$, $\exists N$ such that $\abs{a_n - a} < \varepsilon$ for all $n \geq N$. Since $n_j \geq j$ (by induction), $\abs{a_{n_j} - a} < \varepsilon$ for all $j \geq N$.
              \setcounter{enumi}{4}
        \item $\abs{a_nb_n - ab} \leq \abs{a_nb_n - a_nb} + \abs{a_nb - ab} = \abs{a_n}\abs{b_n - b} + \abs{b}\abs{a_n - a}$.

              If $a_n \to a$, then given $\varepsilon > 0$, $\exists N_1$ such that $\abs{a_n - a} < \varepsilon$ for all $n \geq N_1$. ($\ast$)

              If $b_n \to b$, then given $\varepsilon > 0$, $\exists N_2$ such that $\abs{b_n - b} < \varepsilon$ for all $n \geq N_2$.

              Using ($\ast$), if $n \geq N_1(1)$ (i.e. $\varepsilon = 1$), $\abs{a_n - a} < 1$, so $\abs{a_n} \leq \abs{a} + 1$.

              Therefore $\abs{a_n b_n - ab} \leq \varepsilon(\abs{a} + 1 + \abs{b})$ for all $n \geq N_3(\varepsilon) = \max\{ N_1(1), N_1(\varepsilon), N_2(\varepsilon) \}$.
    \end{enumerate}
\end{proof}

\subsection{Harmonic Series}
\begin{lemma}
    The sequence $\frac{1}{n}$ tends to zero as $n \to \infty$.
\end{lemma}
\begin{proof}
    We know that $\frac{1}{n}$ is a decreasing sequence, and it is bounded below by zero. Hence it converges to a limit $a$. We will prove now that $a = 0$. $\frac{1}{2n} = \frac{1}{2}\cdot \frac{1}{n}$, and by property (v) above, $\frac{1}{2n}$ tends to $\frac{1}{2}\cdot a$. But $\frac{1}{2n}$ is a subsequence of $\frac{1}{n}$, and so by property (ii) it converges to $a$. So by property (i), $\frac{1}{2} \cdot a = a$ hence $a=0$.
\end{proof}

\subsection{Limits in the Complex Plane}
\begin{remark}
    The definition of the limit of a sequence makes perfect sense for $a_n \in \mathbb C$.
\end{remark}
\begin{definition}
    $a_n \to a$ if given $\varepsilon > 0$, $\exists N$ such that $\forall n \geq N$, $\abs{a_n - a} < \varepsilon$.
\end{definition}
From this definition, it is easy to check that properties (i)--(vi) hold for complex numbers. However, property (vii) makes no sense in the world of the complex numbers since they do not have an ordering.

\section{More on Convergence}
\subsection{The Bolzano-Weierstrass Theorem}
\begin{theorem}
    If $x_n$ is a sequence of real numbers, and there exists some $k$ such that $\abs{x_n} \leq k$ for all $n$, then we can find $n_1 < n_2 < n_3 < n_4 < \dots$ and $x \in \mathbb R$ such that $x_{n_j} \to x$ as $j \to \infty$. In other words, any bounded sequence has a convergent subsequence.
\end{theorem}
\begin{remark}
    This theorem does not state anything about the uniqueness of such a subsequence; indeed, there could exist many subsequences that have possibly different limits. For example, $x_n = (-1)^n$ gives $x_{2n+1} \to -1$ and $x_{2n} \to 1$.
\end{remark}
\begin{proof}
    Let $[a_1, b_1]$ be the range of the sequence, i.e. $[-k, k]$. Then let the midpoint $c_1 = \frac{a_1 + b_1}{2}$. Consider the following alternatives:
    \begin{enumerate}
        \item $x_n \in [a_1, c]$ for infinitely many values of $n$.
        \item $x_n \in [c, b_1]$ for infinitely many values of $n$.
    \end{enumerate}
    Note that cases 1 and 2 could hold at the same time. If case 1 holds, we set $a_2 = a_1$ and $b_2 = c$. If case 1 fails, then case 2 must hold, so we can set $a_2 = c$ and $b_2 = b_1$. We have now constructed a subsequence whose range is half as large as the original sequence, and it contains infinitely many values of $x_n$.

    We can proceed inductively to construct sequences $a_n, b_n$ such that $x_m \in [a_n, b_n]$ for infinitely many values of $m$. This is known as a `bisection method'. By construction, $a_{n-1} \leq a_n \leq b_n \leq b_{n-1}$. Since we are dividing by two each time,
    \[ b_n - a_n = \frac{1}{2}(b_{n-1} - a_{n-1}) \tag{$\ast$} \]
    Note that $a_n$ is a bounded, increasing sequence; and $b_n$ is a bounded, decreasing sequence. By the Fundamental Axiom of the Real Numbers, $a_n$ and $b_n$ converge to limits $a \in [a_1, b_1]$ and $b \in [a_1, b_1]$. Using $(\ast)$, $b-a = \frac{b-a}{2} \implies b = a$.

    Since $x_m \in [a_n, b_n]$ for infinitely many values of $m$, having chosen $n_j$ such that $x_{n_j} \in [a_j, b_j]$, there is $n_{j+1} > n_j$ such that $x_{n_{j+1}} \in [a_{j+1}, b_{j+1}]$. Informally, this works because we have an unlimited supply of such $x$ values. Hence
    \[ a_j \leq x_{n_j} \leq b_j \]
    So this $x_{n_j} \to a$, so we have constructed a convergent subsequence.
\end{proof}

\subsection{Cauchy Sequences}
\begin{definition}
    A sequence $a_n$ is called a Cauchy sequence if given $\varepsilon > 0$ there exists $N > 0$ such that $\abs{a_n - a_m} < \varepsilon$ for all $n, m \geq N$. Informally, the terms of the sequence grow ever closer together such that there are infinitely many consecutive terms within a small region.
\end{definition}
\begin{lemma}
    If a sequence converges, it is a Cauchy sequence.
\end{lemma}
\begin{proof}
    If $a_n \to a$, given $\varepsilon > 0$ then $\exists N$ such that $\forall n \geq N, \abs{a_n - a} < \varepsilon$. Then take $m, n \geq N$, and we have
    \[ \abs{a_n - a_m} \leq \abs{a_n - a} + \abs{a_m - a} < 2\varepsilon \]
\end{proof}
\begin{theorem}
    Every Cauchy sequence converges.
\end{theorem}
\begin{proof}
    First, we note that if $a_n$ is a Cauchy sequence then it is bounded. Let us take $\varepsilon = 1$, so $N = N(1)$ in the Cauchy property. Then
    \[ \abs{a_n - a_m} < 1 \]
    for all $m, n \geq N(1)$. So by the triangle inequality,
    \[ \abs{a_m} \leq \abs{a_m - a_N} + \abs{a_N} < 1 + \abs{a_N} \]
    So the sequence after this point is bounded by $1 + \abs{a_N}$. The remaining terms in the sequence are only finitely many, so we can compute the maximum of all of those terms along with $1+\abs{a_N}$ to produce a bound $k$ for all $n$.

    By the Bolzano-Weierstrass Theorem, this sequence $a_n$ has a convergent subsequence $a_{n_j} \to a$. We want to prove that $a_n \to a$. Given $\varepsilon > 0$, there exists $j_0$ such that $\abs{a_{n_j} - a} < \varepsilon$ for all $j \geq j_0$. Also, $\exists N(\varepsilon)$ such that $\abs{a_m - a_n} < \varepsilon$ for all $m, n \geq N(\varepsilon)$. Combining these, we can take a $j$ such that $n_j \geq \max \{ N(\varepsilon), n_{j_0} \}$. Then, if $n \geq N(\varepsilon)$, using the triangle inequality,
    \[ \abs{a_n - a} \leq \abs{a_n - a_{n_j}} + \abs{a_{n_j} - a} < 2\varepsilon \]
\end{proof}
\noindent Therefore, on $\mathbb R$, a sequence is convergent if and only if it is a Cauchy sequence. This is sometimes referred to as the general principle of convergence, however this is a relatively old-fashioned name. This property is very useful, since we don't need to know what the limit actually is.

\subsection{Series}
Let $a_n$ be a real or complex sequence. We say that $\sum_{j=1}^\infty a_j$ converges to $s$ if the sequence of partial sums $s_N$ converges to $s$ as $N \to \infty$, i.e.
\[ s_N = \sum_{j=1}^N a_j \to s \]
If the sequence of partial sums does not converge, then we say that the series diverges. Note that any problem on series can be turned into a problem on sequences, by considering their partial sums.
\begin{lemma}
    \begin{enumerate}[(i)]
        \item If $\sum_{j=1}^\infty a_j$ and $\sum_{j=1}^\infty b_j$ converge, then so does $\sum_{j=1}^\infty (\lambda a_j + \mu b_j)$, where $\lambda, \mu \in \mathbb C$.
        \item Suppose $\exists N$ such that $a_j = b_j$ for all $j \geq N$. Then either $\sum_{j=1}^\infty a_j$ and $\sum_{j=1}^\infty b_j$ both converge, or they both diverge. In other words, the initial terms do not matter for considering convergence (but the sum will change).
    \end{enumerate}
\end{lemma}
\begin{proof}
    \begin{enumerate}[(i)]
        \item We have
              \begin{align*}
                  s_N            & = \sum_{j=1}^\infty (\lambda a_j + \mu b_j)                 \\
                                 & = \sum_{j=1}^\infty \lambda a_j + \sum_{j=1}^\infty \mu b_j \\
                                 & = \lambda c_N + \mu d_N                                     \\
                  \therefore s_N & \to \lambda c + \mu d
              \end{align*}
        \item For any $n \geq N$, we have
              \begin{align*}
                  s_N & = \sum_{j=1}^n a_j = \sum_{j=1}^{N-1} a_j + \sum_{j=n}^N a_j \\
                  d_N & = \sum_{j=1}^n b_j = \sum_{j=1}^{N-1} b_j + \sum_{j=n}^N b_j \\
              \end{align*}
              Taking the difference, we get
              \[ s_N - d_N = \sum_{j=1}^{N-1} a_j - \sum_{j=1}^{N-1} b_j \]
              which is finite. So $s_N$ converges if and only if $d_N$ also converges.
    \end{enumerate}
\end{proof}

\section{Convergence Tests}
\subsection{Geometric Series}
Let $a_n = x^{n-1}$, where $n \geq 1$. Then
\[ s_n = \sum_{j=1}^n a_j = 1 + x + x^2 + \dots + x^{n-1} \]
Then
\[ s_n = \begin{cases}
        \frac{1 - x^n}{1 - x} & \text{if } x \neq 1 \\
        n                     & \text{if } x = 1
    \end{cases} \]
This can be shown by observing that
\[ x s_n = x + x^2 + \dots + x^n = s_n - 1 + x^n \implies s_n(1-x) = 1-x^n \]
If $\abs{x} < 1$, then $x^n \to 0$ as $x \to \infty$. So $s_n \to \frac{1}{1-x}$. If $x > 1$, then $x^n \to \infty$ and so $s_n \to \infty$. If $x < -1$, $s_n$ oscillates. For completeness, if $x=-1$, $s_n$ oscillates between 0 and 1.

Note that the statement $s_n \to \infty$ means that given $a \in \mathbb R$, $\exists N$ such that $s_n > a$ for all $n \geq N$, and a similar statement holds for negative infinity (swapping the inequality). If $s_n$ does not converge or tend to $\pm \infty$, we say that $s_n$ oscillates.

Thus the geometric series converges if and only if $\abs{x} < 1$. Note that to prove that $x^n \to 0$ if $\abs{x} < 1$, we can consider the caase $0 < x < 1$ and write $1/x = 1 + \delta$ for some positive $\delta$. Then $x^n = \frac{1}{(1 + \delta)^n} \leq \frac{1}{1 + \delta n}$ from the binomial expansion, and this tends to zero as required.

\begin{lemma}
    If $\sum_{j=1}^\infty a_j$ converges, then $\lim_{j \to \infty} a_j = 0$.
\end{lemma}
\begin{proof}
    Given $s_n = \sum_{j=1}^n a_j$, we have $a_n = s_n - s_{n-1}$. If $s_n \to a$, then $a_n \to 0$ since $s_{n-1}$ also tends to $a$.
\end{proof}
\begin{remark}
    The converse is not true. For example, the harmonic series diverges, but the terms approach zero. Consider
    \begin{align*}
        s_{2n} & = s_n + \frac{1}{n+1} + \frac{1}{n+2} + \dots + \frac{1}{2n} \\
               & > s_n + \frac{1}{2n} + \frac{1}{2n} + \dots + \frac{1}{2n}   \\
               & = s_n + \frac{1}{2}
    \end{align*}
    So as $n \to \infty$, if the sequence is convergent then the sequences $s_n$ and $s_{2n}$ tend to the same limit, but they clearly do not.
\end{remark}

\subsection{Comparison Test}
In this section, we will let $a_n \in \mathbb R, a_n \geq 0$. In other words, all series contain only non-negative real terms.
\begin{theorem}
    Suppose $0 \leq b_n \leq a_n$ for all $n$. If $\sum_{j=1}^\infty a_j$ converges, then $\sum_{j=1}^\infty b_j$ converges.
\end{theorem}
\begin{proof}
    Let $s_N$ be the $N$th partial sum over the $a_n$, and let $d_N$ be the $N$th partial sum over the $d_N$. Since $b_n \leq a_n, d_N \leq s_N$. But $s_N \to s$, so $d_N \leq s_N \leq s$. So $d_N$ is an increasing sequence that is bounded above by $s$, so it converges.
\end{proof}
\noindent For example, let us analyse the behaviour of the sum of the sequence $\frac{1}{n^2}$. Note that
\[ \frac{1}{n^2} < \frac{1}{n(n-1)} = \frac{1}{n-1} - \frac{1}{n} \]
for $n \geq 2$. By the comparison test, it is sufficient to show that the series on the right hand side converges, in order to show that the original series converges.
\[ \sum_{j=2}^N a_j = 1 - \frac{1}{N} \to 1 \]
as required. So the original series tends to some value less than or equal to 2.

\subsection{Root Test or Cauchy's Test}
\begin{theorem}
    Suppose we have a sequence of non-negative terms $a_n$. Suppose that $a_n^{1/n} \to a$ as $n \to \infty$. Then if $a < 1$, the series $\sum a_n$ converges. If $a > 1$, the series $\sum a_n$ diverges.
\end{theorem}
\begin{remark}
    Nothing can be said if $a=1$. There is an example later of this fact.
\end{remark}
\begin{proof}
    If $a < 1$, let us choose an $r$ such that $a < r < 1$. By the definition of the limit, $\exists N$ such that $\forall n \geq N$, $a_n^{1/n} < r$. This implies that $a_n < r^n$. The geometric series $\sum r^n$ converges. By comparison, the series $a_n$ converges.

    If $a > 1$, for all $n \geq N$, $a_n^{1/n} > 1$ which implies $a_n > 1$, thus $\sum a_n$ diverges, since $a_n$ does not tend to zero.
\end{proof}

\subsection{Ratio Test or d'Alembert's Test}
\begin{theorem}
    Suppose $a_n > 0$, and $\frac{a_{n+1}}{a_n} \to \ell$. If $\ell < 1$, then the series $\sum a_n$ converges. If $\ell > 1$, then the series $\sum a_n$ diverges.
\end{theorem}
\begin{remark}
    Like before, no conclusion can be drawn if $\ell = 1$.
\end{remark}
\begin{proof}
    Suppose $\ell < 1$. We can choose $\ell < r < 1$, $\exists N$ such that $\forall n \geq N$, $\frac{a_{n+1}}{a_n} < r$. Therefore $a_n < r^{n-N} a_N$. Hence, $a_n < k r^n$ where $k$ is independent of $n$. Applying the comparison test, the series $\sum a_n$ must converge.

    If $\ell > 1$, we can choose $\ell > r > 1$. Then $\exists N$ such that $\forall n \geq N$, $\frac{a_{n+1}}{a_n} > r$. As before, $a_n > r^{n-N} a_N$. But the $r^{n-N}$ diverges, so the original series diverges.
\end{proof}

\section{More Convergence Tests}
\subsection{Examples of Ratio and Root Tests}
Consider $\sum_1^\infty \frac{n}{2^n}$. We have
\[ \frac{a_{n+1}}{a_n} = \frac{(n+1)/2^{n+1}}{n/2^n} \to \frac{1}{2} \]
So we have convergence, by the ratio test. Now, consider $\sum_1^\infty \frac{1}{n}$ and $\sum_1^\infty \frac{1}{n^2}$. In both cases, the ratio test gives limit 1. So the ratio test is inconclusive if the limit is 1. Since $n^{1/n} \to 1$, the root test is also inconclusive when the limit is 1. To check this limit, we can write
\[ n^{1/n} = 1 + \delta_n;\quad \delta_n > 0 \]
\[ n = (1 + \delta_n)^n > \frac{n(n-1)}{2}\delta_n^2 \]
using the binomial expansion.
\[ \implies \delta_n^2 < \frac{2}{n-1} \implies \delta_n \to 0 \]
The root test is a good candidate for series that contain powers of $n$, for example
\[ \sum_1^\infty \left[ \frac{n+1}{3n+5} \right]^n \]
In this instance, for example, we have convergence.

\subsection{Cauchy's Condensation Test}
\begin{theorem}
    Let $a_n$ be a decreasing sequence of positive terms. Then $\sum_1^\infty a_n$ converges if and only if $\sum_1^\infty 2^n a_{2^n}$ converges.
\end{theorem}
\begin{proof}
    First, note that if $a_n$ is decreasing, then
    \[ a_{2^k} \underset{(\ast)}{\leq} a_{2^{k-1} + i} \underset{(\dagger)}{\leq} a_{2^{k-1}};\quad 1 \leq i \leq 2^{k-1};\quad k \geq 1 \]
    Now let us assume that $\sum a_n$ converges to $A \in \mathbb R$. Then, by $(\ast)$,
    \begin{align*}
        2^{n-1} a_{2^n} & = a_{2^n} + a_{2^n} + \dots + a_{2^n}                \\
                        & \leq a_{2^{n-1}+1} + a_{2^{n-1}+2} + \dots + a_{2^n} \\
                        & = \sum_{m=2^{n-1}+1}^{2^n}a_m
    \end{align*}
    Thus,
    \[ \sum_{n=1}^N 2^{n-1}a_{2^n} \leq \sum_{n=1}^N \sum_{m=2^{n-1}+1}^{2^n} a_m = \sum_{n=2}^{2^N} a_m \]
    Therefore,
    \[ \sum_{n=1}^N 2^n a_{2^n} \leq 2 \sum_{n=2}^{2^N} a_m \leq 2(A-a_1) \]
    Thus $\sum_{n=1}^N 2^n a_{2^n}$ converges, since it is increasing and bounded above. For the converse, we will assume that $\sum 2^n a_{2^n}$ converges to $B$. Using $(\dagger)$,
    \begin{align*}
        \sum_{m=2^{n-1}}^{2^n} a_m & = a_{2^n} + a_{2^n} + \dots + a_{2^n}                \\
                                   & \leq a_{2^{n-1}} + a_{2^{n-1}} + \dots + a_{2^{n-1}} \\
                                   & = 2^{n-1}a_{2^{n-1}}
    \end{align*}
    So we have
    \[ \sum_{m=2}^{2^N} a_m = \sum_{n=1}^N \sum_{m=2^{n-1}+1}^{2^n} a_m \leq \sum_{n=1}^N 2^{n-1} a_{2^{n-1}} \leq \frac{1}{2} B \]
    Therefore, $\sum_{m=1}^N a_m$ is a bounded, increasing sequence and hence converges.
\end{proof}
\noindent Let us consider an example of this test. Consider the series definition of the Riemann zeta function
\[ \zeta(k) = \sum_{n=1}^\infty \frac{1}{n^k} \]
For what $k \in \mathbb R, k>0$ does this series converge? This is equivalent to asking if the following series converges.
\[ \sum_{n=1}^\infty 2^n \left[ \frac{1}{2^n} \right]^k = \sum_{n=1}^\infty \left( 2^{1-k} \right)^n \]
Hence it converges if and only if $2^{1-k} < 1 \iff k > 1$.

\subsection{Alternating Series}
An alternating series is a series where the sign on each term switches between positive and negative.
\begin{theorem}[Alternating Series Test]
    If $a_n$ decreases and tends to zero as $u \to \infty$, then the alternating series
    \[ \sum_1^\infty (-1)^{n+1} a_n \]
    converges.
\end{theorem}
\begin{proof}
    Let us consider the partial sum
    \[ s_n = a_1 - a_2 + a_3 - a_4 + \dots + (-1)^{n+1}a_n \]
    In particular,
    \[ s_{2n} = (a_1 - a_2) + (a_3 - a_4) + \dots + (a_{2n-1} - a_{2n}) \]
    Since the sequence is decreasing, each parenthesised block is positive. Then $s_{2n} \geq s_{2n-2}$. We can also write the partial sum as
    \[ s_{2n} = a_1 - (a_2 - a_3) - (a_4 - a_5) - \dots - (a_{2n-2} - a_{2n-1}) - a_{2n} \]
    Each parenthesised block here is negative. So $s_{2n} \leq a_1$. So $s_{2n}$ is increasing and bounded above, so it must converge. Now, note that
    \[ s_{2n+1} = s_{2n} + a_{2n+1} \to s_{2n} \]
    since $a_{2n+1} \to 0$. So $s_{2n+1}$ also converges, in fact to the same limit. Hence $s_n$ converges to this same limit.
\end{proof}

\section{Absolute Convergence}
\subsection{Absolute Convergence}
\begin{definition}
    Let $a_n \in \mathbb C$. Then if $\sum_{n=1}^\infty \abs{a_n}$ converges, then the series is called absolutely convergent.
\end{definition}
\begin{remark}
    Since $\abs{a_n} \geq 0$, we can use the previous tests to check for absolute convergence.
\end{remark}
\begin{theorem}
    Let $a_n \in \mathbb C$. If this series is absolutely convergent, it is convergent.
\end{theorem}
\begin{proof}
    Suppose first that $a_n$ is a sequence of real numbers. Then let
    \[ v_n = \begin{cases}
            a_n & \text{if } a_n \geq 0 \\
            0   & \text{if } a_n < 0
        \end{cases};\quad w_n = \begin{cases}
            0    & \text{if } a_n \geq 0 \\
            -a_n & \text{if } a_n < 0
        \end{cases} \]
    Hence,
    \[ v_n = \frac{\abs{a_n} + a_n}{2};\quad w_n = \frac{\abs{a_n} - a_n}{2} \]
    Clearly, $v_n, w_n \geq 0$, and $a_n = v_n - w_n$, and $\abs{a_n} = v_n + w_n$. If $\sum \abs{a_n}$ converges, then by comparison $\sum v_n$ and $\sum w_n$ also converge, and hence $\sum a_n$ converges. Now, let us consider the case where $a_n$ is complex. Then we can write $a_n = x_n + iy_n$ where $x_n, y_n$ are real sequences. Note that $\abs{x_n}, \abs{y_n} \leq \abs{a_n}$. So by comparison $x_n$ and $y_n$ converge, so $a_n$ converges.
\end{proof}
\noindent Here are some examples.
\begin{enumerate}
    \item The alternating harmonic series $\sum \frac{(-1)^n}{n}$ is convergent, but not absolutely convergent.
    \item $\sum \frac{z^n}{2^n}$ is absolutely convergent when $\abs{z} < \abs{2}$, because it reduces to a real geometric series. If $\abs{z} \geq 2$, then $\abs{a_n} \geq 1$, so we do not have absolute convergence.
\end{enumerate}

\subsection{Conditional Convergence and Rearrangement}
If the series is convergent but not absolutely convergent, it is called \textit{conditionally} convergent. The sum to which a series converges depends on the order in which the terms are added.
\begin{definition}
    Let $\sigma$ be a bijection of the positive integers to itself, then
    \[ a_n' = a_{\sigma(n)} \]
    is a rearrangement of $a_n$.
\end{definition}
\begin{theorem}
    If $\sum_1^\infty a_n$ is absolutely convergent, then every rearrangement of this series converges to the same value.
\end{theorem}
\begin{proof}
    First, let us consider the real case. Let $\sum a_n'$ be a rearrangement of $\sum a_n$. Let $s_n = \sum_1^n a_n$, and $t_n = \sum_1^n a_n'$. Let $s_n$ converge to $s$. Suppose first that $a_n \geq 0$. Then given any $n \in \mathbb N$, we can find some $q \in \mathbb N$ such that $s_q$ contains every term of $t_n$. Since the $a_n \geq 0$,
    \[ t_n \leq s_q \leq s \]
    As $n \to \infty$, the $t_n$ is an increasing sequence bounded above, so it must tend to a limit $t$, where $t \leq s$. Note, however, that this argument is symmetric; we can equally derive that $s \leq t$. Therefore $s = t$.

    Now, let us drop the condition that $a_n \geq 0$. We can now consider $v_n, w_n$ from above:
    \[ v_n = \frac{\abs{a_n} + a_n}{2};\quad w_n = \frac{\abs{a_n} - a_n}{2} \]
    Since $\sum\abs{a_n}$ converges, both $\sum v_n, \sum w_n$ converge. Since all $v_n, w_n \geq 0$, we can deduce that $\sum v_n = \sum v_n'$ and $\sum w_n' = \sum w_n$. The claim follows since $a_n = v_n - w_n$.

    For the case $a_n \in \mathbb C$, we can write $a_n = x_n + iy_n$, noting that $\abs{x_n}, \abs{y_n} \geq \abs{a_n}$. By comparison, the series $\sum x_n, \sum y_n$ are absolutely convergent, and by the previous case, $\sum x_n = \sum x_n'$ and $\sum y_n' = \sum y_n'$. Since $a_n' = x_n' + y_n'$, $\sum a_n = \sum a_n'$ as required.
\end{proof}

\section{Continuity}
\subsection{Definitions}
Let $E \subseteq \mathbb C$ be a non-empty set, and $f \colon E \to \mathbb C$ be any function, and let $a \in E$. Certainly, this includes the case in which $f$ is a real-valued function and $E \subseteq \mathbb R$.
\begin{definition}
    $f$ is continuous at $a$ if for every sequence $z_n \in E$ that converges to $a$, we have $f(z_n) \to f(a)$.
\end{definition}
\noindent We can use an alternative definition:
\begin{definition}[$\varepsilon$-$\delta$ definition]
    $f$ is continuous at $a$ if given $\varepsilon > 0$, $\exists \delta > 0$ such that for every $z \in E$, if $\abs{z - a} < \delta$, then $\abs{f(z) - f(a)} < \varepsilon$.
\end{definition}
\noindent We will immediately prove that both definitions are equivalent. First, let us prove that the $\varepsilon$-$\delta$ definition implies the first definition.
\begin{proof}
    We know that given $\varepsilon > 0, \exists \delta > 0$ such that for all $z \in E$, $\abs{z - a} < \delta$ implies $\abs{f(z) - f(a)} < \varepsilon$. Let $z_n \to a$, then by the definition of the limit of the sequence then there exists $n_0$ such that for all $n \geq n_0$ we have $\abs{z_n - a} < \delta$. But this implies that $\abs{f(z_n) - f(a)} < \varepsilon$, i.e. $f(z_n) \to f(a)$.
\end{proof}
\noindent We now prove the converse, that the first definition implies the second.
\begin{proof}
    We know that for every sequence $z_n \in E$ that converges to $a$, $f(z_n) \to f(a)$. Suppose $f$ is not continuous at $a$, according to the $\varepsilon$-$\delta$ definition. Then there exists some $\varepsilon$ such that for all $\delta > 0$, there exists $z \in E$ such that $\abs{z - a} < \delta$ but $\abs{f(z) - f(a)} \geq \varepsilon$. So, let us construct a sequence of $\delta$ values to substitute into this definition. Let $\delta = 1/n$. Then the $z_n$ given by this $\delta$ is such that $\abs{z_n - a} < 1/n$ and $\abs{f(z_n) - f(a)} \geq \varepsilon$. Clearly, $z_n \to a$, but $f(z_n)$ does not tend to $f(a)$ because the difference between the two is always greater than $\varepsilon$. This is a contradiction, since we assumed that $f$ is continuous by the first definition. So $f$ is continuous by the $\varepsilon$-$\delta$ definition.
\end{proof}

\subsection{Making Continuous Functions}
We can create new continuous functions from old ones by manipulating them in a number of ways.
\begin{proposition}
    Let $g, f \colon E \to \mathbb C$ be continous functions at a point $a \in E$. Then all of the functions
    \begin{itemize}
        \item $f(z) + g(z)$
        \item $f(z)g(z)$
        \item $\lambda f(z)$ for some constant $\lambda$
    \end{itemize}
    are all continuous. In addition, if $f(z) \neq 0$ everywhere in $E$, then $\frac{1}{f}$ is a continuous function at $a$.
\end{proposition}
\begin{proof}
    Using the first definition, this is obvious using the fact that limits of sequences behave analogously.
\end{proof}
\noindent Trivially, the function $f(z) = z$ is continuous. From this, we can derive that every polynomial is continuous at every point in $\mathbb C$. Note that we say that $f$ is continuous on the entire set $E$ if it is continuous at every point $a \in E$.

\subsection{Composition of Continuous Functions}
\begin{theorem}
    Let $f \colon A \to \mathbb C$ and $g \colon B \to \mathbb C$ where $A, B \subseteq \mathbb C$ be two functions that can be composed, i.e. $f(A) \subseteq B$. If $f$ is continuous at $a \in A$ and $g$ is continuous at $f(a) \in B$, then $g \circ f \colon A \to \mathbb C$ is continuous at $a$.
\end{theorem}
\begin{proof}
    Take any sequence $z_n \to a$. By assumption, $f(z_n) \to f(a)$. Now, let us define a new sequence $w_n = f(z_n)$. Then $w_n \in B$ and $w_n \to f(a)$. Thus, $g(f(z_n)) = g(w_n) \to g(f(a))$ by continuity, as required.
\end{proof}
\noindent Consider the function $f\colon \mathbb R \to \mathbb R$ defined by
\[ f(x) = \begin{cases}
        \sin\left( \frac{1}{x} \right) & x \neq 0 \\
        0                              & x = 0
    \end{cases} \]
This is assuming the knowledge of $\sin(x)$ being a continuous function $\mathbb R\to \mathbb R$, which we will prove later. So $f(x)$ is certainly continuous at every point on $\mathbb R$ excluding 0, since it is the composition of two continuous functions. We can prove it is discontinuous at $x=0$ by providing a sequence, for example
\[ \frac{1}{x_n} = \left(2n + \frac{1}{2}\right)\pi \]
Then $x_n \to 0$, and $f(x_n) = 1$. But $f(0) \neq 1$, so it is discontinuous. Let us modify the example as follows.
\[ f(x) = \begin{cases}
        x\sin\left( \frac{1}{x} \right) & x \neq 0 \\
        0                               & x = 0
    \end{cases} \]
We can prove that this sequence is continuous at 0. For an arbitrary sequence $x_n \to 0$, then $\abs{f(x_n)} \leq \abs{x_n}$ because $\abs{\sin x} \leq 1$. So $f(x_n)$ is bounded by $x_n$, which tends to zero, so $f(x_n)$ tends to zero as required. Now for a final example, let
\[ f(x) = \begin{cases}
        1 & x \in \mathbb Q    \\
        0 & x \notin \mathbb Q
    \end{cases} \]
This is discontinuous at every point. If $x \in \mathbb Q$, take a sequence $x_n \to x$ with all $x_n$ irrational, then $f(x_n) = 0$ but $f(x) = 1$. Similarly, if $x \notin \mathbb Q$, take a sequence $x_n \to x$ with all $x_n$ rational, then $f(x_n) = 1$ but $f(x) = 0$.

\section{Limit of a Function}
\subsection{Definition}
Let $f \colon E \subseteq \mathbb C \to \mathbb C$. We would like to define what is meant by $\lim_{z \to a} f(z)$, even when $a \notin E$. Further, if we have a set with an isolated point, for example $E = \{ 0 \} \cup [1, 2]$, it does not make sense to talk about limits tending to 0 since there are no points in $E$ close to 0.
\begin{definition}
    Let $E \subseteq \mathbb C,\, a \in \mathbb C$. $a$ is a limit point of $E$ if for any $\delta > 0$, there exists $z \in E$ such that $0 < \abs{z - a} < \delta$.
\end{definition}
\noindent First, note that $a$ is a limit point if and only if there exists a sequence $z_n \in E$ such that $z_n \to a$, but notably $z_n \neq a$ for all $n$.
\begin{definition}
    Let $f \colon E \subseteq \mathbb C \to \mathbb C$, and let $a \in \mathbb C$ be a limit point of $E$. We say that $f \to \ell$ as $z \to a$, if given $\varepsilon > 0$ there exists $\delta > 0$ such that whenever $0 < \abs{z - a} < \delta$ and $z \in E$, $\abs{f(z) - \ell} < \varepsilon$. Equivalently, $f(z_n) \to \ell$ for every sequence $z_n \in E$, such that $z_n \to a$ but $z_n \neq a$.
\end{definition}
\noindent Therefore if $a \in E$ is a limit point, then $\lim_{z \to a} f(z) = f(a)$ if and only if $f$ is continuous at $a$. If $a \in E$ is isolated (not a limit point) then $f$ at $a$ is trivially continuous, since there are no points near $a$ but $a$ itself.

\subsection{Properties}
The limit of a function has very similar properties when compared to the limit of a sequence.
\begin{enumerate}
    \item It is unique. $f(z) \to A$, $f(z) \to B$ implies $A = B$.
    \item $f(z) \to A$, $g(z) \to B$ implies
          \begin{enumerate}
              \item $f(z) + g(z) \to A + B$
              \item $f(z)\cdot g(z) \to AB$
              \item If $B \neq 0$, $\frac{f(z)}{g(z)} \to \frac{A}{B}$
          \end{enumerate}
\end{enumerate}

\subsection{Intermediate Value Theorem}
\begin{theorem}
    Let $f \colon [a, b] \to \mathbb R$ be a continuous function where $f(a) \neq f(b)$. Then $f$ takes all values in the interval $[f(a), f(b)]$.
\end{theorem}
\begin{proof}
    Without loss of generality, let us assume $f(a) < f(b)$. Let us take an $\eta$ such that $f(a) < \eta < f(b)$. We want to prove that there exists some value $c \in [a, b]$ with $f(c) = \eta$. Let $s$ be the set of points defined by
    \[ s = \{ x \in [a, b] \colon f(x) < \eta \} \]
    $a \in s$ therefore the set $s$ is non-empty. The set is also clearly bounded above by $b$. So there is a supremum of this set, say $\sup s = c$ where $c \leq b$. This point $c$ can be visualised as the last point at which $y=f(x)$ crosses the line $y=c$. We intend to show that the function at this rightmost point is $\eta$.

    By the definition of the supremum, given $n$ there exists $x_n \in s$ such that $c - \frac{1}{n} < x_n \leq c$. So the sequence $x_n$ tends to $c$. We know that $f(x_n) < \eta$ for all $x_n$ by definition of the set $s$. By the continuity of $f$, $f(x_n) \to f(c)$. Thus,
    \begin{equation}
        f(c) \leq \eta \tag{$\ast$}
    \end{equation}
    Now, let us consider the fact that $c \neq b$. If $c = b$, then $f(b) \leq \eta$ which is a contradiction since $\eta < f(b)$. So for a large $n$, we can ensure that $c + \frac{1}{n} \in [a,b]$. So by continuity of the function, $f(c + \frac{1}{n}) \to f(c)$. But since $c + \frac{1}{n} > c$, then necessarily $f(c + \frac{1}{n}) \geq \eta$ because $c$ is the supremum of $s$. Thus
    \[ f(c) \geq \eta \]
    Combining this with $(\ast)$ we get $f(c) = \eta$.
\end{proof}
\noindent This theorem is very useful for finding zeroes and fixed points. For example, we can prove the existence of the $N$th root of a positive real number $y$. Let
\[ f(x) = x^N \]
Then $f$ is certainly continuous on the interval $[0, 1+y]$, since
\[ 0 = f(0) < y < (1+y)^N = f(1 + y) \]
By the intermediate value theorem, there exists a point $c \in (0, 1+y)$ such that $f(c) = c^N = y$. So $c$ is a positive $N$th root of $y$. We can also prove the uniqueness of such a point. Suppose $d^N = y$ with $d>0$ and $d \neq c$. Without loss of generality, suppose $d < c$. Then $d^N < c^N$ so $d^N \neq y$, which is a contradiction.

\section{Bounds and Inverses}
\subsection{Bounds of a Continuous Function}
\begin{theorem}
    Let $f \colon [a, b] \to \mathbb R$ be continuous. Then the function is bounded, i.e. there exists $k \in \mathbb R$ such that $\abs{f(x)} \leq k$ for every point $x \in [a, b]$.
\end{theorem}
\begin{proof}
    Suppose that such a function $f$ is not bounded. Then in particular, given any integer $n \geq 1$, there exists $x_n \in [a, b]$ such that $\abs{f(x_n)} > n$. By the Bolzano-Weierstrass theorem, the sequence $x_n$, which is bounded by $a \leq x_n \leq b$, has a convergent subsequence $x_{n_j} \to x$, such that $x \in [a, b]$. Then by continuity of $f$, $f(x_{n_j}) \to f(x)$. But $\abs{f(x_{n_j})} > n_j \to \infty$. This is a contradiction.
\end{proof}
\noindent We can actually improve this statement.
\begin{theorem}
    Suppose $f \colon [a, b] \to \mathbb R$ is a continuous function. Then there exist $x_1, x_2 \in [a, b]$ such that
    \[ f(x_1) \leq f(x) \leq f(x_2) \]
    for all $x \in [a, b]$. In other words, a continuous function on a closed bounded interval is bounded and attains its bounds.
\end{theorem}
\begin{proof}
    Let $A = \{ f(x) \colon x \in [a, b] \}$ be the image of $[a, b]$ under $f$. By the above theorem, $A$ is bounded. It is also non-empty, hence it has a supremum $M = \sup A$ (and analogously an infimum $\inf A$, whose proof is almost identical). Then by the definition of the supremum, given an integer $n \geq 1$ there exists $x_n \in [a, b]$ such that $M - \frac{1}{n} < f(x_n) \leq M$. By the Bolzano-Weierstrass theorem, there exists a convergent subsequence $x_{n_j} \to x \in [a, b]$. Since $f(x_{n_j}) \to M$, then by continuity, $f(x) = M$.
\end{proof}
\noindent Here is an alternative proof of the same theorem.
\begin{proof}
    As before, let $A$ be the image of $f$, and $M$ be the supremum of $A$. Suppose there is no $x_2 \in [a, b]$ such that $f(x_2) = M$. Then let $g(x) = \frac{1}{M - f(x)}$ for $x \in [a, b]$. Since there exists no $x$ such that $M = f(x)$, $g(x)$ is continuous since we are never dividing by zero. So $g$ is bounded. So by the previous theorem, there is some $k > 0$ such that $g(x) \leq k$ for all $x \in [a, b]$. This means that $f(x) \leq M - \frac{1}{k}$ on $[a, b]$ for this $k$, but this cannot happen since $M$ is the supremum.
\end{proof}
\noindent Note that these theorems are certainly false if the interval is not closed: consider the counterexample $(0, 1]$ and the function $x \mapsto x^{-1}$.

\subsection{Inverse Functions}
\begin{definition}
    $f$ is increasing for $x \in [a, b]$ if $f(x_1) \leq f(x_2)$ for all $x_1 \leq x_2 \in [a, b]$. If $f(x_1) < f(x_2)$ then the function is strictly increasing. A function may be called decreasing or strictly decreasing analogously.
\end{definition}
\begin{definition}
    A function $f$ is called monotone if it is either increasing or decreasing.
\end{definition}
\begin{theorem}
    Let $f \colon [a, b] \to \mathbb R$ be continuous and strictly increasing for $x \in [a, b]$. Let $c = f(a)$, $d = f(b)$. Then $f \colon [a, b] \to [c, d]$ is bijective, and the inverse $g := f^{-1} \colon [c, d] \to [a, b]$ is continuous and strictly increasing.
\end{theorem}
\noindent A similar theorem holds for strictly decreasing functions.
\begin{proof}
    Let $c < k < d$. From the intermediate value theorem, there exists $h$ such that $f(h) = k$. This $h$ must be unique since the function is strictly increasing. Then we can define $g(k) = h$, giving us an inverse $g \colon [c, d] \to [a, b]$ for $f$.

    First, note that $g$ is strictly increasing. Indeed, for $y_1 < y_2$ then $y_1 = f(x_1)$, $y_2 = f(x_2)$. This means that if $x_2 \geq x_1$, then since $f$ is increasing $y_2 \leq y_1$ which is a contradiction.

    Now, note that $g$ is continuous. Indeed, given $\varepsilon > 0$, we can let $k_1 = f(h - \varepsilon)$ and $k_2 = f(h + \varepsilon)$. If $f$ is strictly increasing, then $k_1 < k < k_2$. Then $h - \varepsilon < g(y) < h + \varepsilon$. So let $\delta = \min(k_2 - k, k - k_1)$ where $k \in (c, d)$, establishing continuity as claimed.
\end{proof}

\section{Differentiability}
\subsection{Definitions}
Let $f \colon E \subseteq \mathbb C \to \mathbb C$. Mostly we will take $E$ to be an interval in the real numbers, or a disc in the complex plane.
\begin{definition}
    Let $x \in E$ be a point such that there exists a sequence $x_n \in E$ with $x_n \neq x$, but $x_n \to x$, i.e. $x$ is a limit point. $f$ is said to be differentiable at $x$ with derivative $f'(x)$ if
    \[ \lim_{y \to x} \frac{f(y) - f(x)}{y - x} = f'(x) \]
\end{definition}
\noindent If $f$ is differentiable at each point in $E$, we say that $f$ is differentiable on $E$.
\begin{remark}
    One interpretation of the definition is to write it in the form
    \[ \varepsilon(h) := f(x+h) - f(x) - hf'(x);\quad \lim_{h \to 0} \frac{\varepsilon(h)}{h} = 0 \]
    so $\varepsilon$ is $o(h)$. Hence,
    \[ f(x+h) = f(x) + hf'(x) + \varepsilon(h) \]
    We could have made an alternative definition for differentiability. $f$ is differentiable at $x$ if there exists $A$ and $\varepsilon$ such that
    \[ f(x+h) = f(x) + hA + \varepsilon(h) \text{ where } \lim_{h \to 0} \frac{\varepsilon(h)}{h} = 0 \]
    If such an $A$ exists, then it is unique, since $A$ is the limit
    \[ A = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h} \]
    We could have alternatively written the definition as
    \[ f(x+h) = f(x) + hf'(x) + h\varepsilon_f(h) \text{ where } \lim_{h \to 0} \varepsilon_f(h) = 0 \]
    or perhaps
    \[ f(x) = f(a) + (x-a)f'(a) + (x-a)\varepsilon_f(x) \text{ where } \lim_{x \to a} \varepsilon_f(x) = 0 \]
    Note further that if $f$ is differentiable at $x$, $f$ is certainly continuous at $x$. This follows from the fact that $\varepsilon(h) \to 0$, and hence $f(x+h) \to f(x)$ as $h \to 0$.
\end{remark}
\noindent As an example, let us consider $f(x) = \abs{x}$ for $f \colon \mathbb R \to \mathbb R$. Is the function at the point $x=0$ differentiable? If $x > 0$, we have $f'(x) = 1$, but if $x < 0$, we have $f'(x) = -1$. These results can be checked directly using the definitions above. But we have produced two sequences for $h \to 0$ which give different values, so the derivative is not defined here.

\subsection{Differentiation of Sums and Products}
\begin{proposition}
    \begin{enumerate}[(i)]
        \item If $f(x) = c$ for all $x \in E$, then $f$ is differentiable with $f'(x) = 0$.
        \item If $f$ and $g$ are differentiable at $x$, then so is $f+g$, where $(f+g)'(x) = f'(x) + g'(x)$.
        \item If $f$ and $g$ are differentiable at $x$, then so is $fg$, where $(fg)'(x) = f'(x)g(x) + g'(x)f(x)$.
        \item If $f$ is differentiable at $x$ and $f(x) \neq 0$, then so is $\frac{1}{f}$, where $(\frac{1}{f})'(x) = \frac{-f'(x)}{(f(x))^2}$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}[(i)]
        \item $\lim_{h \to 0} \frac{c-c}{h} = 0$ as required.
        \item Since all relevant limits are well-defined,
              \[ \lim_{h \to 0} \frac{f(x+h) + g(x+h) - f(x) - g(x)}{h} = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} + \lim_{h \to 0} \frac{g(x+h) - g(x)}{h} = f'(x) + g'(x) \]
        \item Let $\phi(x) = f(x)g(x)$. Then, since $f$ is continuous at $x$,
              \begin{align*}
                  \lim_{h \to 0} \frac{\phi(x+h) - \phi(x)}{h} & = \lim_{h \to 0} \frac{f(x+h)g(x+h) - f(x)g(x)}{h}                            \\
                                                               & = \lim_{h \to 0} f(x+h) \frac{g(x+h) - g(x)}{h} + g(x)\frac{f(x+h) - f(x)}{h} \\
                                                               & = \lim_{h \to 0} f(x) \frac{g(x+h) - g(x)}{h} + g(x)\frac{f(x+h) - f(x)}{h}   \\
                                                               & = f(x)g'(x) + g(x)f'(x)
              \end{align*}
        \item Let $\phi(x) = \frac{1}{f(x)}$. Then,
              \begin{align*}
                  \lim_{h \to 0} \frac{\phi(x+h) - \phi(x)}{h} & = \lim_{h \to 0} \frac{\frac{1}{f(x+h)} - \frac{1}{f(x)}}{h} \\
                                                               & = \lim_{h \to 0} \frac{f(x) - f(x+h)}{hf(x)f(x+h)}           \\
                                                               & = \frac{-f'(x)}{f(x)f(x)}                                    \\
              \end{align*}
    \end{enumerate}
\end{proof}
\begin{remark}
    From (iii) and (iv), we can immediately find the quotient rule,
    \[ \left( \frac{f(x)}{g(x)} \right)' = \frac{g(x)f'(x) - f(x)g'(x)}{(g(x))^2} \]
\end{remark}

\section{Properties of the Derivative}
\subsection{Differentiating Polynomial Terms}
As an example of the differentiability properties we saw last lecture, we can find the derivative of $f(x) = x^n$ for $n \in \mathbb Z$, $n > 0$. If $n=1$, clearly $f'(x) = 1$. We can show inductively that $f'(x) = nx^{n-1}$. Indeed,
\begin{align*}
    (x^n)' & = x \cdot (x^{n-1})' + (x)' \cdot x^{n-1} \\
           & = (n-1)x^{n-1} + x^{n-1}                  \\
           & = nx^{n-1}
\end{align*}
We can now take $f(x) = x^{-n}$. Using the reciprocal law,
\begin{align*}
    f'(x) & = \frac{-(x^n)'}{(x^n)^2}  \\
          & = \frac{-nx^{n-1}}{x^{2n}} \\
          & = -nx^{-n-1}
\end{align*}

\subsection{Chain Rule}
\begin{theorem}
    Let $f \colon U \to \mathbb C$ be such that $f(x) \in V$ for all $x \in U$. If $f$ is differentiable at $a \in U$, and $g \colon V \to \mathbb C$ is differentiable at $f(a) \in V$, then $g \circ f$ is differentiable at $a$ with
    \[ gf'(a) = f'(a)g'(f(a)) \]
\end{theorem}
\begin{proof}
    We know that we can write
    \[ f(x) = f(a) + (x-a)f'(a) + \varepsilon_f(x)(x-a) \]
    where $\lim_{x \to a} \varepsilon_f(x) = 0$. Further,
    \[ g(y) = g(b) + (y-b)g'(b) + \varepsilon_g(y)(y-b) \]
    where $\lim_{y \to b} \varepsilon_g(y) = 0$, and $b = f(a)$. We will set $\varepsilon_f(a) = 0$ and $\varepsilon_g(b) = 0$, so they are continuous at $x=a$ and $y=b$, so that everything is well-defined when we begin to compose the functions. Now, $y=f(x)$, so
    \begin{align*}
        g(f(x)) & =  g(b) + (f(x) - b)g'(b) + \varepsilon_g(f(x))(f(x) - b)                                                                                                       \\
                & = g(f(a)) + \left[ (x-a)f'(a) + \varepsilon_f(x)(x-a) \right]\left[ g'(b) + \varepsilon_g(f(x)) \right]                                                         \\
                & = g(f(a)) + (x-a)f'(a)g'(b) + (x-a)\underbrace{\left[ \varepsilon_f(x) g'(b) + \varepsilon_g(f(x)) \left( f'(a) + \varepsilon_f(x) \right) \right]}_{\sigma(x)} \\
    \end{align*}
    Now, we just need to show that $\lim_{x \to a} \sigma(x) = 0$ in order to prove the theorem. Clearly
    \[ \sigma(x) = \underbrace{\varepsilon_f(x)}_{\to 0} g'(b) + \underbrace{\varepsilon_g(f(x))}_{\to 0} \left( f'(a) + \varepsilon_f(x) \right) \]
    Hence $\sigma(x) \to 0$ as required.
\end{proof}

\subsection{Rolle's Theorem}
\begin{theorem}
    Let $f \colon [a,b] \to \mathbb R$ be a continuous function on $[a,b]$ and differentiable on $(a, b)$. If $f(a) = f(b)$, then there exists $c \in (a,b)$ such that $f'(c) = 0$.
\end{theorem}
\begin{proof}
    Let $M$ be the maximum point and $m$ be the minimum point of the function. Recall that in Lecture 8 we proved that any function achieves its bounds. Let $k = f(a)$. If $M=m=k$, then $f$ must be a constant, and clearly $f'(c) = 0$ for every value $c \in (a, b)$. Otherwise, either $M > k$ or $m < k$. Suppose $M > k$ (the proof is very similar if $m < k$). Then there exists some value $c \in (a, b)$ such that $f(c) = M$. We would like to show that $f'(c) = 0$, so let us suppose that $f'(c) \neq 0$. If $f'(c) > 0$, then there are values $d > c$ where $f(d) > f(c)$. Indeed,
    \[ f(h+c) - f(c) = h\left[ f'(c) + \varepsilon(h) \right] \]
    For a small, positive $h$, this value is positive. This contradicts the fact that $M$ is the maximum. Similarly, if $f'(c) < 0$ there are values $d < c$ with $f(d) > f(c)$. Hence $f'(c) = 0$.
\end{proof}

\subsection{Mean Value Theorem}
We can make a small change to Rolle's theorem and obtain the mean value theorem.
\begin{theorem}
    Let $f \colon [a,b] \to \mathbb R$ be a continuous function on $[a,b]$ and differentiable on $(a, b)$. Then there exists $c \in (a, b)$ such that
    \[ f(b) - f(a) = f'(c)(b-a) \]
\end{theorem}
\begin{proof}
    Let $\phi$ be a function defined by $\phi(x) = f(x) - kx$, choosing a $k$ such that $\phi(a) = \phi(b)$. We can find that
    \[ f(b) - bk = f(a) - ak \implies k = \frac{f(b) - f(a)}{b - a} \]
    By Rolle's theorem, there exists $c \in (a,b)$ such that $\phi'(c) = 0$. Now, note that $f'(x) = \phi'(x) + k$, hence there exists $c$ such that $f'(c) = k$.
\end{proof}
\begin{remark}
    We will often rewrite the mean value theorem as follows.
    \[ f(a + h) = f(a) + hf'(a + \theta h) \]
    where $\theta \in (0, 1)$. Note, however, that $\theta$ is a function of $h$, so if we begin to shrink $h$ then $\theta$ may change.
\end{remark}

\section{???}
\subsection{Properties of a Function from its Derivative}
We can deduce certain facts about a function by observing the properties its derivative exhibits. These results are mostly trivial corollaries to the mean value theorem, proven in the last lecture.
\begin{corollary}
    Let $f \colon [a,b] \to \mathbb R$ be continuous, and differentiable on $(a, b)$. Then we have
    \begin{enumerate}[(i)]
        \item If $f'(x) > 0$ for all $x \in (a, b)$, then $f$ is strictly increasing on $[a, b]$;
        \item If $f'(x) \geq 0$ for all $x \in (a, b)$, then $f$ is increasing on $[a, b]$;
        \item If $f'(x) = 0$ for all $x \in (a, b)$, then $f$ is constant on $[a, b]$.
    \end{enumerate}
\end{corollary}
\noindent Part (iii) of this corollary is essentially solving the most simple differential equation; we are showing that the only possible solutions to this equation are the constant functions. Note that similar statements about decreasing functions hold.
\begin{proof}
    \begin{enumerate}[(i)]
        \item We have $f(y) - f(x) = f'(c)(y-x)$ for some $c \in (x, y)$. If $f'(c) > 0$, then $f(y) - f(x) > 0$.
        \item Analogously to before, $f(y) - f(x) = f'(c)(y-x)$ for some $c \in (x, y)$. If $f'(c) \geq 0$, then  $f(y) - f(x) \geq 0$.
        \item By the mean value theorem on $[a, x]$, if $f'(c) = 0$, then $f(x) - f(a) = 0$.
    \end{enumerate}
\end{proof}

\subsection{Inverse Function Theorem}
\begin{theorem}
    Let $f \colon [a, b] \to \mathbb R$ be a continuous function on $[a, b]$ and differentiable on $(a, b)$, with $f'(x) > 0$ everywhere on $(a, b)$. Let $f(a) = c, f(b) = d$. Then the function $f \colon [a,b] \to [c,d]$ is bijective, and $f^{-1} \colon [c,d] \to [a,b]$ is differentiable on $(c, d)$ with
    \[ \left( f^{-1} \right)' (x) = \frac{1}{f'\left(f^{-1}(x)\right)} \]
\end{theorem}
\noindent Note, in lecture 8 it was proven that a continuous strictly increasing function has a continous inverse. This strengthens that claim to include the differentiability property if the original function was differentiable.
\begin{proof}
    We know from lecture 8 that there exists $g \colon [c,d] \to [a,b]$ which is a strictly increasing continuous function, which is the inverse of $f$. We must now show that $g$ is differentiable and that its derivative has the required form as stated in the claim. Now, let $y = f(x)$. Given $k \neq 0$, let $h$ be given by
    \[ y + k = f(x+h) \]
    Alternatively, written in terms of $g$,
    \[ x + h = g(y + k) \]
    So clearly $h \neq 0$. Since $g$ is continuous, if $k \to 0$ then $h \to 0$. Then
    \begin{align*}
        \frac{g(y + k) - g(y)}{k}                           & = \frac{x + h - x}{f(x+h) - y}           \\
                                                            & = \frac{h}{f(x+h) - f(x)}                \\
        \therefore \lim_{k \to 0} \frac{g(y + k) - g(y)}{k} & = \lim_{h \to 0} \frac{h}{f(x+h) - f(x)} \\
                                                            & = \frac{1}{f'(x)}
    \end{align*}
    as required.
\end{proof}

\subsection{Derivative of Rational Powers}
First, let $g(x) = x^{1/q}$ for some positive integer $q$. We can find that $f(x) = x^q$ has the derivative $f'(x) = qx^{q-1}$. By the inverse function theorem, $g'(x) = \frac{1}{q}x^{1/q - 1}$. Now, if $g(x) = x^{p/q}$, where $p$ is an integer and $q$ is a positive integer, then by the chain rule $g'(x) = \frac{p}{q}x^{p/q - 1}$ which matches the expected result.

\subsection{Mean Value Theorem Applied to Limits}
Suppose $f, g \colon [a,b] \to \mathbb R$ are continuous, and differentiable on $(a, b)$. Suppose further that $g(a) \neq g(b)$. The mean value theorem can be applied to both functions, and will give two points $s, t \in (a, b)$ such that
\[ \frac{f(b) - f(a)}{g(b) - g(a)} = \frac{(b-a)f'(s)}{(b-a)g'(t)} = \frac{f'(s)}{g'(t)} \]
This gives us a way to simplify a limit of the form of the left hand side (as $b \to a$) by instead considering the right hand side. We can apply Cauchy's mean value theorem, seen in the next lecture.

\section{Extensions of the Mean Value Theorem}
\subsection{Cauchy's Mean Value Theorem}
\begin{theorem}
    If $f, g \colon [a,b] \to \mathbb R$ are continuous, and differentiable on $(a, b)$, there exists $t \in (a,b)$ such that
    \[ (f(b) - f(a))g'(t) = f'(t)(g(b) - g(a)) \]
\end{theorem}
\noindent We can recover the normal mean value theorem from Cauchy's generalisation by taking $g(x) = x$.
\begin{proof}
    Let
    \[ \phi(x) = \begin{vmatrix}
            1    & 1    & 1    \\
            f(a) & f(x) & f(b) \\
            g(a) & g(x) & g(b)
        \end{vmatrix} \]
    Certainly $\phi(x)$ is continuous on $[a,b]$ and differentiable on $(a, b)$, by using previous results. Also, $\phi(a) = \phi(b) = 0$ by observing the linear dependence of the columns. By Rolle's theorem, there exists $t \in (a, b)$ such that $\phi'(t) = 0$. We can expand $\phi'(t)$ and this will show the required result.
    \[ \phi'(x) = f'(x)g(b) - g'(x)f(b) + f(a)g'(x) - g(a)f'(x) = f'(x) [g(b) - g(a)] + g'(x) [f(a) - f(b)] \]
\end{proof}

\subsection{Example of L'H\^opital's Rule}
The derivation of L'H\^opital's rule is on an example sheet, so in this subsection we will consider only a special case of it, using Cauchy's mean value theorem.
\[ \ell = \lim_{x \to 0} \frac{e^x - 1}{\sin x} \]
We can write
\[ \ell = \lim_{x \to 0} \frac{e^x - e^0}{\sin x - \sin 0} = \frac{e^t}{\cos t} \]
for some $t \in (0, x)$. So as $x \to 0$, $t \to 0$ and hence
\[ \frac{e^t}{\cos t} \to 1 \]

\subsection{Taylor's Theorem}
\begin{theorem}[Taylor's Theorem with Lagrange's Remainder]
    Suppose $f$ and its derivatives up to order $n-1$ are continuous in $[a, a+h]$, and $f^{(n)}$ exists for $x \in (a, a+h)$. Then
    \[ f(a+h) = f(a) + hf'(a) + \frac{h^2}{2!} f''(a) + \dots + \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(a) + \frac{h^n}{n!}f^{(n)}(a + \theta h) \]
    where $\theta \in (0, 1)$.
\end{theorem}
\noindent Note that for $n=1$, this is exactly the mean value theorem, so this can be seen as an $n$th order extension of the mean value theorem. We commonly write $R_n$ for the final error term $\frac{h^n}{n!}f^{(n)}(a + \theta h)$. This is known as Lagrange's form of the remainder.
\begin{proof}
    For $0 \leq t \leq h$, we define
    \[ \phi(t) = f(a+t) - f(a) - tf'(a) - \dots - \frac{t^{n-1}}{(n-1)!}f^{(n-1)}(a) - \frac{t^n}{n!}B \]
    where we choose $B$ suitably such that $\phi(h) = 0$. (Recall that in the proof of the mean value theorem, we used $f(x) - kx$ and picked $k$ suitably such that this allowed the use of Rolle's theorem. This is entirely analogous, but generalised to the $n$th derivative). Note that
    \[ \phi(0) = \phi'(0) = \dots = \phi^{(n-1)}(0) = 0 \]
    We can use Rolle's theorem inductively $n$ times. Since $\phi(0) = \phi(h) = 0$, there is a point $0 < h_1 < h$ such that $\phi'(h_1) = 0$. Since $\phi'(0) = \phi'(h_1) = 0$, there is a point $0 < h_2 < h_1$ such that $\phi''(h_2) = 0$. This continues until we find a point $0 < h_n < h$ such that $\phi^{(n)}(h_n) = 0$. Hence $h_n = \theta h$ for some $0 < \theta < 1$. Now, $\phi^{(n)}(t) = f^{(n)}(a + t) - B$. We can see now that $B = f^{(n)}(a + \theta h)$, which gives the required result.
\end{proof}
\noindent We can prove an alternative version of Taylor's theorem with a different error term.
\begin{theorem}[Taylor's Theorem with Cauchy's Remainder]
    Suppose (equivalently to before) $f$ and its derivatives up to order $n-1$ are continuous in $[a, a+h]$, and $f^{(n)}$ exists for $x \in (a, a+h)$. Then
    \[ f(a+h) = f(a) + hf'(a) + \frac{h^2}{2!} f''(a) + \dots + \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(a) + R_n \]
    where
    \[ R_n = \frac{(1 - \theta)^{n-1}h^nf^{(n)}(a + \theta h)}{(n-1)!} \]
    for $\theta \in (0, 1)$.
\end{theorem}
\begin{proof}
    For simplicity, in this proof we let $a = 0$, although the same argument applies when $a \neq 0$. Let us define
    \[ F(t) = f(h) - f(t) - (h-t)f'(t) - \dots - \frac{(h-t)^{n-1}f^{(n-1)}(t)}{(n-1)!} \]
    for $t \in [0, h]$. Then
    \begin{align*}
        F'(t) & = -f'(t) + f'(t) - (h-t)f''(t) + (h-t)f''(t) - \frac{1}{2} (h-t)^2f'''(t) + \frac{1}{2} (h-t)^2f'''(t) \\
              & - \dots - \frac{(h-t)^{n-1}}{(n-1)!}f^{(n)}(t)                                                         \\
              & = - \frac{(h-t)^{n-1}}{(n-1)!}f^{(n)}(t)
    \end{align*}
    Let
    \[ \phi(t) = F(t) - \left[ \frac{h-t}{h} \right]^p F(0) \]
    where $p \in \mathbb N$ and $1 \leq p \leq n$. Then
    \[ \phi(0) = \phi(h) = 0 \]
    By Rolle's theorem, there exists $\theta \in (0, 1)$ such that
    \[ \phi'(\theta h) = 0 \]
    We can compute $\phi'$ to find
    \[ \phi'(\theta h) = F'(\theta h) + \frac{p(1-\theta)^{p-1}}{h} F(0) = 0 \]
    Substituting everything back into $F$ gives
    \[ 0 = \frac{-h^{n-1}(1-\theta)^{n-1}}{(n-1)!}f^{(n)}(\theta h) + \frac{p(1-\theta)^{p-1}}{h}\left[ f(h) - f(0) - h'(0) - \dots - \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(0) \right] \]
    Hence
    \[ f(h) = f(0) + hf'(0) + \frac{h^2}{2!} f''(0) + \dots + \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(0) + \underbrace{\frac{h^n(1 - \theta)^{n-1}f^{(n)}(\theta h)}{(n-1)!\cdot p(1-\theta)^{p-1}}}_{R_n} \]
    By letting $p = n$, we get Lagrange's remainder. If $p=1$, we get Cauchy's remainder.
\end{proof}

\section{Applications of Remainders in Taylor's Theorem}
\subsection{Bounding Error Terms}
Recall that Lagrange's remainder is
\[ R_n = \frac{h^n}{n!}f^{(n)}(a + \theta h) \]
and Cauchy's remainder is
\[ R_n = \frac{(1 - \theta)^{n-1}h^nf^{(n)}(a + \theta h)}{(n-1)!} \]
and that we can write
\[ f(h) = P_{n-1}(h) + R_n \]
where $P_{n-1}$ is the Taylor polynomial to $(n-1)$th order. To get a Taylor series for a function $f$, we need to prove that the $R_n$ tend to zero as $n \to \infty$. In general, this requires estimates for the $R_n$ and it could take a lot of effort to prove whether this limit is zero or not. Note also that the theorems deducing the remainder terms work equally well in an interval $[a+h, a]$ where $h < 0$.

\subsection{Binomial Series}
\begin{proposition}
    Let
    \[ f(x) = (1 + x)^r \]
    for some $r \in \mathbb Q$. If $\abs{x} < 1$, then
    \[ f(x) = 1 + \binom{r}{1}x + \dots + \binom{r}{n}x^n + \dots \]
    where
    \[ \binom{r}{n} = \frac{r(r-1)\cdots(r-n+1)}{n!} \]
\end{proposition}
\begin{proof}
    Clearly,
    \[ f^{(n)}(x) = r(r-1)\cdots(r-n+1)(1+x)^{r-n} \]
    These coefficients correspond exactly with that of the Taylor polynomial. If $r \in \mathbb N$, then $f^{(r+1)}(x) \equiv 0$, so clearly the $R_n$ are zero as $n \to \infty$. In general, using Lagrange's form of the remainder,
    \[ R_n = \frac{x^n}{n!} f^{(n)}(\theta x) = \binom{r}{n} \frac{x^n}{(1 + \theta x)^{n-r}} \]
    Note that in principle, $\theta$ depends both on $x$ and $n$. For $0 < x < 1$, $(1 + \theta x)^{n - r} > 1$ for $n > r$. Now observe that the series given by
    \[ \sum \binom{r}{n} x^n \]
    is absolutely convergent for $\abs{x} < 1$. Indeed, we can apply the ratio test and find that
    \[ \abs{\frac{a_{n+1}}{a_n}} = \abs{\frac{(r-n)x}{n+1}} \]
    which tends to $\abs{x}$ as $n \to \infty$. In particular therefore, the terms $\binom{r}{n}x^n$ tend to zero for $\abs{x} < 1$. Hence for $n > r$ and $0 < x < 1$, we have
    \[ \abs{R_n} \leq \abs{\binom{r}{n}x^n} \to 0 \]
    So the claim is proven in the range $0 \leq x < 1$. If $x < 0$, then the step when we compare $(1 + \theta x)^{n-r}$ with 1 breaks down. Let us instead use the Cauchy form of the remainder to bypass this step.
    \[ R_n = \frac{(1 - \theta)^{n-1}x^nf^{(n)}(\theta x)}{(n-1)!} = \frac{(1-\theta)^{n-1} r(r-1)\cdots(r-n+1)(1+\theta x)^{r-n} x^n}{(n-1)!} \]
    By regrouping terms, we get
    \[ R_n = \frac{r(r-1)\cdots(r-n+1)}{(n-1)!} \cdot \frac{(1-\theta)^{n-1}}{(1 + \theta x)^{n-r}} x^n = r\binom{r-1}{n-1}x^n (1+\theta x)^{r-1} \left( \underbrace{\frac{1-\theta}{1 + \theta x}}_{<1} \right)^{n-1} \]
    Hence
    \[ \abs{R_n} \leq \abs{r \binom{r-1}{n-1}x^n} (1+\theta x)^{r-1} \]
    This will then tend to zero, after a bit more effort; we can bound the $(1 + \theta x)^{r-1}$ term by the maximum of $1$ and $(1 + x)^{r-1}$, which is independent of $n$, and then the result will follow.
\end{proof}

\subsection{Complex Differentiation}
The complex derivative and the real derivative have the same core properties, for instance linearity, the product rule and the chain rule. However, the complex derivative is significantly more restrictive than the real derivative, since we can approach a point in any number of directions. If we can find a function that is complex differentiable with this restriction, we actually get a whole array of features for free. As an example of this restriction, consider the function $f(z) = \overline{z}$. This function is actually nowhere differentiable. If it were differentiable, then any sequence tending to $z$ would yield the same limit when substituted into the definition of the derivative. Consider first the sequence
\[ z_n = z + \frac{1}{n} \to z \]
Then
\[ \frac{f(z_n) - f(z)}{z_n - z} = \frac{\overline{z} + \frac{1}{n} - \overline{z}}{z + \frac{1}{n} - z} = 1 \]
Now consider the sequence
\[ z_n = z + \frac{i}{n} \to z \]
Then
\[ \frac{f(z_n) - f(z)}{z_n - z} = \frac{\overline{z} - \frac{i}{n} - \overline{z}}{z + \frac{i}{n} - z} = -1 \]
Hence $f(z)$ is nowhere differentiable. On the other hand, the real function $f(x, y) = (x, -y)$ is clearly real differentiable, since it is linear; but in the complex world the function $z \mapsto \overline{z}$ is not linear.

\end{document}