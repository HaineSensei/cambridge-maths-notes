\documentclass{article}

\input{../util.tex}

\title{Vector Calculus}
\author{Cambridge University Mathematical Tripos: Part IA}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Differential Geometry of Curves}
\subsection{Notation}
Throughout this course, a column vector e.g.
\[ \begin{pmatrix}
        a \\ b \\ c
    \end{pmatrix} \]
it should be interpreted as the vector
\[ \vb x = a \vb e_x + b \vb e_y + x \vb e_z \]
where $\{ \vb e_x, \vb e_y, \vb e_z \}$ are the basis vectors aligned with the fixed Cartesian $x, y, z$ axes in $\mathbb R^3$. We will be dealing with various kinds of basis vectors through the course, so it is useful to define now that column vectors written as above always represent the standard basis.

\subsection{Parametrised Curves and Smoothness}
A parametrised curve $C$ in $R^3$ is the image of a continuous map $\vb x \colon [a, b] \to \mathbb R^3$, in which $t \mapsto \vb x(t)$. In Cartesian coordinates,
\[ \vb x(t) = \begin{pmatrix}
        x_1(t) \\ x_2(t) \\ x_3(t)
    \end{pmatrix} = \begin{pmatrix}
        x(t) \\ y(t) \\ z(t)
    \end{pmatrix} \]
The resultant curve has a direction, from $\vb x(a)$ to $\vb x(b)$.
\begin{definition}
    We say that $C$ is a differentiable curve if each of the components $\{x_i(t)\}$ are differentiable functions. $C$ is regular if it is differentiable and $\abs{\vb x'(t)} \neq 0$. If $C$ is differentiable and regular, we say that $C$ is smooth.
\end{definition}
\begin{note}
    We need this regularity condition because it is quite easy to create `bad curves' with cusps and spikes using only differentiable functions, for example
    \[ \vb x(t) = (t^2, t^3) \]
    The components are clearly differentiable, but $\vb x(t)$ has a cusp at $t = 0$. At this point, $\abs{\vb x'(0)} = 0$.
\end{note}
\begin{definition}
    Recall that $x_i(t)$ is called `differentiable' at $t$ if
    \[ x_i(t+h) = x_i(t) + x_i'(t)h + o(h) \]
    where $o(h)$ represents a function that obeys
    \[ \lim_{h \to 0} \frac{o(h)}{h} = 0 \]
    In terms of vectors,
    \[ \vb x(t+h) = \vb x(t) + \vb x'(t)h + o(h) \]
    where here $o(h)$ is a vector for which
    \[ \lim_{h \to 0} \frac{\abs{o(h)}}{h} = 0 \]
\end{definition}

\subsection{Arc Length}
We can approximate the length of a curve $C$ by splitting it into small straight lines and summing the lengths of such lines. We will introduce a partition $P$ of $[a, b]$ with $t_0 = a, t_N = b$ and
\[ t_0 < t_1 < t_2 < \dots < t_N \]
Let us now set $\Delta t_i = t_{i+1} - t_i$ and $\Delta t = \max_i \Delta t_i$. The length of the curve relative to $P$ is defined as
\[ \ell(C, P) = \sum_{i=0}^{N-1} \abs{\vb x(t_{i+1}) - \vb x(t_i)} \]
As $\Delta t$ gets smaller, we would expect $\ell(C, P)$ to give a better approximation to the true length of $C$, which we will call $\ell(C)$. Therefore we can define the length of $C$ by
\[ \ell(C) = \lim_{\Delta t \to 0} \sum_{i=0}^{N-1}\abs{\vb x(t_{i+1}) - \vb x(t_i)} = \lim_{\Delta t \to 0} \ell(C, P) \]
If this limit doesn't exist, we say that the curve is \textit{non-rectifiable}. Suppose $C$ is differentiable. Then
\begin{align*}
    \vb x(t_{i+1}) & = \vb x(t_i + t_{i+1} - t_i)                          \\
                   & = \vb x(t_i + \Delta t_i)                             \\
                   & = \vb x(t_i) + \vb x'(t_i) \Delta t_i + o(\Delta t_i)
\end{align*}
It follows then that
\[ \abs{\vb x(t_{i+1}) - \vb x(t_i)} = \abs{\vb x'(t_i)}\Delta t_i + o(\Delta t_i) \]
So if $C$ is differentiable,
\[
    \ell(C, P) = \lim_{\Delta t \to 0} \sum_{i=0}^{N-1} \left( \abs{\vb x'(t_i)}\Delta t_i + o(\Delta t_i)  \right)
\]
Recall that this $o(\Delta t_i)$ term represents a function for which $o(\Delta t_i) / \Delta t_i \to 0$. So for any $\varepsilon > 0$, if $\Delta t = \max_i \Delta t_i$ is sufficiently small, we have $\abs{o(\Delta t_i)} < \frac{\varepsilon}{b-a}\Delta t_i$, for $i = 0, \dots, N-1$. So by the Triangle Inequality, choosing $\Delta t$ sufficiently small,
\[
    \abs{\ell(C, P) - \sum_{i=0}^{N-1} \abs{\vb x'(t_i)}\Delta t_i} = \abs{\sum_{i=0}^{N-1} o(\Delta t_i)} < \frac{\varepsilon}{b-a}\underbrace{\sum_{i=0}^{N-1} \Delta t_i}_{b-a} = \varepsilon
\]
So the left hand side tends to zero as $\Delta t \to 0$. We then get
\begin{align*}
    \ell(C) & = \lim_{\Delta t \to 0} \ell(C, P)                                     \\
            & = \lim_{\Delta t \to 0} \sum_{i=0}^{N - 1} \abs{\vb x'(t_i)}\Delta t_i \\
            & = \int_a^b \abs{\vb x'(t)} \dd{t}
\end{align*}
according to Analysis I, and the definition of the Riemann Integral. So in summary, if $C \colon [a, b] \ni t \mapsto \vb x(t)$, then
\begin{align*}
    \ell(C) & = \int_a^b \abs{\vb x'(t)} \dd{t} \\
            & = \int_C \dd{s}
\end{align*}
where $\dd{s}$ is the `arc length element', i.e. $\dd{s} = \abs{\vb x'(t)} \dd{t}$. Similarly, we define
\[ \int_C f(\vb x) \dd{s} = \int_a^b f(\vb x(t)) \,\abs{\vb x'(t)} \dd{t} \]
If $C$ is made up of $M$ smooth curves $C_1, \dots, C_M$, we say that $C$ is `piecewise smooth'. We write $C = C_1 + \dots + C_M$ and define
\[ \int_C f(\vb x) \dd{s} = \sum_{i=1}^M \int_{C_i}f(\vb x) \dd{s} \]
Now note (informally) that
\[ \dd{s} = \abs{\vb x'(t)}\dd{t} = \sqrt{\left( \frac{\dd{x}}{\dd{t}} \right)^2 + \left( \frac{\dd{y}}{\dd{t}} \right)^2 + \left( \frac{\dd{z}}{\dd{t}} \right)^2} \dd{t} \]
i.e. (now very informally)
\[ \dd{s}^2 = \dd{x}^2 + \dd{y}^2 + \dd{z}^2 \]
which is Pythagoras' Theorem.

\subsection{Example of Arc Length}
Let $C$ be the circle of radius $r>0$ in $\mathbb R^3$
\[ \vb x(t) = \begin{pmatrix}
        r\cos t \\ r\sin t \\ 0
    \end{pmatrix};\quad t \in [0, 2\pi] \]
So
\[ \vb x'(t) = \begin{pmatrix}
        -r\sin t \\ r\cos t \\ 0
    \end{pmatrix} \]
Therefore
\[ \int_C \dd{s} = \int_0^{2\pi} \abs{\vb x'(t)} \dd{t} = \int_0^{2\pi} \sqrt{r^2 \sin^2 t + r^2 \cos^2 t} \dd{t} = \int_0^{2\pi} r \dd{t} = 2\pi r \]
Also, for example,
\[ \int_C x^2 y \dd{s} = \int_0^{2\pi} (r \cos t)^2 (r \sin t) \sqrt{r^2 \sin^2 t + r^2 \cos^2 t} \dd{t} = \int_0^{2\pi} r^3 \cos^2 t \sin t \dd{t} = 0 \]

\subsection{Choice of Parametrisation of Curves}
Does $\ell(C)$ depend on the choice of parametrisation of $\vb x(t)$? For example,
\[ \vb x(t) = \begin{pmatrix}
        r\cos t \\ r \sin t \\ 0
    \end{pmatrix};\quad t \in [0, 2\pi] \]
and
\[ \widetilde{\vb x}(t) = \begin{pmatrix}
        r\cos 2t \\ r \sin 2t \\ 0
    \end{pmatrix};\quad t \in [0, \pi] \]
both give rise to a circle, but have different forms. Suppose that $C$ has two different parametrisations,
\[ \vb x = \vb x_1(t);\quad a \leq t \leq b \]
\[ \vb x = \vb x_2(\tau);\quad \alpha \leq \tau \leq \beta \]
There must be some relationship $\vb x_2(\tau) = \vb x_1(t(\tau))$ for some function $t(\tau)$, since they represent the same curve. We can assume $\frac{\dd{t}}{\dd \tau} \neq 0$, so the map between $t$ and $\tau$ is invertible and differentiable (see IB Analysis and Topology). Note that
\begin{align*}
    \vb x_2'(\tau) & = \frac{\dd}{\dd \tau}\vb x_2(\tau)        \\
                   & = \frac{\dd}{\dd \tau}\vb x_1(t(\tau))     \\
    \intertext{By the Chain Rule,}
                   & = \frac{\dd{t}}{\dd \tau}\vb x_1'(t(\tau))
\end{align*}
And now from the above definitions,
\[ \int_C f(\vb x) \dd{s} = \int_a^b f(\vb x_1(t)) \, \abs{\vb x_1'(t)} \dd{t} \]
Making the substitution $t = t(\tau)$, and assuming $\frac{\dd{t}}{\dd \tau} > 0$, the lattern integral becomes
\[ \int_\alpha^\beta f(\vb x_2(\tau)) \, \underbrace{\abs{\vb x_1'(t(\tau))} \,\frac{\dd{t}}{\dd \tau}\dd \tau}_{\abs{\vb x_2'(\tau)}\,\dd \tau} = \int_\alpha^\beta f(\vb x_2(\tau)) \, \abs{\vb x_2'(\tau)} \,\dd \tau \]
which is precisely the same as $\int_C f(\vb x) \dd{s}$ using the $\vb x_2(\tau)$ parametrisation. When $\frac{\dd{t}}{\dd \tau} < 0$, you get the same result. So the definition of $\int_C f(\vb x) \, \dd{s}$ does \textit{not} depend on the choice of parametrisation of $C$.

\section{Curvature and Torsion}
\subsection{Parametrisation According to Arc Length}
We know that for any curve $C$ there exist multiple unique parametrisations. We will define the arc-length function for a curve $[a, b] \ni t \mapsto \vb x(t)$ by
\[ s(t) = \int_a^t \abs{\vb x'(\tau)} \,\dd \tau \]
So $s(a) = 0, s(b) = \ell(C)$. Using the Fundamental Theorem of Calculus, we have
\[ s'(t) = \abs{\vb x'(t)} \geq 0 \]
For regular curves, we have that
\[ s'(t) > 0 \]
So we can invert the relationship between $s$ and $t$; i.e. we can find $t$ as a function of $s$. Hence, we can parametrise curves with respect to arc length. If we write
\[ \vb r(s) = \vb x(t(s)) \]
where $0 \leq s \leq \ell(C)$, then by the chain rule we have
\[ \frac{\dd{t}}{\dd{s}} = \frac{1}{\frac{\dd{s}}{\dd{t}}} = \frac{1}{\abs{\vb x'(t(s))}} \]
So
\[ \vb r'(s) = \frac{\dd}{\dd{s}} \vb x(t(s)) = \frac{\dd{t}}{\dd{s}} \vb x'(t(s)) = \frac{\vb x'(t(s))}{\abs{\vb x'(t(s))}} \]
In other words, $\vb r'(s)$ is a unit vector tangential to the curve. This (consistently) gives
\[ \ell(C) = \int_0^{\ell(C)} \abs{\vb r'(s)} \dd{s} = \int_0^{\ell(C)} \dd{s} \]
as previously found above.

\subsection{Curvature}
Throughout this section, we will be talking about a generic regular curve $C$, parametrised with respect to arc length, where a position vector on $C$ is given by $\vb r(s)$. We will define the tangent vector
\[ \vb t(s) = \vb r'(s) \]
We already know that $\abs{\vb t(s)} = 1$. Therefore the only part of $\vb t$ that changes with respect to $s$ is its direction. So $\vb t'(s) = \vb r''(s)$ only measures the change in the direction of the tangent as we move along the curve. So intuitively, if $\abs{\vb r''(s)}$ is large then the curve is rapidly changing direction. If $\abs{\vb r''(s)}$ is small, the curve is approximately flat; there is little change in direction. Using this intuition, we will define curvature as
\[ \kappa(s) = \abs{\vb r''(s)} = \abs{\vb t'(s)} \]
In other words $\kappa$ is the magnitude of the acceleration a particle experiences while moving along the curve at unit speed.

\subsection{Torsion}
Since $\vb t = \vb r'(s)$ is a unit vector, differentiating $\vb t \cdot \vb t = 1$ gives $\vb t \cdot \vb t' = 0$. We will define the principal normal $\vb n$ by the formula
\[ \vb t' = \kappa \vb n \]
Note that $\vb n$ is everywhere normal to the curve $C$, since it is always perpendicular to the tangent vector $\vb t$, since $\vb t \cdot \vb n = 0$. We can extend the vectors $\{ \vb t, \vb n \}$ into an orthonormal basis by computing the cross product:
\[ \vb b = \vb t \times \vb n \]
We call $\vb b$ the binormal. It is a unit vector, since it is the cross product of two orthogonal unit vectors in $\mathbb R^3$. We also have that $\vb b \cdot \vb b' = 0$; also since $\vb t \cdot \vb b = 0$ and $\vb n \cdot \vb b = 0$, we must have
\[ 0 = (\vb t \cdot \vb b)' = \vb t' \cdot \vb b + \vb t \cdot \vb b' = \kappa \vb n \cdot b + \vb t \cdot \vb b' = \vb t \cdot \vb b' \]
So $\vb b'$ is orthogonal to both $\vb t$ and $\vb b$, i.e. it is parallel to $\vb n$. We will define the torsion $\tau$ of a curve by
\[ \vb b' = -\tau \vb n \]
A physical interpretation of torsion is a kind of `corkscrew' rotation in three dimensions.

\begin{proposition}[Fundamental Theorem of Differential Geometry of Curves]
    The curvature $\kappa(s)$ and torsion $\tau(s)$ uniquely define a curve in $\mathbb R^3$, up to translation and orientation.
\end{proposition}
\begin{proof}
    Since $\vb n = \vb b \times \vb t$, we have $\vb t' = \kappa(\vb b \times \vb t)$ and $\vb b' = -\tau(\vb b \times \vb t)$. This gives six equations (written in component form) for six unknowns. Given $\kappa(s)$ and $\tau(s)$, and given $\vb t(0)$ and $\vb b(0)$, we can construct the functions $\vb t(s), \vb b(s), \vb n(s) = \vb b(s) \times \vb t(s)$.
\end{proof}

\subsection{Radius of Curvature}
A generic curve $s \mapsto \vb r(s)$ can be Taylor expanded around $s=0$. Writing $\vb t = \vb t(0), \vb n = \vb n(0)$ and so on, we have
\begin{align*}
    \vb r(s) & = \vb r + s\vb r' + \frac{1}{2}s^2 \vb r'' + o(s^2)    \\
             & = \vb r + s\vb t + \frac{1}{2}s^2 \kappa\vb n + o(s^2)
\end{align*}
What circle that touches the curve at $s=0$ would be the best approximation for the curve at this point? Since the circle touches the curve, we know the position vectors (of the curve and the circle) match, and their first derivatives match. So we want to unify the second derivatives. The equation of such a circle of radius $R$ is
\[ \vb x(\theta) = \vb r + R(1 - \cos \theta)\vb n + R(\sin \theta) \vb t \]
Expanding this for small $\theta$ gives
\[ \vb x(\theta) = \vb r + R\theta\vb t + \frac{1}{2} R\theta^2\vb n + o(\theta^2) \]
But the arc length on a circle is simply $R\theta$. So in terms of arc length,
\[ \vb x(\theta) = \vb r + s\vb t + \frac{1}{2} s^2 \frac{1}{R}\vb n + o(s^2) \]
Hence by comparing coefficients,
\[ R = \frac{1}{\kappa} \]
We name this $R(s)$ the radius of curvature.

\subsection{Gaussian Curvature (non-examinable)}
This subsection is non-examinable. How can we find the curvature of a surface? At any point $\vb r$ on a surface, we have a normal vector $\vb n$. We can construct a plane containing this normal; such a plane will then intersect the surface near $\vb r$. This intersection is a curve $C$, which has a curvature $\kappa$. The choice of plane is arbitrary, however. To unify all of these different possible results for $\kappa$, we can compute the Gaussian curvature $\kappa_G$ by
\[ \kappa_G = \kappa_{\text{min}} \kappa_{\text{max}} \]
\begin{itemize}
    \item The Gaussian curvature of a flat plane is zero, since the minimum and maximum curvatures are both zero.
    \item On any point on a sphere of radius $R$, the Gaussian curvature is $\frac{1}{R^2}$, since any plane containing the normal produces a great circle of radius $R$, i.e. of curvature $\frac{1}{\kappa}$.
\end{itemize}

\begin{theorem}[Gauss's Remarkable Theorem]
    The Gaussian curvature of a surface $S$ is invariant under local isometries; i.e. if you bend the surface without stretching it.
\end{theorem}

\section{Coordinates, Differentials and Gradients}
\subsection{Differentials and First Order Changes}
Recall that for a function $f(u_1, \dots, u_n)$, we define the differential of $f$, written $\dd{f}$, by
\[ \dd{f} = \frac{\partial f}{\partial u_i} \dd{u}_i \]
noting that the summation convention applies. The $\dd{u}_i$ are called differential forms, which can be thought of as linearly independent objects (if the coordinates $u_1, \dots, u_n$ are independent), i.e. $\alpha_i \dd{u}_i = 0 \implies \alpha_i = 0$ for all $i$. Similarly, if we have a vector $\vb x(u_1, \dots, u_n)$, we define
\[ \dd \vb x = \frac{\partial \vb x}{\partial u_i} \dd{u}_i \]
As an example, let $f(u, v, w) = u^2 + w \sin(v)$. Then
\[ \dd{f} = 2u \dd{u} + w \cos(v) \dd{v} + \sin(v) \dd{w} \]
Similarly, given
\[ \vb x(u, v, w) = \begin{pmatrix}
        u^2 - v^2 \\ w \\ e^v
    \end{pmatrix} \]
we can compute
\[ \dd \vb x = \begin{pmatrix}
        2u \\ 0 \\ 0
    \end{pmatrix} \dd{u} + \begin{pmatrix}
        -2v \\ 0 \\ e^v
    \end{pmatrix} \dd{v} + \begin{pmatrix}
        0 \\ 1 \\ 0
    \end{pmatrix} \dd{w} \]
Differentials encode information about how a function (or vector field) changes when we change the coordinates by a small amount. By calculus,
\[ f(u + \delta u_1, \dots, u_n + \delta u_n) - f(u_1, \dots, u_n) = \frac{\partial f}{\partial u_i} \delta u_i + o(\delta \vb u) \]
So if $\delta f$ denotes the change in $f(u_1, \dots, u_n)$ under this small change in coordinates, we have, to first order,
\[ \delta f \approx \frac{\partial f}{\partial u_i}\delta u_i \]
The analogous result holds for vector vields:
\[ \delta \vb x \approx \frac{\partial \vb x}{\partial u_i}\delta u_i \]

\subsection{Coordinates and Line Elements in $\mathbb R^2$}
We can create multiple different consistent coordinate systems by defining a relationship between them. For example, polar coordinates $(r, \theta)$ and Cartesian coordinates $(x, y)$ can be related by
\[ x = r \cos \theta;\quad y = r \sin \theta \]
Even though this relationship is not bijective (there are multiple polar coordinates mapping to the origin), it's still a useful coordinate system because the vast majority of points work well. Even coordinate systems with a countable amount of badly-behaved points are still useful.

A general set of coordinates $(u, v)$ on $\mathbb R^2$ can be specified by their relationship to the standard Cartesian coordinates $(x, y)$. We must specify smooth, invertible functions $x(u, v)$, $y(u, v)$. We would also like to have a small change in one coordinate system to be equivalent to a small change in the other coordinate system (i.e. the inverse is also smooth). The same principle applies in $\mathbb R^3$ for three coordinates, for example.

Consider the standard Cartesian coordinates in $\mathbb R^2$.
\[ \vb x(x, y) = \begin{pmatrix}
        x \\ y
    \end{pmatrix} = x \vb e_x + y \vb e_y \]
Note that $\{\vb e_x, \vb e_y\}$ are orthonormal, and point in the same direction regardless of the value of $\vb x$: $\vb e_x$ points in the direction of changing $x$ with $y$ held constant, for example. Equivalently,
\[ \vb e_x = \frac{\frac{\partial}{\partial x} \vb x(x, y)}{\abs{\frac{\partial}{\partial x} \vb x(x, y)}};\quad \vb e_y = \frac{\frac{\partial}{\partial y} \vb x(x, y)}{\abs{\frac{\partial}{\partial y} \vb x(x, y)}} \]
Note that
\[ \dd \vb x = \frac{\partial \vb x}{\partial x}\dd{x} + \frac{\partial \vb x}{\partial y} \dd{y} = \dd{x} \,\vb e_x + \dd{y} \,\vb e_y \]
In other words, when applying the change in coordinate $x \mapsto x + \delta x$, the vector changes (to first order) to $\vb x \mapsto \vb x + \delta x \vb e_x$. In fact, in the case of Cartesian coordinates, this change is precisely correct for any size of $\delta$, since the coordinate basis vectors are the same everywhere. We call $\dd \vb x$ the line element; it tells us how small changes in coordinates produce changes in position vectors.

Now, let us consider polar coordinates in two-dimensional space. We can use the same idea as before, giving
\[ \vb e_r = \frac{\frac{\partial}{\partial r} \vb x(r, \theta)}{\abs{\frac{\partial}{\partial r} \vb x(r, \theta)}} = \begin{pmatrix}
        \cos\theta \\ \sin\theta
    \end{pmatrix};\quad \vb e_\theta = \frac{\frac{\partial}{\partial \theta} \vb x(r, \theta)}{\abs{\frac{\partial}{\partial \theta} \vb x(r, \theta)}} = \begin{pmatrix}
        -\sin\theta \\ \cos\theta
    \end{pmatrix} \]
Therefore, we have
\[ \vb x(r, \theta) = \begin{pmatrix}
        r \cos\theta \\ r \sin\theta
    \end{pmatrix} = r\vb e_r \]
Note that $\{\vb e_r, \vb e_\theta\}$ are also orthonormal at each $(r, \theta)$, but their exact values are not the same everywhere. Since the basis vectors are orthogonal, we can call $r$ and $\theta$ orthogonal curvilinear coordinates. Also, we can compute the line element $\dd \vb x$ as
\[ \dd \vb x = \frac{\partial \vb x}{\partial r} \dd{r} + \frac{\partial \vb x}{\partial \theta} \dd \theta = \begin{pmatrix}
        \cos \theta \\ \sin \theta
    \end{pmatrix} \dd{r} + \begin{pmatrix}
        -r \sin \theta \\ r \cos \theta
    \end{pmatrix} \dd \theta = \dd{r} \, \vb e_r + r\, \dd \theta \, \vb e_\theta \]
We see that a change in $\theta$ produces (up to first order) a change $\vb x \mapsto \vb x + r \,\delta \theta \,\vb e_\theta$, a change proportional to $r$. So a small change in $\theta$ could cause quite a large change in Cartesian coordinates.

\subsection{Orthogonal Curvilinear Coordinates}
We say that $(u, v, w)$ are a set of orthogonal curvilinear coordinates if the vectors
\[ \vb e_u = \frac{\frac{\partial \vb x}{\partial u}}{\abs{\frac{\partial \vb x}{\partial u}}};\quad \vb e_v = \frac{\frac{\partial \vb x}{\partial v}}{\abs{\frac{\partial \vb x}{\partial v}}};\quad \vb e_w = \frac{\frac{\partial \vb x}{\partial w}}{\abs{\frac{\partial \vb x}{\partial w}}} \]
form a right-handed, orthonormal basis for each $(u, v, w)$; but not necessarily the same basis over the entire vector field. It is standard to write
\[ h_u = \abs{\frac{\partial \vb x}{\partial u}};\quad h_v = \abs{\frac{\partial \vb x}{\partial v}};\quad h_w = \abs{\frac{\partial \vb x}{\partial w}} \]
We call $h_u, h_v, h_w$ the scale factors.  Note that the line element is
\begin{align*}
    \dd \vb x & = \frac{\partial \vb x}{\partial u}\dd{u} + \frac{\partial \vb x}{\partial v}\dd{v} + \frac{\partial \vb x}{\partial w} \dd{w} \\
              & = h_u \vb e_u \dd{u} + h_v \vb e_v \dd{v} + h_w \vb e_w \dd{w}
\end{align*}
So the scale factors show how first-order changes in the coordinates are scaled into changes in $\vb x$.

\subsection{Cylindrical Polar Coordinates}
We define $(\rho, \phi, z)$ by
\[ \vb x(\rho, \phi, z) = \begin{pmatrix}
        \rho \cos \phi \\
        \rho \sin \phi \\
        z
    \end{pmatrix} \]
where $0 \leq \rho; 0 \leq \phi < 2 \pi; z \in \mathbb R$. So we can find
\[ \vb e_\rho = \begin{pmatrix}
        \cos \phi \\ \sin \phi \\ 0
    \end{pmatrix};\quad \vb e_\phi = \begin{pmatrix}
        -\sin \phi \\ \cos \phi \\ 0
    \end{pmatrix};\quad \vb e_z = \begin{pmatrix}
        0 \\ 0 \\ 1
    \end{pmatrix} \]
The scale factors are
\[ h_\rho = 1;\quad h_\phi = \rho;\quad h_z = 1 \]
The line element is
\[ \dd \vb x = \dd \rho \, \vb e_\rho + \rho \, \dd \phi \, \vb e_\phi + \dd{z} \, \vb e_z \]
Note that
\[ \vb x = \rho \begin{pmatrix}
        \cos \phi \\ \sin \phi \\ 0
    \end{pmatrix} + z \begin{pmatrix}
        0 \\ 0 \\ 1
    \end{pmatrix} = \rho \vb e_\rho + z \vb e_z \]

\subsection{Spherical Polar Coordinates}
We define $(r, \theta, \phi)$ by
\[ \vb x(r, \theta, \phi) = \begin{pmatrix}
        r \cos \phi \sin \theta \\
        r \sin \phi \sin \theta \\
        r \cos \theta
    \end{pmatrix} \]
where $0 \leq r; 0 \leq \theta < 2 \pi; 0 \leq \phi < 2 \pi$. So we can find
\[ \vb e_r = \begin{pmatrix}
        \cos \phi \sin \theta \\ \sin \phi \sin \theta \\ \cos \theta
    \end{pmatrix};\quad \vb e_\theta = \begin{pmatrix}
        \cos \phi \cos \theta \\ \sin \phi \cos \theta \\ -\sin \theta
    \end{pmatrix};\quad \vb e_\phi = \begin{pmatrix}
        -\sin \phi \\ \cos \phi \\ 0
    \end{pmatrix} \]
The scale factors are
\[ h_r = 1;\quad h_\theta = r;\quad h_\phi = r \sin \theta \]
The line element is
\[ \dd \vb x = \dd{r} \, \vb e_r + r \, \dd \theta \, \vb e_\theta + r \sin \theta \, \dd \phi \, \vb e_\phi \]
Note that
\[ \vb x = r \begin{pmatrix}
        \cos \phi \sin \theta \\ \sin \phi \sin \theta \\ \cos \theta
    \end{pmatrix} = r \vb e_r \]

\section{Gradient Operator}
\subsection{Definition}
For $f \colon \mathbb R^3 \to \mathbb R$, we define the gradient of $f$, written $\grad f$, by
\begin{equation}
    f(\vb x + \vb h) = f(\vb x) + \grad f(\vb x) \cdot \vb h + o(\vb h)
    \tag{$\ast$}
\end{equation}
as $\abs{\vb h} \to 0$. The directional derivative of $f$ in the direction $\vb v$, denoted by $D_{\vb v} f$ or $\frac{\partial f}{\partial \vb v}$, is defined by
\[ D_{\vb v} f(\vb x) = \lim_{t \to 0} \frac{f(\vb x + t\vb v) - f(\vb x)}{t} \]
Alternatively,
\begin{equation}
    f(\vb x + t\vb v) = f(\vb x) + t D_{\vb v}f(\vb x) + o(t)
    \tag{$\dagger$}
\end{equation}
as $t \to 0$. Setting $\vb h = t\vb v$ in $(\ast)$, we have
\[ f(\vb x + t\vb v) = f(\vb x) + t \grad f(\vb x) \cdot \vb v + o(t) \]
This gives another way to interpret the gradient of $f$. Comparing this result to $(\dagger)$, we see that
\[ D_{\vb v} f = \vb v \cdot \grad f \]
By the Cauchy-Schwarz inequality, the dot product is maximised when the two vectors are parallel. Hence, the directional derivative is maximised when $\vb v$ points in the direction of $\grad f$. So $\grad f$ points in the direction of greatest increase of $f$. Similarly, $-\grad f$ points in the direction of greatest decrease of $f$. For example, suppose $f(x) = \frac{1}{2}\abs{\vb x}^2$. Then
\[ f(\vb x + \vb h) = \frac{1}{2}(\vb x + \vb h)\cdot (\vb x + \vb h) = \frac{1}{2}\abs{\vb x}^2 + \frac{1}{2}(2\vb x \cdot \vb h) + \frac{1}{2}\abs{\vb h}^2 = f(\vb x) + \vb x \cdot \vb h + o(\vb h) \]
Hence $\grad f(\vb x) = \vb x$.

\subsection{Gradient on Curves}
Suppose we have a curve $t \mapsto \vb x(t)$. How does some function $f$ change when moving along the curve? We will write $F(t) = f(\vb x(t)), \delta \vb x = \vb x(t + \delta t) - \vb x(t)$.
\begin{align*}
    F(t + \delta t) & = f(\vb x(t + \delta t))                                               \\
                    & = f(\vb x(t) + \delta \vb x)                                           \\
                    & = f(\vb x(t)) + \grad f(\vb x(t)) \cdot \delta \vb x + o(\delta \vb x) \\
    \intertext{Since $\delta \vb x = \vb x'(t) \,\delta t + o(\delta t)$, we have}
    F(t + \delta t) & = F(t) + \vb x'(t) \cdot \grad f(\vb x(t)) \,\delta t + o(\delta t)
\end{align*}
In other words,
\[ \frac{\dd{F}}{\dd{t}} = \frac{\dd}{\dd{t}}f(\vb x(t)) = \frac{\dd \vb x}{\dd{t}} \cdot \grad f(\vb x(t)) \]

\subsection{Gradient on Surfaces}
Suppose we have a surface $S$ in $\mathbb R^3$ defined implicitly by
\[ S = \{ \vb x \in \mathbb R^3 : f(\vb x) = 0 \} \]
If $t \mapsto \vb x(t)$ is any curve in $S$, then $f(\vb x(t)) = 0$ everywhere. So
\[ 0 = \frac{\dd}{\dd{t}}f(\vb x(t)) = \grad f(\vb x(t)) \cdot \frac{\dd \vb x}{\dd{t}} \]
So $\grad f(\vb x(t))$, the gradient, is orthogonal to $\frac{\dd \vb x}{\dd{t}}$, the tangent vector of any chosen curve in $S$. So $\grad f(\vb x(t))$ is normal to the surface.

\subsection{Coordinate-Independent Representation}
If we are working in an orthogonal curvilinear coordinate system $(u, v, w)$, it is not immediately clear how to compute $\grad f$, since we need to represent this arbitrary perturbation $\vb h$ using $(u, v, w)$. In Cartesian coordinates it is simple; to represent the change $\vb x \mapsto \vb x + \vb h$ we simply add the components of $\vb x$ and $\vb h$.
\begin{align*}
    f(\vb x + \vb h) & = f((x + h_1, y + h_2, z + h_3))                                                                                                  \\
                     & = f(\vb x) + \frac{\partial f}{\partial x} h_1 + \frac{\partial f}{\partial y} h_2 + \frac{\partial f}{\partial z} h_3 + o(\vb h) \\
                     & = f(\vb x) + \begin{pmatrix}
        \partial f / \partial x \\ \partial f / \partial y \\ \partial f / \partial z
    \end{pmatrix} \cdot h + o(\vb h)                                                                        \\
\end{align*}
So we have
\[ \implies \grad f = \begin{pmatrix}
        \partial f / \partial x \\ \partial f / \partial y \\ \partial f / \partial z
    \end{pmatrix} \]
Or, using suffix notation,
\[ \grad f = \vb e_i \frac{\partial f}{\partial x_i};\quad [\grad f]_i = \frac{\partial f}{\partial x_i} \]
We see that this $\grad$ is a kind of vector differential operator. In Cartesian coordinates,
\[ \grad = \vb e_x \frac{\partial}{\partial x} + \vb e_y \frac{\partial}{\partial y} + \vb e_z \frac{\partial}{\partial z} \equiv \vb e_i \frac{\partial}{\partial x_i} \]
From our previous example,
\[ f(\vb x) = \frac{1}{2}(x^2 + y^2 + z^2) = \frac{1}{2}\abs{\vb x}^2 \]
\begin{align*}
    [\grad f]_i & = \frac{\partial}{\partial x_i}\left[ \frac{1}{2} x_j x_j \right] \\
                & = \frac{1}{2} \left[ \delta_{ij} x_j + x_j \delta_{ij} \right]    \\
                & = x_i                                                             \\
    \grad f     & = \vb e_i x_i
\end{align*}
Let us return back to computing the gradient in the general case. Recall that in Cartesian coordinates, the line element is simple:
\[ \dd \vb x = \dd{x}_i \vb e_i \]
And also, if we have a function on $\mathbb R^3$ such as $f(x, y, z)$, it has the differential
\[ \dd{f} = \frac{\partial f}{\partial x_i}\dd{x}_i \]
Then,
\begin{align*}
    \grad f \cdot \dd \vb x & = \left( \vb e_i \frac{\partial f}{\partial x_i} \right) \cdot \left( \vb e_j \dd{x}_j \right) \\
                            & = \frac{\partial f}{\partial x_i} \left( \vb e_i \cdot \vb e_j \right) \dd{x}_j                \\
                            & = \frac{\partial f}{\partial x_i} \delta_{ij} \dd{x}_j                                         \\
                            & = \frac{\partial f}{\partial x_i} \dd{x}_i                                                     \\
                            & = \dd{f}
\end{align*}
In other words, in \textit{any} set of coordinates,
\[ \grad f \cdot \dd \vb x = \dd{f} \]

\subsection{Computing the Gradient Vector}
\begin{proposition}
    If $(u, v, w)$ are orthogonal curvilinear coordinates, and $f$ is a function of the position vector $(u, v, w)$, then
    \[ \grad f = \frac{1}{h_u}\frac{\partial f}{\partial u}\vb e_u + \frac{1}{h_v}\frac{\partial f}{\partial v}\vb e_v + \frac{1}{h_w}\frac{\partial f}{\partial w}\vb e_w \]
\end{proposition}
\begin{proof}
    If $f = f(u, v, w)$ and $\vb x = \vb x(u, v, w)$, then
    \[ \dd{f} = \frac{\partial f}{\partial u}\dd{u} + \frac{\partial f}{\partial v}\dd{v} + \frac{\partial f}{\partial w}\dd{w} \]
    \[ \dd{x} = h_u \dd{u} \vb e_u + h_v \dd{v} \vb e_v + h_w \dd{w} \vb e_w \]
    Using the above result, we have
    \[ \grad f \cdot \dd \vb x = \dd{f} \]
    \[ \left( (\grad f)_u \vb e_u + (\grad f)_v \vb e_v + (\grad f)_w \vb e_w \right) \cdot \left( h_u \dd{u} \vb e_u + h_v \dd{v} \vb e_v + h_w \dd{w} \vb e_w \right) = \frac{\partial f}{\partial u}\dd{u} + \frac{\partial f}{\partial v}\dd{v} + \frac{\partial f}{\partial w}\dd{w} \]
    \[  (\grad f)_u h_u \dd{u} + (\grad f)_v h_v \dd{v} + (\grad f)_w h_w \dd{w} = \frac{\partial f}{\partial u}\dd{u} + \frac{\partial f}{\partial v}\dd{v} + \frac{\partial f}{\partial w}\dd{w} \]
    Since $u, v, w$ are independent coordinates, $\dd{u}, \dd{v}, \dd{w}$ are linearly independent. So we can simply compare coefficients, getting
    \[ \grad f = \frac{1}{h_u}\frac{\partial f}{\partial u}\vb e_u + \frac{1}{h_v}\frac{\partial f}{\partial v}\vb e_v + \frac{1}{h_w}\frac{\partial f}{\partial w}\vb e_w \]
    as required.
\end{proof}
\noindent In cylindrical polar coordinates, we have
\[ \grad f = \frac{\partial f}{\partial \rho} \vb e_\rho + \frac{1}{\rho} \frac{\partial f}{\partial \phi} \vb e_\phi + \frac{\partial f}{\partial z} \vb e_z \]
In spherical polar coordinates, we have
\[ \grad f = \frac{\partial f}{\partial r} \vb e_r + \frac{1}{r} \frac{\partial f}{\partial \theta} \vb e_\theta + \frac{1}{r\sin\theta} \frac{\partial f}{\partial \phi} \vb e_\phi \]
Then using the familiar example $f(\vb x) = \frac{1}{2}\abs{\vb x}^2$, we have
\[
    f = \begin{cases}
        \frac{1}{2}(x^2 + y^2 + z^2) & \text{in Cartesian coordinates}         \\
        \frac{1}{2}(\rho^2 + z^2)    & \text{in cylindrical polar coordinates} \\
        \frac{1}{2}r^2               & \text{in spherical polar coordinates}   \\
    \end{cases}
\]
Then we can check the value of $\grad f$ in these different coordinate systems.
\begin{align*}
    \grad f & = \begin{cases}
        x \vb e_x + y \vb e_y + z \vb e_z & \text{in Cartesian coordinates}         \\
        \rho \vb e_\rho + z \vb e_z       & \text{in cylindrical polar coordinates} \\
        r \vb e_r                         & \text{in spherical polar coordinates}   \\
    \end{cases} \\
            & = \vb x
\end{align*}

\section{Integration over Lines}
\subsection{Line Integrals}
For a vector field $\vb F(\vb x)$ and a piecewise smooth parametrised curve $C$ defined by $[a, b] \ni t \mapsto \vb x(t)$, we define the line integral of $F$ along $C$
\[ \int_C \vb F \cdot \dd \vb x = \int_a^b \vb F(\vb x(t)) \cdot \underbrace{\frac{\dd \vb x}{\dd{t}}}_{\mathclap{\text{tangent vector}}} \dd{t} \]
Note that this tangent vector is not necessarily normalised, and note further that the curve direction matters. If we want to integrate in the other direction, it is common to write $\int_{-C}$ instead. We can think of this line integral as the work done by a particle moving along $C$ in the presence of a force $F$. As an example, consider the vector field given by
\[ \vb F = \begin{pmatrix}
        x^2 y \\ yz \\ 2xz
    \end{pmatrix} \]
Consider two curves connecting the origin to the position vector $\begin{pmatrix}
        1 \\ 1 \\ 1
    \end{pmatrix}$.
\[ C_1 \colon [0, 1] \ni t \mapsto \begin{pmatrix}
        t \\ t \\ t
    \end{pmatrix};\quad C_2 \colon [0, 1] \ni t \mapsto \begin{pmatrix}
        t \\ t \\ t^2
    \end{pmatrix} \]
\[ \int_{C_1}\vb F \cdot \dd \vb x = \int_0^1 \begin{pmatrix}
        t^3 \\ t^2 \\ 2t^2
    \end{pmatrix} \cdot \begin{pmatrix}
        1 \\ 1 \\ 1
    \end{pmatrix} \dd{t} = \frac{5}{4} \]
\[ \int_{C_2}\vb F \cdot \dd \vb x = \int_0^1 \begin{pmatrix}
        t^3 \\ t^3 \\ 2t^3
    \end{pmatrix} \cdot \begin{pmatrix}
        1 \\ 1 \\ 2t
    \end{pmatrix} \dd{t} = \frac{13}{10} \]
In general, the result of the line integral depends on the path taken between the two points. In the force analogy, there might be a path between $A$ and $B$ that is very easy to traverse, and another path that is very difficult (i.e. uses a lot of energy).

Now, consider a particle at $\vb x$ experiencing a force $\vb F$, represented in cylindrical polar coordinates as
\[ \vb F(\vb x) = z\rho \vb e_{\phi} \]
Consider the path $C$ given by
\[ C \colon [0, 2\pi] \ni t \mapsto \begin{pmatrix}
        a \cos t \\ a \sin t \\ t
    \end{pmatrix} \]
What is the work done by the particle travelling along $C$? Using the definition of the line element $\dd \vb x$ in cylindrical polar coordinates, we can compute that $\vb F \dd \vb x = z \rho^2 \dd \phi$. Note that in cylindrical polar coordinates, the path can be represented simply as $(\rho, \phi, z) = (a, t, t)$. Hence,
\[ (\dd \rho, \dd \phi, \dd{z}) = (0, \dd{t}, \dd{t}) \]
Therefore, $\vb F \dd \vb x = z \rho^2 \dd{t}$. We can now compute the integral:
\[ \int_C \vb F \cdot \dd \vb x = a^2 \int_0^{2\pi} t\dd{t} = 2\pi^2a^2 \]

\subsection{Closed Curves}
A curve $[a, b] \ni t \mapsto \vb x(t)$ might be such that $\vb x(a) = \vb x(b)$. This is called a closed curve. The line integral around a closed loop is written
\[ \oint_C \vb F \cdot \dd \vb x \]
Sometimes, this is called the `circulation' of $\vb F$ about $C$. Consider the first example from this lecture, with curves $C_1$ and $C_2$. Let $C = C_1 - C_2$. Then
\[ \oint_C \vb F \cdot \dd \vb x = \int_{C_1} \vb F \cdot \dd \vb x - \int_{C_2} \vb F \cdot \dd \vb x = \frac{-2}{15} \]

\subsection{Conservative Forces and Exact Differentials}
We have seen how to interpret things like $\vb F \cdot \dd \vb x$ when inside an integral. This is an example of a differential form; in orthogonal curvilinear coordinates $(u, v, w)$ we have
\[ \vb F \cdot \dd \vb x = a \, \dd{u} + b \, \dd{v} + c \, \dd{w} \]
for some $a, b, c$ dependent on $u, v, w$. We say that $\vb F \cdot \dd \vb x$ is exact if
\[ \vb F \cdot \dd \vb x = \dd{f} \]
for some scalar function $f$. Recall that $\dd{f} = \grad f \cdot \dd \vb x$. So equivalently, $\vb F \cdot \dd \vb x$ is exact if and only if
\[ \vb F = \grad f \]
Such a vector field is called conservative. $\vb F \cdot \dd \vb x$ is exact if and only if $\vb F$ is conservative. Using the properties that $\dd (\alpha f + \beta g) = \alpha \, \dd{f} + \beta \, \dd{g}$, $\dd (fg) = g \, \dd{f} + f \, \dd{g}$ and so on, it is usually easy to see if a differential form is exact.

\begin{proposition}
    If $\theta$ is an exact differential form, then
    \[ \oint_C \theta = 0 \]
    for any closed curve $C$.
\end{proposition}
\begin{proof}
    If $\theta$ is exact, then $\theta = \grad f \cdot \dd \vb x$ for some scalar function $f$. Given a curve $C \colon [a, b] \ni t \mapsto \vb x(t)$,
    \begin{align*}
        \oint_C \theta & = \oint_C \grad f \cdot \dd \vb x                                  \\
                       & = \int_a^b \grad f(\vb x(t)) \cdot \frac{\dd \vb x}{\dd{t}} \dd{t} \\
        \intertext{By the previous lecture,}
                       & = \int_a^b \frac{\dd}{\dd{t}} \left[ f(\vb x(t)) \right] \dd{t}    \\
                       & = f(\vb x(a)) - f(\vb x(b))                                        \\
                       & = 0
    \end{align*}
    since $\vb x(a) = \vb x(b)$.
\end{proof}
\noindent Note, for example in cylindrical polar coordinates, that $f(\rho, \phi, z) = \phi$ is not a function on $\mathbb R^3$, since there are many possible values of $\phi$ for any given position vector. These are called multi-valued functions; for example the contour integral of this function over a circle where $\phi \in [0, 2\pi]$ is not well-defined, since $f(\rho, 0, z) \neq f(\rho, 2 \pi, z)$.

Note that if $\vb F$ is conservative, then the circulation of $\vb F$ around any closed curve $C$ vanishes. This means that the line integral between $A$ and $B$ is not dependent on the path chosen between the two points; simply choose the most convenient curve for the problem.

Let $(u, v, w) = (u_1, u_2, u_3)$ be a set of orthogonal curvilinear coordinates. Let
\[ \vb F \cdot \dd \vb x = \theta = \underbrace{A(u, v, w) \, \dd{u}}_{\theta_1} + \underbrace{B(u, v, w) \, \dd{v}}_{\theta_2} + \underbrace{C(u, v, w) \, \dd{w}}_{\theta_3} = \theta_i \dd{u}_i \]
A necessary condition for $\theta$ to be exact is
\begin{equation}
    \frac{\partial \theta_i}{\partial u_j} = \frac{\partial \theta_j}{\partial u_i}
    \tag{$\dagger$}
\end{equation}
Indeed, if $\theta$ is exact, then $\theta = \dd{f}$, so
\[ \theta = \frac{\partial f}{\partial u_i} \, \dd{u}_i \iff \theta_i = \frac{\partial f}{\partial u_i} \]
and therefore,
\[ \frac{\partial \theta_i}{\partial u_j} = \frac{\partial^2 f}{\partial u_j \partial u_i} = \frac{\partial \theta_j}{\partial u_i} \]
A differential form $\theta = \theta_i \, \dd{u}_i$ that obeys $(\dagger)$ is called a \textit{closed} differential form. Certainly any exact differential form is closed. A differential form is exact if it is closed \textit{and} the domain $\Omega \subset \mathbb R^3$ on which $\theta$ is defined is simply connected, i.e. all closed loops in $\Omega$ can be continuously `shrunk' to any point inside $\Omega$ without leaving it. This is notable, since one direction of implication is related to calculus, but the other direction is related to topology.

Now, let us consider an example. Let
\[ \theta = y \dd{x} - x \dd{y} \]
Is this differential form exact? First, we will check if it is closed.
\[ \frac{\partial}{\partial y} y = 1;\quad \frac{\partial}{\partial x} (-x) = -1 \]
It is not closed, so it is not exact. As another example, let us compute the line integral
\[ \int_C 3x^2y\dd{x} + x^3\dd{y} \]
where
\[ C \colon [\alpha_1, \alpha_{100}] \ni t \mapsto \begin{pmatrix}
        \cos \left( \Im \left( \zeta \left( \frac{1}{2} + it \right) \right) \right) \\
        \sin \left( \Re \left( \zeta \left( \frac{1}{2} + it \right) \right) \right) \\
        0
    \end{pmatrix} \]
where $\alpha_1$ and $\alpha_{100}$ are the 1st and 100th zeroes of $\zeta \left( \frac{1}{2} + it \right)$. The loop is closed and exact; $\dd(x^3 y) = 3x^2 y \dd{x} + x^3 \, \dd{y}$. So the result is zero. As a final example, consider a particle travelling along a curve $C \colon [a, b] \ni t \mapsto \vb x(t)$. Then the work done is
\begin{align*}
    W & = \int_C \vb F \cdot \dd \vb x                    \\
      & = m \int_a^b \ddot{\vb x} + \dot{\vb x} \, \dd{t} \\
      & = \frac{1}{2}m \eval{\abs{\dot{\vb x}}^2}_a^b
\end{align*}
which is the change in kinetic energy. If $\vb F = -\grad V$, i.e. $\vb F$ is conservative,
\[ \int_C \vb F \cdot \dd \vb x = -\int_C \grad V \cdot \dd \vb x = V(\vb x(a)) - V(\vb x(b)) \]
So the change in kinetic energy is equal to the change in potential energy; energy is conserved.

\section{Integration in $\mathbb R^2$}
\subsection{Definition of Integral in $\mathbb R^2$}
We can integrate over a bounded region $D \subset \mathbb R^2$. To do this, we can cover $D$ with small, disjoint sets $A_{ij}$ each with area $\delta A_{ij}$. Each of these sets $A_{ij}$ are contained in a disc of radius $\varepsilon > 0$. Let $(x_i, y_j)$ be points contained in each $A_{ij}$. We now define
\[ \int_D f(\vb x) \dd{A} = \lim_{\varepsilon \to 0} \sum_{i, j} f(x_i, y_j) \,\delta A_{ij} \]
The integral exists if it is independent of the choice of partitions $A_{ij}$ and the points $(x_i, y_j)$. The obvious choice of partitioning $D$ is to use rectangles where the area of each rectangle is $\delta A_{ij} = \delta x_i \delta y_j$. We can create horizontal `strips' of height $\delta y$ which we can integrate over. The possible $x$ coordinates for this strip are $x_y = \{ x \colon (x, y) \in D \}$. We can take the limit as $\delta x \to 0$, giving
\[ \delta y \int_{x_y} f(x, y) \dd{x} \]
Summing over each such strip, taking the limit as $\delta y \to 0$, we have
\[ \int_D f(x, y) \dd{A} = \int_Y \left( \int_{x_y} f(x, y) \dd{x} \right) \dd{y} \]
where $Y$ is the set of all possible $y$ coordinates, i.e. $Y = \{ y \colon \exists x, (x, y) \in D \}$. We can equivalently sum over all vertical strips, and get
\[ \int_D f(x, y) \dd{A} = \int_X \left( \int_{y_x} f(x, y) \dd{y} \right) \dd{x} \]
More concisely, we can write the following (Fubini's Theorem):
\[ \dd{A} = \dd{x} \, \dd{y} = \dd{y} \, \dd{x} \]
Let us consider an example; let $D$ be the triangle with vertices $(0, 0), (1, 0), (0, 1)$. If $f(x, y) = xy^2$, then by integrating over horizontal strips, we have
\begin{align*}
    \int_D f(x, y) \dd{A} & = \int_0^1 \left( \int_0^{1-y} xy^2 \dd{x} \right) \dd{y}  \\
                          & = \int_0^1 \left[ \frac{1}{2}x^2y^2 \right]_0^{1-y} \dd{y} \\
                          & = \int_0^1 \frac{1}{2}(1-y)^2y^2 \dd{y}                    \\
                          & = \frac{1}{60}
\end{align*}
Instead, integrating over vertical strips, we have
\begin{align*}
    \int_D f(x, y) \dd{A} & = \int_0^1 \left( \int_0^{1-x} xy^2 \dd{y} \right) \dd{x} \\
                          & = \int_0^1 \left[ \frac{1}{3} xy^3 \right]_0^{1-x} \dd{x} \\
                          & = \int_0^1 \frac{1}{3} x(1-x)^3 \dd{x}                    \\
                          & = \frac{1}{60}
\end{align*}
Note that if $f(x, y) = g(x) \cdot h(y)$, and $D$ is a rectangle $\{ (x, y) \colon a \leq x \leq b, c \leq y \leq d \}$, then
\[ \int_A f(x, y) \dd{A} = \left( \int_a^b g(x) \dd{x} \right)\left( \int_c^d h(y) \dd{y} \right) \]

\subsection{Change of Variables in $\mathbb R^2$}
It can be useful to introduce a change of variables in order to compute the one-dimensional integral. For example, if $x$ is represented as a function of $u$,
\[ \int_a^b f(x) \dd{x} = \int_{x^{-1}(a)}^{x^{-1}(b)} f(x(u)) \frac{\dd{x}}{\dd{u}}\dd{u} \]
Note that if $\frac{\dd{x}}{\dd{u}} > 0$, then the right hand side integral is taken over a limit from a smaller value to a larger one, but if $\frac{\dd{x}}{\dd{u}} < 0$, then the integral is the `wrong way round'. If $I = [a,b]$ and $I' = x^{-1} I$, we have
\[ \int_I f(x) \dd{x} = \int_{I'} f(x(u)) \abs{\frac{\dd{x}}{\dd{u}}} \dd{u} \]
where the absolute value is used since $I'$ is defined as going from the lower limit to the upper limit. There is a similar formula in 2D.
\begin{proposition}
    Let $\vb x(u, v) = (x(u, v), y(u, v))$ be a smooth, invertible transformation with a smooth inverse that maps the region $D'$ in the $(u, v)$ plane to the region $D$ in the $(x, y)$ plane. (This map must be a bijection; every point must have a unique inverse.) Then
    \[ \iint_D f(x, y) \dd{x} \dd{y} = \iint_{D'} f(x(u, v), y(u, v)) \abs{\frac{\partial (x, y)}{\partial (u, v)}}\dd{u} \dd{v} \]
    where
    \[ \frac{\partial (x, y)}{\partial (u, v)} = J = \det \begin{pmatrix}
            \partial x / \partial u & \partial x / \partial v \\
            \partial y / \partial u & \partial y / \partial v
        \end{pmatrix} = \det \left( \frac{\partial \vb x}{\partial u} \,\middle|\, \frac{\partial \vb x}{\partial v} \right) \]
    is the Jacobian determinant. More concisely,
    \[ \dd{x} \, \dd{y} = \abs{J} \, \dd{u} \, \dd{v} \]
    It doesn't matter if the Jacobian vanishes at a single point, since the area of a single point is zero and hence will have no contribution to the result. The Jacobian being zero means that something non-smooth is happening at this point, so it is important to consider why this point is special.
\end{proposition}
\begin{proof}
    We can form a partition of $D$ by using the image of a rectangular partition of $D'$. Let the rectangular partition be characterised by a horizontal step $\delta x$ and a vertical step of $\delta y$. Then each small rectangle in $D'$ is mapped to some small (not necessarily rectangular) region in $D'$, with vertices
    \[ \vb x(u_i, v_j), \vb x(u_{i+1}, v_j), \vb x(u_{i+1}, v_{j+1}), \vb x(u_i, v_{j+1}) \]
    To first order, the area of this region is the area of the parallelogram with the same vertices. Two of the sides of the parallelogram are
    \[ \vb x(u_{i+1}, v_j) - \vb x(u_i, v_j) \approx \frac{\partial \vb x}{\partial u}(u_i, v_j) \delta u \]
    \[ \vb x(u_i, v_{j+1}) - \vb x(u_i, v_j) \approx \frac{\partial \vb x}{\partial v}(u_i, v_j) \delta v \]
    So the area of the parallelogram is approximately
    \begin{align*}
        \abs{\frac{\partial \vb x}{\partial u}(u_i, v_j) \delta u \cdot \frac{\partial \vb x}{\partial v}(u_i, v_j) \delta v} & = \abs{\det \left( \frac{\partial \vb x}{\partial u}(u_i, v_j) \,\middle|\, \frac{\partial \vb x}{\partial v}(u_i, v_j) \right)} \\
                                                                                                                              & = \abs{J(u_i, v_j)} \,\delta u \,\delta v                                                                                        \\
                                                                                                                              & = \delta A_{ij}
    \end{align*}
    Hence,
    \begin{align*}
        \int_D f \, \dd{A} & = \lim_{\varepsilon \to 0} \sum_{ij} f(x_i, y_j)\,\delta A_{ij}                                           \\
                           & = \lim_{\varepsilon \to 0} \sum_{ij} f(x(u_i, v_j), y(u_i, v_j)) \,\abs{J(u_i, v_j)} \,\delta u\,\delta v \\
                           & = \iint_{D'} f(x(u, v), y(u, v)) \,\abs{J(u_i, v_j)} \dd{u} \dd{v}
    \end{align*}
\end{proof}
\noindent As an example, let us consider polar coordinates $(\rho, \phi)$, where
\[ x(\rho, \phi) = \rho \cos \phi;\quad y(\rho, \phi) = \rho \sin \phi \]
Hence,
\[ \abs{J} = \abs{\det \begin{pmatrix}
            \cos \phi & -\rho \sin \phi \\
            \sin \phi & \rho \cos \phi
        \end{pmatrix}} = \abs{\rho} = \rho \]
If $D = \{ (x, y) \colon x > 0,\, y > 0,\, x^2 + y^2 < r^2 \}$, which is a quarter-circle of radius $r$ in the first quadrant, then $D' = \{ (\rho, \phi) \colon 0 < \rho < r,\, 0 < \phi < \frac{\pi}{2} \}$. This is notably a rectangle in polar coordinates.
\[ \iint_D f(x, y)\dd{x} \dd{y} = \iint_{D'} f(\rho \cos \phi, \rho \sin \phi) \,\rho \,\dd \rho \,\dd \phi \]
So, for example, if we let $r \to \infty$, then
\[ \int_{x=0}^\infty \int_{y=0}^\infty f(x, y) \dd{y}\dd{x} = \int_{\phi = 0}^{\frac{\pi}{2}} \int_{\rho = 0}^\infty f(\rho \cos \phi, \rho \sin \phi) \,\rho\,\dd \rho\,\dd \phi \]
Consider
\[ I = \int_0^\infty e^{-x^2} \dd{x} \]
Then,
\begin{align*}
    I^2        & = \int_0^\infty e^{-x^2} \dd{x} \cdot \int_0^\infty e^{-y^2} \dd{y}                                \\
               & = \int_{x=0}^\infty \int_{y=0}^\infty e^{-x^2-y^2} \dd{y}\dd{x}                                    \\
               & = \int_{\phi = 0}^{\frac{\pi}{2}} \int_{\rho = 0}^\infty e^{-\rho^2} \,\rho\,\dd \rho\,\dd \phi    \\
               & = \frac{\pi}{2} \int_0^\infty \frac{\dd}{\dd \rho} \left( -\frac{1}{2}e^{-\rho^2} \right) \dd \rho \\
               & = \frac{\pi}{4}                                                                                    \\
    \implies I & = \frac{\sqrt{\pi}}{2}
\end{align*}

\section{Integration in $\mathbb R^3$}
\subsection{Example and Change of Variables}
To integrate over regions $V$ in $\mathbb R^3$, we can use similar ideas to those discussed in the previous lecture.
\[ \int_V f(\vb x) \dd{V} = \lim_{\varepsilon \to 0} \sum_{i,j,k} f(x_i, y_j, z_k) \,\delta V_{ijk} \]
where the $\delta V_{ijk}$ partition $V$, and each contain the point $(x_i, y_j, z_k)$. In this case, the volume element satisfies
\[ \dd{V} = \dd{x}\dd{y}\dd{z} \]
The integrals may be computed in any order. As an example, consider the simplex defined by
\[ V = \{ x > 0,\, y > 0,\, z > 0,\, x+y+z < 1 \} \]
We can compute the volume using the integral
\begin{align*}
    I & = \int_{z=0}^1 \int_{y=0}^{1-z} \int_{x=0}^{1-y-z} 1 \dd{x}\dd{y}\dd{z}                                                        \\
      & = \int_{z=0}^1 \int_{y=0}^{1-z} (1-y-z)\dd{y}\dd{z}                                                                            \\
      & = \int_{z=0}^1 \left((1-z) - \frac{1}{2}(1-z)^2 - (1-z)z\right) \dd{z}                                                         \\
      & = \left[ z - \frac{1}{2}z^2 - \frac{1}{2}z + \frac{1}{2}z^2 - \frac{1}{6}z^3 - \frac{1}{2}z^2 + \frac{1}{3}z^3 \right]_{z=0}^1 \\
      & = \frac{1}{6}
\end{align*}
We can compute things like the centre of mass, assuming it has constant density $\rho = 1$. Then
\[ \vb X = \frac{1}{m} \int_V \rho \vb x \dd{V} = \frac{1}{4}\begin{pmatrix}
        1 \\1\\1
    \end{pmatrix} \]
\begin{proposition}
    Let $x(u, v, w), y(u, v, w), z(u, v, w)$ be a continuously differentiable bijection with a continuously differentiable inverse, that maps the volume $V'$ to $V$. The integral
    \[ \iiint_V f(x, y, z)\dd{x}\dd{y}\dd{z} = \iiint_{V'} f(x(u, v, w), y(u, v, w), z(u, v, w))\,\abs{J}\dd{u}\dd{v}\dd{w} \]
    where
    \[ J = \det\left( \frac{\partial \vb x}{\partial u} \,\middle|\, \frac{\partial \vb x}{\partial v} \,\middle|\, \frac{\partial \vb x}{\partial w} \right) \]
    More concisely,
    \[ \dd{x}\dd{y}\dd{z} = \abs{J}\dd{u}\dd{v}\dd{w} \]
\end{proposition}
\noindent The Jacobian comes from the fact that the volume of a parallepiped generated by the vectors
\[ \frac{\partial \vb x}{\partial u} \delta u,\,\frac{\partial \vb x}{\partial v} \delta v,\,\frac{\partial \vb x}{\partial w} \delta w \]
is precisely the determinant of the Jacobian matrix multiplied by $\delta u\,\delta v\,\delta w$. The rest of this proof follows from the two-dimensional case. As an example, let us consider cylindrical polar coordinates $(u, v, w) = (\rho, \phi, z)$.
\[ \dd{V} = \rho \, \dd \rho \, \dd \phi \, \dd{z};\quad \abs{J} = \rho \]
In spherical polar coordinates $(u, v, w) = (r, \theta, \phi)$,
\[ \dd{V} = r^2 \sin\theta \dd{r} \,\dd \theta \,\dd \phi;\quad \abs{J} = r^2\sin\theta \]

\subsection{Calculating Volumes}
We can use the volume element to calculate, for example, the volume of a ball of radius $R$. To begin, let us use Cartesian coordinates.
\begin{align*}
    \int_V \dd{V} & = \int_{z=-R}^R \dd{z} \int_{y = -\sqrt{R^2 - z^2}}^{\sqrt{R^2 - z^2}} \dd{y} \int_{x = -\sqrt{R^2 - z^2 - y^2}}^{\sqrt{R^2 - z^2 - y^2}} \dd{x}                             \\
                  & = \int_{z=-R}^R \dd{z} \int_{y = -\sqrt{R^2 - z^2}}^{\sqrt{R^2 - z^2}} \dd{y} \left[ 2\sqrt{R^2 - z^2 - y^2} \right]                                                         \\
                  & = \int_{z=-R}^R \dd{z} \left[ y\sqrt{R^2 - z^2 - y^2} + (R^2 - z^2) \arctan \left( \frac{y}{\sqrt{R^2 - z^2 - y^2}} \right)_{y=-\sqrt{R^2 - z^2}}^{\sqrt{R^2 - z^2}} \right] \\
                  & = \int_{z=-R}^R \dd{z} \left[ \pi (R^2 - z^2) \right]                                                                                                                        \\
                  & = \frac{4}{3}\pi R^3
\end{align*}
We can alternatively use spherical polar coordinates.
\begin{align*}
    \int_V \dd{V} & = \int_{r=0}^R \dd{r} \int_{\theta=0}^\pi \dd \theta \int_{\phi=0}^{2\pi} \dd \phi \cdot r^2 \sin\theta        \\
                  & = \int_{r=0}^R r^2\dd{r} \int_{\theta=0}^\pi \sin\theta\, \dd \theta \int_{\phi=0}^{2\pi} \dd \phi             \\
                  & = \int_{r=0}^R r^2\dd{r} \cdot \int_{\theta=0}^\pi \sin\theta\, \dd \theta \cdot \int_{\phi=0}^{2\pi} \dd \phi \\
                  & = \frac{1}{3}R^3 \cdot 2 \cdot 2 \pi                                                                           \\
                  & = \frac{4}{3}\pi R^3
\end{align*}
This is clearly a much cleaner computation. Now, consider the a ball of radius $a$ with cylinder of radius $b<a$ removed from the centre aligned with the $z$ axis. To calculate this volume, the symmetry of the problem suggests we might want to use cylindrical polar coordinates.
\[ V = \{ (\rho, \phi, z) \colon 0 < \rho^2 + z^2 < a^2,\, b < \rho < a \} \]
\begin{align*}
    \int_V \dd{V} & = \int_{\rho=b}^a \rho\,\dd \rho \int_{\phi=0}^{2\pi} \dd \phi \int_{z=-\sqrt{a^2 - \rho^2}}^{\sqrt{a^2 - \rho^2}} \dd{z} \\
                  & = 2 \pi \int_b^a 2\rho\sqrt{a^2 - \rho^2}\,\dd \rho                                                                       \\
                  & = \frac{4}{3}\pi (a^2 - b^2)^{\frac{3}{2}}
\end{align*}

\section{Integration over Surfaces}
\subsection{Two-Dimensional Surfaces}
A two-dimensional surface in $\mathbb R^3$ can be defined implicitly using a function $f \colon \mathbb R^3 \to \mathbb R$, with
\[ S = \{ \vb x \in \mathbb R^3 \colon f(\vb x) = 0 \} \]
The normal to $S$ at $\vb x$ is parallel to $\grad f(\vb x)$. We call the surface regular if $\grad f(\vb x) \neq \vb 0$ everywhere on the surface. For example, consider
\[ S = \{ (x, y, z) \colon x^2 + y^2 + z^2 - 1 = 0 \} \]
Then
\[ \grad f(\vb x) = \begin{pmatrix}
        2x \\ 2y \\ 2z
    \end{pmatrix} = 2\vb x \]
which is clearly normal to $S$ at $\vb x$. Some surfaces have a boundary, for instance a hemisphere.
\[ S = \{ (x, y, z) \colon x^2 + y^2 + z^2 - 1 = 0,\,z \geq 0 \} \]
We label the boundary $\partial S$, so
\[ \partial S = \{ (x, y, z) \colon x^2 + y^2 = 1,\,z = 0 \} \]
In this course, a surface will either have no boundary or its boundary will be made of piecewise smooth curves. If $S$ has no boundary, we say that $S$ is a closed surface. It is often useful to parametrise a surface using some coordinates $(u, v)$.
\[ S = \{ \vb x = \vb x(u, v) \colon (u, v) \in D \} \]
where $D$ is some region in the $u$-$v$ plane. For a hemisphere, we can use spherical polar coordinates:
\[ S = \left\{ \vb x = \vb x(\theta, \phi) = \begin{pmatrix}
        \sin\theta \cos\phi \\ \sin\theta \sin\phi \\ \cos\theta
    \end{pmatrix} \colon 0 \leq \theta \leq \frac{\pi}{2},\, 0 \leq \phi \leq 2\pi \right\} \]
We call a parametrisation of $S$ regular if
\[ \frac{\partial \vb x}{\partial u} \times \frac{\partial \vb x}{\partial v} \neq \vb 0 \]
everywhere on the surface. Note that $\frac{\partial \vb x}{\partial u}$ is the tangent in one direction, and $\frac{\partial \vb x}{\partial v}$ is the tangent in another direction, so their cross product should be normal to the surface.
\[ \nhat = \frac{\frac{\partial \vb x}{\partial u} \times \frac{\partial \vb x}{\partial v}}{\abs{\frac{\partial \vb x}{\partial u} \times \frac{\partial \vb x}{\partial v}}} \]
This normal will vary smoothly with respect to $u$ and $v$, if we are moving across a smooth part of the curve. Choosing a consistent normal over $S$ gives a way to give an orientation to the boundary $\partial S$. We make the convention that normal vectors near you should be on your left as you traverse $\partial S$.

\subsection{Areas and Integrals over Surfaces}
Consider a parametrised surface
\[ S = \{ \vb x = \vb x(u, v) \colon (u, v) \in D \} \]
The integral over $S$ cannot be of the form
\[ \iint_D \dd{u}\dd{v} \]
since a patch of area $\delta u \,\delta v$ in $D$ will not in general correspond to a patch of area $\delta u \,\delta v$ in $S$. Note that the small change $u \mapsto u + \delta u$ produces a change
\[ \vb x(u + \delta u, v) - \vb x(u, v) \approx \frac{\partial \vb x}{\partial u} \delta u \]
Similarly, changing $v$, we have
\[ \vb x(u, v + \delta v) - \vb x(u, v) \approx \frac{\partial \vb x}{\partial v} \delta v \]
So the patch of area $\delta u\,\delta v$ in $D$ corresponds (to first order) to a parallelogram of area
\[ \abs{\frac{\partial \vb x}{\partial u} \times \frac{\partial \vb x}{\partial v}} \,\delta u\,\delta v \]
This leads us to define the scalar area element and the vector area element as follows:
\begin{align*}
    \dd{S}    & = \abs{\frac{\partial \vb x}{\partial u} \times \frac{\partial \vb x}{\partial v}}\dd{u}\dd{v}          \\
    \dd \vb S & = \frac{\partial \vb x}{\partial u} \times \frac{\partial \vb x}{\partial v}\dd{u}\dd{v} = \nhat \dd{S}
\end{align*}
So for instance the area of $S$ is given by
\[ \int_S \dd{S} = \iint_D \abs{\frac{\partial \vb x}{\partial u} \times \frac{\partial \vb x}{\partial v}}\dd{u}\dd{v} \]
As an example, consider the hemisphere of radius $R$.
\[ S = \left\{ \vb x = \vb x(\theta, \phi) = \begin{pmatrix}
        R\sin\theta \cos\phi \\ R\sin\theta \sin\phi \\ R\cos\theta
    \end{pmatrix} = R \vb e_r \colon 0 \leq \theta \leq \frac{\pi}{2},\, 0 \leq \phi \leq 2\pi \right\} \]
So
\begin{align*}
    \frac{\partial \vb x}{\partial \theta} & = \begin{pmatrix}
        R\cos\theta \cos\phi \\ R\cos\theta \sin\phi \\ -R\sin\theta
    \end{pmatrix} = R\vb e_\theta         \\
    \frac{\partial \vb x}{\partial \phi}   & = \begin{pmatrix}
        -R\sin\theta \sin\phi \\ R\sin\theta \cos\phi \\ 0
    \end{pmatrix} = R\sin\theta\vb e_\phi \\
\end{align*}
Hence
\[ \dd{S} = R^2 \sin\theta\, \abs{\vb e_\theta \times \vb e_\phi} \,\dd \theta\,\dd \phi = R^2 \sin \theta \,\dd \theta \,\dd \phi \]
So the surface area of the hemisphere is
\[ \int_{\theta = 0}^{\frac{\pi}{2}} \dd \theta \int_{\phi = 0}^{2 \pi} \dd \phi \, R^2 \sin \theta = 2 \pi R^2 \]
Here is another example. Suppose the velocity of a fluid is $\vb u(\vb x)$. Given a surface $S$, we might like to calculate how much fluid passes through it per unit time. On a small patch $\delta S$ on $S$, the fluid passing through the small patch would be $(u \cdot \delta \vb S)\,\delta t$ in time $\delta t$, where $\delta \vb S$ is the normal direction to the area $\delta S$. Over the whole surface, the smount that passes over $S$ in $\delta t$ is
\[ \delta t \int_S \vb u \cdot \dd \vb S \]
This kind of integral is called a `flux integral'.

\subsection{Choice of Parametrisation of Surfaces}
Let $\vb x = \vb x(u, v)$ and $\vb x = \widetilde {\vb x} (\widetilde u, \widetilde v)$ be two different parametrisations of $S$ with $(u, v) \in D$ and $(\widetilde u, \widetilde v) \in \widetilde D'$. Since every coordinate in $S$ has a pre-image in both $D$ and $D'$, there must be a relationship
\[ \vb x(u, v) = \widetilde {\vb x} (\widetilde u(u, v), \widetilde v(u, v)) \]
By the chain rule,
\begin{align*}
    \frac{\partial \vb x}{\partial u} \times \frac{\partial \vb x}{\partial v} & = \left( \frac{\partial \widetilde {\vb x}}{\partial \widetilde u}\frac{\partial \widetilde u}{\partial u} + \frac{\partial \widetilde {\vb x}}{\partial \widetilde v}\frac{\partial \widetilde v}{\partial u} \right) \times \left( \frac{\partial \widetilde {\vb x}}{\partial \widetilde u}\frac{\partial \widetilde u}{\partial v} + \frac{\partial \widetilde {\vb x}}{\partial \widetilde v}\frac{\partial \widetilde v}{\partial v} \right) \\
                                                                               & = \left( \frac{\partial \widetilde u}{\partial u}\frac{\partial \widetilde v}{\partial v} - \frac{\partial \widetilde u}{\partial v}\frac{\partial \widetilde v}{\partial u} \right) \left( \frac{\partial \widetilde {\vb x}}{\partial \widetilde u}\times\frac{\partial \widetilde {\vb x}}{\partial \widetilde v} \right)                                                                                                                       \\
                                                                               & = \frac{\partial (\widetilde u, \widetilde v)}{\partial (u, v)} \left( \frac{\partial \widetilde {\vb x}}{\partial \widetilde u}\times\frac{\partial \widetilde {\vb x}}{\partial \widetilde v} \right)
\end{align*}
Hence,
\begin{align*}
    \int_S f \dd{S} & = \iint_{\widetilde D} f(\widetilde {\vb x}(\widetilde u, \widetilde v)) \,\abs{\frac{\partial \widetilde {\vb x}}{\partial \widetilde u}\times\frac{\partial \widetilde {\vb x}}{\partial \widetilde v}}\,\dd \widetilde u\,\dd \widetilde v \\
                    & = \iint_D f(\vb x(u, v)) \,\abs{\frac{\partial \widetilde {\vb x}}{\partial \widetilde u}\times\frac{\partial \widetilde {\vb x}}{\partial \widetilde v}}\,\abs{\frac{\partial (\widetilde u, \widetilde v)}{\partial (u, v)}}\dd{u}\dd{v}    \\
                    & = \iint_D f(\vb x(u, v)) \,\abs{\frac{\partial \vb x}{\partial u}\times\frac{\partial \vb x}{\partial v}}\dd{u}\dd{v}                                                                                                                         \\
\end{align*}
So the result of the integral over the surface is independent of the choice of parametrisation.

\section{Divergence, Curl and Laplacians}
\subsection{Definitions}
Recall the gradient operator $\grad$, which is defined in Cartesian coordinates as
\[ \grad = \vb e_i \frac{\partial}{\partial x_i} \]
For a vector field $\vb F \colon \mathbb R^3 \to \mathbb R^3$, we define the divergence of $\vb F$ by
\[ \div{\vb F} \]
In Cartesian coordinates,
\[ \div{\vb F} = \left( \vb e_i \frac{\partial}{\partial x_i} \right) \cdot (F_j \vb e_j) = \pdv{F_i}{x_i} \]
Note that the divergence of a vector field is a scalar field. We define the curl of $\vb{F}$ to be
\[ \curl{\vb F} \]
In Cartesian coordinates,
\[ \curl{\vb F} = \left( \vb e_j \pdv{x_j} \right) \times (F_k \vb e_k) = e_j \times \left[ \pdv{x_j}(F_k \vb e_k) \right] = (\vb e_j \times \vb e_k) \pdv{F_k}{x_j} = \varepsilon_{ijk} \pdv{F_k}{x_j}\vb e_i \]
Hence (just in Cartesian coordinates):
\[ \left[ \curl{\vb F} \right]_i = \varepsilon_{ijk} \pdv{F_k}{x_j} \]
The curl of a vector field is another vector field. In terms of a `formal' determinant, we can write
\[ \curl{\vb F} = \det \begin{pmatrix}
        \vb e_1    & \vb e_2    & \vb e_3    \\
        \pdv*{x_1} & \pdv*{x_2} & \pdv*{x_2} \\
        F_1        & F_2        & F_3
    \end{pmatrix} \]
We cannot trivially generalise the curl operator to spaces that do not have three spatial dimensions. Finally, we define the Laplacian of a scalar field $f \colon \mathbb R^3 \to \mathbb R$ as
\[ \laplacian{f} := \div{\grad{f}} \]
In Cartesian coordinates, $[\grad{f}]_i = \pdv*{f}{x_i}$, so
\[ \laplacian{f} = \pdv{f}{x_i}{x_i} \]

\subsection{Example and Explanation of Divergence and Curl}
Consider
\[ \vb F(\vb x) = \vb x \]
Using Cartesian coordinates,
\[ \div{\vb F} = \pdv{x_i}x_i = \delta_{ii} = 3 \]
\[ [\curl{\vb F}]_i = \varepsilon_{ijk} \pdv{x_j} x_k = \varepsilon_{ijk} \delta_{kj} = \varepsilon_{ijj} = 0 \]
A positive divergence at a point indicates that the vector field is generally pointing away from that point. If thought of as a fluid, the point acts as a `source' of fluid. A negative divergence indicates that the vector field is pointing towards that point, so it acts like a `sink'. If a vector field has zero divergence, it can be thought of as representing the velocity of an incompressible fluid. The curl measures the local rotation of the vector field (or the related `fluid') in a given direction. If the vector field was going anticlockwise in the $\vb e_1$-$\vb e_2$ plane, then the component of the curl in the $\vb e_3$ direction would be positive. If there is no local rotation, then the component is zero.

\subsection{Identities}
\begin{proposition}
    For $f, g$ scalar fields, $\vb F, \vb G$ vector fields, the following identities hold.
    \begin{itemize}
        \item $\grad(fg) = (\grad f)g + (\grad g)f$
        \item $\div(f \vb F) = (\grad f)\cdot \vb F + (\div F)f$
        \item $\curl(f \vb F) = (\grad f)\times \vb F + (\curl F)f$
        \item $\grad(\vb F \cdot \vb G) = \vb F \times (\curl{\vb G}) + \vb G \times (\curl{\vb F}) + (\vb F \cdot \grad)\vb G + (\vb G \cdot \grad)\vb F$
        \item $\curl(\vb F \times \vb G) = \vb F(\div{\vb G}) - \vb G(\div{\vb F}) + (\vb G \cdot \grad)\vb F - (\vb F \cdot \grad)\vb G$
        \item $\div(\vb F \times \vb G) = (\curl{\vb F}) \cdot \vb G - \vb F \cdot (\curl{\vb G})$
    \end{itemize}
\end{proposition}
\noindent Note, for example, that we can compute the dot product between vector fields and operators:
\[ \left[ (\vb F \cdot \grad) \vb G \right]_i = \left( F_j \pdv{x_j} \right) G_i = F_j \pdv{G_i}{x_j} \]
Specifically, $\vb F \cdot \grad$ is a vector, and $\div{\vb F}$ is a differential operator; they are not the same thing.
\begin{proof}
    We will only prove the fifth one for now, as all the proofs are similar. The identities hold in any coordinate system, so we will choose the Cartesian coordinate system since the basis vectors are the same everywhere.
    \begin{align*}
        [\curl(\vb F \times \vb G)]_i & = \varepsilon_{ijk} \pdv{x_j} (\vb F \times \vb G)_k                                                            \\
                                      & = \varepsilon_{ijk} \pdv{x_j} \varepsilon_{klm} F_l G_m                                                         \\
                                      & = \varepsilon_{ijk} \varepsilon_{klm} \pdv{x_j} F_l G_m                                                         \\
                                      & = \varepsilon_{ijk} \varepsilon_{klm} \left( F_l \pdv{G_m}{x_j} + G_l \pdv{F_l}{x_j}  \right)                   \\
                                      & = (\delta_{il}\delta_{jm} - \delta_{im}\delta_{jl}) \left( F_l \pdv{G_m}{x_j} + G_l \pdv{F_l}{x_j} \right)      \\
                                      & = F_i \pdv{G_j}{x_j} - F_j \pdv{G_i}{x_j} + G_j \pdv{F_i}{x_j} - G_i \pdv{F_j}{x_j}                             \\
                                      & = [\vb F(\div{\vb G})]_i - [(\vb F \cdot \grad)\vb G]_i + [(\vb G \cdot \grad)\vb F]_i - [(\div{\vb F})\vb G]_i
    \end{align*}
\end{proof}

\subsection{Definitions in Orthogonal Curvilinear Coordinate Systems}
For a general set of orthogonal curvilinear coordinates, divergence is defined by
\[ \div{\vb F} = \left( \vb e_u \frac{1}{h_u} \pdv{u} + \vb e_v \frac{1}{h_v} \pdv{v} + \vb e_w \frac{1}{h_w} \pdv{w} \right) \cdot \left( F_u \vb e_u + F_v \vb e_v + F_w \vb e_w \right) \]
We would get terms like
\begin{align*}
    \left( \vb e_u \frac{1}{h_u} \pdv{u} \right) \cdot (F_v \vb e_v) & = \frac{1}{h_u} \vb e_u \cdot \left[ \pdv{u}(F_v \vb e_v) \right]                       \\
                                                                     & = \frac{1}{h_u} \vb e_u \cdot \left[ \pdv{F_v}{u}\vb e_v + \pdv{\vb e_v}{u} F_v \right] \\
                                                                     & = \frac{F_v}{h_u} \left( \vb e_u \cdot \pdv{\vb e_v}{u} \right)
\end{align*}
We can combine all such terms and then derive that
\[ \div{\vb F} = \frac{1}{h_u h_v h_w} \left[ \pdv{u} (h_v h_w F_u) + \pdv{v} (h_u h_w F_v) + \pdv{w} (h_u h_v F_w) \right] \]
\[ \curl{\vb F} = \frac{1}{h_u h_v h_w} \begin{vmatrix}
        h_u \vb e_u & h_v \vb e_v & h_w \vb e_w \\
        \pdv*{u}    & \pdv*{v}    & \pdv*{w}    \\
        h_u F_u     & h_v F_v     & h_w F_w
    \end{vmatrix} \]
\[ \laplacian{f} = \frac{1}{h_u h_v h_w} \left[ \pdv{u} \left( \frac{h_v h_w}{h_u} \pdv{f}{u} \right) + \pdv{v} \left( \frac{h_u h_w}{h_v} \pdv{f}{v} \right) + \pdv{w} \left( \frac{h_u h_v}{h_w} \pdv{f}{w} \right) \right] \]
For cylindrical polar coordinates $(\rho, \phi, z)$, we have $(h_\rho, h_\phi, h_z) = (1, \rho, 1)$ and hence
\[ \div{\vb F} = \frac{1}{\rho} \pdv{\rho} (\rho F_\rho) + \frac{1}{\rho} \pdv{F_\phi}{\phi} + \pdv{F_z}{z} \]
\[ \curl{\vb F} = \frac{1}{\rho} \begin{vmatrix}
        \vb e_\rho  & \rho \vb e_\phi & \vb e_z  \\
        \pdv*{\rho} & \pdv*{\phi}     & \pdv*{z} \\
        F_\rho      & \rho F_\phi     & F_z
    \end{vmatrix} \]
\[ \laplacian{f} = \frac{1}{\rho} \pdv{\rho} \left( \rho \pdv{f}{\rho} \right) + \frac{1}{\rho^2} \pdv[2]{f}{\phi} + \pdv[2]{f}{z} \]
For spherical polar coordinates $(r, \theta, \phi)$, we have $(h_r, h_\theta, h_\phi) = (1, r, r\sin\theta)$ and hence
\[ \div{\vb F} = \frac{1}{r^2} \pdv{r}(r^2 F_r) + \frac{1}{r\sin\theta} \pdv{\theta}(\sin \theta\, F_\theta) + \frac{1}{r\sin\theta} \pdv{F_\phi}{\phi} \]
\[ \curl{\vb F} = \frac{1}{r^2\sin\theta} \begin{vmatrix}
        \vb e_r  & r \vb e_\theta & r\sin\theta\, \vb e_\phi \\
        \pdv*{r} & \pdv*{\theta}  & \pdv*{\phi}              \\
        F_r      & r F_\theta     & r\sin\theta\, F_\phi
    \end{vmatrix} \]
\[ \laplacian{f} = \frac{1}{r^2} \pdv{r} \left( r^2 \pdv{f}{r} \right) + \frac{1}{r^2 \sin\theta} \pdv{\theta} \left( \sin \theta \pdv{f}{\theta} \right) + \frac{1}{r^2\sin^2 \theta} \pdv[2]{f}{\phi} \]

\subsection{Laplacian of a Vector Field}
The Laplacian of a vector field might be expected to be something like $\div(\grad {\vb F})$. However, we have not defined the gradient of a vector field. In Cartesian coordinates, it would make sense that
\begin{equation}
    \laplacian{\vb F} = \laplacian(F_i \vb e_i) = (\laplacian{F_i})\vb e_i
    \tag{$\dagger$}
\end{equation}
If this is the case, we can show then that, in Cartesian coordinates,
\[ \laplacian{\vb F} = \grad(\div{\vb F}) - \curl(\curl{\vb F}) \]
In other words, in Cartesian coordinates,
\[ [\laplacian{\vb F}]_i = \pdv{F_i}{x_j}{x_j} = \laplacian(F_i) \]
Since the right hand side of $(\dagger)$ is well-defined in any orthogonal curvilinear coordinate system, we will use it as a definition.

\section{Relations Between Differential Operators}
\subsection{Basic Relationships}
\begin{proposition}
    For a scalar field $f$ and a vector field $\vb F$,
    \[ \curl{\grad{f}} = \vb 0 \]
    and
    \[ \div{\curl{\vb F}} = 0 \]
    In other words, curl $\circ$ grad gives zero, and div $\circ$ curl gives zero.
\end{proposition}
\begin{proof}
    We will use Cartesian coordinates for simplicity.
    \begin{align*}
        [\curl{\grad{f}}]_i & = \varepsilon_{ijk} \pdv{x_j} \left( \pdv{f}{x_k} \right) \\
                            & = \varepsilon_{ijk} \pdv{f}{x_j}{x_k}
    \end{align*}
    $\varepsilon_{ijk}$ is antisymmetric in $j$ and $k$, but $\pdv{f}{x_j}{x_k}$ is symmetric in $j$ and $k$. Hence the result is zero. Further,
    \begin{align*}
        \div{\curl{\vb F}} & = \pdv{x_i} \varepsilon_{ijk} \pdv{x_j} F_k \\
                           & = \varepsilon_{ijk} \pdv{F_k}{x_i}{x_j}
    \end{align*}
    Once again the $\varepsilon$ term is antisymmetric and the partial derivative is symmetric, so the result follows.
\end{proof}

\subsection{Irrotational and Solenoidal Forces}
As a short aside, `simply connected' means that any loop in a space can be `shrunk' to any point within that space. It can also be referred to as `1-connected' since the loop is a one-dimensional manifold. For example, $\mathbb R^3$ is 1-connected, but $\mathbb R^3$ with the $z$-axis removed is not 1-connected; a loop around this axis cannot be shrunk to a point away from the axis.

We can write that a space is `2-connected' if it is 1-connected and any 2-manifold (surface) can be shrunk to any point within the space. Certainly $\mathbb R^3$ is 2-connected, but for example $\mathbb R^3$ without the origin is not 2-connected. The space is certainly 1-connected, but it is not 2-connected because a surface around the origin cannot be shrunk to a point away from the origin.

Recall that $\vb F$ is conservative if we can write $\vb F = \grad f$. We say that $\vb F$ is irrotational if $\curl{\vb F} = \vb 0$. Hence, any conservative function is irrotational. The converse is true if the domain of $\vb F$ is 1-connected. We say that $\vb F$ is solenoidal if $\div{\vb F} = 0$. If there exists a vector potential $\vb A$ for $\vb F$, i.e. $\vb F = \curl{\vb A}$, then $\vb F$ is solenoidal. The converse is true if the domain of $\vb F$ is 2-connected.

\subsection{Green's Theorem}
We begin now a section on various integral theorems.
\begin{proposition}
    If $P = P(x, y)$ and $Q = Q(x, y)$ are continuously differentiable on a planar domain $A \cup \partial A$ ($A$ and its boundary), and $\partial A$ is piecewise smooth, then
    \[ \oint_{\partial A} P \dd{x} + Q \dd{y} = \iint_{A} \left( \pdv{Q}{x} - \pdv{P}{y} \right) \dd{x}\dd{y} \]
    where the orientation of $\partial A$ is such that $A$ lies to the left while traversing $\partial A$.
\end{proposition}
\noindent Note that it is easy to arrive at this result for a rectangle. In this case,
\begin{align*}
    \iint_{A} \left( \pdv{Q}{x} - \pdv{P}{y} \right) \dd{x}\dd{y} & = \int_c^d \dd{y} \int_a^b \dd{x} \pdv{Q}{x} - \int_a^b \dd{x} \int_x^d \dd{y} \pdv{P}{y} \\
                                                                  & = \int_c^d [Q(b, y) - Q(a, y)] \dd{y} + \int_a^b [P(x, c) - P(x, d)] \dd{x}               \\
                                                                  & = \oint_{\partial A} P \dd{x} + Q \dd{y}
\end{align*}
It then intuitively follows that we can approximate a surface with a set of small rectangles, and then the theorem should hold. As an example, let
\[ P = -\frac{1}{2}y;\quad Q = \frac{1}{2}x\]
Then the area of some region is given by
\begin{align*}
    \iint_A \dd{x}\dd{y} & = \iint_A \left( \frac{1}{2} + \frac{1}{2} \right) \dd{x}\dd{y} \\
                         & = \iint_A \left( \pdv{Q}{x} - \pdv{P}{y} \right) \dd{x}\dd{y}   \\
                         & = \frac{1}{2}\oint_{\partial A} x\dd{y} - y\dd{x}
\end{align*}
So letting $A$ be the ellipse $\frac{x^2}{a^2} + \frac{y^2}{b^2} \leq 1$, we can parametrise $\partial A$ by
\[ [0, 2 \pi] \ni t \mapsto \begin{pmatrix}
        a \cos t \\ b \sin t
    \end{pmatrix} \]
Hence the area is
\[ \frac{1}{2}\int_0^{2\pi} \left( ab\cos^2 t + ab\sin^2 t \right) \dd{t} = \pi ab \]

\section{Stokes' Theorem}
\subsection{Definition}
\begin{proposition}
    If $\vb F(\vb x)$ is a continuously differentiable vector field, and $S$ is an orientable, piecewise regular surface with a piecewise smooth boundary $\partial S$, then
    \[ \int_S (\curl{\vb F}) \cdot \dd \vb S = \oint_{\partial S} \vb F\cdot \dd{\vb x} \]
\end{proposition}
\noindent This can be thought of as a generalisation to the fundamental theorem of calculus. From the fundamental theorem, we know that the integral of a differentiated function over an interval $I$ is just the original function evaluated at the boundary $\partial I$. Likewise, Stokes' theorem states that the integral of the curl of a function (just another differential operator) over a surface $S$ is just the original function evaluated at the boundary of the surface $\partial S$. In the one-dimensional fundamental theorem of calculus, we say that the function `evaluated over the boundary' is simply the function applied to the final point, minus the function applied to the initial point; we are in some sense considering every point on the boundary $\partial I$. But in the case of $\partial S$ being a curve, we must integrate around the curve boundary, since without an integral we can't consider infinitely many boundary points.

Note that for a surface to be `orientable', it simply means that it has two sides, an inside and an outside. There must be a consistent choice of normal at each point. For example, a sphere is orientable, but a M\"obius strip is not orientable.

\subsection{First Example}
Let $S$ be a cap of a sphere:
\[ S = \left\{ \vb x(\theta, \phi) = \begin{pmatrix}
        \sin\theta\cos\phi \\ \sin\theta\sin\phi \\ \cos\theta
    \end{pmatrix} \equiv \vb e_r; 0 \leq \theta \leq \alpha; 0 \leq \phi < 2 \pi \right\} \]
Now, let
\[ \vb F(\vb x) = \begin{pmatrix}
        -x^2y \\ 0 \\ 0
    \end{pmatrix} \implies \curl{\vb F} = \begin{pmatrix}
        0 \\ 0 \\ x^2
    \end{pmatrix} \]
On $S$,
\[ \dd {\vb S} = \dv{\vb x}{\theta} \times \dv{\vb x}{\phi} \dd{\theta}\dd{\phi} = \vb e_r \sin\theta \dd{\theta}\dd{\phi} \]
Note that since
\[ x^2 \vb e_z \cdot \vb e_r = (\sin\theta\cos\phi)^2 \cos\theta \]
on $S$, we can compute
\[ \int_S (\curl{\vb F}) \cdot \dd{\vb S} = \int_{\phi = 0}^{2 \pi} \left( \int_{\theta = 0}^\alpha (\sin\theta\cos\phi)^2 \cos\theta \sin\theta \dd{\theta} \right) \dd{\phi} = \frac{\pi}{4}\sin^4 \alpha \]
We can instead compute the integral over the boundary. By Stokes' theorem, the results should match. $\partial S$ is described by
\[ [0, 2\pi] \ni t \mapsto \begin{pmatrix}
        \sin\alpha\cos t \\ \sin\alpha\sin t \\ \cos\alpha
    \end{pmatrix} \]
Then
\[ \dd{\vb x} = \dv{\vb x}{t} \dd{t} = \sin \alpha\begin{pmatrix}
        -\sin t \\ \cos t \\ 0
    \end{pmatrix} \dd{t} \]
We can show that
\[ \oint_{\partial S} \vb F \cdot \dd{\vb S} = \sin^4 \alpha \int_0^{2\pi} \left( -\cos^2 t \sin t \right)\left( -\sin t \right) \dd{t} = \frac{\pi}{4}\sin^4 \alpha \]

\subsection{Stokes' Theorem on Closed Surfaces}
If $S$ is an orientable, closed surface, and $\vb F$ is continuously differentiable, then
\[ \int_S (\curl{\vb F}) \cdot \dd{\vb S} = 0 \]
This is clear since $\partial S = \varnothing$.

\subsection{Zero Circulation and Irrotationality}
\begin{proposition}
    If $\vb F$ is continuously differentiable, and for every loop $C$ we have that
    \[ \oint_C \vb F \cdot \dd{\vb x} = 0 \]
    then $\curl{\vb F} = \vb 0$. In other words, $\vb F$ is irrotational if and only if $\vb F$ has zero circulation around all closed loops.
\end{proposition}
\noindent Note that the backward implication is trivial. If $\vb F$ has zero circulation around all loops, we can define that loop to be the boundary of some surface, and so the integral of the curl vanishes.
\begin{proposition}
    Suppose that the result is false; there exists a unit vector $\vu{k}$ such that $\vu{k} \cdot \left( \curl{\vb F(\vb x_0)} \right) = \varepsilon > 0$ for some $\vb x_0$. By continuity, for a sufficiently small $\delta > 0$,
    \[ \vu{k} \cdot \left( \curl{\vb F(\vb x_0)} \right) > \frac{1}{2}\varepsilon;\quad \text{for }\abs{\vb x - \vb x_0} < \delta \]
    Now, we can take a loop in this ball $\{ \vb x \colon \abs{\vb x - \vb x_0} < \delta \}$ that lies entirely in a plane with normal $\vu{k}$. Let this small loop's enclosed surface be $S$, with boundary $\partial S$. Then
    \[ 0 = \oint_{\partial S} \vb F \cdot \dd{\vb x} = \int_S \curl{\vb F} \cdot \vu k \dd{S} > \frac{1}{2}\varepsilon \int \dd{S} > 0 \]
    which is a contradiction.
\end{proposition}

\subsection{Intuition for Curl as Infinitesimal Circulation}
Let $S_\varepsilon$ denote a region contained inside a disk of radius $\varepsilon > 0$, centred at $\vb x_0$ with normal $\vu{k}$.
\begin{align*}
    \int_{S_\varepsilon} \curl{\vb F} \cdot \dd{\vb S} & = \int_{S_\varepsilon} \left( \curl{\vb F(\vb x)} - \curl{\vb F(\vb x_0)} \right) \cdot \dd{\vb S} + \int_{S_\varepsilon} \curl{\vb F(\vb x_0)} \cdot \vu k \dd{S}                                                                                        \\
                                                       & = \underbrace{\int_{S_\varepsilon} \left( \curl{\vb F(\vb x)} - \curl{\vb F(\vb x_0)} \right) \cdot \dd{\vb S}}_{o(\text{area}(S_\varepsilon))} + \curl{\vb F(\vb x_0)} \cdot \vu k \underbrace{\int_{S_\varepsilon} \dd{S}}_{\text{area}(S_\varepsilon)} \\
\end{align*}
As $\varepsilon$ shrinks, the first integral tends to zero faster than the second term. Hence,
\[ \int_{S_\varepsilon} \curl{\vb F} \cdot \dd{\vb S} =  \curl{\vb F(\vb x_0)} \cdot \vu k \cdot \text{area}(S_\varepsilon) + o(\text{area}(S_\varepsilon)) \]
We can then see, by Stokes' theorem, that
\[ \curl{\vb F(\vb x_0)} \cdot \vu k = \lim_{\varepsilon \to 0} \frac{1}{\text{area}(S_\varepsilon)} \oint_{\partial S_\varepsilon} \vb F \cdot \dd{\vb x} \]
So the curl of $\vb F$ at $\vb x_0$ in the direction $\vu k$ is the infinitesimal circulation around $\vb x_0$, per unit area.

\section{Gauss' Divergence Theorem}
\begin{proposition}
    If $\vb F(\vb x)$ is a continuously differentiable vector field, and $V$ is a volume with a piecewise regular boundary $\partial V$, then
    \[ \int_V \div{\vb F} \dd{V} = \int_{\partial V} \vb F \cdot \dd{\vb S} \]
    where the normal of $\partial V$ points out of $V$.
\end{proposition}
\noindent There is also a two-dimensional version. If $D$ is a planar region with a piecewise smooth boundary $\partial D$,
\[ \int_D \div{\vb F} \dd{A} = \oint_{\partial D} \vb F \cdot \vb n \dd{s} \]
where the $\dd{s}$ represents arc length, and where $\vb n$ points out of $D$.

\subsection{First Example}
Let $V$ be a cylinder, defined in cylindrical polar coordinates $(\rho, \phi, z)$ as
\[ V = \left\{ (\rho, \phi, z) \colon 0 \leq \rho \leq R, -h \leq z \leq h, 0 \leq \phi < 2 \pi \right\} \]
Let us label the boundary on the top $S_+$, the boundary on the bottom $S_-$, and the rest of the boundary $S_R$:
\begin{align*}
    S_\pm & = \{ (\rho, \phi, z) \colon 0 \leq \rho \leq R, z = \pm h, 0 \leq \phi < 2 \pi \} \\
    S_R   & = \{ (\rho, \phi, z) \colon \rho = R, -h \leq z \leq h, 0 \leq \phi < 2 \pi \}    \\
\end{align*}
Consider $\vb F(\vb x) = \vb x$, hence $\div{\vb F} = 3$.
\[ \int_V \div{\vb F} \dd{V} = 3 \int_V \dd{V} = 6\pi R^2 h \]
Using instead the divergence theorem,
\[ \int_V \div{\vb F} \dd{V} = \int_{\partial V} \vb F \cdot \dd{\vb S} \]
On $S_R$, $\dd{\vb S} = \vb e_\rho R \dd{\phi}\dd{z}$, and $\vb x \cdot \vb e_\rho = R$. So we have the flux integral
\[ \int_{S_R} \vb F \cdot \dd{\vb S} = \int_{z = -h}^h \int_{\phi = 0}^{2 \pi} R^2 \dd{\phi} \dd{z} = 4\pi R^2 h \]
On $S_\pm$, $\dd{\vb S} = \pm \vb e_z \rho \dd{\rho}\dd{\phi}$, and $\vb x \cdot \vb e_z = \pm h$. Hence
\[ \int_{S_\pm} \vb F \cdot \dd{\vb S} = \int_{\phi = 0}^{2 \pi} \int_{\rho = 0}^R h \rho\dd{\rho}\dd{\phi} = \pi R^2 h \]
The total is $6\pi R^2 h$ as expected.

\subsection{Basic Corollary}
\begin{proposition}
    If $\vb F$ is continuously differentiable, and for every closed surface $S$ we have
    \[ \int_S \vb F \cdot \dd{\vb S} = 0 \]
    then $\div{\vb F} = 0$.
\end{proposition}
\begin{proof}
    Suppose that the result is false; $\div{\vb F}(\vb x_0) = \varepsilon > 0$. By continuity, for some sufficiently small $\delta > 0$ we have
    \[ \div{\vb F}(\vb x) > \frac{1}{2} \varepsilon \text{ for } \abs{\vb x_0 - \vb x} < \delta \]
    Now, we can choose a volume $V$ inside the ball $\abs{\vb x_0 - \vb x} < \delta$, and then by assumption, applying the divergence theorem,
    \[ 0 = \int_{\partial V} \vb F \cdot \dd{\vb S} = \int_V \div{\vb F} \dd{V} > 0 \]
    which is a contradiction. We then conclude that if the vector field has zero net flux through any closed surface, it is solenoidal.
\end{proof}

\subsection{Intuition for Divergence as Infinitesimal Flux}
Let $V_\varepsilon$ be a volume in $\mathbb R^3$, contained inside a ball of radius $\varepsilon > 0$, centred at a point $\vb x_0$. Then
\[ \int_{V_\varepsilon} \div{\vb F} \dd{V} = \mathrm{vol}(V_\varepsilon) \div{\vb F}(\vb x_0) + \underbrace{\int_{V_\varepsilon}  \left[ \div{\vb F}(\vb x) - \div{\vb F}(\vb x_0) \right] \dd{V}}_{o(\mathrm{vol}(V_\varepsilon))} \]
Dividing both sides by the volume of $V_\varepsilon$, and taking $\varepsilon \to 0$, we can apply the divergence theorem to get
\[ \div{\vb F}(\vb x_0) = \lim_{\varepsilon \to 0} \frac{1}{\mathrm{vol}(V_\varepsilon)} \int_{\partial V_\varepsilon} \vb F \cdot \dd{\vb S} \]
The divergence of $\vb F$ measures the infinitesimal flux per unit volume. If the flux is moving `outward' at this point, $\div{\vb F} > 0$, and vice versa.

\subsection{Example of Conservation Laws}
Many equations in mathematical physics can be represented using density $\rho(\vb x, t)$ and a vector field $\vb J(\vb x, t)$, as follows.
\begin{equation}
    \pdv{\rho}{t} + \div{\vb J} = 0 \tag{$\dagger$}
\end{equation}
This kind of equation is called a `conservation law'. Suppose both $\rho$ and $\abs{\vb J}$ decrease rapidly as $\abs{\vb x} \to \infty$. We will define the charge $Q$ by
\[ Q = \int_{\mathbb R^3} \rho(\vb x, t) \dd{V} \]
We have conservation of charge;
\begin{align*}
    \dv{Q}{t} & = \int_{\mathbb R^3} \pdv{\rho}{t} \dd{V}                            \\
              & = -\int_{\mathbb R^3} \div{\vb J} \dd{V}                             \\
              & = -\lim_{R \to \infty} \int_{\abs{\vb x} \leq R} \div{\vb J} \dd{V}  \\
              & = -\lim_{R \to \infty} \int_{\abs{\vb x} = R} \vb J \cdot \dd{\vb S} \\
              & = 0                                                                  \\
\end{align*}
as $\vb J$ decreases rapidly to zero as $\abs{\vb x} \to \infty$. So $(\dagger)$ gives conservation of charge.

\section{Proofs of Integral Theorems}
\subsection{Divergence Theorem}
\begin{proof}
    Suppose first that
    \[ \vb F = F_z(x,y,z) \vb e_z \]
    The divergence theorem states that
    \begin{equation}
        \int_V \underbrace{\pdv{F_z}{z}}_{\div{\vb F}} \dd{V} = \int_{\partial V} F_z \vb e_z \cdot \dd{\vb S}
        \tag{$\dagger$}
    \end{equation}
    We would like to show that these two are really the same. First, let us simplify the problem to a convex volume $V$, such that we can split the boundary into two halves, one with normals in the positive $z$ direction ($S_+$) and one with normals in the negative $z$ direction ($S_-$). Then $\partial V = S_+ \cup S_-$. Project the volume into the $x$-$y$ plane, and call this region $A$. This planar region is then the shape of the `cut' between the $S_+$ and $S_-$ halves. We can write
    \[ S_\pm = \left\{ \vb x(x, y) = \begin{pmatrix}
            x \\ y \\ g_\pm (x, y)
        \end{pmatrix} : (x, y) \in A \right\} \]
    We can then say
    \begin{align*}
        \int_V \pdv{F_z}{z} \dd{V} & = \iint_{A} \left[ \int_{z = g_- (x, y)}^{g_+ (x, y)} \pdv{F_z}{z} \dd{z} \right] \dd{x} \dd{y} \\
                                   & = \iint_A \left[ F_z(x, y, g_+ (x, y)) - F_z(x, y, g_- (x, y)) \right] \dd{x}\dd{y}
    \end{align*}
    To calculate right hand side of $(\dagger)$, we need $\dd{\vb S}$:
    \begin{align*}
        \dd{\vb S} & = \pdv{\vb x}{x} \times \pdv{\vb x}{y} \dd{x}\dd{y} \\
                   & = \begin{pmatrix}
            -\pdv*{g_\pm}{x} \\
            -\pdv*{g_\pm}{y} \\
            1                \\
        \end{pmatrix} \dd{x}\dd{y}
    \end{align*}
    Since we want the normal to point `out' of $V$, on $S_\pm$ we have
    \[ \eval{\dd{\vb S}}_{S_\pm} = \pm \begin{pmatrix}
            -\pdv*{g_\pm}{x} \\
            -\pdv*{g_\pm}{y} \\
            1                \\
        \end{pmatrix} \dd{x}\dd{y} \]
    Therefore,
    \begin{align*}
        \int_{\partial V} \vb F \cdot \dd{\vb S} & = \left[ \int_{S_+} + \int_{S_-} \right] F_z \vb e_z \cdot \dd{\vb S}                   \\
                                                 & = \iint_A F_z(x, y, g_+(x, y)) \dd{x}\dd{y} - \iint_A F_z(x, y, g_-(x, y)) \dd{x}\dd{y}
    \end{align*}
    which matches the expression we found for the left hand side of $(\dagger)$ above. In the same way, we can show that
    \begin{align*}
        \int_V \pdv{F_x}{x} \dd{V} & = \int_{\partial V} F_x \vb e_x \cdot \dd{\vb S} \\
        \int_V \pdv{F_y}{y} \dd{V} & = \int_{\partial V} F_y \vb e_y \cdot \dd{\vb S} \\
    \end{align*}
    and because the integrals are linear, we can compute their sum to find
    \[ \int_V \div{\vb F} \dd{V} = \int_{\partial V} \vb F \cdot \dd{\vb S} \]
    which is exactly the divergence theorem.
\end{proof}

\subsection{Green's Theorem}
We can use the two-dimensional divergence theorem to prove Green's theorem.
\begin{proof}
    Let
    \[ \vb F = \begin{pmatrix}
            Q(x, y) \\ -P(x, y)
        \end{pmatrix} \]
    Then
    \[ \iint_A \left( \pdv{Q}{x} - \pdv{P}{y} \right) \dd{x}\dd{y} = \int_A \div{\vb F} \dd{A} = \oint_{\partial A} \vb F \cdot \vb n \dd{s} \]
    If $\partial A$ is parametrised with respect to arc length, which means that the unit tangent vector is
    \[ \vb t = \begin{pmatrix}
            x'(s) \\ y'(s)
        \end{pmatrix} \]
    then the normal vector is
    \[ \vb n = \begin{pmatrix}
            y'(s) \\ -x'(s)
        \end{pmatrix} \]
    Therefore,
    \[ \oint_{\partial A} \vb F \cdot \vb n \dd{s} = \oint_{\partial A} \begin{pmatrix}
            Q \\ -P
        \end{pmatrix} \cdot \begin{pmatrix}
            y'(s) \\ -x'(s)
        \end{pmatrix} \dd{s} = \oint_{\partial A} P \dv{x}{s} \dd{s} + Q \dv{y}{s} \dd{s} = \oint_{\partial A} P \dd{x} + Q \dd{y} \]
\end{proof}

\subsection{Stokes' Theorem}
We can now use Green's theorem to derive Stokes' theorem.
\begin{proof}
    Consider a regular surface
    \[ S = \left\{ \vb x = \vb x(u, v) \colon (u, v) \in A \right\} \]
    Then the boundary is
    \[ \partial S = \left\{ \vb x = \vb x(u, v) \colon (u, v) \in \partial A \right\} \]
    Green's theorem gives
    \begin{equation}
        \oint_{\partial A} P \dd{u} + Q \dd{v} = \iint_A \left( \pdv{Q}{u} - \pdv{P}{v} \right) \dd{u}\dd{v}
        \tag{$\dagger$}
    \end{equation}
    We will now set
    \[ P(u, v) = \vb F(\vb x(u, v)) \cdot \pdv{\vb x}{u};\quad Q(u, v) = \vb F(\vb x(u, v)) \cdot \pdv{\vb x}{v} \]
    Then
    \[ P \dd{u} + Q \dd{v} = \vb F(\vb x(u, v)) \cdot \left( \pdv{\vb x}{u} \dd{u} + \pdv{\vb v} \dd{v} \right) = \vb F(\vb x(u, v)) \cdot \dd{\vb x(u, v)} \]
    And so we can compute the left hand side of $(\dagger)$:
    \[ \oint_{\partial A} P \dd{u} + Q \dd{v} = \oint_{\partial S} \vb F \cdot \dd{\vb x} \]
    For the right hand side, we must first compute some derivatives.
    \[ Q = F_i(\vb x(u, v))\pdv{x_i}{v} \implies \pdv{Q}{u} = \pdv{x_j}{u}\pdv{F_i}{x_j}\pdv{x_i}{v} + F_i \pdv{x_i}{u}{v} \]
    \[ P = F_i(\vb x(u, v))\pdv{x_i}{u} \implies \pdv{Q}{v} = \pdv{x_j}{v}\pdv{F_i}{x_j}\pdv{x_i}{u} + F_i \pdv{x_i}{v}{u} \]
    Hence
    \begin{align*}
        \pdv{Q}{u} - \pdv{P}{v} & = \left( \pdv{x_i}{v}\pdv{x_j}{u} - \pdv{x_i}{u}\pdv{x_j}{v} \right) \pdv{F_i}{x_j}                       \\
                                & = \left( \delta_{ip} \delta_{jq} - \delta_{iq} \delta_{jp} \right) \pdv{F_i}{x_j}\pdv{x_p}{v}\pdv{x_q}{u} \\
                                & = \varepsilon_{ijk} \varepsilon_{pqk} \pdv{F_i}{x_j}\pdv{x_p}{v}\pdv{x_q}{u}                              \\
                                & = \left[ -\curl{\vb F} \right]_k \left( -\pdv{\vb x}{u} \times \pdv{\vb x}{v} \right)_k                   \\
                                & = \left( \curl{\vb F} \right) \cdot \left( \pdv{\vb x}{u} \times \pdv{\vb x}{v} \right)                   \\
    \end{align*}
    Therefore,
    \[ \iint_{A} \left( \pdv{Q}{u} - \pdv{P}{v} \right) \dd{u}\dd{v} = \iint_{A} \left( \curl{\vb F} \right) \cdot \left( \pdv{\vb x}{u} \times \pdv{\vb x}{v} \right) = \iint_{S} (\curl{\vb F}) \cdot \dd{\vb S} \]
    which gives Stokes' theorem as required.
\end{proof}

\section{Maxwell's Equations}
\subsection{Introduction and the Equations}
We will denote the magnetic field by $\vb B(\vb x, t)$, and the electric field by $\vb E(\vb x, t)$. These fields will depend on the current density $\vb J(\vb x, t)$, the electric current per unit area, and the charge density $\rho(\vb x, t)$, the electric charge per unit volume.
\begin{align}
    \div{\vb E}                                      & = \frac{\rho}{\varepsilon_0} \tag{1} \\
    \div{\vb B}                                      & = 0                          \tag{2} \\
    \curl{\vb E} + \pdv{\vb B}{t}                    & = 0                          \tag{3} \\
    \curl{\vb B} - \mu_0\varepsilon_0 \pdv{\vb E}{t} & = \mu_0 \vb J \tag{4}
\end{align}
The constants $\varepsilon_0$ and $\mu_0$ denote the permittivity and permeability of free space, which obey
\[ \frac{1}{\mu_0 \varepsilon_0} = c^2 \]
where $c$ is the speed of light, \SI{299792458}{\metre\per\second}. Note that if we take the divergence of equation (4), we find
\begin{align*}
    \mu_0 \varepsilon_0 \pdv{t}\left( \div{\vb E} \right) + \mu_0 \div{\vb J}              & = 0 \\
    (1) \implies \mu_0 \varepsilon_0 \pdv{t}\frac{\rho}{\varepsilon_0} + \mu_0 \div{\vb J} & = 0 \\
    \pdv{\rho}{t} + \div{\vb J}                                                            & = 0 \\
\end{align*}
which is a conservation law for charge.

\subsection{Integral Formulations of Maxwell's Equations}
Integrating $(1)$ over some volume $V$, and applying the divergence theorem, gives
\begin{align*}
    \div{\vb E}                          & = \frac{\rho}{\varepsilon_0}                 \\
    \int_V \div{\vb E} \dd{V}            & = \frac{1}{\varepsilon_0} \int_V \rho \dd{V} \\
    \int_{\partial V} \vb E \cdot \dd{S} & = \frac{Q}{\varepsilon_0}                    \\
\end{align*}
where $Q$ is the total charge in $V$. This is known as Gauss' law. For magnetic fields, we can integrate $(2)$:
\[ \int_{V} \div{\vb B} \dd{V} = \int_{\partial V} \vb B \cdot \dd{S} = 0 \]
Hence there is no net magnetic flux over any closed surface $\partial V$. This implies that we cannot have a magnetic field with only a north pole or only a south pole. Integrating $(3)$ over a surface, and applying Stokes' theorem, gives
\begin{align*}
    \curl{\vb E} + \pdv{\vb B}{t}                                                      & = 0                                     \\
    \int_S \qty(\curl{\vb E} + \pdv{\vb B}{t}) \cdot \dd{\vb S}                        & = 0                                     \\
    \oint_{\partial S} \vb E \cdot \dd{\vb x} + \int_S \pdv{\vb B}{t} \cdot \dd{\vb S} & = 0                                     \\
    \oint_{\partial S} \vb E \cdot \dd{\vb x}                                          & = -\dv{t} \int_S \vb B \cdot \dd{\vb S}
\end{align*}
So a change in the magnetic flux through a surface $S$ induces a circulation in $\vb E$ about the boundary. Integrating $(4)$ over a surface, again using Stokes' theorem, we have
\begin{align*}
    \int_S \qty(\curl{\vb B} - \mu_0\varepsilon_0 \pdv{\vb E}{t}) \cdot \dd{\vb S} & = \int_S \mu_0 \vb J \cdot \dd{\vb S}                                                             \\
    \oint_{\partial S} \vb B \cdot \dd{\vb x}                                      & = \int_S \mu_0 \vb J \cdot \dd{\vb S} + \int_S \mu_0\varepsilon_0 \pdv{\vb E}{t} \cdot \dd{\vb S} \\
    \oint_{\partial S} \vb B \cdot \dd{\vb x}                                      & = \mu_0 \int_S \vb J \cdot \dd{\vb S} + \mu_0\varepsilon_0 \dv{t} \int_S \vb E \cdot \dd{\vb S}   \\
\end{align*}
So if an electric current flows through a wire, this generates a circulation of the magnetic field around the wire.

\subsection{Electromagnetic Waves}
In empty space, $\rho = 0$ and $\vb J = \vb 0$. Maxwell's equations show that
\begin{align}
    \div{\vb E}                                       & = 0 \tag{1} \\
    \div{\vb B}                                       & = 0 \tag{2} \\
    \curl{\vb E} + \pdv{\vb B}{t}                     & = 0 \tag{3} \\
    \curl{\vb B} - \mu_0 \varepsilon_0 \pdv{\vb E}{t} & = 0 \tag{4}
\end{align}
Recall that the Laplacian of a vector field $\vb F$ is
\[ \laplacian \vb F = \grad(\div{\vb F}) - \curl(\curl{\vb F}) \]
We can deduce that
\begin{align*}
    \laplacian \vb E & = \grad(\div{\vb E}) - \curl(\curl{\vb E})  \\
                     & = \grad(0) - \curl(-\pdv{\vb B}{t})         \\
                     & = \curl(\pdv{\vb B}{t})                     \\
                     & = \dv{t} \curl{\vb B}                       \\
                     & = \dv{t} \mu_0 \varepsilon_0 \pdv{\vb E}{t} \\
                     & = \frac{1}{c^2} \pdv[2]{\vb E}{t}
\end{align*}
\[ \therefore \laplacian \vb E - \frac{1}{c^2} \pdv[2]{\vb E}{t} = \vb 0 \]
which is the wave equation for waves travelling at speed $c$. Hence, in a vacuum, the electric field propagates at speed $c$. Similarly, for the magnetic field,
\begin{align*}
    \laplacian \vb B & = \grad(\div{\vb B}) - \curl(\curl{\vb B})             \\
                     & = \grad(0) - \curl(\mu_0 \varepsilon_0 \pdv{\vb E}{t}) \\
                     & = -\mu_0\varepsilon_0 \dv{t} \curl{\vb E}              \\
                     & = \mu_0\varepsilon_0 \dv{t} \pdv{\vb B}{t}             \\
                     & = \frac{1}{c^2} \pdv[2]{\vb B}{t}
\end{align*}
\[ \therefore \laplacian \vb B - \frac{1}{c^2} \pdv[2]{\vb B}{t} = \vb 0 \]
Hence the magnetic field also propagates at speed $c$. So in general, we can say that electromagnetic waves always travel at speed $c$ in a vacuum.

\subsection{Electrostatics and Magnetostatics}
Suppose that all fields and source terms are independent of $t$. Then Maxwell's equations decouple into
\begin{align}
    \div{\vb E}  & = \frac{\rho}{\varepsilon_0} \tag{1} \\
    \div{\vb B}  & = 0                          \tag{2} \\
    \curl{\vb E} & = 0                          \tag{3} \\
    \curl{\vb B} & = \mu_0 \vb J \tag{4}
\end{align}
which gives one system of equations for $\vb E$, and one for $\vb B$. When considering the whole of $\mathbb R^3$, which is 2-connected, then equations (2) and (3) imply
\[ \vb E = -\grad \phi;\quad \vb B = \curl \vb A \]
where $\phi$ is the electric potential, and $\vb A$ is the magnetic potential. Substituting into the other two equations, we have
\begin{align*}
    (1) \implies -\div{\grad \phi} & = \frac{\rho}{\varepsilon_0} \\
    -\laplacian \phi               & = \frac{\rho}{\varepsilon_0}
\end{align*}
and
\[ (4) \implies \curl(\curl{\vb A}) = \mu_0 \vb J \]

\section{Poisson's and Laplace's Equations}
\subsection{The Boundary Value Problem}
Many problems in mathematical physics can be reduced to the form
\[ \laplacian{\phi} = F \]
This is called Poisson's equation. In the case that $F \equiv 0$, this is called Laplace's equation. We are interested in solving this equation on $\Omega \subseteq \mathbb R^n$ for $n = 2, 3$. This is too general to solve at the moment, so we will need to supply boundary conditions, which are very common in physical problems. In other words, $\phi$ will be known on $\partial \Omega$, or as $\abs{\vb x} \to \infty$ if $\Omega = \mathbb R^n$. For instance, the Dirichlet problem is
\[ \laplacian{\phi} = F \text{ inside } \Omega;\quad \phi = f \text{ on } \partial \Omega \]
The Neumann problem is
\[ \laplacian{\phi} = F \text{ inside } \Omega;\quad \pdv{\phi}{\vb n} = g \text{ on } \partial \Omega \]
where $\vb n$ is the normal to the surface, and $\pdv{\phi}{\vb n} := \vb n \cdot \grad{\phi}$. As a further restriction, we must interpret the boundary conditions in an `appropriate' manner; we assume that $\phi$ (or $\pdv{\phi}{\vb n}$) approaches the behaviour at the boundary continuously as $\vb x \to \partial \Omega$. More precisely, $\phi$ and $\grad{\phi}$ are continuous on $\Omega \cup \partial\Omega$. Note that if we are solving some equation $\laplacian{\phi} = 0$ in $\Omega$, we must be certain that $\phi$ is actually well-defined on the entire set. As a worked example, consider
\[ \laplacian{\phi} = r \text{ inside } \left\{ r < a \right\};\quad \phi = 1 \text{ on } \left\{ r = a \right\} \]
We might guess that the solution is of the form $\phi(r)$. We can use the formula
\[ \laplacian{\phi} = \frac{1}{r^2} \dv{r}\left( r^2 \dv{\phi}{r} \right) \]
to get
\[ r^3 = \dv{r}\left( r^2 \dv{\phi}{r} \right) \text{ inside } \left\{ r < a \right\};\quad \phi(a) = 1 \]
The general solution to the first part is
\[ \phi(r) = A + \frac{B}{r} + \frac{1}{12}r^3 \]
The $\frac{B}{r}$ term is \textit{not} well-defined inside $\left\{ r < a \right\}$, therefore $B=0$ to eliminate the problematic term. By the second part, we can solve for $A$:
\[ 1 = \phi(a) = A + \frac{1}{12}a^3 \implies A = 1 - \frac{1}{12}a^3 \]
Hence the solution is
\[ \phi(r) = 1 + \frac{1}{12}\left(r^3 - a^3\right) \]

\subsection{Uniqueness of Solutions}
When solving Poisson's or Laplace's equation, we want to ensure that the solution we find is unique. If it is unique, then we can apply similar logic to solving differential equations, where we can guess the form of an equation and then derive the solution from that, and we don't need to worry about solutions that do not have this form. Consider a generic linear problem
\begin{equation}
    L\phi = F \text{ in } \Omega;\quad B \phi = f \text{ on } \partial\Omega \tag{$\dagger$}
\end{equation}
where $L$ and $B$ are linear differential operators. If $\phi_1$ and $\phi_2$ are both solutions to $(\dagger)$, then consider $\psi = \phi_1 - \phi_2$. By linearity,
\[ L\psi = L\phi_1 - L\phi_2 = F - F = 0 \text{ in } \Omega \]
and
\[ B\psi = B\phi_1 - B\phi_2 = f - f = 0 \text{ on } \partial\Omega \]
If we can show that the only solution to these new equations is $\psi = 0$, we must conclude that $\phi_1 = \phi_2$, which means that there is only one solution to $(\dagger)$. Hence the solution to a linear problem is unique if and only if the only solution to the homogeneous problem is zero.

\begin{proposition}
    The solution to the Dirichlet problem is unique. The solution to the Neumann problem is unique up to the addition of an arbitrary constant.
\end{proposition}
\begin{proof}
    Let $\psi = \phi_1 - \phi_2$ be the difference between two solutions. In the Dirichlet case, we want to show that $\psi = 0$, and in the Neumann case, we want to show that $\psi$ is an arbitrary constant. We know that
    \[ \laplacian{\psi} = 0 \text{ in } \Omega;\quad B\psi = 0 \text{ on } \partial\Omega \]
    where $B\psi = \psi$ in the Dirichlet problem, or $B\psi = \pdv{\psi}{\vb n}$ in the Neumann problem. Consider the non-negative functional
    \[ I[\psi] = \int_\Omega \abs{\grad{\psi}}^2 \dd{V} \geq 0 \]
    Clearly,
    \[ I[\psi] = 0 \iff \grad{\psi} = 0 \text{ everywhere in } \Omega \]
    Now, note that we can apply the divergence theorem to get
    \begin{align*}
        I[\psi] & = \int_\Omega \abs{\grad{\psi}}^2 \dd{V}                                          \\
                & = \int_\Omega \grad{\psi} \cdot \grad{\psi} \dd{V}                                \\
                & = \int_\Omega \left( \div(\psi \grad{\psi}) - \psi\laplacian{\psi} \right) \dd{V} \\
                & = \int_\Omega \div(\psi \grad{\psi}) \dd{V}                                       \\
                & = \int_{\partial\Omega} \psi \grad{\psi} \cdot\dd{\vb S}                          \\
                & = \int_{\partial\Omega} \psi \grad{\psi} \cdot \vb n \dd{S}                       \\
                & = \int_{\partial\Omega} \psi \dv{\psi}{\vb n} \dd{S}                              \\
    \end{align*}
    In the Dirichlet case, $I[\psi] = 0$ since $\psi = 0$ on the boundary. In the Neumann case, $I[\psi] = 0$ as well, since $\dv{\psi}{\vb n} = 0$. Hence, in either case, $\grad{\psi} = 0$ everywhere in $\Omega$. Therefore, $\psi$ is a constant throughout $\Omega$. In the Dirichlet case, we know that $\psi = 0$ on the boundary, hence $\psi = 0$ everywhere as it is continuous. However, in the Neumann problem, no such deduction can be made.
\end{proof}
\noindent Here is an example from electrostatics. Consider the charge density $\rho$ defined by
\[ \rho(\vb x) = \begin{cases}
        0    & r < a    \\
        F(r) & r \geq a
    \end{cases} \]
We can show that there is no electric field in the region $r < a$. We know that the electric potential $\phi$ will satisfy
\[ \laplacian{\phi} = \frac{-\rho(\vb x)}{\varepsilon_0} = 0 \text{ if } r<a \]
By symmetry, we will try a $\phi$ of the form $\phi(r)$. Hence, $\phi(a)$ is constant on the boundary $r=a$. Note that the unique solution to
\[ \laplacian{\phi} = 0 \text{ for } r<a;\quad \phi = \text{constant on } r = a \]
is exactly that $\phi$ is constant everywhere. Hence
\[ \vb E = -\grad{\psi} = 0 \text{ throughout } r<a \]
This can be viewed as a version of Newton's shell theorem.

\section{Gauss' Flux Method}
\subsection{Spherical Symmetry}
Suppose the source term (the $F$ on the right hand side of Poisson's equation) is spherically symmetric, so $F$ is a function of $r = \abs{\vb x}$. Assuming we are trying to solve the equation for $\Omega = \mathbb R^3$, we can rewrite the problem as
\begin{equation}
    \div{\grad{\phi}} = F
    \tag{$\ast$}
\end{equation}
Since the right hand side only depends on $r$, the same is true of the left hand side. So we might guess a $\phi$ of the form $\phi(r)$. In which case, we can compute
\[ \grad{\phi} = \phi'(r) \vb e_r \]
Using Gauss' flux method, we will integrate $(\ast)$ over some spherical region $\abs{\vb x} < R$, and use the divergence theorem.
\[ \int_{\abs{\vb x} < R} \div{\grad{\phi}}\dd{V} = \int_{\abs{\vb x} = R} \grad{\phi} \cdot \dd{\vb S} = \int_{\abs{\vb x} < R} F(r) \dd{V} \]
Thinking of the source term $F$ as some kind of density, for instance charge density or mass density, the right hand side can be thought of as the total amount of charge or mass inside the ball. We will call this term $Q(R)$.
\[ \int_{\abs{\vb x} = R} \grad{\phi} \cdot \dd{\vb S} = Q(R) \]
Recall that on a sphere of radius $R$, $\dd{\vb S} = \vb e_r R^2 \sin\theta \dd{\theta}\dd{\phi}$. Therefore, on the boundary $\abs{\vb x} = R$,
\[ \grad{\phi} \cdot \dd{\vb S} = \phi'(r) \vb e_r \cdot \vb e_r R^2 \sin\theta \dd{\theta}\dd{\phi} = \phi'(r) R^2 \sin\theta \dd{\theta}\dd{\phi} = \phi'(r) \dd{S} \]
Hence,
\[ Q(R) = \int_{\abs{\vb x} = R} \phi'(r) \dd{S} \]
But $\phi'(r)$ is a constant on the surface we are integrating over. Therefore,
\[ Q(R) = \phi'(R) \int_{\abs{\vb x} = R} \dd{S} = 4\pi R^2 \phi'(R) \]
In summary,
\[ \phi'(R) = \frac{Q(R)}{4\pi R^2} \implies \grad{\phi} = \frac{Q(R)}{4\pi R^2} \vb e_r \]

\subsection{Example in Electrostatics}
Recall the first of Maxwell's equations:
\[ \div{\vb E} = \frac{\rho}{\varepsilon_0} \]
Since we are dealing with electrostatics, the curl of $\vb E$ is zero. Hence $\vb E = -\grad{\phi}$, so
\[ \laplacian{\phi} = -\frac{\rho}{\varepsilon_0} \]
Consider a charge density $\rho$ of the form
\[ \rho(r) = \begin{cases}
        \rho_0, & 0 \leq r \leq a \\
        0,      & r > a
    \end{cases} \]
By the previous result,
\[ \phi'(r) = \frac{1}{4 \pi \varepsilon_0} \frac{Q(r)}{r^2} \]
where
\[ Q(r) = \int_{\abs{\vb x} \leq R} \rho(r) \dd{V} \]
Note, if $R > a$ then $Q(R) = Q(a)$, which we will denote $Q$ for the total charge. Hence, we have the following solution:
\[ \vb E(\vb x) = \begin{cases}
        \frac{1}{4 \pi \varepsilon_0} \frac{Q(r)}{r^2}\vb e_r, & r \leq a \\
        \frac{1}{4 \pi \varepsilon_0} \frac{Q}{r^2}\vb e_r,    & r > a
    \end{cases} \]
If we take $a \to 0$, but keeping $Q$ fixed, this represents a point charge. Then
\[ \vb E(\vb x) = \frac{1}{4 \pi \varepsilon_0} \frac{Q}{r^2}\vb e_r \]
In this case, the charge density $\rho$ is
\[ \rho(\vb x) = Q\delta(\vb x) \]
where $\delta$ is the Dirac delta function.

\subsection{Cylindrical Symmetry}
Suppose instead that the source term $F$ is cylindrically symmetric, so $F$ is a function of $\rho$, the distance from the $z$ axis. Similarly as before, we can guess that $\phi$ is a function only of $\rho$. We can integrate $\div{\grad{\phi}} = F(\rho)$ over a cylinder $V$ of radius $R$ and height $a$.
\[ \grad{\phi} = \phi'(\rho) \vb e_\rho \]
Hence,
\[ \int_{V} \div{\grad{\phi}} \dd{V} = \int_{V} F(\rho) \dd{V} \]
The left hand side becomes
\[ \int_{\partial V} \grad\phi \cdot \dd{\vb S} \]
On the top circle, the normal $\vb n$ would be in the $\vb e_z$ direction, and on the bottom circle, $\vb n$ would be in the $-\vb e_z$ direction. On the curved surface, $\vb n$ would be in the $\vb e_\rho$ direction. Note that since $\grad\phi$ only has a component in the $\vb e_\rho$ direction, on both the top and bottom circles will provide no contribution to the final result for this boundary integral. $\dd{\vb S} = R \dd{\phi} \dd{z} \vb e_\rho$, hence
\[
    \int_{\partial V} \grad\phi \cdot \dd{\vb S}
    = \int_{\phi = 0}^{2 \pi} \int_{z = z_0}^{z_0 + a} \phi'(R) R \dd{\phi} \dd{z}
    = 2\pi \int_{z = z_0}^{z_0 + a} \phi'(R) R \dd{z}
    = 2\pi a R \phi'(R)
\]
Substituting into the above equation gives
\[ \phi'(R) = \frac{1}{2\pi a R} \int_{V} F(\rho) \dd{V} \]
Note that the integral $\int_{V} F(\rho) \dd{V}$ is given by
\[ \int_{V} F(\rho) \dd{V} = \int_{\phi = 0}^{2 \pi} \dd{\phi} \int_{z = z_0}^{z_0 + a} \dd{z} \int_{\rho = 0}^R \dd{\rho} F(\rho) \rho = 2 \pi a \int_0^R F(\rho) \rho \dd{\rho} \]
In conclusion,
\[ \phi'(\rho) = \frac{1}{\rho} \int_0^\rho sF(s) \dd{s} \]

\subsection{Example of an Infinitesimally Thick Wire}
Consider a line of charge density $\lambda$ per unit length along the wire. We could proceed analogously to the last example before, by considering a cylinder with positive radius $a$, using Gauss' flux method, and then letting $a \to 0$. However, we will use a different method. Let $F(\rho)$ be the desired charge density. So if we integrate $F(\rho)$ over any cylinder $C$ of length 1, we should retrieve the value $\lambda$.
\begin{align*}
    \lambda = \int_{C} F(\rho) \dd{V} & = \int_{z = z_0}^{z_0 + 1} \dd{z} \int_{\phi = 0}^{2 \pi} \dd{\phi} \int_{\rho = 0}^R \dd{\rho} \rho F(\rho) \\
                                      & = 2 \pi \int_{0}^R \dd{\rho} \rho F(\rho)
\end{align*}
By inspection, $F$ must have the form of a delta function, so $F(\rho) = \lambda \delta(\rho) \frac{1}{2 \pi \rho}$. Hence the corresponding electric potential $\phi$ is given by
\[ \phi'(\rho) = -\frac{1}{\varepsilon_0 \rho} \int_0^\rho \lambda \delta(s) \frac{1}{2 \pi} \dd{s} = \frac{-\lambda}{2\pi\varepsilon_0 \rho} \]
Hence,
\[ E(\vb x) = \frac{1}{2 \pi \varepsilon_0} \frac{\vb e_\rho}{\rho} \]

\section{Superposition Principle and Integral Solutions}
\subsection{Superposition Principle}
Consider a linear operator $L$. If we have solutions $L \psi_n = F_n$ for $n = 1, 2, \dots$, then we have $L\qty(\sum_n \psi_n) = \sum_n F_n$ by linearity. In other words, we can superimpose solutions. We can often break up a forcing term into several smaller, simpler components, and if $L$ is a linear differential operator we can solve for these components separately. For example, we can consider the electric potential due to a pair of point charges $Q_a$ at $\vb x = \vb a$, and $Q_b$ at $\vb x = \vb b$. The charge density would be
\[ \rho(\vb x) = Q_a \delta(\vb x - \vb a) + Q_b \delta(\vb x - \vb b) \]
For one point charge, we know that the electric potential obeys
\[ -\laplacian{\phi} = \frac{Q_a}{\varepsilon_0} \delta(\vb x - \vb a) \]
Hence,
\[ \phi(\vb x) = \frac{Q_a}{4\pi \varepsilon_0} \frac{1}{\abs{\vb x - \vb a}} \]
Then by the superposition principle, for two particles,
\[ \phi(\vb x) = \frac{Q_a}{4\pi \varepsilon_0} \frac{1}{\abs{\vb x - \vb a}} + \frac{Q_b}{4\pi \varepsilon_0} \frac{1}{\abs{\vb x - \vb b}} \]
Now, consider the electric potential outside a ball of radius $\abs{\vb x} < R$ of uniform charge density $\rho_0$. Suppose that the ball has several balls removed from its interior. These `subtracted' balls have the form
\[ \abs{\vb x - \vb a_i} < R_i;\quad i = 1, \dots, N \]
We further require that the balls lay inside the main ball, and do not intersect:
\[ \abs{\vb a_i} + R_i < R;\quad \abs{\vb a_i - \vb a_j} > R_i + R_j \]
We can use the superposition principle to represent each hole as a ball of uniform charge density $-\rho_0$. So the effective potential in $\abs{\vb x} > R$ (outside the vall) from each hole is
\[ \phi(x) = -\frac{Q_i}{4\pi\varepsilon_0} \frac{1}{\abs{\vb x - \vb a_i}};\quad Q_i = \frac{4}{3}\pi R_i^3 \rho_0 \]
Hence, the total potential from the ball and its holes is
\[ \phi(x) = \frac{Q}{4\pi\varepsilon_0} \frac{1}{\abs{\vb x}} - \sum_i\frac{Q_i}{4\pi\varepsilon_0} \frac{1}{\abs{\vb x - \vb a_i}} \]

\subsection{Integral Solutions}
We know that the electric potential due to a point charge at $\vb a$ is proportional to the inverse of the distance to the particle. We can think of a generic distribution of charge density as an infinite collection of superimposed particles, which leads us to consider an integral form for a superposition.
\[ \int_{\mathbb R^3} \frac{F(\vb y)}{\abs{\vb x - \vb y}} \dd{V(\vb y)} \]
where $F$ is the forcing term.
\begin{proposition}
    Suppose $F \to 0$ `rapidly' as $\abs{\vb x} \to \infty$. The unique solution to the Dirichlet problem
    \[ \begin{cases}
            \laplacian{\phi} = F & \vb x \in \mathbb R^3   \\
            \abs{\phi} \to 0     & \abs{\vb{x}} \to \infty
        \end{cases} \]
    is given by
    \[ \phi(\vb x) = -\frac{1}{4\pi}\int_{\mathbb R^3} \frac{F(\vb y)}{\abs{\vb x - \vb y}} \dd{V(\vb y)} \]
\end{proposition}
\noindent This result is another way of saying that
\[ \laplacian(\frac{-1}{4\pi\abs{\vb x}}) = \delta(\vb x) \]
since by differentiating with respect to $x$ under the integral sign,
\begin{align*}
    \laplacian(\frac{-1}{4\pi} \int_{\mathbb R^3} \frac{F(\vb y)}{\abs{\vb x - \vb y}} \dd{V(\vb y)}) & = \frac{-1}{4\pi} \int_{\mathbb R^3} F(\vb y) \laplacian(\frac{1}{\abs{\vb x - \vb y}}) \dd{V(\vb y)} \\
                                                                                                      & = \int_{\mathbb R^3} F(\vb y) \delta(\vb x - \vb y) \dd{V(\vb y)}                                     \\
                                                                                                      & = F(\vb x)
\end{align*}
\noindent so it is sufficient to prove that this Laplacian identity holds. A full proof will not be given here, but here is some intuition to guide the idea. Note that for $r \neq 0$,
\begin{align*}
    \laplacian(\frac{1}{r}) & = \pdv{}{x_i}{x_i} \qty(\frac{1}{r})              \\
                            & = \pdv{x_i} \qty(\frac{-x_i}{r^3})                \\
                            & = \frac{-\delta_{ii}}{r^3} + \frac{3x_i x_i}{r^5} \\
                            & = \frac{-3}{r^3} + \frac{3r^2}{r^5}               \\
                            & = 0
\end{align*}
So certainly $\laplacian(-\frac{1}{4\pi\abs{\vb x}}) = \delta(\vb x)$ for $\vb x \neq 0$. Assuming that the divergence theorem holds for delta functions, for any ball $\abs{\vb x} < R$ we would also have
\begin{align*}
    \int_{\abs{\vb x} < R} \laplacian(\frac{1}{\abs{\vb x}}) \dd{V} & = \int_{\abs{\vb x} = R} \grad(\frac{1}{\abs{\vb x}}) \cdot \dd{\vb S}                                                          \\
                                                                    & = \int_{\theta = 0}^{\pi} \dd{\theta} \int_{\phi = 0}^{2 \pi} \dd{\phi} \qty(\frac{-\vb e_r}{R^2}) \cdot \vb e_r R^2 \sin\theta \\
                                                                    & = \int_{\theta = 0}^{\pi} \dd{\theta} \int_{\phi = 0}^{2 \pi} \dd{\phi} \qty(\frac{-1}{R^2}) R^2 \sin\theta                     \\
                                                                    & = -4\pi
\end{align*}
So for any $R > 0$,
\[ \int_{\abs{\vb x} < R} \laplacian(\frac{-1}{4\pi\abs{\vb x}}) \dd{V} = 1 = \int_{\abs{\vb x} < R} \delta(\vb x) \dd{V} \]
So we might conclude that this Laplacian operator really does give the Dirac delta function.

\section{Harmonic Functions}
\subsection{Introduction}
Harmonic functions are solutions to Laplace's equation,
\[ \laplacian{\phi} = 0 \]
\begin{proposition}
    If $\phi$ is harmonic on $\Omega \subset \mathbb R^3$, then
    \begin{equation}
        \phi(\vb a) = \frac{1}{4 \pi r^2} \int_{\abs{\vb x - \vb a} = r} \phi(\vb x) \dd{S}\tag{$\ast$}
    \end{equation}
    for $\vb a \in \Omega$, and $r$ sufficiently small such that all $\vb x$ are in $\Omega$.
\end{proposition}
\noindent This is known as the `mean value' property; it essentially shows that the value of $\phi$ at any given point $\vb a$ is the average of $\phi$ on the surface of any ball around $\vb a$.
\begin{proof}
    Let $F(r)$ denote the right hand side of $(\ast)$, $\frac{1}{4 \pi r^2} \int_{\abs{\vb x - \vb a} = r} \phi(\vb x) \dd{S}$. Then,
    \[ F(r) = \frac{1}{4\pi r^2} \int_{\abs{\vb x} = r} \phi(\vb a + \vb x) \dd{S} \]
    We can parametrise this sphere using spherical polar coordinates, giving
    \begin{align}
        F(r) & = \frac{1}{4\pi r^2} \int_{\phi = 0}^{2 \pi} \left[ \int_{\theta = 0}^\pi \phi(\vb a + r\vb e_r) r^2 \sin \theta \dd{\theta} \right] \dd{\phi} \notag  \\
             & = \frac{1}{4\pi} \int_{\phi = 0}^{2 \pi} \left[ \int_{\theta = 0}^\pi \phi(\vb a + r\vb e_r) \sin \theta \dd{\theta} \right] \dd{\phi} \tag{$\dagger$}
    \end{align}
    Differentiating with respect to $r$, using $\dv{r}\phi(\vb a + r \vb e_r) = \vb e_r \cdot \grad\phi(\vb a + r \vb e_r)$,
    \begin{align*}
        F'(r) & = \frac{1}{4\pi} \int_{\phi = 0}^{2 \pi} \int_{\theta = 0}^\pi \vb e_r \cdot \grad\phi(\vb a + r\vb e_r) \sin \theta \dd{\theta} \dd{\phi}         \\
              & = \frac{1}{4\pi r^2} \int_{\phi = 0}^{2 \pi} \int_{\theta = 0}^\pi \vb e_r \cdot \grad\phi(\vb a + r\vb e_r) r^2 \sin \theta \dd{\theta} \dd{\phi} \\
              & = \frac{1}{4\pi r^2} \int_{\abs{\vb x} = r} \vb e_r \cdot \grad\phi(\vb a + r\vb e_r) \dd{S}                                                       \\
              & = \frac{1}{4\pi r^2} \int_{\abs{\vb x} = r} \grad\phi(\vb a + \vb x) \cdot \dd{\vb S}                                                              \\
              & = \frac{1}{4\pi r^2} \int_{\abs{\vb x - \vb a} = r} \grad\phi \cdot \dd{\vb S}                                                                     \\
              & = \frac{1}{4\pi r^2} \int_{\abs{\vb x - \vb a} < r} \div{\grad\phi} \dd{V}                                                                         \\
              & = \frac{1}{4\pi r^2} \int_{\abs{\vb x - \vb a} < r} \laplacian{\phi} \dd{V}                                                                        \\
              & = \frac{1}{4\pi r^2} \int_{\abs{\vb x - \vb a} < r} 0 \dd{V}                                                                                       \\
              & = 0
    \end{align*}
    Now, note from $(\dagger)$ that if $r \to 0$, then $F(r) \to \phi(\vb a)$, and the result follows.
\end{proof}

\subsection{Intuitive Explanation of Laplacian}
We can use the central idea of the above proof to examine what the Laplacian operator is really doing.
\begin{proposition}
    For any smooth function $\phi \colon \mathbb R^3 \to \mathbb R$,
    \[ \laplacian\phi(\vb a) = \lim_{r \to 0} \frac{6}{r^2} \left[ \frac{1}{4\pi r^2} \int_{\abs{\vb x - \vb a} = r} \phi(\vb x) \dd{S} \right] - \phi(\vb a) \]
    In particular, if $\phi$ satisfies the mean value property, then it is harmonic.
\end{proposition}
\noindent In some sense, the Laplacian is measuring how the value of $\phi$ at a point differs from its average over a small sphere centred at this point.
\begin{proof}
    Consider a function $G(r)$ defined by
    \[ G(r) = \frac{1}{4\pi r^2} \int_{\abs{\vb x - \vb a} = r} \phi(\vb x) \dd{S} - \phi(\vb a) \]
    $G$ measures the extent to which $\phi$ differs from its average. From the previous proof,
    \[ G(r) = F(a) - \phi(\vb a) \implies G'(r) = F'(r) \]
    So,
    \[ G'(r) = \frac{1}{4\pi r^2} \int_{\abs{\vb x - \vb a} < r} \laplacian{\phi} \dd{V} \]
    Now, note that as $r \to 0$,
    \begin{align*}
        \int_{\abs{\vb x - \vb a} < r} \laplacian \phi(\vb x) \dd{V} & = \laplacian\phi(\vb a) \int_{\abs{\vb x - \vb a} < r} \dd{V} + \int_{\abs{\vb x - \vb a} < r} \left( \laplacian\phi(\vb x) - \laplacian\phi(\vb a) \right) \dd{V} \\
                                                                     & = \frac{4\pi}{3r^3} \laplacian\phi(\vb a) + o(r^3)
    \end{align*}
    Now, as $r \to 0$,
    \[ G'(r) = \frac{1}{4\pi r^2} \left[ \frac{4\pi}{3r^3} \laplacian\phi(\vb a) + o(r^3) \right] = \frac{r}{3}\laplacian\phi(\vb a) + o(r) \]
    Comparing this to the Taylor expansion,
    \[ G'(r) = G'(0) + rG''(0) + o(r) \]
    So certainly, $G'(0) = 0$ since there is no constant term in $G'(r)$. Further, $G''(0) = \frac{1}{3}\laplacian\phi(\vb a)$. Now,
    \[ G(r) = G(0) + rG'(0) + \frac{r^2}{2}G''(0) + o(r^2) \]
    We know that $G(0) = F(0) - \phi(\vb a) = 0$, hence
    \[ G(r) = \frac{1}{6}\laplacian\phi(\vb a) r^2 + o(r^2) \implies \laplacian\phi(\vb a) = \lim_{r \to 0} \frac{6}{r^2}G(r) \]
    which gives the result as required.
\end{proof}

\subsection{Non-Existence of Maximum Points}
\begin{proposition}
    If $\phi$ is harmonic on some volume $\Omega \subset \mathbb R^3$, then $\phi$ cannot have a maximum point at any interior point on $\Omega$, unless $\phi$ is constant.
\end{proposition}
\begin{proof}
    Suppose that there exists a maximum point at $\vb a \in \Omega$. Then $\phi(\vb a) \geq \phi(\vb x)$ for all $\vb x \in \Omega$. Then,
    \[ \phi(\vb a) \geq \phi(\vb x) \text{ on } \abs{\vb x - \vb a} \leq \varepsilon \]
    for some $\varepsilon$ small enough such that the ball is inside $\Omega$. By the mean value property,
    \[ \phi(\vb a) = \frac{1}{4\pi \varepsilon^2} \int_{\abs{\vb x - \vb a} = \varepsilon} \phi(\vb x) \dd{S} \]
    Hence,
    \[ 0 = \frac{1}{4\pi \varepsilon^2} \int_{\abs{\vb x - \vb a} = \varepsilon} \left( \phi(\vb a) - \phi(\vb x) \right) \dd{S} \]
    Note that the integrand is always non-negative, so in order for the integral to equal zero, the integrand must be zero everywhere on the ball. So $\phi(\vb a) = \phi(\vb x)$. Since $\varepsilon$ was arbitrary, we can shrink the ball to a smaller ball around the same point, so $\phi(\vb a) = \phi(\vb x)$ for all $\vb x$ such that $\abs{\vb x - \vb a} \leq \varepsilon$. Hence, $\phi$ is locally constant.

    Now, given any other point $\vb y$, we can introduce a finite sequence of overlapping balls such that the centre of the $(n+1)$th ball is contained inside the $n$th ball, and where the first ball is centred at $\vb a$ and the last ball is centred at $\vb y$. Inductively, the function is constant on each such ball. Hence $\phi$ is actually constant everywhere, since $\vb y$ was arbitrarily chosen.
\end{proof}
\begin{corollary}
    If $\phi$ is harmonic on $\Omega$, then for $\vb x \in \Omega$,
    \[ \phi(x) \leq \max_{\vb y \in \partial \Omega} \phi(\vb y) \]
    This is called the maximum principle.
\end{corollary}

\end{document}