\documentclass{article}

\input{../util.tex}

\title{Probability}
\author{Cambridge University Mathematical Tripos: Part IA}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Probability Spaces}
\subsection{Probability Spaces and $\sigma$-algebras}
\begin{definition}
    Suppose $\Omega$ is a set, and $\mathcal F$ is a collection of subsets of $\Omega$. We call $\mathcal F$ a $\sigma$-algebra if
    \begin{enumerate}
        \item $\Omega \in \mathcal F$
        \item if $A \in \mathcal F$, then $\stcomp{A} \in \mathcal F$
        \item for any countable collection $(A_n)_{n \geq 1}$ with $A_n \in \mathcal F$ for all $n$, we must also have that $\bigcup_n A_n \in \mathcal F$
    \end{enumerate}
\end{definition}
\begin{definition}
    Suppose that $\mathcal F$ is a $\sigma$-algebra on $\Omega$. A function $\mathbb P \colon \mathcal F \to [0, 1]$ is called a probability measure if
    \begin{enumerate}
        \item $\prob{\Omega} = 1$
        \item for any countable disjoint collection of sets $(A_n)_{n \geq 1}$ in $\mathcal F$ ($A_n \in \mathcal F$ for all $n$), then $\prob{\bigcup_{n \geq 1}A_n} = \sum_{n \geq 1} \prob{A_n}$ (this is called `countable additivity')
    \end{enumerate}
    We say that $\prob{A}$ is `the probability of $A$'. We call $(\Omega, \mathcal F, \mathbb P)$ a probability space, where $\Omega$ is the sample space, $\mathcal F$ is the $\sigma$-algebra, and $\mathbb P$ is the probability measure.
\end{definition}

\begin{remark}
    When $\Omega$ is countable, we take $\mathcal F$ to be all subsets of $\Omega$, i.e. $\mathcal F = \mathcal P(\Omega)$, its power set.
\end{remark}
\begin{definition}
    The elements of $\Omega$ are called outcomes, and the elements of $\mathcal F$ are called events.
\end{definition}
Note that $\mathbb P$ is dependent on $\mathcal F$ but not on $\Omega$. We talk about probabilities of \textit{events}, not probabilities of \textit{outcomes}. For example, if you pick a uniform number from the interval $[0, 1]$; then the probability of getting any specific outcome is zero --- but we can define useful events that have non-zero probabilities.

\subsection{Properties of the Probability Measure}
\begin{itemize}
    \item $\prob{\stcomp{A}} = 1 - \prob{A}$, since $A$ and $\stcomp{A}$ are disjoint sets, whose union is $\Omega$
    \item $\prob{\varnothing} = 0$, since it is the complement of $\Omega$
    \item if $A \subseteq B$, then $\prob{A} \leq \prob{B}$
    \item $\prob{A \cup B} = \prob{A} + \prob{B} - \prob{A \cap B}$ using the Inclusion-Exclusion theorem
\end{itemize}

\subsection{Examples of Probability Spaces}
\begin{itemize}
    \item Rolling a fair 6-sided die:
          \begin{itemize}
              \item $\Omega = \{ 1, 2, 3, 4, 5, 6 \}$
              \item $\mathcal F = \mathcal P(\Omega)$
              \item $\forall \omega \in \Omega, \prob{\{ \omega \}} = \frac{1}{6}$, and if $A \subseteq \Omega$ then $\prob{A} = \frac{\abs{A}}{6}$
          \end{itemize}

    \item Equally likely outcomes (more generally). Suppose $\Omega$ is some finite set, e.g. $\Omega = \{ \omega_1, \omega_2, \dots, \omega_n \}$. Then we define $\prob{A} = \frac{\abs{A}}{\abs{\Omega}}$. In classical probability, this models picking a random element of $\Omega$.

    \item Picking balls from a bag. Suppose we have $n$ balls with $n$ labels from the set $\{1, \dots, n\}$, indistinguishable by touching. Let us pick $k \leq n$ balls at random from the bag, \textit{without replacement}. Here, `at random' just means that all possible outcomes are equally likely, and their probability measures should be equal.

          We will take $\Omega = \{ A \subseteq \{1, \dots, n\} : \abs{A} = k \}$. Then $\abs{\Omega} = \binom{n}{k}$. Then of course $\prob{\{ \omega \}} = \frac{1}{\abs*{\Omega}}$, since all outcomes (combinations, in this case) are equally likely.

    \item Consider a well-shuffled deck of 52 cards, i.e. it is equally likely that each possible permutation of the 52 cards will appear. $\Omega = \{ \text{all permutations of 52 cards} \}$, and $\abs*{\Omega} = 52!$

          The probability that the top two cards are aces is therefore $\frac{4 \times 3 \times 50!}{52!} = \frac{1}{221}$, since there are $4 \times 3 \times 50!$ outcomes that produce such an event.

    \item Consider a string of $n$ random digits from $\{0, \dots, 9\}$. Then $\Omega = \{ 0, \dots, 9 \}^n$, and $\abs*{\Omega} = 10^n$. We define $A_k = \{ \text{no digit exceeds } k \}$, and $B_k = \{ \text{largest digit is } k \}$. Then $\prob{B_k} = \frac{\abs*{B_k}}{\abs*{\Omega}}$. Notice that $B_k = A_k \setminus A_{k-1}$. $\abs*{A_k} = (k+1)^n$, so $\abs*{B_k} = (k+1)^n - k^n$, so $\prob{B_k} = \frac{(k+1)^n - k^n}{10^n}$.

    \item The birthday problem. There are $n$ people; what is the probability that at least two of them share a birthday? We assume that each year has exactly 365 days, i.e. nobody is born on 29\textsuperscript{th} of February, and that the probabilities of being born on any given day are equal.

          Let $\Omega = \{1, \dots, 365\}^n$, and $\mathcal F = \mathcal P(\Omega)$. Since all outcomes are equally likely, we take $\prob{\{\omega\}} = \frac{1}{365^n}$.

          Let $A = \{ \text{at least two people share the same birthday} \}$. $\stcomp{A} = \{ \text{all } n \text{ birthdays are different} \}$. Since $\prob{A} = 1 - \prob{\stcomp{A}}$, it suffices to calculate $\prob{\stcomp{A}}$, which is $\frac{\abs*{\stcomp{A}}}{\abs*{\Omega}} = \frac{365!}{(365 - n)!365^n}$. So the answer is $\prob{A} = 1 - \frac{365!}{(365 - n)!365^n}$

          Note that at $n=22$, $\prob{A} \approx 0.476$ and at $n=23$, $\prob{A} \approx 0.507$. So when there are at least 23 people in a room, the probability that two of them share a birthday is around 50\%.
\end{itemize}

\subsection{Combinatorial Analysis}
Let $\Omega$ be a finite set, and suppose $\abs*{\Omega} = n$. We want to partition $\Omega$ into $k$ disjoint subsets $\Omega_1, \dots, \Omega_k$ with $\abs*{\Omega_i} = n_i$ and $\sum_{i=1}^k n_i = n$. How many ways of doing such a partition are there? The result is
\[ \underbrace{\binom{n}{n_1}}_{\text{choose first set}}\underbrace{\binom{n-n_1}{n_2}}_{\text{choose second set}}\dots\underbrace{\binom{n-(n_1 + n_2 + \dots + n_{k+1})}{n_k}}_{\text{choose last set}} = \frac{n!}{n_1!n_2!\dots n_k!} \]
So we will write
\[ \binom{n}{n_1, \dots, n_k} = \frac{n!}{n_1!n_2!\dots n_k!} \]

Now, let $f\colon \{1, \dots, k\} \to \{1, \dots, n\}$. $f$ is strictly increasing if $x < y \implies f(x) < f(y)$. $f$ is increasing if $x < y \implies f(x) \leq f(y)$. How many strictly increasing functions $f$ exist? Note that if we know the range of $f$, the function is completely determined. The range is a subset of $\{1, \dots, n\}$ of size $k$, i.e. a $k$-subset of an $n$-set, which yields $\binom{n}{k}$ choices, and thus there are $\binom{n}{k}$ strictly increasing functions.

How many increasing functions $f$ exist? Let us define a bijection from the set of increasing functions $\{f\colon \{1, \dots, k\} \to \{1, \dots, n\}\}$ to the set of \textit{strictly} increasing functions $\{g\colon \{1, \dots, k\} \to \{1, \dots, n+k-1\}\}$. For any increasing function $f$, we define $g(i) = f(i) + i - 1$. Then $g$ is clearly strictly increasing, and takes values in the range $\{1, \dots, n+k-1\}$. By extension, we can define an increasing function $f$ from any strictly increasing function $g$. So the total number of increasing functions $f\colon \{1, \dots, k\} \to \{1, \dots, n\}$ is $\binom{n+k-1}{k}$.

\section{Stirling's Formula and Countable Subadditivity}
\subsection{Stirling's Formula}
Let $(a_n)$ and $(b_n)$ be sequences. We will write $a_n \sim b_n$ if $\frac{a_n}{b_n} \to 1$ as $n \to \infty$. This is asymptotic equality.
\begin{theorem}[Stirling's Formula]
    $n! \sim n^n\, \sqrt{2 \pi n}\, e^{-n}$ as $n \to \infty$.
\end{theorem}
\noindent Let us first prove the weaker statement $\log (n!) \sim n \log n$.
\begin{proof}
    Let us define $l_n = \log (n!) = \log 2 + \log 3 + \dots + \log n$. For $x \in \mathbb R$, we write $\floor{x}$ for the integer part of $x$. Note that
    \[ \log \floor x \leq \log x \leq \log \floor{x+1} \]
    Let us integrate this from 1 to $n$.
    \[ \sum_{k=1}^{n-1} \log k \leq \int_1^n \log x\, \dd{x} \leq \sum_{k=2}^{n} \log k \]
    \[ l_{n-1} \leq n \log n - n + 1 \leq l_n \]
    For all $n$, therefore:
    \[ n \log n - n + 1 \leq l_n \leq (n+1) \log (n+1) - (n+1) + 1 \]
    Dividing through by $n\log n$, we get
    \[ \frac{l_n}{n \log n} \to 1 \]
    as $n \to \infty$.
\end{proof}
\noindent The following complete proof is non-examinable.
\begin{proof}
    For any twice-differentiable function $f$, for any $a < b$ we have
    \[ \int_a^b f(x) \dd{x} = \frac{f(a) + f(b)}{2} (b - a) - \frac{1}{2}\int_a^b (x-a)(b-x)f''(x)\dd{x} \]
    Now let $f(x) = \log x$, $a=k$ and $b=k+1$. Then
    \begin{align*}
        \int_k^{k+1} \log x \dd{x} & = \frac{\log k + \log(k+1)}{2} + \frac{1}{2}\int_k^{k+1} \frac{(x-k)(k+1-x)}{x^2}\dd{x}                            \\
                                   & = \frac{\log k + \log(k+1)}{2} + \frac{1}{2}\int_0^1 \frac{x(1-x)}{(x+k)^2}\dd{x}                                  \\
        \intertext{Let us take the sum for $k=1, \dots, n-1$ of the equality.}
        \int_1^n \log x \dd{x}     & = \frac{\log ((n-1)!) + \log(n!)}{2} + \frac{1}{2}\sum_{k=1}^{n-1}\int_0^1 \frac{x(1-x)}{(x+k)^2}\dd{x}            \\
        n\log n - n + 1            & = \log (n!) - \frac{\log n}{2} + \sum_{k=1}^{n-1} a_k;\quad a_k = \frac{1}{2}\int_0^1 \frac{x(1-x)}{(x+k)^2}\dd{x} \\
        \log (n!)                  & = n \log n - n + \frac{\log n}{2} + 1 - \sum_{k=1}^{n-1} a_k                                                       \\
        n!                         & = n^n \, e^{-n} \, \sqrt n \, \exp\left( 1 - \sum_{k=1}^{n-1} a_k \right)
    \end{align*}
    Now, note that
    \[ a_k \leq \frac{1}{2}\int_0^1 \frac{x(1-x)}{k^2}\dd{x} = \frac{1}{12k^2} \]
    So the sum of all $a_k$ converges. We set
    \[ A = \exp\left( 1 - \sum_{k=1}^\infty a_k \right) \]
    and then
    \[ n! = n^n \, e^{-n} \, \sqrt n \, A \, \exp\left( \underbrace{\sum_{k=n}^\infty a_k}_{\text{converges to zero}} \right) \]
    Therefore,
    \[ n! \sim n^n\, \sqrt{n}\, e^{-n}\, A \]
    To finish the proof, we must show that $A = \sqrt{2 \pi}$. We can utilise the fact that $n! \sim n^n\, \sqrt{n}\, e^{-n}\, A$.
    \begin{align*}
        2^{-2n} \binom{2n}{n} & = 2^{-2n} \cdot \frac{2n!}{(n!)^2}                                                                                           \\
                              & \sim 2^{-2n} \frac{(2n)^{2n} \cdot \sqrt{2n} \cdot A \cdot e^{-2n}}{n^n\, e^{-n}\, \sqrt n\, A\, n^n\, e^{-n}\, \sqrt n\, A} \\
                              & = \frac{\sqrt{2}}{A\sqrt{n}}
    \end{align*}
    Using a different method, we will prove that $2^{-2n} \binom{2n}{n} \sim \frac{1}{\sqrt{\pi n}}$, which then forces $A = \sqrt{2\pi}$. Consider
    \[ I_n = \int_0^{\frac{\pi}{2}} (\cos \theta)^n \, \dd \theta;\quad n \geq 0 \]
    So $I_0 = \frac{\pi}{2}$ and $I_1 = 1$. By integrating by parts,
    \[ I_n = \frac{n-1}{n}I_{n-2} \]
    Therefore,
    \[ I_{2n} = \frac{2n-1}{2n}I_{2n-2} = \frac{(2n-1)(2n-3)\dots(3)(1)}{(2n)(2n-2)\dots(2)}I_0 \]
    Multiplying the numerator and denominator by the denominator, we have
    \[ I_{2n} = \frac{(2n)!}{(n! \cdot 2^n)^2} \cdot \frac{\pi}{2} = 2^{-2n} \frac{2n}{n} \cdot \frac{\pi}{2} \]
    In the same way, we can deduce that
    \[ I_{2n+1} = \frac{(2n)(2n-2)\dots(2)}{(2n+1)(2n-1)\dots(3)(1)}I_1 = \frac{1}{2n+1} \left( 2^{-2n} \binom{2n}{n} \right)^{-1} \]
    From $I_n = \frac{n-1}{n}I_{n-2}$, we get that
    \[ \frac{I_n}{I_{n-2}} \to 1 \]
    as $n \to \infty$. We now want to show that $\frac{I_{2n}}{I_{2n+1}} \to 1$. We see from the definition of $I_n$ that $I$ is a decreasing function of $n$. Therefore,
    \[ \frac{I_{2n}}{I_{2n+1}} \leq \frac{I_{2n-1}}{I_{2n+1}} \to 1 \]
    and also
    \[ \frac{I_{2n}}{I_{2n+1}} \geq \frac{I_{2n}}{I_{2n-2}} \to 1 \]
    So
    \[ \frac{I_{2n}}{I_{2n+1}} \to 1 \]
    which means that
    \[ \frac{2^{-2n} \binom{2n}{n} \frac{\pi}{2}}{\left( 2^{-2n} \binom{2n}{n} \right)^{-1} \frac{1}{2n+1}} \to 1 \implies \left( 2^{-2n} \binom{2n}{n} \right)^2 \frac{\pi}{2} (2n+1) \to 1 \]
    Therefore,
    \[ \left( 2^{-2n} \binom{2n}{n} \right)^2 \sim \frac{2}{\pi (2n+1)} \sim \frac{1}{\pi n} \]
    Finally,
    \[ A = \sqrt{2 \pi} \]
    completes the proof.
\end{proof}

\subsection{Countable Subadditivity}
Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space, and let $(A_n)_{n \geq 1}$ be a (not necessarily disjoint) sequence of events in $\mathcal F$. Then
\[ \prob{\bigcup_{n=1}^\infty A_n} \leq \sum_{n=1}^\infty \prob{A_n} \]
\begin{proof}
    Let us define a new sequence $B_1 = A_1$ and $B_n = A_n \setminus (A_1 \cup A_2 \cup \dots \cup A_{n-1})$. So by construction, the sequence $B_n$ is a disjoint sequence of events in $\mathcal F$. Note further that the union of all $B_n$ is equal to the union of all $A_n$. So
    \[ \prob{\bigcup_{n=1}^\infty A_n} = \prob{\bigcup_{n=1}^\infty B_n} \]
    By the countable additivity axiom,
    \[ \prob{\bigcup_{n=1}^\infty B_n} = \sum_{n=1}^\infty \prob{B_n} \]
    But $B_n \subseteq A_n$. So $\prob{B_n} \leq A_n$. Therefore,
    \[ \prob{\bigcup_{n=1}^\infty A_n} \leq \sum_{n=1}^\infty \prob{A_n} \]
\end{proof}

\section{Inclusion-Exclusion}
\subsection{Continuity of Probability Measures}
Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $(A_n)_{n \geq 1}$ be an increasing sequence in $\mathcal F$, i.e. $A_n \in \mathcal F$, and $A_n \subseteq A_{n+1}$. Then $\prob{A_n} \leq \prob{A_{n+1}}$. We want to show that
\[ \lim_{n \to \infty}\prob{A_n} = \prob{\bigcup_n A_n} \]
\begin{proof}
    Let $B_1 = A_1$, and for all $n \geq 2$, let $B_n = A_n \setminus (A_1 \cup A_2 \cup \dots \cup A_{n-1})$. Then the union over $B_i$ up to $n$ is equal to the union over $A_i$ up to $n$. So
    \[ \prob{A_n} = \prob{\bigcup_{k=1}^n B_k} = \sum_{k=1}^n \prob{B_k} \to \sum_{k=1}^\infty \prob{B_k} = \prob{\bigcup_n B_n} = \prob{\bigcup_n A_n} \]
\end{proof}
\noindent We can say that probability measures are continuous; an increasing sequence of events has a probability which tends to some limit. Similarly, if $(A_n)$ is decreasing, then the limit probability is the probability of the intersection of all $A_n$.

\subsection{Inclusion-Exclusion Formula}
Suppose that $A, B \in \mathcal F$. Then $\prob{A \cup B} = \prob{A} + \prob{B} - \prob{A \cap B}$. Now let also $C \in \mathcal F$. Then
\begin{align*}
    \prob{A \cup B \cup C} & = \prob{A \cup B} + \prob{C} - \prob{(A \cup B) \cap C} \\
                           & = \prob{A} + \prob{B} + \prob{C}                        \\
                           & - \prob{A \cap B} - \prob{A \cap C} - \prob{B \cap C}   \\
                           & + \prob{A \cap B \cap C}
\end{align*}
Let $A_1, \dots, A_n$ be events in $\mathcal F$. Then
\begin{align*}
    \prob{\bigcup_{i=1}^n A_i} & = \sum_{i=1}^n \prob{A_i}                                                      \\
                               & - \sum_{1 \leq i_1 < i_2 \leq n}\prob{A_{i_1} \cap A_{i_2}}                    \\
                               & + \sum_{1 \leq i_1 < i_2 < i_3 \leq n}\prob{A_{i_1} \cap A_{i_2} \cap A_{i_3}} \\
                               & - \cdots                                                                       \\
                               & + (-1)^{n+1}\prob{A_1 \cap \dots \cap A_n}
\end{align*}
Or more concisely,
\[ \prob{\bigcup_{i=1}^n A_i} = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < i_2 < \dots < i_k \leq n} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \]

\begin{proof}
    The case for $n=2$ has been verified, so we can use induction on $n$. Now, let us assume this holds for $n-1$ events.
    \begin{align*}
        \prob{\bigcup_{i=1}^n A_i} & = \prob{\left( \bigcup_{i=1}^{n-1} A_i  \right) \cup A_n}                                                     \\
                                   & = \prob{\bigcup_{i=1}^{n-1} A_i} + \prob{A_n} - \prob{\left( \bigcup_{i=1}^{n-1} A_i  \right) \cap A_n}       \\
                                   & = \prob{\bigcup_{i=1}^{n-1} A_i} + \prob{A_n} - \prob{\bigcup_{i=1}^{n-1} (A_i \cap A_n)}                     \\
        \intertext{Let $B_i = A_i \cap A_n$ for all $i$. By the inductive hypothesis, we have}
        \prob{\bigcup_{i=1}^n A_i} & = \prob{\bigcup_{i=1}^{n-1} A_i} + \prob{A_n} - \prob{\bigcup_{i=1}^{n-1} B_n}                                \\
                                   & = \sum_{k=1}^{n-1} (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n-1} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \\&- \sum_{k=1}^{n-1} (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n-1} \prob{B_{i_1} \cap \dots \cap B_{i_k}} \\ &+ \prob{A_n}
    \end{align*}
    which gives the clain as required.
\end{proof}

Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space with $\abs{\Omega} < \infty$ and $\prob{A} = \frac{\abs{A}}{\abs{\Omega}}$. Let $A_1, \dots, A_n \in \mathcal F$. Then
\[ \abs{A_1 \cup \dots \cup A_n} = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} \abs{A_{i_1} \cap \dots \cap A_{i_k}} \]

\subsection{Bonferroni Inequalities}
Truncating the sum in the inclusion-exclusion formula at the $r$th term yields an estimate for the probability. The Bonferroni Inequalities state that if $r$ is odd, it is an overestimate, and if $r$ is even, it is an underestimate.
\begin{align*}
    r \text{ odd}  & \implies \prob{\bigcup_{i=1}^n A_i} \leq \sum_{k=1}^r (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \\
    r \text{ even} & \implies \prob{\bigcup_{i=1}^n A_i} \geq \sum_{k=1}^r (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} \prob{A_{i_1} \cap \dots \cap A_{i_k}}
\end{align*}
\begin{proof}
    Again, we will use induction. The $n=2$ case is trivial. Suppose that $r$ is odd. Then
    \begin{equation}
        \prob{\bigcup_{i=1}^n A_i} = \prob{\bigcup_{i=1}^{n-1} A_i} + \prob{A_n} - \prob{\bigcup_{i=1}^n B_i} \tag{$\ast$}
    \end{equation}
    where $B_i = A_i \cap A_n$. Since $r$ is odd,
    \[ \prob{\bigcup_{i=1}^{n-1} A_i} \leq \sum_{k=1}^r (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n-1} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \]
    Since $r-1$ is even, we can apply the inductive hypothesis to $\prob{\bigcup_{i=1}^{n-1} B_i}$.
    \[ \prob{\bigcup_{i=1}^{n-1} B_i} \geq \sum_{k=1}^{r-1} (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n-1} \prob{B_{i_1} \cap \dots \cap B_{i_k}} \]
    We can substitute both bounds into ($\ast$) to get an overestimate.
\end{proof}

\subsection{Counting using Inclusion-Exclusion}
We can apply the Inclusion-Exclusion formula to count various things. How many functions $f \colon \{ 1, \dots, n \} \to \{ 1, \dots, m \}$ are surjective? Let $\Omega$ be the set of such functions, and $A = \{ f \in \Omega : f \text{ is a surjection} \}$. For all $i \in \{ 1, \dots, m \}$, we define $A_i = \{ f \in \Omega : i \notin \{ f(1), f(2), \dots, f(n) \} \}$. Then $A = \stcomp{A_1} \cap \stcomp{A_2} \cap \dots \cap \stcomp{A_m} = \stcomp{(A_1 \cup A_2 \cup \dots \cup A_m)}$. Then
\[ \abs{A} = \abs{\Omega} - \abs{A_1 \cup \dots \cup A_m} = m^n - \abs{A_1 \cup \dots \cup A_m} \]
Now, let us use the Inclusion-Exclusion formula.
\begin{align*}
    \abs{A_1 \cup \dots \cup A_m} & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} \abs{A_{i_1} \cap \dots \cap A_{i_k}} \\
    \intertext{Note that $A_{i_1} \cap \dots \cap A_{i_k}$ is the set of functions where $k$ distinct numbers are not included in the function's range. There are $(m-k)^n$ such functions.}
    \abs{A_1 \cup \dots \cup A_m} & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} (m-k)^n                               \\
                                  & = \sum_{k=1}^n (-1)^{k+1} \binom{m}{k} (m-k)^n                                                         \\
    \abs{A}                       & = m^n -  \sum_{k=1}^n (-1)^{k+1} \binom{m}{k} (m-k)^n                                                  \\
    \abs{A}                       & = \sum_{k=0}^n (-1)^k \binom{m}{k} (m-k)^n
\end{align*}

\section{Independence and Dependence of Events}
\subsection{Counting Derangements}
A derangement is a permutation which has no fixed point, i.e. $\forall i, \sigma(i) \neq i$. We will let $\Omega$ be the set of permutations of $\{1, \dots, n\}$, i.e. $S_n$. Let $A$ be the set of derangements in $\Omega$. Let us pick a permutation $\sigma$ at random from $\Omega$. What is the probability that it is a derangement? We define $A_i = \{ f \in \Omega \colon f(i) = i \}$, then $A = \stcomp{A_1} \cap \dots \stcomp{A_n} = \stcomp{\left( \bigcup_{i=1}^n A_i  \right)}$, so $\prob{A} = 1 - \prob{\bigcup_{i=1}^n A_i}$. By the inclusion-exclusion formula,
\begin{align*}
    \prob{\bigcup_{i=1}^n A_i} & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < i_2 < \dots < i_k \leq n} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \\
                               & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < i_2 < \dots < i_k \leq n} \frac{(n-k)!}{\abs{\Omega}}            \\
                               & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < i_2 < \dots < i_k \leq n} \frac{(n-k)!}{n!}                      \\
                               & = \sum_{k=1}^n (-1)^{k+1} \binom{n}{k} \frac{(n-k)!}{n!}                                                      \\
                               & = \sum_{k=1}^n (-1)^{k+1} \frac{n!}{k!(n-k)!} \cdot \frac{(n-k)!}{n!}                                         \\
                               & = \sum_{k=1}^n (-1)^{k+1} \frac{1}{k!}                                                                        \\
\end{align*}
So
\[ \prob{A} = 1 - \prob{\bigcup_{i=1}^n A_i} = 1 - \sum_{k=1}^n \frac{(-1)^{k+1}}{k!} = \sum_{k=0}^n \frac{(-1)^k}{k!} \]
As $n \to \infty$, this value tends to $e^{-1} \approx 0.3678$.

\subsection{Independence of Events}
\begin{definition}
    Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $A, B \in \mathcal F$. $A$ and $B$ are called independent if
    \[ \prob{A \cap B} = \prob{A} \cdot \prob{B} \]
    We write $A \perp B$, or $A \perp\!\!\!\perp B$. A countable collection of events $(A_n)$ is said to be independent if for all distinct $i_1, \dots, i_k$, we have
    \[ \prob{A_{i_1} \cap \dots \cap A_{i_k}} = \prod_{j=1}^k \prob{A_{i_j}} \]
\end{definition}
\begin{remark}
    To show that a collection of events is independent, it is insufficient to show that events are pairwise independent. For example, consider tossing a fair coin twice, so $\Omega = \{ (0, 0), (0, 1), (1, 0), (1, 1) \}$. $\prob{\{\omega\}} = \frac{1}{4}$. Consider the events $A, B, C$ where
    \[ A = \{ (0, 0), (0, 1) \};\quad B = \{ (0, 0), (1, 0) \};\quad C = \{ (1, 0), (0, 1) \} \]
    \[ \prob{A} = \prob{B} = \prob{C} = \frac{1}{2} \]
    \begin{align*}
        \prob{A \cap B} = \prob{\{ (0, 0) \}} & = \frac{1}{4} = \prob{A} \cdot \prob{B} \\
        \prob{A \cap C} = \prob{\{ (0, 1) \}} & = \frac{1}{4} = \prob{A} \cdot \prob{C} \\
        \prob{B \cap C} = \prob{\{ (1, 0) \}} & = \frac{1}{4} = \prob{B} \cdot \prob{C}
    \end{align*}
    \[ \prob{A \cap B \cap C} = \prob{\varnothing} = 0 \]
\end{remark}
\begin{claim}
    If $A \perp B$, then $A \perp \stcomp{B}$.
\end{claim}
\begin{proof}
    \begin{align*}
        \prob{A \cap \stcomp{B}} & = \prob{A} - \prob{A \cap B}           \\
                                 & = \prob{A} - \prob{A} \cdot \prob{B}   \\
                                 & = \prob{A} \left( 1 - \prob{B} \right) \\
                                 & = \prob{A} \prob{\stcomp{B}}
    \end{align*}
    as required.
\end{proof}

\subsection{Conditional Probability}
\begin{definition}
    Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $B \in \mathcal F$ with $\prob{B} > 0$. We define the conditional probability of $A$ given $B$, written $\prob{A \mid B}$ as
    \[ \prob{A \mid B} = \frac{\prob{A \cap B}}{\prob{B}} \]
\end{definition}
\noindent Note that if $A$ and $B$ are independent, then
\[ \prob{A \mid B} = \frac{\prob{A \cap B}}{\prob{B}} = \frac{\prob{A} \cdot \prob{B}}{\prob{B}} = \prob{A} \]
\begin{claim}
    Suppose that $(A_n)$ is a disjoint sequence in $\mathcal F$. Then
    \[ \prob{\bigcup A_n \mathrel{\Big|} B} = \sum_{n} \prob{A_n \mid B} \]
    This is the countable additivity property for conditional probability.
\end{claim}
\begin{proof}
    \begin{align*}
        \prob{\bigcup A_n \mathrel{\Big|} B} & = \frac{\prob{(\bigcup A_n) \cap B}}{\prob{B}} \\
                                             & = \frac{\prob{\bigcup (A_n \cap B)}}{\prob{B}} \\
        \intertext{By countable additivity, since that $(A_n \cap B)$ are disjoint,}
                                             & = \sum_n \frac{\prob{A_n \cap B}}{\prob{B}}    \\
                                             & = \sum_n \prob{A_n \mid B}
    \end{align*}
\end{proof}
\noindent We can think of $\prob{\dots \mid B}$ as a new probability measure for the same $\Omega$.

\subsection{Law of Total Probability}
\begin{claim}
    Suppose $(B_n)$ is a disjoint collection of events in $\mathcal F$, such that $\bigcup B = \Omega$, and for all $n$, we have $\prob{B_n} > 0$. If $A \in \mathcal F$ then
    \[ \prob{A} = \sum_n \prob{A \mid B_n} \cdot \prob{B_n} \]
\end{claim}
\begin{proof}
    \begin{align*}
        \prob{A} & = \prob{A \cap \Omega}                   \\
                 & = \prob{A \cap \left(\bigcup B_n\right)} \\
                 & = \prob{\bigcup(A \cap B_n)}             \\
        \intertext{By countable additivity,}
                 & = \sum_n \prob{A \cap B_n}               \\
                 & = \sum_n \prob{A \mid B_n} \prob{B_n}
    \end{align*}
\end{proof}

\subsection{Bayes' Formula}
\begin{claim}
    Suppose $(B_n)$ is a disjoint sequence of events with $\bigcup B_n = \Omega$ and $\prob{B_n} > 0$ for all $n$. Then
    \[ \prob{B_n \mid A} = \frac{\prob{A \mid B_n} \prob{B_n}}{\sum_k \prob{A \mid B_k} \prob{B_k}} \]
\end{claim}
\begin{proof}
    \begin{align*}
        \prob{B_n \mid A} & = \frac{\prob{B_n \cap A}}{\prob{A}}                                       \\
                          & = \frac{\prob{A \mid B_n} \prob{B_n}}{\prob{A}}                            \\
        \intertext{By the Law of Total Probability,}
                          & = \frac{\prob{A \mid B_n} \prob{B_n}}{\sum_k \prob{A \mid B_k} \prob{B_k}}
    \end{align*}
\end{proof}
\noindent Note that on the right hand side, the numerator appears somewhere in the denominator. This formula is the basis of Bayesian statistics. It allows us to reverse the direction of a conditional probability --- knowing the probabilities of the events $(B_n)$, and given a model of $\prob{A \mid B_n}$, we can calculuate the posterior probabilities of $B_n$ given that $A$ occurs.

\section{Examples of Conditional Probability}
\subsection{Bayes' Formula for Medical Tests}
Consider the probability of getting a false positive on a test for a rare condition. Suppose 0.1\% of the population have condition $A$, and we have a test which is positive for 98\% of the affected population, and 1\% of those unaffected by the disease. Picking an individual at random that that they suffer from $A$, given that they have a positive test?

We define $A$ to be the set of individuals suffering from the condition, and $P$ is the set of individuals testing positive. Then by Bayes' formula,
\[ \prob{A \mid P} = \frac{\prob{P \mid A}\prob{A}}{\prob{P \mid A}\prob{A} + \prob{P \mid \stcomp{A}}\prob{\stcomp{A}}} = \frac{0.98 \cdot 0.001}{0.98 \cdot 0.001 + 0.01 \cdot 0.999} \approx 0.09 = 9\% \]
Why is this so low? We can rewrite this instance of Bayes' formula as
\[ \prob{A \mid P} = \frac{1}{1 + \frac{\prob{P \mid \stcomp{A}}\prob{\stcomp{A}}}{\prob{P \mid A}\prob{A}}} \]
Here, $\prob{\stcomp{A}} \approx 1, \prob{P \mid A} \approx 1$. So
\[ \prob{A \mid P} \approx \frac{1}{1 + \frac{\prob{P \mid \stcomp{A}}}{\prob{A}}} \]
So this is low because the probability that $\prob{P \mid \stcomp{A}} \gg \prob{A}$. Suppose that there is a population of 1000 people and about 1 suffers from the disease. Among the 999 not suffering from $A$, about 10 will test positive. So there will be about 11 people who test positive, and only 1 out of 11 (9\%) of those actually has the disease.

\subsection{Probability Changes under Extra Knowledge}
Consider these three statements:
\begin{enumerate}[(a)]
    \item I have two children, (at least) one of whom is a boy.
    \item I have two children, and the eldest one is a boy.
    \item I have two children, one of whom is a boy born on a Thursday.
\end{enumerate}
What is the probability that I have two boys, given $a$, $b$ or $c$? Since no further information is given, we will assume that all outcomes are equally likely. We define:
\begin{itemize}
    \item $BG$ is the event that the elder sibling is a boy, and the younger is a girl;
    \item $GB$ is the event that the elder sibling is a girl, and the younger is a boy;
    \item $BB$ is the event that both children are boys; and
    \item $GG$ is the event that both children are girls.
\end{itemize}
Now, we have
\begin{enumerate}[(a)]
    \item $\prob{BB \mid BB \cup BG \cup GB} = \frac{1}{3}$
    \item $\prob{BB \mid BB \cup BG} = \frac{1}{2}$
    \item Let us define $GT$ to be the event that the elder sibling is a girl, and the younger is a boy born on a Thursday, and define $TN$ to be the event that the elder sibling is a boy born on a Thursday and the younger is a boy not born on a Thursday, and other events are defined similarly. So
          \begin{align*}
              \prob{TT \cup TN \cup NT \mid GT \cup TG \cup TT \cup TN \cup NT} & = \frac{\prob{TT \cup TN \cup NT}}{\prob{GT \cup TG \cup TT \cup TN \cup NT}}                                                                                                                                                                                \\
                                                                                & = \frac{\frac{1}{2}\frac{1}{7}\frac{1}{2}\frac{1}{7} + 2 \cdot \frac{1}{2}\frac{1}{7}\frac{1}{2}\frac{6}{7}}{2\cdot \frac{1}{2}\frac{1}{2}\frac{1}{7} + \frac{1}{2}\frac{1}{7}\frac{1}{2}\frac{1}{7} + 2 \cdot \frac{1}{2}\frac{1}{7}\frac{1}{2}\frac{6}{7}} \\
                                                                                & = \frac{13}{27} \approx 48\%
          \end{align*}
\end{enumerate}

\subsection{Simpson's Paradox}
Consider admissions by men and women from state and independent schools to a university given by the tables
\medskip\begin{center}
    \begin{tabular}{c | c c c}
        All applicants & Admitted & Rejected & \% Admitted \\ \hline
        State          & 25       & 25       & 50\%        \\
        Independent    & 28       & 22       & 56\%        \\
    \end{tabular}
\end{center}
\medskip\begin{center}
    \begin{tabular}{c | c c c}
        Men only    & Admitted & Rejected & \% Admitted \\ \hline
        State       & 15       & 22       & 41\%        \\
        Independent & 5        & 8        & 38\%        \\
    \end{tabular}
\end{center}
\medskip\begin{center}
    \begin{tabular}{c | c c c}
        Women only  & Admitted & Rejected & \% Admitted \\ \hline
        State       & 10       & 3        & 77\%        \\
        Independent & 23       & 14       & 62\%        \\
    \end{tabular}
\end{center}
\medskip\noindent This is seemingly a paradox; both women and men are more likely to be admitted if they come from a state school, but when looking at all applicants, they are more likely to be admitted if they come from an independent school. This is called Simpson's paradox; it arises when we aggregate data from disparate populations. Let $A$ be the event that an individual is admitted, $B$ be the event that an individual is a man, and $C$ be the event that an individual comes from a state school. We see that
\begin{align*}
    \prob{A \mid B \cap C}          & > \prob{A \mid B \cap \stcomp{C}}          \\
    \prob{A \mid \stcomp{B} \cap C} & > \prob{A \mid \stcomp{B} \cap \stcomp{C}} \\
    \prob{A \mid C}                 & < \prob{A \mid \stcomp{C}}
\end{align*}
First, note that
\begin{align*}
    \prob{A \mid C} & = \prob{A \cap B \mid C} + \prob{A \cap \stcomp{B} \mid C}                                                                           \\
                    & = \frac{\prob{A \cap B \cap C}}{\prob{C}} + \frac{\prob{A \cap \stcomp{B} \cap C}}{\prob{C}}                                         \\
                    & = \frac{\prob{A \mid B \cap C} \prob{B \cap C}}{\prob{C}} + \frac{\prob{A \mid \stcomp{B} \cap C}\prob{\stcomp{B} \cap C}}{\prob{C}} \\
                    & = \prob{A \mid B \cap C} \prob{B \mid C} + \prob{A \mid \stcomp{B} \cap C} \prob{\stcomp{B} \mid C}                                  \\
                    & > \prob{A \mid B \cap \stcomp{C}} \prob{B \mid C} + \prob{A \mid \stcomp{B} \cap \stcomp{C}} \prob{\stcomp{B} \mid C}
\end{align*}
Let us also assume that $\prob{B \mid C} = \prob{B \mid \stcomp{C}}$. Then
\begin{align*}
    \prob{A \mid C} & > \prob{A \mid B \cap \stcomp{C}} \prob{B \mid \stcomp{C}} + \prob{A \mid \stcomp{B} \cap \stcomp{C}} \prob{\stcomp{B} \mid \stcomp{C}} \\
                    & = \prob{A \mid \stcomp{C}}
\end{align*}
So we needed to further assume that $\prob{B \mid C} = \prob{B \mid \stcomp{C}}$ in order for the `intuitive' result to hold. The assumption was not valid in the example, so the result did not hold.

\section{Discrete Distributions and Random Variables}
\subsection{Discrete Distributions}
In a discrete probability distribution on a probability space $(\Omega, \mathcal F, \mathbb P)$, $\Omega$ is either finite or countable, i.e. $\Omega = \{ \omega_1, \omega_2, \dots \}$, and as stated before, $\mathcal F$ is the power set of $\Omega$. If we know $\prob{\{\omega_i\}}$, then this completely determines $\mathbb P$. Indeed, let $A \subseteq \Omega$, then
\[ \prob{A} = \prob{\bigcup_{i \colon \omega_i \in A} \{ \omega_i \}} = \sum_{i \colon \omega_i \in A}\prob{\{\omega_i\}} \]
by countable additivity. We will see later that this is not true if $\Omega$ is uncountable. We write $p_i = \prob{\{\omega_i\}}$, and we then call this a discrete probability distribution. It has the following key properties:
\begin{itemize}
    \item $p_i \geq 0$
    \item $\sum_i p_i = 1$
\end{itemize}

\subsection{Bernoulli Distribution}
We model the outcome of a test with two outcomes (e.g. the toss of a coin) with the Bernoulli distribution. Let $\Omega = \{ 0, 1 \}$. We will denote $p = p_1$, then clearly $p_0 = 1 - p$.

\subsection{Binomial Distribution}
The binomial distribution $B$ has parameters $N \in \mathbb Z^+, p \in [0, 1]$. This distribution models a sequence of $N$ independent Bernoulli distributions of parameter $p$. We then count the amount of `successes', i.e. trials in which the result was 1. $\Omega = \{ 0, 1, \dots, N \}$.
\[ \prob{\{ k \}} = p_k = \binom{N}{k}p^k(1-p)^{N-k} \]

\subsection{Multinomial Distribution}
The multinomial distribution is a generalisation of the binomial distribution. $M$ has parameters $N \in \mathbb Z^+$ and $p_1, p_2, \dots \in [0, 1]$ where $\sum_{i=1}^k p_i = 1$. This models a sequence of $N$ independent trials in which a number from 1 to $N$ is selected, where the probability of selecting $i$ is $p_i$. $\Omega = \{ (n_1, \dots, n_k) \in \mathbb N^k \colon \sum_{i=1}^k n_i = N \}$, in other words, ordered partitions of $N$. Therefore
\[ \prob{n_1 \text{ outcomes had value 1}, \dots, n_k \text{ outcomes had value }k} = \prob{(n_1, \dots, n_k)} = \binom{N}{n_1,\dots,n_k}p_1^{n_1}\dots p_k^{n_k} \]

\subsection{Geometric Distribution}
Consider a Bernoulli distribution of parameter $p$. The geometric distribution models running this trial many times independently until the first `success' (i.e. the first result of value 1). Then $\Omega = \{ 1, 2, \dots \} = \mathbb Z^+$. Then
\[ p_k = (1-p)^{k-1}p \]
We can compute the infinite geometric series $\sum p_k$ which gives 1. We could alternatively model the distribution using $\Omega' = \{ 0, 1, \dots \} = \mathbb N$ which records the amount of failures before the first success. Then
\[ p_k' = (1-p)^k p \]
Again, the sum converges to 1.

\subsection{Poisson Distribution}
This is used to model the number of occurences of an event in a given interval of time. $\Omega = \{ 0, 1, 2, \dots \} = \mathbb N$. This distribution has one parameter $\lambda \in \mathbb R$. We have
\[ p_k = e^{-\lambda} \frac{\lambda^k}{k!} \]
Then
\[ \sum_{k=0}^\infty p_k = e^{-\lambda}  \sum_{k=0}^\infty \frac{\lambda^k}{k!} = e^{-\lambda} \cdot e^{\lambda} = 1 \]
Suppose customers arrive into a shop during the time interval $[0, 1]$. We will subdivide $[0, 1]$ into $N$ intervals $\left[ \frac{i-1}{N}, \frac{i}{N} \right]$. In each interval, a single customer arrives with probability $p$, independent of other time intervals. In this example,
\[ \prob{k \text{ customers arrive}} = \binom{N}{k} p^k (1-p)^{N-k} \]
Let $p = \frac{\lambda}{N}$ for $\lambda > 0$. We will show that as $N \to \infty$, this binomial distribution converges to the Poisson distribution.
\begin{align*}
    \binom{N}{k} p^k (1-p)^{N-k} & = \frac{N!}{k!(N-k)!} \left( \frac{\lambda}{n} \right)^k \cdot \left( 1 - \frac{\lambda}{n} \right)^{N-k} \\
                                 & = \frac{\lambda_k}{k!} \cdot \frac{N!}{N^k(N-k)!} \cdot \left( 1 - \frac{\lambda}{N} \right)^{N-k}        \\
                                 & \to \frac{\lambda_k}{k!} \cdot 1 \cdot e^{-\lambda}
\end{align*}
which matches the Poisson distribution.

\subsection{Random Variables}
\begin{definition}
    Consider the probability space $(\Omega, \mathcal F, \mathbb P)$. A random variable $X$ is a function $X \colon \Omega \to \mathbb R$ satisfying
    \[ \{ \omega \in \Omega \colon X(\omega) \leq x \} \in \mathcal F \]
    for any given $x$.
\end{definition}
\noindent Suppose $A \subseteq \mathbb R$. Then typically we write
\[ \{ X \in A \} = \{ \omega \colon X(\omega) \in A \} \]
as shorthand. Given $A \in \mathcal F$, we define the indicator of $A$ to be
\[ 1_A(\omega) = 1(\omega \in A) = \begin{cases}
        1 & \text{if } \omega \in A \\
        0 & \text{otherwise}
    \end{cases} \]
Because $A \in \mathbb F$, $1_A$ is a random variable. Suppose $X$ is a random variable. We define probability distribution function of $X$ to be
\[ F_X \colon \mathbb R \to [0, 1];\quad F_X(x) = \prob{X \leq x} \]
\begin{definition}
    $(X_1, \dots, X_n)$ is called a random variable in $\mathbb R^n$ if $(X_1, \dots, X_n) \colon \Omega \to \mathbb R^n$, and for all $x_1, \dots, x_n \in \mathbb R$ we have
    \[ \{ X_1 \leq x_1, \dots, X_n \leq x_n \} = \{ \omega \colon X_1(\omega) \leq x_1, \dots, X_n(\omega) \leq x_n \} \in \mathcal F \]
\end{definition}
\noindent This definition is equivalent to saying that $X_1, \dots, X_n$ are all random variables in $\mathbb R$. Indeed,
\[ \{ X_1 \leq x_1, \dots, X_n \leq x_n \} = \{ X_1 \leq x_1 \} \cap \dots \cap \{ X_n \leq x_n \} \]
which, since $\mathcal F$ is a $\sigma$-algebra, is an element of $\mathcal F$.

\section{Discrete Random Variables}
\subsection{Definition and Example}
\begin{definition}
    A random variable $X$ is called discrete if it takes values in a countable set. Suppose $X$ takes values in the countable set $S$. For every $x \in S$, we write
    \[ p_x = \prob{X = x} = \prob{\{ \omega \colon X(\omega) = x \}} \]
    We call $(p_x)_{x \in S}$ the probability mass function of $X$, or the distribution of $X$. If $(p_x)$ is Bernoulli for example, then we say that $X$ is a Bernoulli (or such) random variable, or that $X$ has the Bernoulli distribution.
\end{definition}
\begin{definition}
    Suppose $X_1, \dots, X_n$ are discrete random variables taking variables in $S_1, \dots, S_n$. We say that the random variables $X_1, \dots, X_n$ are independent if
    \[ \prob{X_1 = x_1, \dots, X_n = x_n} = \prob{X_1 = x_1} \cdots \prob{X_n = x_n}\quad \forall x_1 \in S_1, \dots, x_n \in S_n \]
\end{definition}
\noindent As an example, suppose we toss a $p$-biased coin $n$ times independently. Let $\Omega = \{ 0, 1 \}^n$. For every $\omega \in \Omega$,
\[ p_\omega = \prod_{k=1}^n p^{\omega_k} (1-p)^{1-\omega_k};\quad \text{where we write } \omega = (\omega_1, \dots, \omega_n) \]
We define a set of discrete random variables $X_k(\omega) = \omega_k$. Then $X_k$ gives the output of the $k$th toss. We have
\[ \prob{X_k = 1} = \prob{\omega_k = 1} = p;\quad \prob{X_k = 0} = \prob{\omega_k = 0} = 1-p \]
So $X_k$ has the Bernoulli distribution with parameter $p$. We can also show that the $X_i$ are independent. Let $x_1, \dots, x_n \in \{ 0, 1 \}$. Then
\begin{align*}
    \prob{X_1 = x_1, \dots, X_n = x_n} & = \prob{\omega = (x_1, \dots, x_n)}   \\
                                       & = p_{(x_1, \dots, x_n)}               \\
                                       & = \prod_{k=1}^N p^{x_k} (1-p)^{1-x_k} \\
                                       & = \prod_{k=1}^N \prob{X_k = x_k}
\end{align*}
as required. Now, we define $S_n(\omega) = X_1(\omega) + \dots + X_n(\omega)$. This is the number of heads in $N$ tosses. So $S_n \colon \Omega \to \{ 0, \dots, N \}$, and
\[ \prob{S_n = k} = \binom{n}{k} p^k (1-p)^{n-k} \]
So $S_n$ has the binomial distribution with parameters $n$ and $p$.

\subsection{Expectation}
Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space such that $\Omega$ is countable. Let $X \colon \Omega \to \mathbb R$ be a random variable, which is necessarily discrete. We say that $X$ is non-negative if $X \geq 0$. We define the expectation of $X$ to be
\[ \expect{X} = \sum_\omega X(\omega) \cdot \prob{\{ \omega \}} \]
We will write
\[ \Omega_X = \{ X(\omega) \colon \omega \in \Omega \} \]
So
\[ \Omega = \bigcup_{x \in \Omega_X} \{ X = x \} \]
So we have partitioned $\Omega$ using $X$. Note that
\[ \expect{X} = \sum_\omega X(\omega) \prob{\{\omega\}} = \sum_{x \in \Omega_X} \sum_{\omega \in \{ X = x\}} X(\omega) \prob{\{\omega\}} = \sum_{x \in \Omega_X} \sum_{\omega \in \{ X = x\}} x \prob{\{\omega\}} = \sum_{x \in \Omega_X} x\prob{\{X = x \}} \]
which matches the more familiar definition of the expectation; the average of the values taken by $X$, weighted by the probability of the event occcuring. So
\[ \expect{X} = \sum_{x \in \Omega_X} x p_x \]

\subsection{Expectation of Binomial Distribution}
Let $X \sim \text{Bin}(N, p)$. Then
\[ \forall k = 0, \dots, N,\quad \prob{X = k} = \binom{N}{k} p^k (1-p)^{N-k} \]
So using the second definition,
\begin{align*}
    \expect{X} & = \sum_{k=0}^N k \prob{X = k}                                                         \\
               & = \sum_{k=0}^N k \binom{n}{k} p^k (1-p)^{N-k}                                         \\
               & = \sum_{k=0}^N \frac{k \cdot N!}{k! \cdot (N-k)!} p^k (1-p)^{N-k}                     \\
               & = \sum_{k=1}^N \frac{(N-1)! \cdot N \cdot p}{(k-1)! \cdot (N-k)!} p^{k-1} (1-p)^{N-k} \\
               & = Np \sum_{k=1}^N \binom{N-1}{k-1} p^{k-1} (1-p)^{N-k}                                \\
               & = Np \sum_{k=0}^{N-1} \binom{N-1}{k} p^{k} (1-p)^{N-1-k}                              \\
               & = Np (p + 1 - p)^{N-1}                                                                \\
               & = Np
\end{align*}

\subsection{Expectation of Poisson Distribution}
Let $X \sim \text{Poi}(\lambda)$, so
\[ \prob{X = k} = e^{-\lambda} \frac{\lambda^k}{k!} \]
Hence
\begin{align*}
    \expect{X} & = \sum_{k=0}^\infty k e^{-\lambda} \frac{\lambda^k}{k!}              \\
               & =\sum_{k=1}^\infty e^{-\lambda} \frac{\lambda^{k-1} \lambda}{(k-1)!} \\
               & = e^{-\lambda} \cdot e^{\lambda} \cdot \lambda                       \\
               & = \lambda
\end{align*}

\subsection{Expectation of a General Random Variable}
Let $X$ be a general (not necessarily non-negative) discrete random variable. Then we define
\[ X^+ = \max(X, 0);\quad X^- = \max(-X, 0) \]
Then $X = X^+ - X^-$. Note that $X^+$ and $X^-$ are non-negative random variables, which has a well-defined expectation. So if at least one of $\expect{X^+}, \expect{X^-}$ is finite, we define
\[ \expect{X} = \expect{X^+} - \expect{X^-} \]
If both are infinite, then we say that the expectation of $X$ is not defined. Whenever we write $\expect{X}$, it is assumed to be well-defined. If $\expect{\abs{X}} < \infty$, we say that $X$ is integrable. When $\expect{X}$ is well-defined, we have again that
\[ \expect{X} = \sum_{x \in \Omega_x} x \cdot \prob{X = x} \]

\subsection{Properties of the Expectation}
The following properties follow immediately from the definition.
\begin{enumerate}
    \item If $X \geq 0$, then $\expect{X} \geq 0$.
    \item If $X \geq 0$ and $\expect{X} = 0$, then $\prob{X = 0} = 1$.
    \item If $c \in \mathbb R$, then $\expect{cX} = c\expect{X}$, and $\expect{c + X} = c + \expect{X}$.
    \item If $X$, $Y$ are two integrable random variables, then $\expect{X + Y} = \expect{X} + \expect{Y}$.
    \item More generally, let $c_1, \dots, c_n \in \mathbb R$ and $X_1, \dots, X_n$ integrable random variables. Then
          \[ \expect{c_1X_1 + \dots + c_nX_n} = c_1 \expect{X_1} + \dots + c_n \expect{X_n} \]
          So the expectation is a linear operator over finitely many inputs.
\end{enumerate}

\section{Expectation and Variance}
\subsection{Countable Additivity for the Expectation}
Suppose $X_1, X_2, \dots$ are non-negative random variables. Then
\[ \expect{\sum_n X_n} = \sum_n \expect{X_n} \]
The non-negativity constraint allows us to guarantee that the sums are well-defined; they could be infinite, but at least their values are well-defined. We will construct a proof assuming that $\Omega$ is countable, however the result holds regardless of the choice of $\Omega$.
\begin{proof}
    \begin{align*}
        \expect{\sum_n X_n} & = \sum_\omega \sum_n X_n(\omega) \prob{\{ \omega \}} \\
                            & = \sum_n \sum_\omega X_n(\omega) \prob{\{ \omega \}} \\
                            & = \sum_n \expect{X_n}
    \end{align*}
\end{proof}
\noindent We are allowed to rearrange the sums since all relevant terms are non-negative.

\subsection{Expectation of Indicator Function}
If $X = 1(A)$ where $A \in \mathcal F$, then $\expect{X} = \prob{A}$. This is obvious from the second definition of the expectation.

\subsection{Expectation under Function Application}
If $g \colon \mathbb R \to \mathbb R$, we can define $g(X)$ to be the random variable given by
\[ g(X)(\omega) = g(X(\omega)) \]
Then
\[ \expect{g(X)} = \sum_{x \in \Omega_X} g(x) \cdot \prob{X = x} \]
\begin{proof}
    Let $Y = g(X)$. Then
    \[ \expect{Y} = \sum_{y \in \Omega_Y} y \cdot \prob{Y = y} \]
    Note that
    \begin{align*}
        \{ Y = y \} & = \{ \omega \colon Y(\omega) = y \}              \\
                    & = \{ \omega \colon g(X(\omega)) = y \}           \\
                    & = \{ \omega \colon X(\omega) \in g^{-1}(\{y\})\} \\
                    & = \{ X \in g^{-1} (\{ y \}) \}
    \end{align*}
    where $g^{-1}(\{ y \})$ is the set of all $x$ such that $g(x) \in \{ y \}$. So
    \begin{align*}
        \expect{Y} & = \sum_{y \in \Omega_y} y \cdot \prob{X \in g^{-1}(\{ y \})}              \\
                   & = \sum_{y \in \Omega_Y} y \cdot \sum_{x \in g^{-1}(\{ y \})} \prob{X = x} \\
                   & = \sum_{y \in \Omega_Y} \sum_{x \in g^{-1}(\{y\})} g(x) \prob{X = x}      \\
                   & = \sum_{x \in \Omega_X} g(x) \prob{X = x}
    \end{align*}
\end{proof}

\subsection{Calculating Expectation with Cumulative Probabilities}
If $X \geq 0$ and takes integer values, then
\[ \expect{X} = \sum_{k=1}^\infty \prob{X \geq k} = \sum_{k=0}^\infty \prob{X > k} \]
\begin{proof}
    Since $X$ takes non-negative integer values,
    \[ X = \sum_{k=1}^\infty 1(X \geq k) = \sum_{k=0}^\infty 1(X > k) \]
    This represents the fact that any integer is the sum of that many ones, e.g. $4 = 1 + 1 + 1 + 1 + 0 + 0 + \dots$ to infinity. Taking the expectation of the above formula, using that $\expect{1(A)} = \prob{A}$ and countable additivity, we have the result as claimed.
\end{proof}

\subsection{Inclusion-Exclusion Formula with Indicators}
We can provide another proof of the inclusion-exclusion formula, using some basic properties of indicator functions.
\begin{itemize}
    \item $1(\stcomp{A}) = 1 - 1(A)$
    \item $1(A \cap B) = 1(A) \cdot 1(B)$
    \item Following from the above, $1(A \cup B) = 1-(1-1(A))(1-1(B))$.
\end{itemize}
More generally,
\[ 1(A_1 \cup \dots \cup A_n) = 1-\prod_{i=1}^n(1-1(A_i)) \]
which gives the inclusion-exclusion formula. Taking the expectation of both sides, we can see that
\[ \prob{A_1 \cup \dots \cup A_n} = \sum_{i=1}^n \prob{A_i} - \sum_{i_1 < i_2} \prob{A_{i_1} \cap A_{i_2}} + \dots + (-1)^{n+1}\prob{A_1 \cap \dots \cap A_n} \]
which is the result as previously found.

\subsection{Variance}
Let $X$ be a random variable, and $r \in \mathbb N$. If it is well-defined, we call $\expect{X^r}$ the $r$th moment of $X$. We define the variance of $X$ by
\[ \Var{X} = \expect{(X - \expect{X})^2} \]
If the variance is small, $X$ is highly concentrated around $\expect{X}$. If the variance is large, $X$ has a wide distribution including values not necessarily near $\expect{X}$. We call $\sqrt{\Var{X}}$ the standard deviation of $X$, denoted with $\sigma$. The variance has the following basic properties:
\begin{itemize}
    \item $\Var{X} \geq 0$, and if $\Var{X} = 0$, $\prob{X = \expect{X}} = 1$.
    \item If $c \in \mathbb R$, then $\Var{cX} = c^2\Var{X}$, and $\Var{X + c} = \Var{X}$.
    \item $\Var{X} = \expect{X^2} - \expect{X}^2$. This follows since $\expect{(X - \expect{X})^2} = \expect{X^2 - 2X\expect{X} + \expect{X}^2} = \expect{X^2} - 2\expect{X} \expect{X} + \expect{X}^2 = \expect{X^2} - \expect{X}^2$.
    \item $\Var{X} = \min_{c \in \mathbb R} \expect{(X - c)^2}$, and this minimum is achieved at $c = \expect{X}$. Indeed, if we let $f(c) = \expect{(X - c)^2}$, then $f(c) = \expect{X^2} - 2c\expect{X} + c^2$. Minimising $f$, we get $f(\expect{X}) = \Var{X}$ as required.
\end{itemize}
As an example, consider $X \sim \text{Bin}(n, p)$. Then $\expect{X} = np$, as we found before. Note that we can also represent this binomial distribution as the sum of $n$ Bernoulli distributions of parameter $p$ to get the same result. The variance of $X$ is
\[ \Var{X} = \expect{X^2} - \expect{X}^2 \]
In fact, in order to compute $\expect{X^2}$ it is easier to find $\expect{X(X-1)}$.
\begin{align*}
    \expect{X(X-1)} & = \sum_{k=2}^n k \cdot (k-1) \cdot \binom{n}{k} \cdot p^k \cdot (1-p)^{n-k} \\
                    & = \sum_{k=2}^n \frac{k(k-1)n! p^k (1-p)^{n-k}}{(n-k)!k!}                    \\
                    & = \sum_{k=2}^n \frac{n! p^k (1-p)^{n-k}}{((n-2)-(k-2))!(k-2)!}              \\
                    & = n(n-1)p^2 \sum_{k=2}^n \binom{n-2}{k-2} p^{k-2} (1-p)^{n-k}               \\
                    & = n(n-1)p^2
\end{align*}
Hence,
\[ \Var{X} = \expect{X(X-1)} + \expect{X} - \expect{X}^2 = n(n-1)p^2 + np - (np)^2 = np(1-p) \]
As a second example, if $X \sim \text{Poi}(\lambda)$, we have $\expect{X} = \lambda$. Because of the factorial term, it is easier to use $X(X-1)$ than $X^2$.
\begin{align*}
    \expect{X(X-1)} & = \sum_{k=2}^\infty k(k-1) e^{-\lambda} \frac{\lambda^k}{k!}                  \\
                    & = e^{-\lambda} \sum_{k=2}^\infty \frac{\lambda_{k-2}}{(k-2)!} \cdot \lambda^2 \\
                    & = \lambda^2
\end{align*}
Hence,
\[ \Var{X} = \lambda^2 + \lambda - \lambda^2 = \lambda \]

\section{Covariance and Inequalities}
\subsection{Covariance}
\begin{definition}
    Let $X$ and $Y$ be random variables. Their covariance is defined
    \[ \Cov{X,Y} = \expect{(X - \expect{X})(Y - \expect{Y})} \]
    It is a measure of how dependent $X$ and $Y$ are.
\end{definition}
\noindent Immediately we can deduce the following properties.
\begin{itemize}
    \item $\Cov{X,Y} = \Cov{Y,X}$
    \item $\Cov{X,X} = \Var{X}$
    \item $\Cov{X,Y} = \expect{XY} - \expect{X}\cdot\expect{Y}$. Indeed, $(X - \expect{X})(Y - \expect{Y}) = XY - X\expect{Y} - Y\expect{X} + \expect{X}\expect{Y}$ and the result follows.
    \item Let $c \in \mathbb R$. Then $\Cov{cX,Y} = c\Cov{X,Y}$, and $\Cov{c + X,Y} = \Cov{X,Y}$.
    \item $\Var{X + Y} = \Var{X} + \Var{Y} + 2\Cov{X,Y}$. Indeed, we have

          $\Var{X + Y} = \expect{(X - \expect{X} + Y - \expect{Y})^2}$ which gives

          $\expect{(X - \expect{X})^2} + \expect{(Y - \expect{Y})^2} + 2\expect{(X - \expect{X})(Y - \expect{Y})}$ as required.
    \item For all $c \in \mathbb R$, $\Cov{c, X} = 0$
    \item If $X$, $Y$, $Z$ are random variables, then $\Cov{X + Y,Z} = \Cov{X,Z} + \Cov{Y,Z}$. More generally, for $c_1, \dots, c_n, d_1, \dots, d_m$ real numbers, and for $X_1, \dots, X_n, Y_1, \dots, Y_m$ random variables, we have
          \[ \Cov{\sum_{i=1}^n c_i X_i,\sum_{j=1}^m d_j Y_j} = \sum_{i=1}^n \sum_{j=1}^m c_i d_j \Cov{X_i,Y_j} \]
          In particular, if we apply this to $X_i = Y_i$, and $c_i = d_i = 1$, then we have
          \[ \Var{\sum_{i=1}^n X_i} = \sum_{i=1}^n \Var{X_i} + \sum_{i \neq j} \Cov{X_i,X_j} \]
\end{itemize}

\subsection{Expectation of Functions of a Random Variable}
Recall that $X$ and $Y$ are independent if for all $x$ and $y$,
\[ \prob{X = x, Y = y} = \prob{X = x} \cdot \prob{Y = y} \]
We would like to prove that given positive functions $f, g \colon \mathbb R \to \mathbb R_+$, if $X$ and $Y$ are independent we have
\[ \expect{f(X)g(Y)} = \expect{f(X)} \cdot \expect{g(Y)} \]
\begin{proof}
    \begin{align*}
        \expect{f(X)g(Y)} & = \sum_{(x, y)} f(x) g(y) \prob{X = x, Y = y}                 \\
                          & = \sum_{(x, y)} f(x) g(y) \prob{X = x} \prob{Y = y}           \\
                          & = \sum_{x} f(x) \prob{X = x} \cdot \sum_{y} g(y) \prob{Y = y} \\
                          & = \expect{f(X)} \cdot \expect{g(Y)}
    \end{align*}
\end{proof}
\noindent The same result holds for general functions, provided the required expectations exist.

\subsection{Covariance of Independent Variables}
Suppose $X$ and $Y$ are independent. Then
\[ \Cov{X,Y} = 0 \]
This is because
\begin{align*}
    \Cov{X,Y} & = \expect{(X - \expect{X})(Y - \expect{Y})}             \\
              & = \expect{X - \expect{X}} \cdot \expect{Y - \expect{Y}} \\
              & = 0 \cdot 0                                             \\
              & = 0
\end{align*}
\noindent In particular, we can deduce that
\[ \Var{X + Y} = \Var{X} + \Var{Y} \]
Note, however, that the covariance being equal to zero does not imply independence. For instance, let $X_1, X_2, X_3$ be independent Bernoulli random variables with parameter $\frac{1}{2}$. Let us now define $Y_1 = 2X_1 - 1$, $Y_2 = 2X_2 - 1$, and $Z_1 = X_3 Y_1$, $Z_2 = X_3 Y_2$. Now, we have
\[ \expect{Y_1} = \expect{Y_2} = \expect{Z_1} = \expect{Z_2} = 0 \]
We can find that
\[ \Cov{Z_1, Z_2} = \expect{Z_1 \cdot Z_2} = \expect{X_3^2 Y_1 Y_2} = \expect{X_3^2} \cdot 0 \cdot 0 = 0 \]
However, $Z_1$ and $Z_2$ are in fact not independent. Since $Y_1, Y_2$ are never zero,
\[ \prob{Z_1 = 0, Z_2 = 0} = \prob{X_3 = 0} = \frac{1}{2} \]
But also
\[ \prob{Z_1 = 0} = \prob{Z_2 = 0} = \prob{X_3 = 0} = \frac{1}{2} \implies \prob{Z_1 = 0} \cdot \prob{Z_2 = 0} = 0 \]
So the events are not independent.

\subsection{Markov's Inequality}
The following useful inequality, and the others derived from it, hold in the discrete and the continuous case.
\begin{theorem}
    Let $X \geq 0$ be a non-negative random variable. Then for all $a > 0$,
    \[ \prob{X \geq a} \leq \frac{\expect{X}}{a} \]
\end{theorem}
\begin{proof}
    Observe that $X \geq a \cdot 1(X \geq a)$. This can be seen to be true simply by checking both cases, $X < a$ and $X \geq a$. Taking expectations, we get
    \[ \expect{X} \geq \expect{a \cdot 1(X \geq a)} = \expect{a \cdot \prob{X \geq a}} = a \cdot \prob{X \geq a} \]
    and the result follows.
\end{proof}

\subsection{Chebyshev's Inequality}
\begin{theorem}
    Let $X$ be a random variable with finite expectation. Then for all $a > 0$,
    \[ \prob{\abs{X - \expect{X}} \geq a} \leq \frac{\Var{X}}{a^2} \]
\end{theorem}
\begin{proof}
    Note that $\prob{\abs{X - \expect{X}} \geq a} = \prob{\abs{X - \expect{X}}^2 \geq a^2}$. Then we can apply Markov's inequality to this non-negative random variable to get
    \[ \prob{\abs{X - \expect{X}}^2 \geq a^2} \leq \frac{\expect{(X - \expect{X})^2}}{a^2} = \frac{\Var{X}}{a^2} \]
\end{proof}

\subsection{Cauchy-Schwarz Inequality}
\begin{theorem}
    If $X$ and $Y$ are random variables, then
    \[ \expect{\abs{XY}} \leq \sqrt{\expect{X^2}\cdot\expect{Y^2}} \]
\end{theorem}
\begin{proof}
    It suffices to prove this statement for $X$ and $Y$ which have finite second moments, i.e. $\expect{X^2}$ and $\expect{X^2}$ are finite. Clearly if they are infinite, then the upper bound is infinite which is trivially true. We need to show that $\expect{\abs{XY}}$ is finite. Here we can apply the additional assumption that $X$ and $Y$ are non-negative, since we are taking the absolute value:
    \[ XY \leq \frac{1}{2}\left(X^2 + Y^2\right) \implies \expect{XY} \leq \frac{1}{2}\left( \expect{X^2} + \expect{Y^2}  \right) \]
    Now, we can assume $\expect{X^2} > 0$ and $\expect{Y^2} > 0$. If this were not the case, the result is trivial since if at least one of them were equal to zero, the corresponding random variable would be identically zero. Let $t \in \mathbb R$ and consider
    \[ 0 \leq (X - tY)^2 = X^2 - 2tXY + t^2Y^2 \]
    Hence
    \[ \expect{X^2} - 2t\expect{XY} + t^2\expect{Y^2} \geq 0 \]
    We can view this left hand side as a function $f(t)$. The minimum value of this function is achieved at $t_\ast = \frac{\expect{XY}}{\expect{Y^2}}$. Then
    \[ f(t_\ast) \geq 0 \implies \expect{X^2} - \frac{2\expect{XY}}{\expect{Y^2}} + \frac{\expect{XY}^2}{\expect{Y^2}} \geq 0 \]
    Hence,
    \[ \expect{XY}^2 \leq \expect{X^2}\cdot \expect{Y^2} \]
    and the result follows.
\end{proof}

\section{More Inequalities and Conditional Expectation}
\subsection{Equality in Cauchy-Schwarz}
In what cases do we get equality in the Cauchy-Schwarz inequality? Recall that the inequality states
\[ \expect{\abs{XY}} \leq \sqrt{\expect{X^2}\cdot\expect{Y^2}} \]
Recall that in the proof, we considered the random variable $(X - tY)^2$ where $X$ and $Y$ were non-negative, and had finite second moments. The expectation of this random variable was called $f(t)$, and we found that $f(t)$ was minimised when $t = \frac{\expect{XY}}{\expect{Y^2}}$. We have equality exactly when $f(t) = 0$ for this value of $t$. But $(X - tY)^2$ is a non-negative random variable, with expectation zero, so it must be zero with probability 1. So we have equality if and only if $X$ is exactly $tY$.

\subsection{Jensen's Inequality}
\begin{definition}
    A function $f\colon \mathbb R \to \mathbb R$ is called convex if $\forall x, y \in \mathbb R$ and for all $t \in [0, 1]$,
    \[ f(tx + (1-t)y) \leq tf(x) + (1-t)f(y) \]
    This can be visualised as linearly interpolating the values of the function at two points, $x$ and $y$. The linear interpolation of those points is always greater than the function applied to the linear interpolation of the input points.
\end{definition}
\begin{theorem}
    Let $X$ be a random variable, and let $f$ be a convex function. Then
    \[ \expect{f(X)} \geq f(\expect{X}) \]
\end{theorem}
\noindent We can remember the direction of this inequality by considering the variance: $\Var{X} = \expect{(X - \expect{X})^2}$ which is non-negative. Further, $\Var{X} = \expect{X^2} - \expect{X}^2$ hence $\expect{X^2} \geq \expect{X}^2$. Squaring is an example of a convex function, so Jensen's inequality holds in this case. We will first prove a basic lemma about convex functions.
\begin{lemma}
    Let $f \colon \mathbb R \to \mathbb R$ be a convex function. Then $f$ is the supremum of all the lines lying below it. More formally, $\forall m \in \mathbb R$, $\exists a, b \in \mathbb R$ such that $f(m) = am + b$ and $f(x) \geq ax + b$ for all $x$.
\end{lemma}
\begin{proof}
    Let $m \in \mathbb R$. Let $x < m < y$. Then we can express $m$ as $tx + (1-t)y$ for some $t$ in the interval $[0, 1]$. By convexity,
    \[ f(m) \leq tf(x) + (1-t)f(y) \]
    And hence,
    \begin{align*}
        tf(m) + (1-t)f(m)         & \leq tf(x) + (1-t)f(y)       \\
        t(f(m) - f(x))            & \leq (1-t)(f(y) - f(m))      \\
        \frac{f(m) - f(x)}{m - x} & \leq \frac{f(y) - f(m)}{y-m}
    \end{align*}
    So the slope of the line joining $m$ to a point on its left is smaller than the slope of the line joining $m$ to a point on its right. So we can produce a value $a \in \mathbb R$ given by
    \[ a = \sum_{x < m} \frac{f(m) - f(x)}{m - x} \]
    such that
    \[ \frac{f(m) - f(x)}{m - x} \leq a \leq \frac{f(y) - f(m)}{y - m} \]
    for all $x < m < y$. We can rearrange this to give
    \[ f(x) \geq a(x-m) + f(m) = ax + (f(m) - am) \]
    for all $x$.
\end{proof}
\noindent We may now prove Jensen's inequality.
\begin{proof}
    Set $m = \expect{X}$. Then from the lemma above, there exists $a, b \in \mathbb R$ such that
    \begin{equation}
        f(m) = am + b \implies f(\expect{X}) = a\expect{X} + b \tag{$\ast$}
    \end{equation}
    and for all $x$, we have
    \[ f(x) \geq ax + b \]
    We can now apply this inequality to $X$ to get
    \[ f(X) \geq aX + b \]
    Taking the expectation, by $(\ast)$ we get
    \[ \expect{f(X)} \geq a\expect{X} + b = f(\expect{X}) \]
    as required.
\end{proof}
\noindent Like the Cauchy-Schwarz inequality, we would like to consider the cases of equality. Let $X$ be a random variable, and $f$ be a convex function such that if $m = \expect{X}$, then $\exists a, b \in \mathbb R$ such that
\[ f(m) = am + b;\quad \forall x \neq m,\, f(x) > ax + b \]
We know that $f(X) \geq aX + b$, since $f$ is convex. Then $f(X) - (aX+b) \geq 0$ is a non-negative random variable. Taking expectations,
\[ \expect{f(X) - (aX+b)} \geq 0 \]
But $\expect{aX + b} = am + b = f(m) = f(\expect{X})$. We assumed that $\expect{f(X)} = f(\expect{X})$, hence $\expect{aX+b} = \expect{f(X)}$ and $\expect{f(X) - (aX+b)} = 0$. But since $f(X) \geq aX+b$, this forces $f(X) = aX+b$ everywhere. By our assumption, for all $x \neq m$, $f(x) > ax+b$. This forces $X=m$ with probability 1.

\subsection{Arithmetic Mean and Geometric Mean Inequality}
Let $f$ be a convex function. Suppose $x_1, \dots, x_n \in \mathbb R$. Then, from Jensen's inequality,
\[ \frac{1}{n} \sum_{k=1}^n f(x_k) \geq f\left( \frac{1}{n} \sum_{k=1}^n x_k \right) \]
Indeed, we can define a random variable $X$ to take values $x_1, \dots, x_n$ all with equal probability. Then, $\expect{f(X)}$ gives the left hand side, and $f(\expect{X})$ gives the right hand side. Now, let $f(x) = -\log x$. This is a convex function as required. Hence
\begin{align*}
    -\frac{1}{n} \sum_{k=1}^n \log x_k             & \geq -\log\left( \frac{1}{n} \sum_{k=1}^n x_k \right) \\
    \left( \prod_{k=1}^n x_k \right)^{\frac{1}{n}} & \leq \frac{1}{n} \sum_{k=1}^n x_k
\end{align*}
Hence the geometric mean is less than or equal to the arithmetic mean.

\subsection{Conditional Expectation and Law of Total Expectation}
Recall that if $B \in \mathcal F$ with $\prob{B} \geq 0$, we defined
\[ \prob{A \mid B} = \frac{\prob{A \cap B}}{\prob{B}} \]
Now, let $X$ be a random variable, and let $B$ be an event as above with non-zero probability. We can then define
\[ \expect{X \mid B} = \frac{\expect{X \cdot 1(B)}}{\prob{B}} \]
The numerator is notably zero when $1(B) = 0$, so in essence we are excluding the case where $X$ is not $B$.
\begin{theorem}[Law of Total Expectation]
    Suppose $X \geq 0$. Let $(\Omega_n)$ be a partition of $\Omega$ into disjoint events, so $\Omega = \bigcup_n \Omega_n$. Then
    \[ \expect{X} = \sum_n \expect{X \mid \Omega_n} \cdot \prob{\Omega_n} \]
\end{theorem}
\begin{proof}
    We can write $X = X \cdot 1(\Omega)$, where
    \[ 1(\Omega) = \sum_n 1(\Omega_n) \]
    Taking expectations, we get
    \[ \expect{X} = \expect{ \sum_{n} X \cdot 1(\Omega_n) } \]
    By countable additivity of expectation, we have
    \[ \expect{X} = \sum_{n} \expect{ X \cdot 1(\Omega_n) } = \sum_n \expect{X \mid \Omega_n} \cdot \prob{\Omega_n} \]
    as required.
\end{proof}

\section{Joint Distribution, Convolution and Conditional Expectation}
\subsection{Joint Distribution}
\begin{definition}
    Let $X_1, \dots, X_n$ be discrete random variables. Their joint distribution is defined as
    \[ \prob{X_1 = x_1, \dots, X_n = x_n} \]
    for all $x_i \in \Omega_i$.
\end{definition}
\noindent Now, we have
\[ \prob{X_1 = x_1} = \prob{\{ X_1 = x_1\} \cap \bigcup_{i=2}^n \bigcup_{x_i} \{ X_i = x_i \} } = \sum_{x_2, \dots, x_n} \prob{X_1 = x_1, X_2 = x_2, \dots, X_n = x_n} \]
In general,
\[ \prob{X_i = x_i} = \sum_{x_1, x_2, \dots, x_{i-1}, x_{i+1}, \dots, x_n} \prob{X_1 = x_1, X_2 = x_2, \dots, X_n = x_n} \]
We call $(\prob{X_i = x_i})_i$ the marginal distribution of $X_i$. Let $X, Y$ be random variables. The conditional distribution of $X$ given $Y = y$ where $y \in \Omega_y$ is defined to be
\[ \prob{X = x \mid Y = y} = \frac{\prob{X = x, Y = y}}{\prob{Y = y}} \]
We can find
\[ \prob{X = x} = \sum_y \prob{X = x, Y = y} = \sum_y \prob{X = x \mid Y = y} \prob{Y = y} \]
which is the law of total probability.

\subsection{Convolution}
Let $X$ and $Y$ be independent, discrete random variables. We would like to find $\prob{X + Y = z}$. Clearly this is
\begin{align*}
    \prob{X + Y = z} & = \sum_y \prob{X + Y = z, Y = y}           \\
                     & = \sum_y \prob{X = z-y, Y = y}             \\
                     & = \sum_y \prob{X = z-y} \cdot \prob{Y = y} \\
\end{align*}
This last sum is called the convolution of the distributions of $X$ and $Y$. Similarly,
\[ \prob{X + Y = z} = \sum_x \prob{X = x} \cdot \prob{Y = z-x} \]
As an example, let $X \sim \mathrm{Poi}(\lambda)$ and $Y \sim \mathrm{Poi}(\mu)$ be independent. Then
\begin{align*}
    \prob{X + Y = n} & = \sum_{r = 0}^n \prob{X = r} \prob{Y = n - r}                                              \\
                     & = \sum_{r = 0}^n e^{-\lambda} \frac{\lambda^r}{r!} \cdot e^{-\mu} \frac{\mu^{n-r}}{(n-r)!}  \\
                     & = e^{-(\lambda+\mu)} \sum_{r = 0}^n \frac{\lambda^r\mu^{n-r}}{r!(n-r)!}                     \\
                     & = \frac{e^{-(\lambda+\mu)}}{n!} \sum_{r = 0}^n \frac{\lambda^r\mu^{n-r} \cdot n!}{r!(n-r)!} \\
                     & = \frac{e^{-(\lambda+\mu)}}{n!} \sum_{r = 0}^n \binom{n}{r} \lambda^r\mu^{n-r}              \\
                     & = \frac{e^{-(\lambda+\mu)}}{n!} (\lambda + \mu)^n                                           \\
\end{align*}
which is the probability mass function of a Poisson random variable with parameter $\lambda + \mu$. In other words, $X + Y \sim \mathrm{Poi}(\lambda + \mu)$.

\subsection{Conditional Expectation}
Let $X$ and $Y$ be discrete random variables. Then the conditional expectation of $X$ given that $Y = y$ is
\[ \expect{X \mid Y = y} = \frac{\expect{X \cdot 1(Y = y)}}{\prob{Y = y}} = \frac{1}{\prob{Y = y}} \sum_x x \cdot \prob{X = x, Y = y} = \sum_x x \cdot \prob{X = x \mid Y = y} \]
Observe that for every $y \in \Omega_y$, this expectation is purely a function of $y$. Let $g(y) = \expect{X \mid Y = y}$. Now, we define the conditional expectation of $X$ given $Y$ as $\expect{X \mid Y} = g(Y)$. Note that $\expect{X \mid Y}$ is a random variable, dependent only on $Y$. We have
\begin{align*}
    \expect{X \mid Y} & = g(Y) \cdot 1                                \\
                      & = g(Y) \sum_y 1(Y = y)                        \\
                      & = \sum_y g(Y) \cdot 1(Y = y)                  \\
                      & = \sum_y g(y) \cdot 1(Y = y)                  \\
                      & = \sum_y \expect{X \mid Y = y} \cdot 1(Y = y)
\end{align*}
This is perhaps a clearer way to see that it depends only on $Y$. As an example, let us consider tossing a $p$-biased coin $n$ times independently. We write $X_i$ for the indicator function that the $i$th toss was a head. Let $Y_n = X_1 + \dots + X_n$. What is $\expect{X_1 \mid Y_n}$? Let $g(y) = \expect{X_1 \mid Y_n = y}$. Then $\expect{X_1 \mid Y_n} = g(Y_n)$. We therefore need to find $g$. Let $y \in \{ 0, \dots, n \}$, then
\begin{align*}
    g(y) & = \expect{X_1 \mid Y_n = y}   \\
         & = \prob{X_1 = 1 \mid Y_n = y} \\
\end{align*}
Clearly if $y = 0$, then $\prob{X_1 = 1 \mid Y_n = 0} = 0$. Now, suppose $y \neq 0$. We have
\begin{align*}
    \prob{X_1 = 1 \mid Y_n = y} & = \frac{\prob{X_1 = 1, Y_n = y}}{\prob{Y_n = y}}                             \\
                                & = \frac{\prob{X_1 = 1, X_2 + \dots + X_n = y-1}}{\prob{Y_n = y}}             \\
                                & = \frac{\prob{X_1 = 1} \cdot \prob{X_2 + \dots + X_n = y-1}}{\prob{Y_n = y}} \\
                                & = \frac{p \cdot \binom{n-1}{y-1} \cdot p^{y-1}(1-p)^{n-y}}{\prob{Y_n = y}}   \\
                                & = \frac{\binom{n-1}{y-1} \cdot p^y(1-p)^{n-y}}{\binom{n}{y}p^y (1-p)^{n-y}}  \\
                                & = \frac{\binom{n-1}{y-1}}{\binom{n}{y}}                                      \\
                                & = \frac{y}{n}
\end{align*}
Hence
\[ g(y) = \frac{y}{n} \]
We can then find that
\[ \expect{X_1 \mid Y_n} = g(Y_n) = \frac{Y_n}{n} \]
which is indeed a random variable dependent only on $Y_n$.

\subsection{Properties of Conditional Expectation}
The following properties hold.
\begin{itemize}
    \item For all $c \in \mathbb R$, $\expect{cX \mid Y} = c\expect{X \mid Y}$, and $\expect{c \mid Y} = c$.
    \item Let $X_1, \dots, X_n$ be random variables. Then $\expect{\sum_{i=1}^n X_i \mid Y} = \sum_{i=1}^n \expect{X_i \mid Y}$.
    \item $\expect{\expect{X \mid Y}} = \expect{X}$.
\end{itemize}
The last property is not obvious from the definition, so it warrants its own proof. We can see by the standard properties of the expectation that
\begin{align*}
    \expect{X \mid Y}                     & = \sum_y 1(Y = y) \expect{X \mid Y = y}                              \\
    \therefore \expect{\expect{X \mid Y}} & = \sum_y \expect{1(Y = y)} \expect{X \mid Y = y}                     \\
                                          & = \sum_y \prob{Y = y} \expect{X \mid Y = y}                          \\
                                          & = \sum_y \prob{Y = y} \frac{\expect{X \cdot 1(Y = y)}}{\prob{Y = y}} \\
                                          & = \sum_y \expect{X \cdot 1(Y = y)}                                   \\
                                          & = \expect{\sum_y X \cdot 1(Y = y)}                                   \\
                                          & = \expect{X \sum_y 1(Y = y)}                                         \\
                                          & = \expect{X}                                                         \\
\end{align*}
Alternatively, we could expand the inner expectation as a sum:
\[ \sum_y \expect{X \mid Y = y} \cdot \prob{Y = y} = \sum_x \sum_y x \cdot \prob{X = x \mid Y = y} \cdot \prob{Y = y} \]
and the result follows as required. The final property relates conditional probability to independence. Let $X$ and $Y$ be independent. Then $\expect{X \mid Y} = \expect{X}$. Indeed,
\begin{align*}
    \expect{X \mid Y} & = \sum_y 1(Y = y) \expect{X \mid Y = y} \\
                      & = \sum_y 1(Y = y) \expect{X}            \\
                      & = \expect{X}                            \\
\end{align*}

\section{Conditional Expectation and Random Walks}
\subsection{More Properties of Conditional Expectation}
\begin{proposition}
    Suppose $Y$ and $Z$ are independent random variables. Then
    \[ \expect{\expect{X \mid Y} \mid Z} = \expect{X} \]
\end{proposition}
\begin{proof}
    Let $\expect{X \mid Y} = g(Y)$ be a random variable that is a function only of $Y$. Since $Y$ and $Z$ are independent, $f(Y)$ is also independent of $Z$ for any function $f$. Then $\expect{g(Y) \mid Z} = \expect{g(Y)} = \expect{X}$.
\end{proof}
\begin{proposition}
    Suppose $h \colon \mathbb R \to \mathbb R$ is a function. Then
    \[ \expect{h(Y) \cdot X \mid Y} = h(Y) \cdot \expect{X \mid Y} \]
    We can `take out what is known', since we know what $Y$ is.
\end{proposition}
\begin{proof}
    Note that $\expect{h(Y) \cdot X \mid Y = y} = \expect{h(y) \cdot X \mid Y = y} = h(y) \expect{X \mid Y = y}$. Then $\expect{h(Y) \cdot X \mid Y} = h(y) \cdot \expect{X \mid Y}$ as required.
\end{proof}
\begin{corollary}
    $\expect{\expect{X \mid Y} \mid Y} = \expect{X \mid Y}$, and $\expect{X \mid X} = X$.
\end{corollary}
\noindent Let $X_i = 1(i\text{th toss is a head})$, and $Y_n = X_1 + \dots + X_n$. We found before that $\expect{X_1 \mid Y_n} = \frac{Y_n}{n}$. By symmetry, for all $i$ we have $\expect{X_i \mid Y_n} = \expect{X_1 \mid Y_n}$. Hence
\[ \expect{Y_n \mid Y_n} = \expect{\sum_{i=1}^n X_i \mid Y_n} = \sum_{i=1}^n \expect{X_i \mid Y_n} = n \cdot \expect{X_1 \mid Y_n} \]
which yields the same result.

\subsection{Random Walks}
A random process, also known as a stochastic process, is a sequence of random variables $X_n$ for $n \in \mathbb N$. A random walk is a random process that can be expressed as
\[ X_n = x + Y_1 + \dots + Y_n \]
where the $Y_i$ are independent and identically distributed, and $x$ is a deterministic number. We will focus on the simple random walk on $\mathbb Z$, which is defined by taking
\[ \prob{Y_i = 1} = p;\quad \prob{Y_i = -1} = 1-p = q \]
This can be thought of as a specific case of a Markov chain; it has the property that the path to $X_i$ does not matter, all that matters is the value that we are at, at any point in time.

\subsection{Gambler's Ruin Estimate}
What is the probability that $X_n$ reaches some value $a$ before it falls to 0? We will write $\mathbb P_x$ for the probability measure $\mathbb P$ with the condition that $X_0 = x$, i.e.
\[ \psub{x}{A} = \prob{A \mid X_0 = x} \]
We define $h(x) = \psub{x}{(X_n) \text{ hits $a$ before hitting 0}}$. We can define a recurrence relation. By the law of total probability, we have, for $0 < x < a$,
\begin{align*}
    h(x) & = \psubx{(X_n) \text{ hits $a$ before hitting 0} \mid Y_1 = 1} \cdot \psubx{Y_1 = 1}   \\
         & + \psubx{(X_n) \text{ hits $a$ before hitting 0} \mid Y_1 = -1} \cdot \psubx{Y_1 = -1} \\
         & = p \cdot h(x+1) + q \cdot h(x-1)
\end{align*}
Note that
\[ h(0) = 0;\quad h(a) = 1 \]
There are two cases; $p=q = \frac{1}{2}$ and $p \neq q$. If $p=q=\frac{1}{2}$, then
\[ h(x) - h(x+1) = h(x-1) - h(x) \]
We can then solve this to find
\[ h(x) = \frac{x}{a} \]
If $p \neq q$, then
\[ h(x) = ph(x+1) + qh(x-1) \]
We can try a solution of the form $\lambda^x$. Substituting gives
\[ p\lambda^2 - \lambda + q = 0 \implies \lambda = 1, \frac{q}{p} \]
The general solution can be found by using the boundary conditions.
\[ h(x) = A + B \left( \frac{q}{p} \right)^x \implies h(x) = \frac{\left( \frac{q}{p} \right)^x - 1}{\left( \frac{q}{p} \right)^a - 1} \]
This is known as the `gambler's ruin' estimate, since it determines whether a gambler will reach a target before going bankrupt.

\subsection{Expected Time to Absorption}
Let us define $T$ to be the first time that $x = 0$ or $x = a$. Then $T = \min \{ n \geq 0 \colon X_n \in \{ 0, a \} \}$. We want to find $\esubx{T} = \tau_x$. We can apply a condition on the first step, and use the law of total expectation to give
\[ \tau_x = p \esubx{T \mid Y_1 = 1} + q \esubx{T \mid Y_1 = -1} \]
Hence
\[ \tau_x = p (\tau_{x + 1} + 1) + q (\tau_{x - 1} + 1) \]
We can deduce that, for $0 < x < a$,
\[ \tau_x = 1 + p \tau_{x+1} + q \tau_{x-1} \]
and $\tau_0 = \tau_a = 0$. If $p = q = \frac{1}{2}$, then we can try a solution of the form $Ax^2$.
\[ Ax^2 = 1 + \frac{1}{2}A(x+1)^2 + \frac{1}{2}A(x-1)^2 \]
This gives a general solution of the form
\[ A = -1 \implies \tau_x = -x^2 + Bx + C \implies \tau_x = x(a-x) \]
If $p \neq q$, then we will try a solution of the form $Cx$, giving
\[ C = \frac{1}{q-p} \]
The general solution has the form
\[ \tau_x = \frac{x}{q-p} + A + B\left( \frac{q}{p} \right)^x \implies \tau_x = \frac{x}{q-p} - \frac{q}{q-p} \cdot \frac{\left( \frac{q}{p} \right)^x - 1}{\left( \frac{q}{p} \right)^a - 1}  \]

\section{Probability Generating Functions}
\subsection{Definition}
Let $X$ be a random variable with values in the positive integers, $\mathbb N$. Let $p_r = \prob{X = r}$ be the probability mass function. Then the probability generating function is defined to be
\[ p(z) = \sum_{r=0}^\infty p_r z^r = \expect{z^X} \text{ for } \abs{z} \leq 1 \]
When $\abs{z} \leq 1$, the probability generating function converges absolutely, since $\abs{\sum_{r=0}^\infty p_r z^r} \leq \sum_{r=0}^\infty p_r = 1$. So $p(z)$ is well-defined and has a radius of convergence of at least 1.
\begin{theorem}
    The probability generating function of $X$ uniquely determines the distribution of $X$.
\end{theorem}
\begin{proof}
    Suppose $(p_r)$ and $(q_r)$ are two probability mass functions with
    \[ \sum_{r=0}^\infty p_r z^r = \sum_{r=0}^\infty q_r z^r, \forall \abs{z} \leq 1 \]
    We will show that $p_r = q_r$ for all $r$. First, set $z = 0$, then clearly $p_0 = q_0$. Then by induction, suppose that $p_r = q_r$ for all $r \leq n$. Then we would like to show that $p_{n+1} = q_{n+1}$. We know that
    \[ \sum_{r=n+1}^\infty p_r z^r = \sum_{r=n+1}^\infty q_r z^r \]
    Hence, dividing by $z^{n+1}$, and taking the limit as $z \to 0$, we have $p_{n+1} = q_{n+1}$ as required.
\end{proof}

\subsection{Finding Moments and Probabilities}
\begin{theorem}
    \[ \lim_{z \to 1^-} p'(z) = p'(1^-) = \expect{X} \]
\end{theorem}
\begin{proof}
    We will first assume that $\expect{X}$ is finite; we will then extend the proof to the infinite case. Let $0 < z < 1$, then since the series $p(z)$ is absolutely convergent, we can interchange the sum and the derivative operators, giving
    \[ p'(z) = \sum_{r=0}^\infty r p_r z^{r-1} \]
    We can make an upper bound for this sum:
    \[ \sum_{r=0}^\infty r p_r z^{r-1} \leq \sum_{r=0}^\infty r p_r = \expect{X} \]
    Since $0 < z < 1$, we see that $p'(z)$ is an increasing function of $z$. This implies that there exists a limit of $p'(z)$ as $z \to 1^-$, which is upper bounded by $\expect{X}$. Now, let $\varepsilon > 0$ and let $N$ be an integer large enough such that
    \[ \sum_{r = 0}^N rp_r \geq \expect{X} - \varepsilon \]
    We have further that, since $0 < z < 1$,
    \[ p'(z) \geq \sum_{r=1}^N rp_r z^{r-1} \]
    So
    \[ \lim_{z \to 1^-} p'(z) \geq \sum_{r=1}^N rp_r \geq \expect{X} - \varepsilon \]
    which is true for any $\varepsilon$. Therefore $\lim_{z \to 1^-} p'(z) = \expect{X}$. Now, in the case that $\expect{X}$ is infinite, for any $M$ we can find a sufficiently large $N$ such that
    \[ \sum_{r = 0}^N rp_r \geq M \]
    From above, we know that
    \[ \lim_{z \to 1^-} p'(z) \geq \sum_{r=1}^N rp_r \geq M \]
    Since this is true for any $M$, this limit is equal to $\infty$.
\end{proof}
\noindent In exactly the same way, we can prove that
\[ p''(1^-) = \expect{X(X-1)} \]
and in general,
\[ p^{(k)}(1^-) = \expect{X(X-1)\cdots(X-k+1)} \]
In particular,
\[ \Var{X} = p''(1^-) + p'(1^-) - p'(1^-)^2 \]
Further,
\[ \prob{X = n} = \frac{1}{n!} \eval{\dv[n]{z} p(z)}_{z = 0} \]

\subsection{Sums of Random Variables}
Suppose that $X_1, \dots, X_n$ are independent random variables with probability generating functions $q_1, \dots, q_n$ respectively. Then
\[ p(z) = \expect{z^{X_1 + \dots + X_n}} \]
Recall that if $X$ and $Y$ are independent, then for all functions $f$ and $g$, we have $\expect{f(X)g(Y)} = \expect{f(X)}\expect{g(Y)}$. Therefore,
\[ p(z) = \expect{z^{X_1} z^{X_2} \cdots z^{X_n}} = \expect{z^{X_1}} \cdots \expect{z^{X_n}} = q_1(z) \cdots q_n(z) \]
So the probability generating function factorises into its independent parts. In particular, if all the $X_i$ are independent and identically distributed, then
\[ p(z) = q(z)^n \]

\subsection{Common Probability Generating Functions}
Suppose that $X \sim \mathrm{Bin}(n, p)$. Then
\begin{align*}
    p(z) & = \expect{z^X}                                  \\
         & = \sum_{r=0}^n z^r \binom{n}{r} p^r (1-p)^{n-r} \\
         & = \sum_{r=0}^n \binom{n}{r} (pz)^r (1-p)^{n-r}  \\
         & = (pz + 1 - p)^{n}
\end{align*}
Now, let $X \sim \mathrm{Bin}(n, p)$, $Y \sim \mathrm{Bin}(m, p)$ be independent random variables. Then the probability generating function of $X+Y$ is
\[ (pz + 1 - p)^{n} \cdot (pz + 1 - p)^{m} = (pz + 1 - p)^{n+m} \]
which is the probability generating function of a binomial distribution where the number of trials is $n+m$. Now, suppose that $X \sim \mathrm{Geo}(p)$. Then
\begin{align*}
    p(z) & = \expect{z^X}               \\
         & = \sum_{r=0}^n z^r (1-p)^r p \\
         & = \frac{p}{1-z(1-p)}         \\
\end{align*}
Now, suppose that $X \sim \mathrm{Poi}(\lambda)$. Then
\begin{align*}
    p(z) & = \expect{z^X}                                       \\
         & = \sum_{r=0}^n z^r e^{-\lambda} \frac{\lambda^r}{r!} \\
         & = e^{\lambda(z-1)}                                   \\
\end{align*}

\subsection{Random Sums of Random Variables}
Consider the sum of a random number of random variables. Let $X_1, \dots$ be independent and identically distributed, and let $N$ be an independent random variable with values in $\mathbb N$. Now, we can define the random variables $S_n$ to be
\[ S_n = X_1 + \dots + X_n \]
Then
\[ S_N = X_1 + \dots + X_N \]
is a random variable dependent on $N$. For all $\omega \in \Omega$,
\begin{align*}
    S_N(\omega) & = X_1(\omega) + \dots + X_{N(\omega)}(\omega) \\
                & = \sum_{i=1}^{N(\omega)} X_i(\omega)
\end{align*}
Now, let $q$ be the probability generating function of $N$, and $p$ be the probability generating function of $X_1$ (or equivalently, any $X_i$). Then let
\begin{align*}
    r(z) & = \expect{z^{S_N}}                                          \\
         & = \sum_{n} \expect{z^{X_1 + \dots + X_N} \cdot 1(N = n)}    \\
         & = \sum_{n} \expect{z^{X_1 + \dots + X_n} \cdot 1(N = n)}    \\
         & = \sum_{n} \expect{z^{X_1 + \dots + X_n}} \expect{1(N = n)} \\
         & = \sum_{n} \expect{z^{X_1 + \dots + X_n}} \prob{N = n}      \\
         & = \sum_{n} \expect{z^{X_1}}^n \prob{N = n}                  \\
         & = \sum_{n} p(z)^n \prob{N = n}                              \\
         & = q(p(z))
\end{align*}
Here is an alternative proof using conditional expectation.
\begin{align*}
    r(z) & = \expect{z^{S_N}}                 \\
         & = \expect{\expect{z^{S_N} \mid N}} \\
\end{align*}
We can see that
\begin{align*}
    \expect{z^{S_N} \mid N = n} & = \expect{z^{S_n} \mid N = n} \\
                                & = \expect{z^{X_1}}^n          \\
                                & = p(z)^n
\end{align*}
Therefore,
\begin{align*}
    r(z) & = \expect{p(z)^N} \\
         & = q(p(z))
\end{align*}
Using this expression for $r$, we can find that
\[ \expect{S_N} = r'(1^-) = q'(p(1^-)) \cdot p'(1^-) = q'(1^-) \cdot p'(1^-) = \expect{N} \expect{X_1} \]
Similarly,
\[ \Var{S_N} = \expect{N} \Var{X_1} + \Var{N} \left( \expect{X_1} \right)^2 \]

\section{Branching Processes}
\subsection{Introduction}
Let $(X_n \colon n \geq 0)$ be a random process, where $X_n$ is the number of individuals in generation $n$, and $X_0 = 1$. The individual in generation 0 produces a random number of offspring with distribution
\[ g_k = \prob{X_1 = k} \]
Then every individual in generation 1 produces an independent number of offspring with the same distribution. This is called a branching process. We can write a recursive formula for $X_n$. First, let $(Y_{k, n} \colon k \geq 1, n \geq 0)$ be an independent and identically distributed sequence with distribution $(g_k)_k$. So $Y_{k, n}$ is the number of offspring of the $k$th individual in generation $n$.
\[ X_{n+1} = \begin{cases}
        Y_{1,n} + \cdots + Y_{X_n,n} & \text{when } X_n \geq 1 \\
        0                            & \text{otherwise}
    \end{cases} \]

\subsection{Expectation of Generation Size}
\begin{theorem}
    \[ \expect{X_n} = \expect{X_1}^n \]
\end{theorem}
\begin{proof}
    Inductively,
    \begin{align*}
        \expect{X_{n+1}}                     & = \expect{\expect{X_{n+1} \mid X_n}}                 \\
        \expect{X_{n+1} \mid X_n = m}        & = \expect{Y_{1,n} + \cdots + Y_{X_n,n} \mid X_n = m} \\
                                             & = \expect{Y_{1,n} + \cdots + Y_{m,n} \mid X_n = m}   \\
                                             & = m \expect{Y_{1,n}}                                 \\
                                             & = m \expect{X_1}                                     \\
        \therefore \expect{X_{n+1} \mid X_n} & = X_n \cdot \expect{X_1}                             \\
        \therefore \expect{X_{n+1}}          & = \expect{X_n \cdot \expect{X_1}}                    \\
                                             & = \expect{X_n} \cdot \expect{X_1}
    \end{align*}
\end{proof}

\subsection{Probability Generating Functions}
\begin{theorem}
    Let $G(z) = \expect{z^{X_1}}$ be the probability generating function of $X_1$, and $G_n(z) = \expect{z^{X_n}}$ be the probability generating function of $X_n$. Then
    \[ G_{n+1}(z) = G(G_n(z)) = G(G(\cdots G(z)\cdots)) = G_n(G(z)) \]
\end{theorem}
\begin{proof}
    \begin{align*}
        G_{n+1}(z)                                            & = \expect{z^{X_{n+1}}}                                 \\
                                                              & = \expect{\expect{z^{X_{n+1}} \mid X_n}}               \\
        \expect{z^{X_{n+1}} \mid X_n = m}                     & = \expect{z^{Y_{1, n} + \dots + Y_{m,n}} \mid X_n = m} \\
                                                              & = \expect{z^{X_1}}^m                                   \\
                                                              & = G(z)^m                                               \\
        \therefore \expect{\expect{z^{X_{n+1}} \mid X_n = m}} & = \expect{G(z)^m}                                      \\
                                                              & = G_n(G(z))
    \end{align*}
\end{proof}

\subsection{Probability of Extinction}
We define the extinction probability $q$ as the probability that $X_n = 0$ for some $n \geq 1$, and $q_n = \prob{X_n = 0}$. It is clear that $X_n = 0$ implies that $X_{n+1} = 0$. So the sequence of events $(A_n) = (\{ X_n = 0 \})$ is an increasing sequence of events. So by the continuity of the probability measure, $\prob{A_n}$ converges to $\prob{\bigcup A_n}$ as $n \to \infty$. Note that the event $\bigcup A_n$ is the event that there will be extinction. Therefore, $q_n \to q$ as $n \to \infty$.
\begin{claim}
    $q_{n+1} = G(q_n)$ and $q = G(q)$.
\end{claim}
\begin{proof}
    Using the above theorem on $Q$,
    \begin{align*}
        q_{n+1} &= \prob{X_{n+1} = 0} \\
        &= G_{n+1}(0) \\
        &= G(G_n(0)) \\
        &= G(q_n)
    \end{align*}
    Since $G$ is continuous, taking the limit as $n \to \infty$ and using that $q_n \to q$ gives $G(q) = q$.
\end{proof}
\noindent We can form another proof for the first part of the above claim.
\begin{proof}
    Instead of conditioning on the previous generation, let us condition on the first generation, i.e. $X_1 = m$. Note that after the first generation, we will have $m$ independent subtrees on the family tree. Each tree is identically distributed to the entire tree as a whole. Hence,
    \[ X_{n+1} = X_n^{(1)} + \dots + X_n^{(m)} \]
    where the $X_i^{(j)}$ are independent and identically distributed random processes each with the same offspring distribution. By the law of total probability,
    \begin{align*}
        q_{n+1} &= \prob{X_{n+1} = 0} \\
        &= \sum_m \prob{X_{n+1} = 0 \mid X_1 = m} \cdot \prob{X_1 = m} \\
        &= \sum_m \prob{X_n^{(1)} = 0, \dots, X_n^{(m)} = 0} \cdot \prob{X_1 = m} \\
        &= \sum_m \prob{X_n^{(1)} = 0}^m \cdot \prob{X_1 = m} \\
        &= \sum_m q_n^m \cdot \prob{X_1 = m} \\
        &= G(q_n)
    \end{align*}
\end{proof}
\begin{theorem}
    Assume $\prob{X_1 = 1} < 1$. Then, the extinction probability $q$ is the minimal non-negative solution to $G(t) = t$. Further, $q < 1$ if and only if $\expect{X_1} > 1$.
\end{theorem}
\begin{proof}
    First, we will prove the minimality of $q$. Let $t$ be the smallest non-negative solution to $G(t) = t$. We will prove inductively that $q_n \leq t$ for all $n$, and then by taking limits we have that $q \leq t$. Since $q$ is a solution, this will imply that $q=t$. Now, as a base case, $q_0 = 0 = \prob{X_0 = 0} \leq t$. Inductively let us suppose that $q_n \leq t$. We know that $q_{n+1} = G(q_n)$. $G$ is an increasing function on $[0, 1]$, and since $q_n \leq t$ we have $q_{n+1} = G(q_n) \leq G(t) = t$.
\end{proof}

\end{document}