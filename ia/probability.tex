\documentclass{article}

\input{../util.tex}

\title{Probability}
\author{Cambridge University Mathematical Tripos: Part IA}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Probability Spaces}
\subsection{Probability Spaces and $\sigma$-algebras}
\begin{definition}
    Suppose $\Omega$ is a set, and $\mathcal F$ is a collection of subsets of $\Omega$. We call $\mathcal F$ a $\sigma$-algebra if
    \begin{enumerate}
        \item $\Omega \in \mathcal F$
        \item if $A \in \mathcal F$, then $\stcomp{A} \in \mathcal F$
        \item for any countable collection $(A_n)_{n \geq 1}$ with $A_n \in \mathcal F$ for all $n$, we must also have that $\bigcup_n A_n \in \mathcal F$
    \end{enumerate}
\end{definition}
\begin{definition}
    Suppose that $\mathcal F$ is a $\sigma$-algebra on $\Omega$. A function $\mathbb P \colon \mathcal F \to [0, 1]$ is called a probability measure if
    \begin{enumerate}
        \item $\prob{\Omega} = 1$
        \item for any countable disjoint collection of sets $(A_n)_{n \geq 1}$ in $\mathcal F$ ($A_n \in \mathcal F$ for all $n$), then $\prob{\bigcup_{n \geq 1}A_n} = \sum_{n \geq 1} \prob{A_n}$ (this is called `countable additivity')
    \end{enumerate}
    We say that $\prob{A}$ is `the probability of $A$'. We call $(\Omega, \mathcal F, \mathbb P)$ a probability space, where $\Omega$ is the sample space, $\mathcal F$ is the $\sigma$-algebra, and $\mathbb P$ is the probability measure.
\end{definition}

\begin{remark}
    When $\Omega$ is countable, we take $\mathcal F$ to be all subsets of $\Omega$, i.e. $\mathcal F = \mathcal P(\Omega)$, its power set.
\end{remark}
\begin{definition}
    The elements of $\Omega$ are called outcomes, and the elements of $\mathcal F$ are called events.
\end{definition}
Note that $\mathbb P$ is dependent on $\mathcal F$ but not on $\Omega$. We talk about probabilities of \textit{events}, not probabilities of \textit{outcomes}. For example, if you pick a uniform number from the interval $[0, 1]$; then the probability of getting any specific outcome is zero --- but we can define useful events that have non-zero probabilities.

\subsection{Properties of the Probability Measure}
\begin{itemize}
    \item $\prob{\stcomp{A}} = 1 - \prob{A}$, since $A$ and $\stcomp{A}$ are disjoint sets, whose union is $\Omega$
    \item $\prob{\varnothing} = 0$, since it is the complement of $\Omega$
    \item if $A \subseteq B$, then $\prob{A} \leq \prob{B}$
    \item $\prob{A \cup B} = \prob{A} + \prob{B} - \prob{A \cap B}$ using the Inclusion-Exclusion theorem
\end{itemize}

\subsection{Examples of Probability Spaces}
\begin{itemize}
    \item Rolling a fair 6-sided die:
          \begin{itemize}
              \item $\Omega = \{ 1, 2, 3, 4, 5, 6 \}$
              \item $\mathcal F = \mathcal P(\Omega)$
              \item $\forall \omega \in \Omega, \prob{\{ \omega \}} = \frac{1}{6}$, and if $A \subseteq \Omega$ then $\prob{A} = \frac{\abs{A}}{6}$
          \end{itemize}

    \item Equally likely outcomes (more generally). Suppose $\Omega$ is some finite set, e.g. $\Omega = \{ \omega_1, \omega_2, \dots, \omega_n \}$. Then we define $\prob{A} = \frac{\abs{A}}{\abs{\Omega}}$. In classical probability, this models picking a random element of $\Omega$.

    \item Picking balls from a bag. Suppose we have $n$ balls with $n$ labels from the set $\{1, \dots, n\}$, indistinguishable by touching. Let us pick $k \leq n$ balls at random from the bag, \textit{without replacement}. Here, `at random' just means that all possible outcomes are equally likely, and their probability measures should be equal.

          We will take $\Omega = \{ A \subseteq \{1, \dots, n\} : \abs{A} = k \}$. Then $\abs{\Omega} = \binom{n}{k}$. Then of course $\prob{\{ \omega \}} = \frac{1}{\abs*{\Omega}}$, since all outcomes (combinations, in this case) are equally likely.

    \item Consider a well-shuffled deck of 52 cards, i.e. it is equally likely that each possible permutation of the 52 cards will appear. $\Omega = \{ \text{all permutations of 52 cards} \}$, and $\abs*{\Omega} = 52!$

          The probability that the top two cards are aces is therefore $\frac{4 \times 3 \times 50!}{52!} = \frac{1}{221}$, since there are $4 \times 3 \times 50!$ outcomes that produce such an event.

    \item Consider a string of $n$ random digits from $\{0, \dots, 9\}$. Then $\Omega = \{ 0, \dots, 9 \}^n$, and $\abs*{\Omega} = 10^n$. We define $A_k = \{ \text{no digit exceeds } k \}$, and $B_k = \{ \text{largest digit is } k \}$. Then $\prob{B_k} = \frac{\abs*{B_k}}{\abs*{\Omega}}$. Notice that $B_k = A_k \setminus A_{k-1}$. $\abs*{A_k} = (k+1)^n$, so $\abs*{B_k} = (k+1)^n - k^n$, so $\prob{B_k} = \frac{(k+1)^n - k^n}{10^n}$.

    \item The birthday problem. There are $n$ people; what is the probability that at least two of them share a birthday? We assume that each year has exactly 365 days, i.e. nobody is born on 29\textsuperscript{th} of February, and that the probabilities of being born on any given day are equal.

          Let $\Omega = \{1, \dots, 365\}^n$, and $\mathcal F = \mathcal P(\Omega)$. Since all outcomes are equally likely, we take $\prob{\{\omega\}} = \frac{1}{365^n}$.

          Let $A = \{ \text{at least two people share the same birthday} \}$. $\stcomp{A} = \{ \text{all } n \text{ birthdays are different} \}$. Since $\prob{A} = 1 - \prob{\stcomp{A}}$, it suffices to calculate $\prob{\stcomp{A}}$, which is $\frac{\abs*{\stcomp{A}}}{\abs*{\Omega}} = \frac{365!}{(365 - n)!365^n}$. So the answer is $\prob{A} = 1 - \frac{365!}{(365 - n)!365^n}$

          Note that at $n=22$, $\prob{A} \approx 0.476$ and at $n=23$, $\prob{A} \approx 0.507$. So when there are at least 23 people in a room, the probability that two of them share a birthday is around 50\%.
\end{itemize}

\subsection{Combinatorial Analysis}
Let $\Omega$ be a finite set, and suppose $\abs*{\Omega} = n$. We want to partition $\Omega$ into $k$ disjoint subsets $\Omega_1, \dots, \Omega_k$ with $\abs*{\Omega_i} = n_i$ and $\sum_{i=1}^k n_i = n$. How many ways of doing such a partition are there? The result is
\[ \underbrace{\binom{n}{n_1}}_{\text{choose first set}}\underbrace{\binom{n-n_1}{n_2}}_{\text{choose second set}}\dots\underbrace{\binom{n-(n_1 + n_2 + \dots + n_{k+1})}{n_k}}_{\text{choose last set}} = \frac{n!}{n_1!n_2!\dots n_k!} \]
So we will write
\[ \binom{n}{n_1, \dots, n_k} = \frac{n!}{n_1!n_2!\dots n_k!} \]

Now, let $f\colon \{1, \dots, k\} \to \{1, \dots, n\}$. $f$ is strictly increasing if $x < y \implies f(x) < f(y)$. $f$ is increasing if $x < y \implies f(x) \leq f(y)$. How many strictly increasing functions $f$ exist? Note that if we know the range of $f$, the function is completely determined. The range is a subset of $\{1, \dots, n\}$ of size $k$, i.e. a $k$-subset of an $n$-set, which yields $\binom{n}{k}$ choices, and thus there are $\binom{n}{k}$ strictly increasing functions.

How many increasing functions $f$ exist? Let us define a bijection from the set of increasing functions $\{f\colon \{1, \dots, k\} \to \{1, \dots, n\}\}$ to the set of \textit{strictly} increasing functions $\{g\colon \{1, \dots, k\} \to \{1, \dots, n+k-1\}\}$. For any increasing function $f$, we define $g(i) = f(i) + i - 1$. Then $g$ is clearly strictly increasing, and takes values in the range $\{1, \dots, n+k-1\}$. By extension, we can define an increasing function $f$ from any strictly increasing function $g$. So the total number of increasing functions $f\colon \{1, \dots, k\} \to \{1, \dots, n\}$ is $\binom{n+k-1}{k}$.

\section{Stirling's Formula and Countable Subadditivity}
\subsection{Stirling's Formula}
Let $(a_n)$ and $(b_n)$ be sequences. We will write $a_n \sim b_n$ if $\frac{a_n}{b_n} \to 1$ as $n \to \infty$. This is asymptotic equality.
\begin{theorem}[Stirling's Formula]
    $n! \sim n^n\, \sqrt{2 \pi n}\, e^{-n}$ as $n \to \infty$.
\end{theorem}
\noindent Let us first prove the weaker statement $\log (n!) \sim n \log n$.
\begin{proof}
    Let us define $l_n = \log (n!) = \log 2 + \log 3 + \dots + \log n$. For $x \in \mathbb R$, we write $\floor{x}$ for the integer part of $x$. Note that
    \[ \log \floor x \leq \log x \leq \log \floor{x+1} \]
    Let us integrate this from 1 to $n$.
    \[ \sum_{k=1}^{n-1} \log k \leq \int_1^n \log x\, \dd x \leq \sum_{k=2}^{n} \log k \]
    \[ l_{n-1} \leq n \log n - n + 1 \leq l_n \]
    For all $n$, therefore:
    \[ n \log n - n + 1 \leq l_n \leq (n+1) \log (n+1) - (n+1) + 1 \]
    Dividing through by $n\log n$, we get
    \[ \frac{l_n}{n \log n} \to 1 \]
    as $n \to \infty$.
\end{proof}
\noindent The following complete proof is non-examinable.
\begin{proof}
    For any twice-differentiable function $f$, for any $a < b$ we have
    \[ \int_a^b f(x) \,\dd x = \frac{f(a) + f(b)}{2} (b - a) - \frac{1}{2}\int_a^b (x-a)(b-x)f''(x)\,\dd x \]
    Now let $f(x) = \log x$, $a=k$ and $b=k+1$. Then
    \begin{align*}
        \int_k^{k+1} \log x \,\dd x & = \frac{\log k + \log(k+1)}{2} + \frac{1}{2}\int_k^{k+1} \frac{(x-k)(k+1-x)}{x^2}\,\dd x                            \\
                                    & = \frac{\log k + \log(k+1)}{2} + \frac{1}{2}\int_0^1 \frac{x(1-x)}{(x+k)^2}\,\dd x                                  \\
        \intertext{Let us take the sum for $k=1, \dots, n-1$ of the equality.}
        \int_1^n \log x \,\dd x     & = \frac{\log ((n-1)!) + \log(n!)}{2} + \frac{1}{2}\sum_{k=1}^{n-1}\int_0^1 \frac{x(1-x)}{(x+k)^2}\,\dd x            \\
        n\log n - n + 1             & = \log (n!) - \frac{\log n}{2} + \sum_{k=1}^{n-1} a_k;\quad a_k = \frac{1}{2}\int_0^1 \frac{x(1-x)}{(x+k)^2}\,\dd x \\
        \log (n!)                   & = n \log n - n + \frac{\log n}{2} + 1 - \sum_{k=1}^{n-1} a_k                                                        \\
        n!                          & = n^n \, e^{-n} \, \sqrt n \, \exp\left( 1 - \sum_{k=1}^{n-1} a_k \right)
    \end{align*}
    Now, note that
    \[ a_k \leq \frac{1}{2}\int_0^1 \frac{x(1-x)}{k^2}\,\dd x = \frac{1}{12k^2} \]
    So the sum of all $a_k$ converges. We set
    \[ A = \exp\left( 1 - \sum_{k=1}^\infty a_k \right) \]
    and then
    \[ n! = n^n \, e^{-n} \, \sqrt n \, A \, \exp\left( \underbrace{\sum_{k=n}^\infty a_k}_{\text{converges to zero}} \right) \]
    Therefore,
    \[ n! \sim n^n\, \sqrt{n}\, e^{-n}\, A \]
    To finish the proof, we must show that $A = \sqrt{2 \pi}$. We can utilise the fact that $n! \sim n^n\, \sqrt{n}\, e^{-n}\, A$.
    \begin{align*}
        2^{-2n} \binom{2n}{n} & = 2^{-2n} \cdot \frac{2n!}{(n!)^2}                                                                                           \\
                              & \sim 2^{-2n} \frac{(2n)^{2n} \cdot \sqrt{2n} \cdot A \cdot e^{-2n}}{n^n\, e^{-n}\, \sqrt n\, A\, n^n\, e^{-n}\, \sqrt n\, A} \\
                              & = \frac{\sqrt{2}}{A\sqrt{n}}
    \end{align*}
    Using a different method, we will prove that $2^{-2n} \binom{2n}{n} \sim \frac{1}{\sqrt{\pi n}}$, which then forces $A = \sqrt{2\pi}$. Consider
    \[ I_n = \int_0^{\frac{\pi}{2}} (\cos \theta)^n \, \dd \theta;\quad n \geq 0 \]
    So $I_0 = \frac{\pi}{2}$ and $I_1 = 1$. By integrating by parts,
    \[ I_n = \frac{n-1}{n}I_{n-2} \]
    Therefore,
    \[ I_{2n} = \frac{2n-1}{2n}I_{2n-2} = \frac{(2n-1)(2n-3)\dots(3)(1)}{(2n)(2n-2)\dots(2)}I_0 \]
    Multiplying the numerator and denominator by the denominator, we have
    \[ I_{2n} = \frac{(2n)!}{(n! \cdot 2^n)^2} \cdot \frac{\pi}{2} = 2^{-2n} \frac{2n}{n} \cdot \frac{\pi}{2} \]
    In the same way, we can deduce that
    \[ I_{2n+1} = \frac{(2n)(2n-2)\dots(2)}{(2n+1)(2n-1)\dots(3)(1)}I_1 = \frac{1}{2n+1} \left( 2^{-2n} \binom{2n}{n} \right)^{-1} \]
    From $I_n = \frac{n-1}{n}I_{n-2}$, we get that
    \[ \frac{I_n}{I_{n-2}} \to 1 \]
    as $n \to \infty$. We now want to show that $\frac{I_{2n}}{I_{2n+1}} \to 1$. We see from the definition of $I_n$ that $I$ is a decreasing function of $n$. Therefore,
    \[ \frac{I_{2n}}{I_{2n+1}} \leq \frac{I_{2n-1}}{I_{2n+1}} \to 1 \]
    and also
    \[ \frac{I_{2n}}{I_{2n+1}} \geq \frac{I_{2n}}{I_{2n-2}} \to 1 \]
    So
    \[ \frac{I_{2n}}{I_{2n+1}} \to 1 \]
    which means that
    \[ \frac{2^{-2n} \binom{2n}{n} \frac{\pi}{2}}{\left( 2^{-2n} \binom{2n}{n} \right)^{-1} \frac{1}{2n+1}} \to 1 \implies \left( 2^{-2n} \binom{2n}{n} \right)^2 \frac{\pi}{2} (2n+1) \to 1 \]
    Therefore,
    \[ \left( 2^{-2n} \binom{2n}{n} \right)^2 \sim \frac{2}{\pi (2n+1)} \sim \frac{1}{\pi n} \]
    Finally,
    \[ A = \sqrt{2 \pi} \]
    completes the proof.
\end{proof}

\subsection{Countable Subadditivity}
Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space, and let $(A_n)_{n \geq 1}$ be a (not necessarily disjoint) sequence of events in $\mathcal F$. Then
\[ \prob{\bigcup_{n=1}^\infty A_n} \leq \sum_{n=1}^\infty \prob{A_n} \]
\begin{proof}
    Let us define a new sequence $B_1 = A_1$ and $B_n = A_n \setminus (A_1 \cup A_2 \cup \dots \cup A_{n-1})$. So by construction, the sequence $B_n$ is a disjoint sequence of events in $\mathcal F$. Note further that the union of all $B_n$ is equal to the union of all $A_n$. So
    \[ \prob{\bigcup_{n=1}^\infty A_n} = \prob{\bigcup_{n=1}^\infty B_n} \]
    By the countable additivity axiom,
    \[ \prob{\bigcup_{n=1}^\infty B_n} = \sum_{n=1}^\infty \prob{B_n} \]
    But $B_n \subseteq A_n$. So $\prob{B_n} \leq A_n$. Therefore,
    \[ \prob{\bigcup_{n=1}^\infty A_n} \leq \sum_{n=1}^\infty \prob{A_n} \]
\end{proof}

\section{Inclusion-Exclusion}
\subsection{Continuity of Probability Measures}
Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $(A_n)_{n \geq 1}$ be an increasing sequence in $\mathcal F$, i.e. $A_n \in \mathcal F$, and $A_n \subseteq A_{n+1}$. Then $\prob{A_n} \leq \prob{A_{n+1}}$. We want to show that
\[ \lim_{n \to \infty}\prob{A_n} = \prob{\bigcup_n A_n} \]
\begin{proof}
    Let $B_1 = A_1$, and for all $n \geq 2$, let $B_n = A_n \setminus (A_1 \cup A_2 \cup \dots \cup A_{n-1})$. Then the union over $B_i$ up to $n$ is equal to the union over $A_i$ up to $n$. So
    \[ \prob{A_n} = \prob{\bigcup_{k=1}^n B_k} = \sum_{k=1}^n \prob{B_k} \to \sum_{k=1}^\infty \prob{B_k} = \prob{\bigcup_n B_n} = \prob{\bigcup_n A_n} \]
\end{proof}
\noindent We can say that probability measures are continuous; an increasing sequence of events has a probability which tends to some limit. Similarly, if $(A_n)$ is decreasing, then the limit probability is the probability of the intersection of all $A_n$.

\subsection{Inclusion-Exclusion Formula}
Suppose that $A, B \in \mathcal F$. Then $\prob{A \cup B} = \prob{A} + \prob{B} - \prob{A \cap B}$. Now let also $C \in \mathcal F$. Then
\begin{align*}
    \prob{A \cup B \cup C} & = \prob{A \cup B} + \prob{C} - \prob{(A \cup B) \cap C} \\
                           & = \prob{A} + \prob{B} + \prob{C}                        \\
                           & - \prob{A \cap B} - \prob{A \cap C} - \prob{B \cap C}   \\
                           & + \prob{A \cap B \cap C}
\end{align*}
Let $A_1, \dots, A_n$ be events in $\mathcal F$. Then
\begin{align*}
    \prob{\bigcup_{i=1}^n A_i} & = \sum_{i=1}^n \prob{A_i}                                                      \\
                               & - \sum_{1 \leq i_1 < i_2 \leq n}\prob{A_{i_1} \cap A_{i_2}}                    \\
                               & + \sum_{1 \leq i_1 < i_2 < i_3 \leq n}\prob{A_{i_1} \cap A_{i_2} \cap A_{i_3}} \\
                               & - \cdots                                                                       \\
                               & + (-1)^{n+1}\prob{A_1 \cap \dots \cap A_n}
\end{align*}
Or more concisely,
\[ \prob{\bigcup_{i=1}^n A_i} = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < i_2 < \dots < i_k \leq n} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \]

\begin{proof}
    The case for $n=2$ has been verified, so we can use induction on $n$. Now, let us assume this holds for $n-1$ events.
    \begin{align*}
        \prob{\bigcup_{i=1}^n A_i} & = \prob{\left( \bigcup_{i=1}^{n-1} A_i  \right) \cup A_n}                                                     \\
                                   & = \prob{\bigcup_{i=1}^{n-1} A_i} + \prob{A_n} - \prob{\left( \bigcup_{i=1}^{n-1} A_i  \right) \cap A_n}       \\
                                   & = \prob{\bigcup_{i=1}^{n-1} A_i} + \prob{A_n} - \prob{\bigcup_{i=1}^{n-1} (A_i \cap A_n)}                     \\
        \intertext{Let $B_i = A_i \cap A_n$ for all $i$. By the inductive hypothesis, we have}
        \prob{\bigcup_{i=1}^n A_i} & = \prob{\bigcup_{i=1}^{n-1} A_i} + \prob{A_n} - \prob{\bigcup_{i=1}^{n-1} B_n}                                \\
                                   & = \sum_{k=1}^{n-1} (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n-1} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \\&- \sum_{k=1}^{n-1} (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n-1} \prob{B_{i_1} \cap \dots \cap B_{i_k}} \\ &+ \prob{A_n}
    \end{align*}
    which gives the clain as required.
\end{proof}

Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space with $\abs{\Omega} < \infty$ and $\prob{A} = \frac{\abs{A}}{\abs{\Omega}}$. Let $A_1, \dots, A_n \in \mathcal F$. Then
\[ \abs{A_1 \cup \dots \cup A_n} = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} \abs{A_{i_1} \cap \dots \cap A_{i_k}} \]

\subsection{Bonferroni Inequalities}
Truncating the sum in the inclusion-exclusion formula at the $r$th term yields an estimate for the probability. The Bonferroni Inequalities state that if $r$ is odd, it is an overestimate, and if $r$ is even, it is an underestimate.
\begin{align*}
    r \text{ odd}  & \implies \prob{\bigcup_{i=1}^n A_i} \leq \sum_{k=1}^r (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \\
    r \text{ even} & \implies \prob{\bigcup_{i=1}^n A_i} \geq \sum_{k=1}^r (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} \prob{A_{i_1} \cap \dots \cap A_{i_k}}
\end{align*}
\begin{proof}
    Again, we will use induction. The $n=2$ case is trivial. Suppose that $r$ is odd. Then
    \begin{equation}
        \prob{\bigcup_{i=1}^n A_i} = \prob{\bigcup_{i=1}^{n-1} A_i} + \prob{A_n} - \prob{\bigcup_{i=1}^n B_i} \tag{$\ast$}
    \end{equation}
    where $B_i = A_i \cap A_n$. Since $r$ is odd,
    \[ \prob{\bigcup_{i=1}^{n-1} A_i} \leq \sum_{k=1}^r (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n-1} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \]
    Since $r-1$ is even, we can apply the inductive hypothesis to $\prob{\bigcup_{i=1}^{n-1} B_i}$.
    \[ \prob{\bigcup_{i=1}^{n-1} B_i} \geq \sum_{k=1}^{r-1} (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n-1} \prob{B_{i_1} \cap \dots \cap B_{i_k}} \]
    We can substitute both bounds into ($\ast$) to get an overestimate.
\end{proof}

\subsection{Counting using Inclusion-Exclusion}
We can apply the Inclusion-Exclusion formula to count various things. How many functions $f \colon \{ 1, \dots, n \} \to \{ 1, \dots, m \}$ are surjective? Let $\Omega$ be the set of such functions, and $A = \{ f \in \Omega : f \text{ is a surjection} \}$. For all $i \in \{ 1, \dots, m \}$, we define $A_i = \{ f \in \Omega : i \notin \{ f(1), f(2), \dots, f(n) \} \}$. Then $A = \stcomp{A_1} \cap \stcomp{A_2} \cap \dots \cap \stcomp{A_m} = \stcomp{(A_1 \cup A_2 \cup \dots \cup A_m)}$. Then
\[ \abs{A} = \abs{\Omega} - \abs{A_1 \cup \dots \cup A_m} = m^n - \abs{A_1 \cup \dots \cup A_m} \]
Now, let us use the Inclusion-Exclusion formula.
\begin{align*}
    \abs{A_1 \cup \dots \cup A_m} & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} \abs{A_{i_1} \cap \dots \cap A_{i_k}} \\
    \intertext{Note that $A_{i_1} \cap \dots \cap A_{i_k}$ is the set of functions where $k$ distinct numbers are not included in the function's range. There are $(m-k)^n$ such functions.}
    \abs{A_1 \cup \dots \cup A_m} & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} (m-k)^n                               \\
                                  & = \sum_{k=1}^n (-1)^{k+1} \binom{m}{k} (m-k)^n                                                         \\
    \abs{A}                       & = m^n -  \sum_{k=1}^n (-1)^{k+1} \binom{m}{k} (m-k)^n                                                  \\
    \abs{A}                       & = \sum_{k=0}^n (-1)^k \binom{m}{k} (m-k)^n
\end{align*}

\section{Independence and Dependence of Events}
\subsection{Counting Derangements}
A derangement is a permutation which has no fixed point, i.e. $\forall i, \sigma(i) \neq i$. We will let $\Omega$ be the set of permutations of $\{1, \dots, n\}$, i.e. $S_n$. Let $A$ be the set of derangements in $\Omega$. Let us pick a permutation $\sigma$ at random from $\Omega$. What is the probability that it is a derangement? We define $A_i = \{ f \in \Omega \colon f(i) = i \}$, then $A = \stcomp{A_1} \cap \dots \stcomp{A_n} = \stcomp{\left( \bigcup_{i=1}^n A_i  \right)}$, so $\prob{A} = 1 - \prob{\bigcup_{i=1}^n A_i}$. By the inclusion-exclusion formula,
\begin{align*}
    \prob{\bigcup_{i=1}^n A_i} & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < i_2 < \dots < i_k \leq n} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \\
                               & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < i_2 < \dots < i_k \leq n} \frac{(n-k)!}{\abs{\Omega}}            \\
                               & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < i_2 < \dots < i_k \leq n} \frac{(n-k)!}{n!}                      \\
                               & = \sum_{k=1}^n (-1)^{k+1} \binom{n}{k} \frac{(n-k)!}{n!}                                                      \\
                               & = \sum_{k=1}^n (-1)^{k+1} \frac{n!}{k!(n-k)!} \cdot \frac{(n-k)!}{n!}                                         \\
                               & = \sum_{k=1}^n (-1)^{k+1} \frac{1}{k!}                                                                        \\
\end{align*}
So
\[ \prob{A} = 1 - \prob{\bigcup_{i=1}^n A_i} = 1 - \sum_{k=1}^n \frac{(-1)^{k+1}}{k!} = \sum_{k=0}^n \frac{(-1)^k}{k!} \]
As $n \to \infty$, this value tends to $e^{-1} \approx 0.3678$.

\subsection{Independence of Events}
\begin{definition}
    Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $A, B \in \mathcal F$. $A$ and $B$ are called independent if
    \[ \prob{A \cap B} = \prob{A} \cdot \prob{B} \]
    We write $A \perp B$, or $A \perp\!\!\!\perp B$. A countable collection of events $(A_n)$ is said to be independent if for all distinct $i_1, \dots, i_k$, we have
    \[ \prob{A_{i_1} \cap \dots \cap A_{i_k}} = \prod_{j=1}^k \prob{A_{i_j}} \]
\end{definition}
\begin{remark}
    To show that a collection of events is independent, it is insufficient to show that events are pairwise independent. For example, consider tossing a fair coin twice, so $\Omega = \{ (0, 0), (0, 1), (1, 0), (1, 1) \}$. $\prob{\{\omega\}} = \frac{1}{4}$. Consider the events $A, B, C$ where
    \[ A = \{ (0, 0), (0, 1) \};\quad B = \{ (0, 0), (1, 0) \};\quad C = \{ (1, 0), (0, 1) \} \]
    \[ \prob{A} = \prob{B} = \prob{C} = \frac{1}{2} \]
    \begin{align*}
        \prob{A \cap B} = \prob{\{ (0, 0) \}} & = \frac{1}{4} = \prob{A} \cdot \prob{B} \\
        \prob{A \cap C} = \prob{\{ (0, 1) \}} & = \frac{1}{4} = \prob{A} \cdot \prob{C} \\
        \prob{B \cap C} = \prob{\{ (1, 0) \}} & = \frac{1}{4} = \prob{B} \cdot \prob{C}
    \end{align*}
    \[ \prob{A \cap B \cap C} = \prob{\varnothing} = 0 \]
\end{remark}
\begin{claim}
    If $A \perp B$, then $A \perp \stcomp{B}$.
\end{claim}
\begin{proof}
    \begin{align*}
        \prob{A \cap \stcomp{B}} & = \prob{A} - \prob{A \cap B}           \\
                                 & = \prob{A} - \prob{A} \cdot \prob{B}   \\
                                 & = \prob{A} \left( 1 - \prob{B} \right) \\
                                 & = \prob{A} \prob{\stcomp{B}}
    \end{align*}
    as required.
\end{proof}

\subsection{Conditional Probability}
\begin{definition}
    Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $B \in \mathcal F$ with $\prob{B} > 0$. We define the conditional probability of $A$ given $B$, written $\prob{A \mid B}$ as
    \[ \prob{A \mid B} = \frac{\prob{A \cap B}}{\prob{B}} \]
\end{definition}
\noindent Note that if $A$ and $B$ are independent, then
\[ \prob{A \mid B} = \frac{\prob{A \cap B}}{\prob{B}} = \frac{\prob{A} \cdot \prob{B}}{\prob{B}} = \prob{A} \]
\begin{claim}
    Suppose that $(A_n)$ is a disjoint sequence in $\mathcal F$. Then
    \[ \prob{\bigcup A_n \mathrel{\Big|} B} = \sum_{n} \prob{A_n \mid B} \]
    This is the countable additivity property for conditional probability.
\end{claim}
\begin{proof}
    \begin{align*}
        \prob{\bigcup A_n \mathrel{\Big|} B} & = \frac{\prob{(\bigcup A_n) \cap B}}{\prob{B}} \\
                                             & = \frac{\prob{\bigcup (A_n \cap B)}}{\prob{B}} \\
        \intertext{By countable additivity, since that $(A_n \cap B)$ are disjoint,}
                                             & = \sum_n \frac{\prob{A_n \cap B}}{\prob{B}}    \\
                                             & = \sum_n \prob{A_n \mid B}
    \end{align*}
\end{proof}
\noindent We can think of $\prob{\dots \mid B}$ as a new probability measure for the same $\Omega$.

\subsection{Law of Total Probability}
\begin{claim}
    Suppose $(B_n)$ is a disjoint collection of events in $\mathcal F$, such that $\bigcup B = \Omega$, and for all $n$, we have $\prob{B_n} > 0$. If $A \in \mathcal F$ then
    \[ \prob{A} = \sum_n \prob{A \mid B_n} \cdot \prob{B_n} \]
\end{claim}
\begin{proof}
    \begin{align*}
        \prob{A} & = \prob{A \cap \Omega}                   \\
                 & = \prob{A \cap \left(\bigcup B_n\right)} \\
                 & = \prob{\bigcup(A \cap B_n)}             \\
        \intertext{By countable additivity,}
                 & = \sum_n \prob{A \cap B_n}               \\
                 & = \sum_n \prob{A \mid B_n} \prob{B_n}
    \end{align*}
\end{proof}

\subsection{Bayes' Formula}
\begin{claim}
    Suppose $(B_n)$ is a disjoint sequence of events with $\bigcup B_n = \Omega$ and $\prob{B_n} > 0$ for all $n$. Then
    \[ \prob{B_n \mid A} = \frac{\prob{A \mid B_n} \prob{B_n}}{\sum_k \prob{A \mid B_k} \prob{B_k}} \]
\end{claim}
\begin{proof}
    \begin{align*}
        \prob{B_n \mid A} & = \frac{\prob{B_n \cap A}}{\prob{A}}                                       \\
                          & = \frac{\prob{A \mid B_n} \prob{B_n}}{\prob{A}}                            \\
        \intertext{By the Law of Total Probability,}
                          & = \frac{\prob{A \mid B_n} \prob{B_n}}{\sum_k \prob{A \mid B_k} \prob{B_k}}
    \end{align*}
\end{proof}
\noindent Note that on the right hand side, the numerator appears somewhere in the denominator. This formula is the basis of Bayesian statistics. It allows us to reverse the direction of a conditional probability --- knowing the probabilities of the events $(B_n)$, and given a model of $\prob{A \mid B_n}$, we can calculuate the posterior probabilities of $B_n$ given that $A$ occurs.

\section{Examples of Conditional Probability}
\subsection{Bayes' Formula for Medical Tests}
Consider the probability of getting a false positive on a test for a rare condition. Suppose 0.1\% of the population have condition $A$, and we have a test which is positive for 98\% of the affected population, and 1\% of those unaffected by the disease. Picking an individual at random that that they suffer from $A$, given that they have a positive test?

We define $A$ to be the set of individuals suffering from the condition, and $P$ is the set of individuals testing positive. Then by Bayes' formula,
\[ \prob{A \mid P} = \frac{\prob{P \mid A}\prob{A}}{\prob{P \mid A}\prob{A} + \prob{P \mid \stcomp{A}}\prob{\stcomp{A}}} = \frac{0.98 \cdot 0.001}{0.98 \cdot 0.001 + 0.01 \cdot 0.999} \approx 0.09 = 9\% \]
Why is this so low? We can rewrite this instance of Bayes' formula as
\[ \prob{A \mid P} = \frac{1}{1 + \frac{\prob{P \mid \stcomp{A}}\prob{\stcomp{A}}}{\prob{P \mid A}\prob{A}}} \]
Here, $\prob{\stcomp{A}} \approx 1, \prob{P \mid A} \approx 1$. So
\[ \prob{A \mid P} \approx \frac{1}{1 + \frac{\prob{P \mid \stcomp{A}}}{\prob{A}}} \]
So this is low because the probability that $\prob{P \mid \stcomp{A}} \gg \prob{A}$. Suppose that there is a population of 1000 people and about 1 suffers from the disease. Among the 999 not suffering from $A$, about 10 will test positive. So there will be about 11 people who test positive, and only 1 out of 11 (9\%) of those actually has the disease.

\subsection{Probability Changes under Extra Knowledge}
Consider these three statements:
\begin{enumerate}[(a)]
    \item I have two children, (at least) one of whom is a boy.
    \item I have two children, and the eldest one is a boy.
    \item I have two children, one of whom is a boy born on a Thursday.
\end{enumerate}
What is the probability that I have two boys, given $a$, $b$ or $c$? Since no further information is given, we will assume that all outcomes are equally likely. We define:
\begin{itemize}
    \item $BG$ is the event that the elder sibling is a boy, and the younger is a girl;
    \item $GB$ is the event that the elder sibling is a girl, and the younger is a boy;
    \item $BB$ is the event that both children are boys; and
    \item $GG$ is the event that both children are girls.
\end{itemize}
Now, we have
\begin{enumerate}[(a)]
    \item $\prob{BB \mid BB \cup BG \cup GB} = \frac{1}{3}$
    \item $\prob{BB \mid BB \cup BG} = \frac{1}{2}$
    \item Let us define $GT$ to be the event that the elder sibling is a girl, and the younger is a boy born on a Thursday, and define $TN$ to be the event that the elder sibling is a boy born on a Thursday and the younger is a boy not born on a Thursday, and other events are defined similarly. So
          \begin{align*}
              \prob{TT \cup TN \cup NT \mid GT \cup TG \cup TT \cup TN \cup NT} & = \frac{\prob{TT \cup TN \cup NT}}{\prob{GT \cup TG \cup TT \cup TN \cup NT}}                                                                                                                                                                                \\
                                                                                & = \frac{\frac{1}{2}\frac{1}{7}\frac{1}{2}\frac{1}{7} + 2 \cdot \frac{1}{2}\frac{1}{7}\frac{1}{2}\frac{6}{7}}{2\cdot \frac{1}{2}\frac{1}{2}\frac{1}{7} + \frac{1}{2}\frac{1}{7}\frac{1}{2}\frac{1}{7} + 2 \cdot \frac{1}{2}\frac{1}{7}\frac{1}{2}\frac{6}{7}} \\
                                                                                & = \frac{13}{27} \approx 48\%
          \end{align*}
\end{enumerate}

\subsection{Simpson's Paradox}
Consider admissions by men and women from state and independent schools to a university given by the tables
\medskip\begin{center}
    \begin{tabular}{c | c c c}
        All applicants & Admitted & Rejected & \% Admitted \\ \hline
        State          & 25       & 25       & 50\%        \\
        Independent    & 28       & 22       & 56\%        \\
    \end{tabular}
\end{center}
\medskip\begin{center}
    \begin{tabular}{c | c c c}
        Men only    & Admitted & Rejected & \% Admitted \\ \hline
        State       & 15       & 22       & 41\%        \\
        Independent & 5        & 8        & 38\%        \\
    \end{tabular}
\end{center}
\medskip\begin{center}
    \begin{tabular}{c | c c c}
        Women only  & Admitted & Rejected & \% Admitted \\ \hline
        State       & 10       & 3        & 77\%        \\
        Independent & 23       & 14       & 62\%        \\
    \end{tabular}
\end{center}
\medskip\noindent This is seemingly a paradox; both women and men are more likely to be admitted if they come from a state school, but when looking at all applicants, they are more likely to be admitted if they come from an independent school. This is called Simpson's paradox; it arises when we aggregate data from disparate populations. Let $A$ be the event that an individual is admitted, $B$ be the event that an individual is a man, and $C$ be the event that an individual comes from a state school. We see that
\begin{align*}
    \prob{A \mid B \cap C}          & > \prob{A \mid B \cap \stcomp{C}}          \\
    \prob{A \mid \stcomp{B} \cap C} & > \prob{A \mid \stcomp{B} \cap \stcomp{C}} \\
    \prob{A \mid C}                 & < \prob{A \mid \stcomp{C}}
\end{align*}
First, note that
\begin{align*}
    \prob{A \mid C} & = \prob{A \cap B \mid C} + \prob{A \cap \stcomp{B} \mid C}                                                                           \\
                    & = \frac{\prob{A \cap B \cap C}}{\prob{C}} + \frac{\prob{A \cap \stcomp{B} \cap C}}{\prob{C}}                                         \\
                    & = \frac{\prob{A \mid B \cap C} \prob{B \cap C}}{\prob{C}} + \frac{\prob{A \mid \stcomp{B} \cap C}\prob{\stcomp{B} \cap C}}{\prob{C}} \\
                    & = \prob{A \mid B \cap C} \prob{B \mid C} + \prob{A \mid \stcomp{B} \cap C} \prob{\stcomp{B} \mid C}                                  \\
                    & > \prob{A \mid B \cap \stcomp{C}} \prob{B \mid C} + \prob{A \mid \stcomp{B} \cap \stcomp{C}} \prob{\stcomp{B} \mid C}
\end{align*}
Let us also assume that $\prob{B \mid C} = \prob{B \mid \stcomp{C}}$. Then
\begin{align*}
    \prob{A \mid C} & > \prob{A \mid B \cap \stcomp{C}} \prob{B \mid \stcomp{C}} + \prob{A \mid \stcomp{B} \cap \stcomp{C}} \prob{\stcomp{B} \mid \stcomp{C}} \\
                    & = \prob{A \mid \stcomp{C}}
\end{align*}
So we needed to further assume that $\prob{B \mid C} = \prob{B \mid \stcomp{C}}$ in order for the `intuitive' result to hold. The assumption was not valid in the example, so the result did not hold.

\end{document}