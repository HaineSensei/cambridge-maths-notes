\documentclass{article}

\input{../util.tex}

\title{Probability}
\author{Cambridge University Mathematical Tripos: Part IA}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Probability Spaces}
\subsection{Probability Spaces and $\sigma$-algebras}
\begin{definition}
	Suppose $\Omega$ is a set, and $\mathcal F$ is a collection of subsets of $\Omega$. We call $\mathcal F$ a $\sigma$-algebra if
	\begin{enumerate}
		\item $\Omega \in \mathcal F$
		\item if $A \in \mathcal F$, then $\stcomp{A} \in \mathcal F$
		\item for any countable collection $(A_n)_{n \geq 1}$ with $A_n \in \mathcal F$ for all $n$, we must also have that $\bigcup_n A_n \in \mathcal F$
	\end{enumerate}
\end{definition}
\begin{definition}
	Suppose that $\mathcal F$ is a $\sigma$-algebra on $\Omega$. A function $\mathbb P \colon \mathcal F \to [0, 1]$ is called a probability measure if
	\begin{enumerate}
		\item $\prob{\Omega} = 1$
		\item for any countable disjoint collection of sets $(A_n)_{n \geq 1}$ in $\mathcal F$ ($A_n \in \mathcal F$ for all $n$), then $\prob{\bigcup_{n \geq 1}A_n} = \sum_{n \geq 1} \prob{A_n}$ (this is called `countable additivity')
	\end{enumerate}
	We say that $\prob{A}$ is `the probability of $A$'. We call $(\Omega, \mathcal F, \mathbb P)$ a probability space, where $\Omega$ is the sample space, $\mathcal F$ is the $\sigma$-algebra, and $\mathbb P$ is the probability measure.
\end{definition}

\begin{remark}
	When $\Omega$ is countable, we take $\mathcal F$ to be all subsets of $\Omega$, i.e. $\mathcal F = \mathcal P(\Omega)$, its power set.
\end{remark}
\begin{definition}
	The elements of $\Omega$ are called outcomes, and the elements of $\mathcal F$ are called events.
\end{definition}
Note that $\mathbb P$ is dependent on $\mathcal F$ but not on $\Omega$. We talk about probabilities of \textit{events}, not probabilities of \textit{outcomes}. For example, if you pick a uniform number from the interval $[0, 1]$; then the probability of getting any specific outcome is zero --- but we can define useful events that have non-zero probabilities.

\subsection{Properties of the Probability Measure}
\begin{itemize}
	\item $\prob{\stcomp{A}} = 1 - \prob{A}$, since $A$ and $\stcomp{A}$ are disjoint sets, whose union is $\Omega$
	\item $\prob{\varnothing} = 0$, since it is the complement of $\Omega$
	\item if $A \subseteq B$, then $\prob{A} \leq \prob{B}$
	\item $\prob{A \cup B} = \prob{A} + \prob{B} - \prob{A \cap B}$ using the Inclusion-Exclusion theorem
\end{itemize}

\subsection{Examples of Probability Spaces}
\begin{itemize}
	\item Rolling a fair 6-sided die:
	      \begin{itemize}
		      \item $\Omega = \{ 1, 2, 3, 4, 5, 6 \}$
		      \item $\mathcal F = \mathcal P(\Omega)$
		      \item $\forall \omega \in \Omega, \prob{\{ \omega \}} = \frac{1}{6}$, and if $A \subseteq \Omega$ then $\prob{A} = \frac{\abs{A}}{6}$
	      \end{itemize}

	\item Equally likely outcomes (more generally). Suppose $\Omega$ is some finite set, e.g. $\Omega = \{ \omega_1, \omega_2, \dots, \omega_n \}$. Then we define $\prob{A} = \frac{\abs{A}}{\abs{\Omega}}$. In classical probability, this models picking a random element of $\Omega$.

	\item Picking balls from a bag. Suppose we have $n$ balls with $n$ labels from the set $\{1, \dots, n\}$, indistinguishable by touching. Let us pick $k \leq n$ balls at random from the bag, \textit{without replacement}. Here, `at random' just means that all possible outcomes are equally likely, and their probability measures should be equal.

	      We will take $\Omega = \{ A \subseteq \{1, \dots, n\} : \abs{A} = k \}$. Then $\abs{\Omega} = \binom{n}{k}$. Then of course $\prob{\{ \omega \}} = \frac{1}{\abs*{\Omega}}$, since all outcomes (combinations, in this case) are equally likely.

	\item Consider a well-shuffled deck of 52 cards, i.e. it is equally likely that each possible permutation of the 52 cards will appear. $\Omega = \{ \text{all permutations of 52 cards} \}$, and $\abs*{\Omega} = 52!$

	      The probability that the top two cards are aces is therefore $\frac{4 \times 3 \times 50!}{52!} = \frac{1}{221}$, since there are $4 \times 3 \times 50!$ outcomes that produce such an event.

	\item Consider a string of $n$ random digits from $\{0, \dots, 9\}$. Then $\Omega = \{ 0, \dots, 9 \}^n$, and $\abs*{\Omega} = 10^n$. We define $A_k = \{ \text{no digit exceeds } k \}$, and $B_k = \{ \text{largest digit is } k \}$. Then $\prob{B_k} = \frac{\abs*{B_k}}{\abs*{\Omega}}$. Notice that $B_k = A_k \setminus A_{k-1}$. $\abs*{A_k} = (k+1)^n$, so $\abs*{B_k} = (k+1)^n - k^n$, so $\prob{B_k} = \frac{(k+1)^n - k^n}{10^n}$.

	\item The birthday problem. There are $n$ people; what is the probability that at least two of them share a birthday? We assume that each year has exactly 365 days, i.e. nobody is born on 29\textsuperscript{th} of February, and that the probabilities of being born on any given day are equal.

	      Let $\Omega = \{1, \dots, 365\}^n$, and $\mathcal F = \mathcal P(\Omega)$. Since all outcomes are equally likely, we take $\prob{\{\omega\}} = \frac{1}{365^n}$.

	      Let $A = \{ \text{at least two people share the same birthday} \}$. $\stcomp{A} = \{ \text{all } n \text{ birthdays are different} \}$. Since $\prob{A} = 1 - \prob{\stcomp{A}}$, it suffices to calculate $\prob{\stcomp{A}}$, which is $\frac{\abs*{\stcomp{A}}}{\abs*{\Omega}} = \frac{365!}{(365 - n)!365^n}$. So the answer is $\prob{A} = 1 - \frac{365!}{(365 - n)!365^n}$

	      Note that at $n=22$, $\prob{A} \approx 0.476$ and at $n=23$, $\prob{A} \approx 0.507$. So when there are at least 23 people in a room, the probability that two of them share a birthday is around 50\%.
\end{itemize}

\subsection{Combinatorial Analysis}
Let $\Omega$ be a finite set, and suppose $\abs*{\Omega} = n$. We want to partition $\Omega$ into $k$ disjoint subsets $\Omega_1, \dots, \Omega_k$ with $\abs*{\Omega_i} = n_i$ and $\sum_{i=1}^k n_i = n$. How many ways of doing such a partition are there? The result is
\[ \underbrace{\binom{n}{n_1}}_{\text{choose first set}}\underbrace{\binom{n-n_1}{n_2}}_{\text{choose second set}}\dots\underbrace{\binom{n-(n_1 + n_2 + \dots + n_{k+1})}{n_k}}_{\text{choose last set}} = \frac{n!}{n_1!n_2!\dots n_k!} \]
So we will write
\[ \binom{n}{n_1, \dots, n_k} = \frac{n!}{n_1!n_2!\dots n_k!} \]

Now, let $f\colon \{1, \dots, k\} \to \{1, \dots, n\}$. $f$ is strictly increasing if $x < y \implies f(x) < f(y)$. $f$ is increasing if $x < y \implies f(x) \leq f(y)$. How many strictly increasing functions $f$ exist? Note that if we know the range of $f$, the function is completely determined. The range is a subset of $\{1, \dots, n\}$ of size $k$, i.e. a $k$-subset of an $n$-set, which yields $\binom{n}{k}$ choices, and thus there are $\binom{n}{k}$ strictly increasing functions.

How many increasing functions $f$ exist? Let us define a bijection from the set of increasing functions $\{f\colon \{1, \dots, k\} \to \{1, \dots, n\}\}$ to the set of \textit{strictly} increasing functions $\{g\colon \{1, \dots, k\} \to \{1, \dots, n+k-1\}\}$. For any increasing function $f$, we define $g(i) = f(i) + i - 1$. Then $g$ is clearly strictly increasing, and takes values in the range $\{1, \dots, n+k-1\}$. By extension, we can define an increasing function $f$ from any strictly increasing function $g$. So the total number of increasing functions $f\colon \{1, \dots, k\} \to \{1, \dots, n\}$ is $\binom{n+k-1}{k}$.

\section{Stirling's Formula and Countable Subadditivity}
\subsection{Stirling's Formula}
Let $(a_n)$ and $(b_n)$ be sequences. We will write $a_n \sim b_n$ if $\frac{a_n}{b_n} \to 1$ as $n \to \infty$. This is asymptotic equality.
\begin{theorem}[Stirling's Formula]
	$n! \sim n^n\, \sqrt{2 \pi n}\, e^{-n}$ as $n \to \infty$.
\end{theorem}
\noindent Let us first prove the weaker statement $\log (n!) \sim n \log n$.
\begin{proof}
	Let us define $l_n = \log (n!) = \log 2 + \log 3 + \dots + \log n$. For $x \in \mathbb R$, we write $\floor{x}$ for the integer part of $x$. Note that
	\[ \log \floor x \leq \log x \leq \log \floor{x+1} \]
	Let us integrate this from 1 to $n$.
	\[ \sum_{k=1}^{n-1} \log k \leq \int_1^n \log x\, \dd{x} \leq \sum_{k=2}^{n} \log k \]
	\[ l_{n-1} \leq n \log n - n + 1 \leq l_n \]
	For all $n$, therefore:
	\[ n \log n - n + 1 \leq l_n \leq (n+1) \log (n+1) - (n+1) + 1 \]
	Dividing through by $n\log n$, we get
	\[ \frac{l_n}{n \log n} \to 1 \]
	as $n \to \infty$.
\end{proof}
\noindent The following complete proof is non-examinable.
\begin{proof}
	For any twice-differentiable function $f$, for any $a < b$ we have
	\[ \int_a^b f(x) \dd{x} = \frac{f(a) + f(b)}{2} (b - a) - \frac{1}{2}\int_a^b (x-a)(b-x)f''(x)\dd{x} \]
	Now let $f(x) = \log x$, $a=k$ and $b=k+1$. Then
	\begin{align*}
		\int_k^{k+1} \log x \dd{x} & = \frac{\log k + \log(k+1)}{2} + \frac{1}{2}\int_k^{k+1} \frac{(x-k)(k+1-x)}{x^2}\dd{x}                            \\
		                           & = \frac{\log k + \log(k+1)}{2} + \frac{1}{2}\int_0^1 \frac{x(1-x)}{(x+k)^2}\dd{x}                                  \\
		\intertext{Let us take the sum for $k=1, \dots, n-1$ of the equality.}
		\int_1^n \log x \dd{x}     & = \frac{\log ((n-1)!) + \log(n!)}{2} + \frac{1}{2}\sum_{k=1}^{n-1}\int_0^1 \frac{x(1-x)}{(x+k)^2}\dd{x}            \\
		n\log n - n + 1            & = \log (n!) - \frac{\log n}{2} + \sum_{k=1}^{n-1} a_k;\quad a_k = \frac{1}{2}\int_0^1 \frac{x(1-x)}{(x+k)^2}\dd{x} \\
		\log (n!)                  & = n \log n - n + \frac{\log n}{2} + 1 - \sum_{k=1}^{n-1} a_k                                                       \\
		n!                         & = n^n \, e^{-n} \, \sqrt n \, \exp\left( 1 - \sum_{k=1}^{n-1} a_k \right)
	\end{align*}
	Now, note that
	\[ a_k \leq \frac{1}{2}\int_0^1 \frac{x(1-x)}{k^2}\dd{x} = \frac{1}{12k^2} \]
	So the sum of all $a_k$ converges. We set
	\[ A = \exp\left( 1 - \sum_{k=1}^\infty a_k \right) \]
	and then
	\[ n! = n^n \, e^{-n} \, \sqrt n \, A \, \exp\left( \underbrace{\sum_{k=n}^\infty a_k}_{\text{converges to zero}} \right) \]
	Therefore,
	\[ n! \sim n^n\, \sqrt{n}\, e^{-n}\, A \]
	To finish the proof, we must show that $A = \sqrt{2 \pi}$. We can utilise the fact that $n! \sim n^n\, \sqrt{n}\, e^{-n}\, A$.
	\begin{align*}
		2^{-2n} \binom{2n}{n} & = 2^{-2n} \cdot \frac{2n!}{(n!)^2}                                                                                           \\
		                      & \sim 2^{-2n} \frac{(2n)^{2n} \cdot \sqrt{2n} \cdot A \cdot e^{-2n}}{n^n\, e^{-n}\, \sqrt n\, A\, n^n\, e^{-n}\, \sqrt n\, A} \\
		                      & = \frac{\sqrt{2}}{A\sqrt{n}}
	\end{align*}
	Using a different method, we will prove that $2^{-2n} \binom{2n}{n} \sim \frac{1}{\sqrt{\pi n}}$, which then forces $A = \sqrt{2\pi}$. Consider
	\[ I_n = \int_0^{\frac{\pi}{2}} (\cos \theta)^n \, \dd \theta;\quad n \geq 0 \]
	So $I_0 = \frac{\pi}{2}$ and $I_1 = 1$. By integrating by parts,
	\[ I_n = \frac{n-1}{n}I_{n-2} \]
	Therefore,
	\[ I_{2n} = \frac{2n-1}{2n}I_{2n-2} = \frac{(2n-1)(2n-3)\dots(3)(1)}{(2n)(2n-2)\dots(2)}I_0 \]
	Multiplying the numerator and denominator by the denominator, we have
	\[ I_{2n} = \frac{(2n)!}{(n! \cdot 2^n)^2} \cdot \frac{\pi}{2} = 2^{-2n} \frac{2n}{n} \cdot \frac{\pi}{2} \]
	In the same way, we can deduce that
	\[ I_{2n+1} = \frac{(2n)(2n-2)\dots(2)}{(2n+1)(2n-1)\dots(3)(1)}I_1 = \frac{1}{2n+1} \left( 2^{-2n} \binom{2n}{n} \right)^{-1} \]
	From $I_n = \frac{n-1}{n}I_{n-2}$, we get that
	\[ \frac{I_n}{I_{n-2}} \to 1 \]
	as $n \to \infty$. We now want to show that $\frac{I_{2n}}{I_{2n+1}} \to 1$. We see from the definition of $I_n$ that $I$ is a decreasing function of $n$. Therefore,
	\[ \frac{I_{2n}}{I_{2n+1}} \leq \frac{I_{2n-1}}{I_{2n+1}} \to 1 \]
	and also
	\[ \frac{I_{2n}}{I_{2n+1}} \geq \frac{I_{2n}}{I_{2n-2}} \to 1 \]
	So
	\[ \frac{I_{2n}}{I_{2n+1}} \to 1 \]
	which means that
	\[ \frac{2^{-2n} \binom{2n}{n} \frac{\pi}{2}}{\left( 2^{-2n} \binom{2n}{n} \right)^{-1} \frac{1}{2n+1}} \to 1 \implies \left( 2^{-2n} \binom{2n}{n} \right)^2 \frac{\pi}{2} (2n+1) \to 1 \]
	Therefore,
	\[ \left( 2^{-2n} \binom{2n}{n} \right)^2 \sim \frac{2}{\pi (2n+1)} \sim \frac{1}{\pi n} \]
	Finally,
	\[ A = \sqrt{2 \pi} \]
	completes the proof.
\end{proof}

\subsection{Countable Subadditivity}
Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space, and let $(A_n)_{n \geq 1}$ be a (not necessarily disjoint) sequence of events in $\mathcal F$. Then
\[ \prob{\bigcup_{n=1}^\infty A_n} \leq \sum_{n=1}^\infty \prob{A_n} \]
\begin{proof}
	Let us define a new sequence $B_1 = A_1$ and $B_n = A_n \setminus (A_1 \cup A_2 \cup \dots \cup A_{n-1})$. So by construction, the sequence $B_n$ is a disjoint sequence of events in $\mathcal F$. Note further that the union of all $B_n$ is equal to the union of all $A_n$. So
	\[ \prob{\bigcup_{n=1}^\infty A_n} = \prob{\bigcup_{n=1}^\infty B_n} \]
	By the countable additivity axiom,
	\[ \prob{\bigcup_{n=1}^\infty B_n} = \sum_{n=1}^\infty \prob{B_n} \]
	But $B_n \subseteq A_n$. So $\prob{B_n} \leq A_n$. Therefore,
	\[ \prob{\bigcup_{n=1}^\infty A_n} \leq \sum_{n=1}^\infty \prob{A_n} \]
\end{proof}

\section{Inclusion-Exclusion}
\subsection{Continuity of Probability Measures}
Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $(A_n)_{n \geq 1}$ be an increasing sequence in $\mathcal F$, i.e. $A_n \in \mathcal F$, and $A_n \subseteq A_{n+1}$. Then $\prob{A_n} \leq \prob{A_{n+1}}$. We want to show that
\[ \lim_{n \to \infty}\prob{A_n} = \prob{\bigcup_n A_n} \]
\begin{proof}
	Let $B_1 = A_1$, and for all $n \geq 2$, let $B_n = A_n \setminus (A_1 \cup A_2 \cup \dots \cup A_{n-1})$. Then the union over $B_i$ up to $n$ is equal to the union over $A_i$ up to $n$. So
	\[ \prob{A_n} = \prob{\bigcup_{k=1}^n B_k} = \sum_{k=1}^n \prob{B_k} \to \sum_{k=1}^\infty \prob{B_k} = \prob{\bigcup_n B_n} = \prob{\bigcup_n A_n} \]
\end{proof}
\noindent We can say that probability measures are continuous; an increasing sequence of events has a probability which tends to some limit. Similarly, if $(A_n)$ is decreasing, then the limit probability is the probability of the intersection of all $A_n$.

\subsection{Inclusion-Exclusion Formula}
Suppose that $A, B \in \mathcal F$. Then $\prob{A \cup B} = \prob{A} + \prob{B} - \prob{A \cap B}$. Now let also $C \in \mathcal F$. Then
\begin{align*}
	\prob{A \cup B \cup C} & = \prob{A \cup B} + \prob{C} - \prob{(A \cup B) \cap C} \\
	                       & = \prob{A} + \prob{B} + \prob{C}                        \\
	                       & - \prob{A \cap B} - \prob{A \cap C} - \prob{B \cap C}   \\
	                       & + \prob{A \cap B \cap C}
\end{align*}
Let $A_1, \dots, A_n$ be events in $\mathcal F$. Then
\begin{align*}
	\prob{\bigcup_{i=1}^n A_i} & = \sum_{i=1}^n \prob{A_i}                                                      \\
	                           & - \sum_{1 \leq i_1 < i_2 \leq n}\prob{A_{i_1} \cap A_{i_2}}                    \\
	                           & + \sum_{1 \leq i_1 < i_2 < i_3 \leq n}\prob{A_{i_1} \cap A_{i_2} \cap A_{i_3}} \\
	                           & - \cdots                                                                       \\
	                           & + (-1)^{n+1}\prob{A_1 \cap \dots \cap A_n}
\end{align*}
Or more concisely,
\[ \prob{\bigcup_{i=1}^n A_i} = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < i_2 < \dots < i_k \leq n} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \]

\begin{proof}
	The case for $n=2$ has been verified, so we can use induction on $n$. Now, let us assume this holds for $n-1$ events.
	\begin{align*}
		\prob{\bigcup_{i=1}^n A_i} & = \prob{\left( \bigcup_{i=1}^{n-1} A_i  \right) \cup A_n}                                                     \\
		                           & = \prob{\bigcup_{i=1}^{n-1} A_i} + \prob{A_n} - \prob{\left( \bigcup_{i=1}^{n-1} A_i  \right) \cap A_n}       \\
		                           & = \prob{\bigcup_{i=1}^{n-1} A_i} + \prob{A_n} - \prob{\bigcup_{i=1}^{n-1} (A_i \cap A_n)}                     \\
		\intertext{Let $B_i = A_i \cap A_n$ for all $i$. By the inductive hypothesis, we have}
		\prob{\bigcup_{i=1}^n A_i} & = \prob{\bigcup_{i=1}^{n-1} A_i} + \prob{A_n} - \prob{\bigcup_{i=1}^{n-1} B_n}                                \\
		                           & = \sum_{k=1}^{n-1} (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n-1} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \\&- \sum_{k=1}^{n-1} (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n-1} \prob{B_{i_1} \cap \dots \cap B_{i_k}} \\ &+ \prob{A_n}
	\end{align*}
	which gives the clain as required.
\end{proof}

Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space with $\abs{\Omega} < \infty$ and $\prob{A} = \frac{\abs{A}}{\abs{\Omega}}$. Let $A_1, \dots, A_n \in \mathcal F$. Then
\[ \abs{A_1 \cup \dots \cup A_n} = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} \abs{A_{i_1} \cap \dots \cap A_{i_k}} \]

\subsection{Bonferroni Inequalities}
Truncating the sum in the inclusion-exclusion formula at the $r$th term yields an estimate for the probability. The Bonferroni Inequalities state that if $r$ is odd, it is an overestimate, and if $r$ is even, it is an underestimate.
\begin{align*}
	r \text{ odd}  & \implies \prob{\bigcup_{i=1}^n A_i} \leq \sum_{k=1}^r (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \\
	r \text{ even} & \implies \prob{\bigcup_{i=1}^n A_i} \geq \sum_{k=1}^r (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} \prob{A_{i_1} \cap \dots \cap A_{i_k}}
\end{align*}
\begin{proof}
	Again, we will use induction. The $n=2$ case is trivial. Suppose that $r$ is odd. Then
	\begin{equation}
		\prob{\bigcup_{i=1}^n A_i} = \prob{\bigcup_{i=1}^{n-1} A_i} + \prob{A_n} - \prob{\bigcup_{i=1}^n B_i} \tag{$\ast$}
	\end{equation}
	where $B_i = A_i \cap A_n$. Since $r$ is odd,
	\[ \prob{\bigcup_{i=1}^{n-1} A_i} \leq \sum_{k=1}^r (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n-1} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \]
	Since $r-1$ is even, we can apply the inductive hypothesis to $\prob{\bigcup_{i=1}^{n-1} B_i}$.
	\[ \prob{\bigcup_{i=1}^{n-1} B_i} \geq \sum_{k=1}^{r-1} (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n-1} \prob{B_{i_1} \cap \dots \cap B_{i_k}} \]
	We can substitute both bounds into ($\ast$) to get an overestimate.
\end{proof}

\subsection{Counting using Inclusion-Exclusion}
We can apply the Inclusion-Exclusion formula to count various things. How many functions $f \colon \{ 1, \dots, n \} \to \{ 1, \dots, m \}$ are surjective? Let $\Omega$ be the set of such functions, and $A = \{ f \in \Omega : f \text{ is a surjection} \}$. For all $i \in \{ 1, \dots, m \}$, we define $A_i = \{ f \in \Omega : i \notin \{ f(1), f(2), \dots, f(n) \} \}$. Then $A = \stcomp{A_1} \cap \stcomp{A_2} \cap \dots \cap \stcomp{A_m} = \stcomp{(A_1 \cup A_2 \cup \dots \cup A_m)}$. Then
\[ \abs{A} = \abs{\Omega} - \abs{A_1 \cup \dots \cup A_m} = m^n - \abs{A_1 \cup \dots \cup A_m} \]
Now, let us use the Inclusion-Exclusion formula.
\begin{align*}
	\abs{A_1 \cup \dots \cup A_m} & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} \abs{A_{i_1} \cap \dots \cap A_{i_k}} \\
	\intertext{Note that $A_{i_1} \cap \dots \cap A_{i_k}$ is the set of functions where $k$ distinct numbers are not included in the function's range. There are $(m-k)^n$ such functions.}
	\abs{A_1 \cup \dots \cup A_m} & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < \dots < i_k \leq n} (m-k)^n                               \\
	                              & = \sum_{k=1}^n (-1)^{k+1} \binom{m}{k} (m-k)^n                                                         \\
	\abs{A}                       & = m^n -  \sum_{k=1}^n (-1)^{k+1} \binom{m}{k} (m-k)^n                                                  \\
	\abs{A}                       & = \sum_{k=0}^n (-1)^k \binom{m}{k} (m-k)^n
\end{align*}

\section{Independence and Dependence of Events}
\subsection{Counting Derangements}
A derangement is a permutation which has no fixed point, i.e. $\forall i, \sigma(i) \neq i$. We will let $\Omega$ be the set of permutations of $\{1, \dots, n\}$, i.e. $S_n$. Let $A$ be the set of derangements in $\Omega$. Let us pick a permutation $\sigma$ at random from $\Omega$. What is the probability that it is a derangement? We define $A_i = \{ f \in \Omega \colon f(i) = i \}$, then $A = \stcomp{A_1} \cap \dots \stcomp{A_n} = \stcomp{\left( \bigcup_{i=1}^n A_i  \right)}$, so $\prob{A} = 1 - \prob{\bigcup_{i=1}^n A_i}$. By the inclusion-exclusion formula,
\begin{align*}
	\prob{\bigcup_{i=1}^n A_i} & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < i_2 < \dots < i_k \leq n} \prob{A_{i_1} \cap \dots \cap A_{i_k}} \\
	                           & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < i_2 < \dots < i_k \leq n} \frac{(n-k)!}{\abs{\Omega}}            \\
	                           & = \sum_{k=1}^n (-1)^{k+1} \sum_{1 \leq i_1 < i_2 < \dots < i_k \leq n} \frac{(n-k)!}{n!}                      \\
	                           & = \sum_{k=1}^n (-1)^{k+1} \binom{n}{k} \frac{(n-k)!}{n!}                                                      \\
	                           & = \sum_{k=1}^n (-1)^{k+1} \frac{n!}{k!(n-k)!} \cdot \frac{(n-k)!}{n!}                                         \\
	                           & = \sum_{k=1}^n (-1)^{k+1} \frac{1}{k!}                                                                        \\
\end{align*}
So
\[ \prob{A} = 1 - \prob{\bigcup_{i=1}^n A_i} = 1 - \sum_{k=1}^n \frac{(-1)^{k+1}}{k!} = \sum_{k=0}^n \frac{(-1)^k}{k!} \]
As $n \to \infty$, this value tends to $e^{-1} \approx 0.3678$.

\subsection{Independence of Events}
\begin{definition}
	Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $A, B \in \mathcal F$. $A$ and $B$ are called independent if
	\[ \prob{A \cap B} = \prob{A} \cdot \prob{B} \]
	We write $A \perp B$, or $A \perp\!\!\!\perp B$. A countable collection of events $(A_n)$ is said to be independent if for all distinct $i_1, \dots, i_k$, we have
	\[ \prob{A_{i_1} \cap \dots \cap A_{i_k}} = \prod_{j=1}^k \prob{A_{i_j}} \]
\end{definition}
\begin{remark}
	To show that a collection of events is independent, it is insufficient to show that events are pairwise independent. For example, consider tossing a fair coin twice, so $\Omega = \{ (0, 0), (0, 1), (1, 0), (1, 1) \}$. $\prob{\{\omega\}} = \frac{1}{4}$. Consider the events $A, B, C$ where
	\[ A = \{ (0, 0), (0, 1) \};\quad B = \{ (0, 0), (1, 0) \};\quad C = \{ (1, 0), (0, 1) \} \]
	\[ \prob{A} = \prob{B} = \prob{C} = \frac{1}{2} \]
	\begin{align*}
		\prob{A \cap B} = \prob{\{ (0, 0) \}} & = \frac{1}{4} = \prob{A} \cdot \prob{B} \\
		\prob{A \cap C} = \prob{\{ (0, 1) \}} & = \frac{1}{4} = \prob{A} \cdot \prob{C} \\
		\prob{B \cap C} = \prob{\{ (1, 0) \}} & = \frac{1}{4} = \prob{B} \cdot \prob{C}
	\end{align*}
	\[ \prob{A \cap B \cap C} = \prob{\varnothing} = 0 \]
\end{remark}
\begin{claim}
	If $A \perp B$, then $A \perp \stcomp{B}$.
\end{claim}
\begin{proof}
	\begin{align*}
		\prob{A \cap \stcomp{B}} & = \prob{A} - \prob{A \cap B}           \\
		                         & = \prob{A} - \prob{A} \cdot \prob{B}   \\
		                         & = \prob{A} \left( 1 - \prob{B} \right) \\
		                         & = \prob{A} \prob{\stcomp{B}}
	\end{align*}
	as required.
\end{proof}

\subsection{Conditional Probability}
\begin{definition}
	Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $B \in \mathcal F$ with $\prob{B} > 0$. We define the conditional probability of $A$ given $B$, written $\prob{A \mid B}$ as
	\[ \prob{A \mid B} = \frac{\prob{A \cap B}}{\prob{B}} \]
\end{definition}
\noindent Note that if $A$ and $B$ are independent, then
\[ \prob{A \mid B} = \frac{\prob{A \cap B}}{\prob{B}} = \frac{\prob{A} \cdot \prob{B}}{\prob{B}} = \prob{A} \]
\begin{claim}
	Suppose that $(A_n)$ is a disjoint sequence in $\mathcal F$. Then
	\[ \prob{\bigcup A_n \mathrel{\Big|} B} = \sum_{n} \prob{A_n \mid B} \]
	This is the countable additivity property for conditional probability.
\end{claim}
\begin{proof}
	\begin{align*}
		\prob{\bigcup A_n \mathrel{\Big|} B} & = \frac{\prob{(\bigcup A_n) \cap B}}{\prob{B}} \\
		                                     & = \frac{\prob{\bigcup (A_n \cap B)}}{\prob{B}} \\
		\intertext{By countable additivity, since that $(A_n \cap B)$ are disjoint,}
		                                     & = \sum_n \frac{\prob{A_n \cap B}}{\prob{B}}    \\
		                                     & = \sum_n \prob{A_n \mid B}
	\end{align*}
\end{proof}
\noindent We can think of $\prob{\dots \mid B}$ as a new probability measure for the same $\Omega$.

\subsection{Law of Total Probability}
\begin{claim}
	Suppose $(B_n)$ is a disjoint collection of events in $\mathcal F$, such that $\bigcup B = \Omega$, and for all $n$, we have $\prob{B_n} > 0$. If $A \in \mathcal F$ then
	\[ \prob{A} = \sum_n \prob{A \mid B_n} \cdot \prob{B_n} \]
\end{claim}
\begin{proof}
	\begin{align*}
		\prob{A} & = \prob{A \cap \Omega}                   \\
		         & = \prob{A \cap \left(\bigcup B_n\right)} \\
		         & = \prob{\bigcup(A \cap B_n)}             \\
		\intertext{By countable additivity,}
		         & = \sum_n \prob{A \cap B_n}               \\
		         & = \sum_n \prob{A \mid B_n} \prob{B_n}
	\end{align*}
\end{proof}

\subsection{Bayes' Formula}
\begin{claim}
	Suppose $(B_n)$ is a disjoint sequence of events with $\bigcup B_n = \Omega$ and $\prob{B_n} > 0$ for all $n$. Then
	\[ \prob{B_n \mid A} = \frac{\prob{A \mid B_n} \prob{B_n}}{\sum_k \prob{A \mid B_k} \prob{B_k}} \]
\end{claim}
\begin{proof}
	\begin{align*}
		\prob{B_n \mid A} & = \frac{\prob{B_n \cap A}}{\prob{A}}                                       \\
		                  & = \frac{\prob{A \mid B_n} \prob{B_n}}{\prob{A}}                            \\
		\intertext{By the Law of Total Probability,}
		                  & = \frac{\prob{A \mid B_n} \prob{B_n}}{\sum_k \prob{A \mid B_k} \prob{B_k}}
	\end{align*}
\end{proof}
\noindent Note that on the right hand side, the numerator appears somewhere in the denominator. This formula is the basis of Bayesian statistics. It allows us to reverse the direction of a conditional probability --- knowing the probabilities of the events $(B_n)$, and given a model of $\prob{A \mid B_n}$, we can calculuate the posterior probabilities of $B_n$ given that $A$ occurs.

\section{Examples of Conditional Probability}
\subsection{Bayes' Formula for Medical Tests}
Consider the probability of getting a false positive on a test for a rare condition. Suppose 0.1\% of the population have condition $A$, and we have a test which is positive for 98\% of the affected population, and 1\% of those unaffected by the disease. Picking an individual at random that that they suffer from $A$, given that they have a positive test?

We define $A$ to be the set of individuals suffering from the condition, and $P$ is the set of individuals testing positive. Then by Bayes' formula,
\[ \prob{A \mid P} = \frac{\prob{P \mid A}\prob{A}}{\prob{P \mid A}\prob{A} + \prob{P \mid \stcomp{A}}\prob{\stcomp{A}}} = \frac{0.98 \cdot 0.001}{0.98 \cdot 0.001 + 0.01 \cdot 0.999} \approx 0.09 = 9\% \]
Why is this so low? We can rewrite this instance of Bayes' formula as
\[ \prob{A \mid P} = \frac{1}{1 + \frac{\prob{P \mid \stcomp{A}}\prob{\stcomp{A}}}{\prob{P \mid A}\prob{A}}} \]
Here, $\prob{\stcomp{A}} \approx 1, \prob{P \mid A} \approx 1$. So
\[ \prob{A \mid P} \approx \frac{1}{1 + \frac{\prob{P \mid \stcomp{A}}}{\prob{A}}} \]
So this is low because the probability that $\prob{P \mid \stcomp{A}} \gg \prob{A}$. Suppose that there is a population of 1000 people and about 1 suffers from the disease. Among the 999 not suffering from $A$, about 10 will test positive. So there will be about 11 people who test positive, and only 1 out of 11 (9\%) of those actually has the disease.

\subsection{Probability Changes under Extra Knowledge}
Consider these three statements:
\begin{enumerate}[(a)]
	\item I have two children, (at least) one of whom is a boy.
	\item I have two children, and the eldest one is a boy.
	\item I have two children, one of whom is a boy born on a Thursday.
\end{enumerate}
What is the probability that I have two boys, given $a$, $b$ or $c$? Since no further information is given, we will assume that all outcomes are equally likely. We define:
\begin{itemize}
	\item $BG$ is the event that the elder sibling is a boy, and the younger is a girl;
	\item $GB$ is the event that the elder sibling is a girl, and the younger is a boy;
	\item $BB$ is the event that both children are boys; and
	\item $GG$ is the event that both children are girls.
\end{itemize}
Now, we have
\begin{enumerate}[(a)]
	\item $\prob{BB \mid BB \cup BG \cup GB} = \frac{1}{3}$
	\item $\prob{BB \mid BB \cup BG} = \frac{1}{2}$
	\item Let us define $GT$ to be the event that the elder sibling is a girl, and the younger is a boy born on a Thursday, and define $TN$ to be the event that the elder sibling is a boy born on a Thursday and the younger is a boy not born on a Thursday, and other events are defined similarly. So
	      \begin{align*}
		      \prob{TT \cup TN \cup NT \mid GT \cup TG \cup TT \cup TN \cup NT} & = \frac{\prob{TT \cup TN \cup NT}}{\prob{GT \cup TG \cup TT \cup TN \cup NT}}                                                                                                                                                                                \\
		                                                                        & = \frac{\frac{1}{2}\frac{1}{7}\frac{1}{2}\frac{1}{7} + 2 \cdot \frac{1}{2}\frac{1}{7}\frac{1}{2}\frac{6}{7}}{2\cdot \frac{1}{2}\frac{1}{2}\frac{1}{7} + \frac{1}{2}\frac{1}{7}\frac{1}{2}\frac{1}{7} + 2 \cdot \frac{1}{2}\frac{1}{7}\frac{1}{2}\frac{6}{7}} \\
		                                                                        & = \frac{13}{27} \approx 48\%
	      \end{align*}
\end{enumerate}

\subsection{Simpson's Paradox}
Consider admissions by men and women from state and independent schools to a university given by the tables
\medskip\begin{center}
	\begin{tabular}{c | c c c}
		All applicants & Admitted & Rejected & \% Admitted \\ \hline
		State          & 25       & 25       & 50\%        \\
		Independent    & 28       & 22       & 56\%        \\
	\end{tabular}
\end{center}
\medskip\begin{center}
	\begin{tabular}{c | c c c}
		Men only    & Admitted & Rejected & \% Admitted \\ \hline
		State       & 15       & 22       & 41\%        \\
		Independent & 5        & 8        & 38\%        \\
	\end{tabular}
\end{center}
\medskip\begin{center}
	\begin{tabular}{c | c c c}
		Women only  & Admitted & Rejected & \% Admitted \\ \hline
		State       & 10       & 3        & 77\%        \\
		Independent & 23       & 14       & 62\%        \\
	\end{tabular}
\end{center}
\medskip\noindent This is seemingly a paradox; both women and men are more likely to be admitted if they come from a state school, but when looking at all applicants, they are more likely to be admitted if they come from an independent school. This is called Simpson's paradox; it arises when we aggregate data from disparate populations. Let $A$ be the event that an individual is admitted, $B$ be the event that an individual is a man, and $C$ be the event that an individual comes from a state school. We see that
\begin{align*}
	\prob{A \mid B \cap C}          & > \prob{A \mid B \cap \stcomp{C}}          \\
	\prob{A \mid \stcomp{B} \cap C} & > \prob{A \mid \stcomp{B} \cap \stcomp{C}} \\
	\prob{A \mid C}                 & < \prob{A \mid \stcomp{C}}
\end{align*}
First, note that
\begin{align*}
	\prob{A \mid C} & = \prob{A \cap B \mid C} + \prob{A \cap \stcomp{B} \mid C}                                                                           \\
	                & = \frac{\prob{A \cap B \cap C}}{\prob{C}} + \frac{\prob{A \cap \stcomp{B} \cap C}}{\prob{C}}                                         \\
	                & = \frac{\prob{A \mid B \cap C} \prob{B \cap C}}{\prob{C}} + \frac{\prob{A \mid \stcomp{B} \cap C}\prob{\stcomp{B} \cap C}}{\prob{C}} \\
	                & = \prob{A \mid B \cap C} \prob{B \mid C} + \prob{A \mid \stcomp{B} \cap C} \prob{\stcomp{B} \mid C}                                  \\
	                & > \prob{A \mid B \cap \stcomp{C}} \prob{B \mid C} + \prob{A \mid \stcomp{B} \cap \stcomp{C}} \prob{\stcomp{B} \mid C}
\end{align*}
Let us also assume that $\prob{B \mid C} = \prob{B \mid \stcomp{C}}$. Then
\begin{align*}
	\prob{A \mid C} & > \prob{A \mid B \cap \stcomp{C}} \prob{B \mid \stcomp{C}} + \prob{A \mid \stcomp{B} \cap \stcomp{C}} \prob{\stcomp{B} \mid \stcomp{C}} \\
	                & = \prob{A \mid \stcomp{C}}
\end{align*}
So we needed to further assume that $\prob{B \mid C} = \prob{B \mid \stcomp{C}}$ in order for the `intuitive' result to hold. The assumption was not valid in the example, so the result did not hold.

\section{Discrete Distributions and Random Variables}
\subsection{Discrete Distributions}
In a discrete probability distribution on a probability space $(\Omega, \mathcal F, \mathbb P)$, $\Omega$ is either finite or countable, i.e. $\Omega = \{ \omega_1, \omega_2, \dots \}$, and as stated before, $\mathcal F$ is the power set of $\Omega$. If we know $\prob{\{\omega_i\}}$, then this completely determines $\mathbb P$. Indeed, let $A \subseteq \Omega$, then
\[ \prob{A} = \prob{\bigcup_{i \colon \omega_i \in A} \{ \omega_i \}} = \sum_{i \colon \omega_i \in A}\prob{\{\omega_i\}} \]
by countable additivity. We will see later that this is not true if $\Omega$ is uncountable. We write $p_i = \prob{\{\omega_i\}}$, and we then call this a discrete probability distribution. It has the following key properties:
\begin{itemize}
	\item $p_i \geq 0$
	\item $\sum_i p_i = 1$
\end{itemize}

\subsection{Bernoulli Distribution}
We model the outcome of a test with two outcomes (e.g. the toss of a coin) with the Bernoulli distribution. Let $\Omega = \{ 0, 1 \}$. We will denote $p = p_1$, then clearly $p_0 = 1 - p$.

\subsection{Binomial Distribution}
The binomial distribution $B$ has parameters $N \in \mathbb Z^+, p \in [0, 1]$. This distribution models a sequence of $N$ independent Bernoulli distributions of parameter $p$. We then count the amount of `successes', i.e. trials in which the result was 1. $\Omega = \{ 0, 1, \dots, N \}$.
\[ \prob{\{ k \}} = p_k = \binom{N}{k}p^k(1-p)^{N-k} \]

\subsection{Multinomial Distribution}
The multinomial distribution is a generalisation of the binomial distribution. $M$ has parameters $N \in \mathbb Z^+$ and $p_1, p_2, \dots \in [0, 1]$ where $\sum_{i=1}^k p_i = 1$. This models a sequence of $N$ independent trials in which a number from 1 to $N$ is selected, where the probability of selecting $i$ is $p_i$. $\Omega = \{ (n_1, \dots, n_k) \in \mathbb N^k \colon \sum_{i=1}^k n_i = N \}$, in other words, ordered partitions of $N$. Therefore
\[ \prob{n_1 \text{ outcomes had value 1}, \dots, n_k \text{ outcomes had value }k} = \prob{(n_1, \dots, n_k)} = \binom{N}{n_1,\dots,n_k}p_1^{n_1}\dots p_k^{n_k} \]

\subsection{Geometric Distribution}
Consider a Bernoulli distribution of parameter $p$. The geometric distribution models running this trial many times independently until the first `success' (i.e. the first result of value 1). Then $\Omega = \{ 1, 2, \dots \} = \mathbb Z^+$. Then
\[ p_k = (1-p)^{k-1}p \]
We can compute the infinite geometric series $\sum p_k$ which gives 1. We could alternatively model the distribution using $\Omega' = \{ 0, 1, \dots \} = \mathbb N$ which records the amount of failures before the first success. Then
\[ p_k' = (1-p)^k p \]
Again, the sum converges to 1.

\subsection{Poisson Distribution}
This is used to model the number of occurences of an event in a given interval of time. $\Omega = \{ 0, 1, 2, \dots \} = \mathbb N$. This distribution has one parameter $\lambda \in \mathbb R$. We have
\[ p_k = e^{-\lambda} \frac{\lambda^k}{k!} \]
Then
\[ \sum_{k=0}^\infty p_k = e^{-\lambda}  \sum_{k=0}^\infty \frac{\lambda^k}{k!} = e^{-\lambda} \cdot e^{\lambda} = 1 \]
Suppose customers arrive into a shop during the time interval $[0, 1]$. We will subdivide $[0, 1]$ into $N$ intervals $\left[ \frac{i-1}{N}, \frac{i}{N} \right]$. In each interval, a single customer arrives with probability $p$, independent of other time intervals. In this example,
\[ \prob{k \text{ customers arrive}} = \binom{N}{k} p^k (1-p)^{N-k} \]
Let $p = \frac{\lambda}{N}$ for $\lambda > 0$. We will show that as $N \to \infty$, this binomial distribution converges to the Poisson distribution.
\begin{align*}
	\binom{N}{k} p^k (1-p)^{N-k} & = \frac{N!}{k!(N-k)!} \left( \frac{\lambda}{n} \right)^k \cdot \left( 1 - \frac{\lambda}{n} \right)^{N-k} \\
	                             & = \frac{\lambda_k}{k!} \cdot \frac{N!}{N^k(N-k)!} \cdot \left( 1 - \frac{\lambda}{N} \right)^{N-k}        \\
	                             & \to \frac{\lambda_k}{k!} \cdot 1 \cdot e^{-\lambda}
\end{align*}
which matches the Poisson distribution.

\subsection{Random Variables}
\begin{definition}
	Consider the probability space $(\Omega, \mathcal F, \mathbb P)$. A random variable $X$ is a function $X \colon \Omega \to \mathbb R$ satisfying
	\[ \{ \omega \in \Omega \colon X(\omega) \leq x \} \in \mathcal F \]
	for any given $x$.
\end{definition}
\noindent Suppose $A \subseteq \mathbb R$. Then typically we write
\[ \{ X \in A \} = \{ \omega \colon X(\omega) \in A \} \]
as shorthand. Given $A \in \mathcal F$, we define the indicator of $A$ to be
\[ 1_A(\omega) = 1(\omega \in A) = \begin{cases}
		1 & \text{if } \omega \in A \\
		0 & \text{otherwise}
	\end{cases} \]
Because $A \in \mathbb F$, $1_A$ is a random variable. Suppose $X$ is a random variable. We define probability distribution function of $X$ to be
\[ F_X \colon \mathbb R \to [0, 1];\quad F_X(x) = \prob{X \leq x} \]
\begin{definition}
	$(X_1, \dots, X_n)$ is called a random variable in $\mathbb R^n$ if $(X_1, \dots, X_n) \colon \Omega \to \mathbb R^n$, and for all $x_1, \dots, x_n \in \mathbb R$ we have
	\[ \{ X_1 \leq x_1, \dots, X_n \leq x_n \} = \{ \omega \colon X_1(\omega) \leq x_1, \dots, X_n(\omega) \leq x_n \} \in \mathcal F \]
\end{definition}
\noindent This definition is equivalent to saying that $X_1, \dots, X_n$ are all random variables in $\mathbb R$. Indeed,
\[ \{ X_1 \leq x_1, \dots, X_n \leq x_n \} = \{ X_1 \leq x_1 \} \cap \dots \cap \{ X_n \leq x_n \} \]
which, since $\mathcal F$ is a $\sigma$-algebra, is an element of $\mathcal F$.

\section{Discrete Random Variables}
\subsection{Definition and Example}
\begin{definition}
	A random variable $X$ is called discrete if it takes values in a countable set. Suppose $X$ takes values in the countable set $S$. For every $x \in S$, we write
	\[ p_x = \prob{X = x} = \prob{\{ \omega \colon X(\omega) = x \}} \]
	We call $(p_x)_{x \in S}$ the probability mass function of $X$, or the distribution of $X$. If $(p_x)$ is Bernoulli for example, then we say that $X$ is a Bernoulli (or such) random variable, or that $X$ has the Bernoulli distribution.
\end{definition}
\begin{definition}
	Suppose $X_1, \dots, X_n$ are discrete random variables taking variables in $S_1, \dots, S_n$. We say that the random variables $X_1, \dots, X_n$ are independent if
	\[ \prob{X_1 = x_1, \dots, X_n = x_n} = \prob{X_1 = x_1} \cdots \prob{X_n = x_n}\quad \forall x_1 \in S_1, \dots, x_n \in S_n \]
\end{definition}
\noindent As an example, suppose we toss a $p$-biased coin $n$ times independently. Let $\Omega = \{ 0, 1 \}^n$. For every $\omega \in \Omega$,
\[ p_\omega = \prod_{k=1}^n p^{\omega_k} (1-p)^{1-\omega_k};\quad \text{where we write } \omega = (\omega_1, \dots, \omega_n) \]
We define a set of discrete random variables $X_k(\omega) = \omega_k$. Then $X_k$ gives the output of the $k$th toss. We have
\[ \prob{X_k = 1} = \prob{\omega_k = 1} = p;\quad \prob{X_k = 0} = \prob{\omega_k = 0} = 1-p \]
So $X_k$ has the Bernoulli distribution with parameter $p$. We can also show that the $X_i$ are independent. Let $x_1, \dots, x_n \in \{ 0, 1 \}$. Then
\begin{align*}
	\prob{X_1 = x_1, \dots, X_n = x_n} & = \prob{\omega = (x_1, \dots, x_n)}   \\
	                                   & = p_{(x_1, \dots, x_n)}               \\
	                                   & = \prod_{k=1}^N p^{x_k} (1-p)^{1-x_k} \\
	                                   & = \prod_{k=1}^N \prob{X_k = x_k}
\end{align*}
as required. Now, we define $S_n(\omega) = X_1(\omega) + \dots + X_n(\omega)$. This is the number of heads in $N$ tosses. So $S_n \colon \Omega \to \{ 0, \dots, N \}$, and
\[ \prob{S_n = k} = \binom{n}{k} p^k (1-p)^{n-k} \]
So $S_n$ has the binomial distribution with parameters $n$ and $p$.

\subsection{Expectation}
Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space such that $\Omega$ is countable. Let $X \colon \Omega \to \mathbb R$ be a random variable, which is necessarily discrete. We say that $X$ is non-negative if $X \geq 0$. We define the expectation of $X$ to be
\[ \expect{X} = \sum_\omega X(\omega) \cdot \prob{\{ \omega \}} \]
We will write
\[ \Omega_X = \{ X(\omega) \colon \omega \in \Omega \} \]
So
\[ \Omega = \bigcup_{x \in \Omega_X} \{ X = x \} \]
So we have partitioned $\Omega$ using $X$. Note that
\[ \expect{X} = \sum_\omega X(\omega) \prob{\{\omega\}} = \sum_{x \in \Omega_X} \sum_{\omega \in \{ X = x\}} X(\omega) \prob{\{\omega\}} = \sum_{x \in \Omega_X} \sum_{\omega \in \{ X = x\}} x \prob{\{\omega\}} = \sum_{x \in \Omega_X} x\prob{\{X = x \}} \]
which matches the more familiar definition of the expectation; the average of the values taken by $X$, weighted by the probability of the event occcuring. So
\[ \expect{X} = \sum_{x \in \Omega_X} x p_x \]

\subsection{Expectation of Binomial Distribution}
Let $X \sim \text{Bin}(N, p)$. Then
\[ \forall k = 0, \dots, N,\quad \prob{X = k} = \binom{N}{k} p^k (1-p)^{N-k} \]
So using the second definition,
\begin{align*}
	\expect{X} & = \sum_{k=0}^N k \prob{X = k}                                                         \\
	           & = \sum_{k=0}^N k \binom{n}{k} p^k (1-p)^{N-k}                                         \\
	           & = \sum_{k=0}^N \frac{k \cdot N!}{k! \cdot (N-k)!} p^k (1-p)^{N-k}                     \\
	           & = \sum_{k=1}^N \frac{(N-1)! \cdot N \cdot p}{(k-1)! \cdot (N-k)!} p^{k-1} (1-p)^{N-k} \\
	           & = Np \sum_{k=1}^N \binom{N-1}{k-1} p^{k-1} (1-p)^{N-k}                                \\
	           & = Np \sum_{k=0}^{N-1} \binom{N-1}{k} p^{k} (1-p)^{N-1-k}                              \\
	           & = Np (p + 1 - p)^{N-1}                                                                \\
	           & = Np
\end{align*}

\subsection{Expectation of Poisson Distribution}
Let $X \sim \text{Poi}(\lambda)$, so
\[ \prob{X = k} = e^{-\lambda} \frac{\lambda^k}{k!} \]
Hence
\begin{align*}
	\expect{X} & = \sum_{k=0}^\infty k e^{-\lambda} \frac{\lambda^k}{k!}              \\
	           & =\sum_{k=1}^\infty e^{-\lambda} \frac{\lambda^{k-1} \lambda}{(k-1)!} \\
	           & = e^{-\lambda} \cdot e^{\lambda} \cdot \lambda                       \\
	           & = \lambda
\end{align*}

\subsection{Expectation of a General Random Variable}
Let $X$ be a general (not necessarily non-negative) discrete random variable. Then we define
\[ X^+ = \max(X, 0);\quad X^- = \max(-X, 0) \]
Then $X = X^+ - X^-$. Note that $X^+$ and $X^-$ are non-negative random variables, which has a well-defined expectation. So if at least one of $\expect{X^+}, \expect{X^-}$ is finite, we define
\[ \expect{X} = \expect{X^+} - \expect{X^-} \]
If both are infinite, then we say that the expectation of $X$ is not defined. Whenever we write $\expect{X}$, it is assumed to be well-defined. If $\expect{\abs{X}} < \infty$, we say that $X$ is integrable. When $\expect{X}$ is well-defined, we have again that
\[ \expect{X} = \sum_{x \in \Omega_x} x \cdot \prob{X = x} \]

\subsection{Properties of the Expectation}
The following properties follow immediately from the definition.
\begin{enumerate}
	\item If $X \geq 0$, then $\expect{X} \geq 0$.
	\item If $X \geq 0$ and $\expect{X} = 0$, then $\prob{X = 0} = 1$.
	\item If $c \in \mathbb R$, then $\expect{cX} = c\expect{X}$, and $\expect{c + X} = c + \expect{X}$.
	\item If $X$, $Y$ are two integrable random variables, then $\expect{X + Y} = \expect{X} + \expect{Y}$.
	\item More generally, let $c_1, \dots, c_n \in \mathbb R$ and $X_1, \dots, X_n$ integrable random variables. Then
	      \[ \expect{c_1X_1 + \dots + c_nX_n} = c_1 \expect{X_1} + \dots + c_n \expect{X_n} \]
	      So the expectation is a linear operator over finitely many inputs.
\end{enumerate}

\section{Expectation and Variance}
\subsection{Countable Additivity for the Expectation}
Suppose $X_1, X_2, \dots$ are non-negative random variables. Then
\[ \expect{\sum_n X_n} = \sum_n \expect{X_n} \]
The non-negativity constraint allows us to guarantee that the sums are well-defined; they could be infinite, but at least their values are well-defined. We will construct a proof assuming that $\Omega$ is countable, however the result holds regardless of the choice of $\Omega$.
\begin{proof}
	\begin{align*}
		\expect{\sum_n X_n} & = \sum_\omega \sum_n X_n(\omega) \prob{\{ \omega \}} \\
		                    & = \sum_n \sum_\omega X_n(\omega) \prob{\{ \omega \}} \\
		                    & = \sum_n \expect{X_n}
	\end{align*}
\end{proof}
\noindent We are allowed to rearrange the sums since all relevant terms are non-negative.

\subsection{Expectation of Indicator Function}
If $X = 1(A)$ where $A \in \mathcal F$, then $\expect{X} = \prob{A}$. This is obvious from the second definition of the expectation.

\subsection{Expectation under Function Application}
If $g \colon \mathbb R \to \mathbb R$, we can define $g(X)$ to be the random variable given by
\[ g(X)(\omega) = g(X(\omega)) \]
Then
\[ \expect{g(X)} = \sum_{x \in \Omega_X} g(x) \cdot \prob{X = x} \]
\begin{proof}
	Let $Y = g(X)$. Then
	\[ \expect{Y} = \sum_{y \in \Omega_Y} y \cdot \prob{Y = y} \]
	Note that
	\begin{align*}
		\{ Y = y \} & = \{ \omega \colon Y(\omega) = y \}              \\
		            & = \{ \omega \colon g(X(\omega)) = y \}           \\
		            & = \{ \omega \colon X(\omega) \in g^{-1}(\{y\})\} \\
		            & = \{ X \in g^{-1} (\{ y \}) \}
	\end{align*}
	where $g^{-1}(\{ y \})$ is the set of all $x$ such that $g(x) \in \{ y \}$. So
	\begin{align*}
		\expect{Y} & = \sum_{y \in \Omega_y} y \cdot \prob{X \in g^{-1}(\{ y \})}              \\
		           & = \sum_{y \in \Omega_Y} y \cdot \sum_{x \in g^{-1}(\{ y \})} \prob{X = x} \\
		           & = \sum_{y \in \Omega_Y} \sum_{x \in g^{-1}(\{y\})} g(x) \prob{X = x}      \\
		           & = \sum_{x \in \Omega_X} g(x) \prob{X = x}
	\end{align*}
\end{proof}

\subsection{Calculating Expectation with Cumulative Probabilities}
If $X \geq 0$ and takes integer values, then
\[ \expect{X} = \sum_{k=1}^\infty \prob{X \geq k} = \sum_{k=0}^\infty \prob{X > k} \]
\begin{proof}
	Since $X$ takes non-negative integer values,
	\[ X = \sum_{k=1}^\infty 1(X \geq k) = \sum_{k=0}^\infty 1(X > k) \]
	This represents the fact that any integer is the sum of that many ones, e.g. $4 = 1 + 1 + 1 + 1 + 0 + 0 + \dots$ to infinity. Taking the expectation of the above formula, using that $\expect{1(A)} = \prob{A}$ and countable additivity, we have the result as claimed.
\end{proof}

\subsection{Inclusion-Exclusion Formula with Indicators}
We can provide another proof of the inclusion-exclusion formula, using some basic properties of indicator functions.
\begin{itemize}
	\item $1(\stcomp{A}) = 1 - 1(A)$
	\item $1(A \cap B) = 1(A) \cdot 1(B)$
	\item Following from the above, $1(A \cup B) = 1-(1-1(A))(1-1(B))$.
\end{itemize}
More generally,
\[ 1(A_1 \cup \dots \cup A_n) = 1-\prod_{i=1}^n(1-1(A_i)) \]
which gives the inclusion-exclusion formula. Taking the expectation of both sides, we can see that
\[ \prob{A_1 \cup \dots \cup A_n} = \sum_{i=1}^n \prob{A_i} - \sum_{i_1 < i_2} \prob{A_{i_1} \cap A_{i_2}} + \dots + (-1)^{n+1}\prob{A_1 \cap \dots \cap A_n} \]
which is the result as previously found.

\subsection{Variance}
Let $X$ be a random variable, and $r \in \mathbb N$. If it is well-defined, we call $\expect{X^r}$ the $r$th moment of $X$. We define the variance of $X$ by
\[ \Var{X} = \expect{(X - \expect{X})^2} \]
If the variance is small, $X$ is highly concentrated around $\expect{X}$. If the variance is large, $X$ has a wide distribution including values not necessarily near $\expect{X}$. We call $\sqrt{\Var{X}}$ the standard deviation of $X$, denoted with $\sigma$. The variance has the following basic properties:
\begin{itemize}
	\item $\Var{X} \geq 0$, and if $\Var{X} = 0$, $\prob{X = \expect{X}} = 1$.
	\item If $c \in \mathbb R$, then $\Var{cX} = c^2\Var{X}$, and $\Var{X + c} = \Var{X}$.
	\item $\Var{X} = \expect{X^2} - \expect{X}^2$. This follows since $\expect{(X - \expect{X})^2} = \expect{X^2 - 2X\expect{X} + \expect{X}^2} = \expect{X^2} - 2\expect{X} \expect{X} + \expect{X}^2 = \expect{X^2} - \expect{X}^2$.
	\item $\Var{X} = \min_{c \in \mathbb R} \expect{(X - c)^2}$, and this minimum is achieved at $c = \expect{X}$. Indeed, if we let $f(c) = \expect{(X - c)^2}$, then $f(c) = \expect{X^2} - 2c\expect{X} + c^2$. Minimising $f$, we get $f(\expect{X}) = \Var{X}$ as required.
\end{itemize}
As an example, consider $X \sim \text{Bin}(n, p)$. Then $\expect{X} = np$, as we found before. Note that we can also represent this binomial distribution as the sum of $n$ Bernoulli distributions of parameter $p$ to get the same result. The variance of $X$ is
\[ \Var{X} = \expect{X^2} - \expect{X}^2 \]
In fact, in order to compute $\expect{X^2}$ it is easier to find $\expect{X(X-1)}$.
\begin{align*}
	\expect{X(X-1)} & = \sum_{k=2}^n k \cdot (k-1) \cdot \binom{n}{k} \cdot p^k \cdot (1-p)^{n-k} \\
	                & = \sum_{k=2}^n \frac{k(k-1)n! p^k (1-p)^{n-k}}{(n-k)!k!}                    \\
	                & = \sum_{k=2}^n \frac{n! p^k (1-p)^{n-k}}{((n-2)-(k-2))!(k-2)!}              \\
	                & = n(n-1)p^2 \sum_{k=2}^n \binom{n-2}{k-2} p^{k-2} (1-p)^{n-k}               \\
	                & = n(n-1)p^2
\end{align*}
Hence,
\[ \Var{X} = \expect{X(X-1)} + \expect{X} - \expect{X}^2 = n(n-1)p^2 + np - (np)^2 = np(1-p) \]
As a second example, if $X \sim \text{Poi}(\lambda)$, we have $\expect{X} = \lambda$. Because of the factorial term, it is easier to use $X(X-1)$ than $X^2$.
\begin{align*}
	\expect{X(X-1)} & = \sum_{k=2}^\infty k(k-1) e^{-\lambda} \frac{\lambda^k}{k!}                  \\
	                & = e^{-\lambda} \sum_{k=2}^\infty \frac{\lambda_{k-2}}{(k-2)!} \cdot \lambda^2 \\
	                & = \lambda^2
\end{align*}
Hence,
\[ \Var{X} = \lambda^2 + \lambda - \lambda^2 = \lambda \]

\section{Covariance and Inequalities}
\subsection{Covariance}
\begin{definition}
	Let $X$ and $Y$ be random variables. Their covariance is defined
	\[ \Cov{X,Y} = \expect{(X - \expect{X})(Y - \expect{Y})} \]
	It is a measure of how dependent $X$ and $Y$ are.
\end{definition}
\noindent Immediately we can deduce the following properties.
\begin{itemize}
	\item $\Cov{X,Y} = \Cov{Y,X}$
	\item $\Cov{X,X} = \Var{X}$
	\item $\Cov{X,Y} = \expect{XY} - \expect{X}\cdot\expect{Y}$. Indeed, $(X - \expect{X})(Y - \expect{Y}) = XY - X\expect{Y} - Y\expect{X} + \expect{X}\expect{Y}$ and the result follows.
	\item Let $c \in \mathbb R$. Then $\Cov{cX,Y} = c\Cov{X,Y}$, and $\Cov{c + X,Y} = \Cov{X,Y}$.
	\item $\Var{X + Y} = \Var{X} + \Var{Y} + 2\Cov{X,Y}$. Indeed, we have

	      $\Var{X + Y} = \expect{(X - \expect{X} + Y - \expect{Y})^2}$ which gives

	      $\expect{(X - \expect{X})^2} + \expect{(Y - \expect{Y})^2} + 2\expect{(X - \expect{X})(Y - \expect{Y})}$ as required.
	\item For all $c \in \mathbb R$, $\Cov{c, X} = 0$
	\item If $X$, $Y$, $Z$ are random variables, then $\Cov{X + Y,Z} = \Cov{X,Z} + \Cov{Y,Z}$. More generally, for $c_1, \dots, c_n, d_1, \dots, d_m$ real numbers, and for $X_1, \dots, X_n, Y_1, \dots, Y_m$ random variables, we have
	      \[ \Cov{\sum_{i=1}^n c_i X_i,\sum_{j=1}^m d_j Y_j} = \sum_{i=1}^n \sum_{j=1}^m c_i d_j \Cov{X_i,Y_j} \]
	      In particular, if we apply this to $X_i = Y_i$, and $c_i = d_i = 1$, then we have
	      \[ \Var{\sum_{i=1}^n X_i} = \sum_{i=1}^n \Var{X_i} + \sum_{i \neq j} \Cov{X_i,X_j} \]
\end{itemize}

\subsection{Expectation of Functions of a Random Variable}
Recall that $X$ and $Y$ are independent if for all $x$ and $y$,
\[ \prob{X = x, Y = y} = \prob{X = x} \cdot \prob{Y = y} \]
We would like to prove that given positive functions $f, g \colon \mathbb R \to \mathbb R_+$, if $X$ and $Y$ are independent we have
\[ \expect{f(X)g(Y)} = \expect{f(X)} \cdot \expect{g(Y)} \]
\begin{proof}
	\begin{align*}
		\expect{f(X)g(Y)} & = \sum_{(x, y)} f(x) g(y) \prob{X = x, Y = y}                 \\
		                  & = \sum_{(x, y)} f(x) g(y) \prob{X = x} \prob{Y = y}           \\
		                  & = \sum_{x} f(x) \prob{X = x} \cdot \sum_{y} g(y) \prob{Y = y} \\
		                  & = \expect{f(X)} \cdot \expect{g(Y)}
	\end{align*}
\end{proof}
\noindent The same result holds for general functions, provided the required expectations exist.

\subsection{Covariance of Independent Variables}
Suppose $X$ and $Y$ are independent. Then
\[ \Cov{X,Y} = 0 \]
This is because
\begin{align*}
	\Cov{X,Y} & = \expect{(X - \expect{X})(Y - \expect{Y})}             \\
	          & = \expect{X - \expect{X}} \cdot \expect{Y - \expect{Y}} \\
	          & = 0 \cdot 0                                             \\
	          & = 0
\end{align*}
\noindent In particular, we can deduce that
\[ \Var{X + Y} = \Var{X} + \Var{Y} \]
Note, however, that the covariance being equal to zero does not imply independence. For instance, let $X_1, X_2, X_3$ be independent Bernoulli random variables with parameter $\frac{1}{2}$. Let us now define $Y_1 = 2X_1 - 1$, $Y_2 = 2X_2 - 1$, and $Z_1 = X_3 Y_1$, $Z_2 = X_3 Y_2$. Now, we have
\[ \expect{Y_1} = \expect{Y_2} = \expect{Z_1} = \expect{Z_2} = 0 \]
We can find that
\[ \Cov{Z_1, Z_2} = \expect{Z_1 \cdot Z_2} = \expect{X_3^2 Y_1 Y_2} = \expect{X_3^2} \cdot 0 \cdot 0 = 0 \]
However, $Z_1$ and $Z_2$ are in fact not independent. Since $Y_1, Y_2$ are never zero,
\[ \prob{Z_1 = 0, Z_2 = 0} = \prob{X_3 = 0} = \frac{1}{2} \]
But also
\[ \prob{Z_1 = 0} = \prob{Z_2 = 0} = \prob{X_3 = 0} = \frac{1}{2} \implies \prob{Z_1 = 0} \cdot \prob{Z_2 = 0} = 0 \]
So the events are not independent.

\subsection{Markov's Inequality}
The following useful inequality, and the others derived from it, hold in the discrete and the continuous case.
\begin{theorem}
	Let $X \geq 0$ be a non-negative random variable. Then for all $a > 0$,
	\[ \prob{X \geq a} \leq \frac{\expect{X}}{a} \]
\end{theorem}
\begin{proof}
	Observe that $X \geq a \cdot 1(X \geq a)$. This can be seen to be true simply by checking both cases, $X < a$ and $X \geq a$. Taking expectations, we get
	\[ \expect{X} \geq \expect{a \cdot 1(X \geq a)} = \expect{a \cdot \prob{X \geq a}} = a \cdot \prob{X \geq a} \]
	and the result follows.
\end{proof}

\subsection{Chebyshev's Inequality}
\begin{theorem}
	Let $X$ be a random variable with finite expectation. Then for all $a > 0$,
	\[ \prob{\abs{X - \expect{X}} \geq a} \leq \frac{\Var{X}}{a^2} \]
\end{theorem}
\begin{proof}
	Note that $\prob{\abs{X - \expect{X}} \geq a} = \prob{\abs{X - \expect{X}}^2 \geq a^2}$. Then we can apply Markov's inequality to this non-negative random variable to get
	\[ \prob{\abs{X - \expect{X}}^2 \geq a^2} \leq \frac{\expect{(X - \expect{X})^2}}{a^2} = \frac{\Var{X}}{a^2} \]
\end{proof}

\subsection{Cauchy-Schwarz Inequality}
\begin{theorem}
	If $X$ and $Y$ are random variables, then
	\[ \expect{\abs{XY}} \leq \sqrt{\expect{X^2}\cdot\expect{Y^2}} \]
\end{theorem}
\begin{proof}
	It suffices to prove this statement for $X$ and $Y$ which have finite second moments, i.e. $\expect{X^2}$ and $\expect{X^2}$ are finite. Clearly if they are infinite, then the upper bound is infinite which is trivially true. We need to show that $\expect{\abs{XY}}$ is finite. Here we can apply the additional assumption that $X$ and $Y$ are non-negative, since we are taking the absolute value:
	\[ XY \leq \frac{1}{2}\left(X^2 + Y^2\right) \implies \expect{XY} \leq \frac{1}{2}\left( \expect{X^2} + \expect{Y^2}  \right) \]
	Now, we can assume $\expect{X^2} > 0$ and $\expect{Y^2} > 0$. If this were not the case, the result is trivial since if at least one of them were equal to zero, the corresponding random variable would be identically zero. Let $t \in \mathbb R$ and consider
	\[ 0 \leq (X - tY)^2 = X^2 - 2tXY + t^2Y^2 \]
	Hence
	\[ \expect{X^2} - 2t\expect{XY} + t^2\expect{Y^2} \geq 0 \]
	We can view this left hand side as a function $f(t)$. The minimum value of this function is achieved at $t_\ast = \frac{\expect{XY}}{\expect{Y^2}}$. Then
	\[ f(t_\ast) \geq 0 \implies \expect{X^2} - \frac{2\expect{XY}}{\expect{Y^2}} + \frac{\expect{XY}^2}{\expect{Y^2}} \geq 0 \]
	Hence,
	\[ \expect{XY}^2 \leq \expect{X^2}\cdot \expect{Y^2} \]
	and the result follows.
\end{proof}

\section{More Inequalities and Conditional Expectation}
\subsection{Equality in Cauchy-Schwarz}
In what cases do we get equality in the Cauchy-Schwarz inequality? Recall that the inequality states
\[ \expect{\abs{XY}} \leq \sqrt{\expect{X^2}\cdot\expect{Y^2}} \]
Recall that in the proof, we considered the random variable $(X - tY)^2$ where $X$ and $Y$ were non-negative, and had finite second moments. The expectation of this random variable was called $f(t)$, and we found that $f(t)$ was minimised when $t = \frac{\expect{XY}}{\expect{Y^2}}$. We have equality exactly when $f(t) = 0$ for this value of $t$. But $(X - tY)^2$ is a non-negative random variable, with expectation zero, so it must be zero with probability 1. So we have equality if and only if $X$ is exactly $tY$.

\subsection{Jensen's Inequality}
\begin{definition}
	A function $f\colon \mathbb R \to \mathbb R$ is called convex if $\forall x, y \in \mathbb R$ and for all $t \in [0, 1]$,
	\[ f(tx + (1-t)y) \leq tf(x) + (1-t)f(y) \]
	This can be visualised as linearly interpolating the values of the function at two points, $x$ and $y$. The linear interpolation of those points is always greater than the function applied to the linear interpolation of the input points.
\end{definition}
\begin{theorem}
	Let $X$ be a random variable, and let $f$ be a convex function. Then
	\[ \expect{f(X)} \geq f(\expect{X}) \]
\end{theorem}
\noindent We can remember the direction of this inequality by considering the variance: $\Var{X} = \expect{(X - \expect{X})^2}$ which is non-negative. Further, $\Var{X} = \expect{X^2} - \expect{X}^2$ hence $\expect{X^2} \geq \expect{X}^2$. Squaring is an example of a convex function, so Jensen's inequality holds in this case. We will first prove a basic lemma about convex functions.
\begin{lemma}
	Let $f \colon \mathbb R \to \mathbb R$ be a convex function. Then $f$ is the supremum of all the lines lying below it. More formally, $\forall m \in \mathbb R$, $\exists a, b \in \mathbb R$ such that $f(m) = am + b$ and $f(x) \geq ax + b$ for all $x$.
\end{lemma}
\begin{proof}
	Let $m \in \mathbb R$. Let $x < m < y$. Then we can express $m$ as $tx + (1-t)y$ for some $t$ in the interval $[0, 1]$. By convexity,
	\[ f(m) \leq tf(x) + (1-t)f(y) \]
	And hence,
	\begin{align*}
		tf(m) + (1-t)f(m)         & \leq tf(x) + (1-t)f(y)       \\
		t(f(m) - f(x))            & \leq (1-t)(f(y) - f(m))      \\
		\frac{f(m) - f(x)}{m - x} & \leq \frac{f(y) - f(m)}{y-m}
	\end{align*}
	So the slope of the line joining $m$ to a point on its left is smaller than the slope of the line joining $m$ to a point on its right. So we can produce a value $a \in \mathbb R$ given by
	\[ a = \sum_{x < m} \frac{f(m) - f(x)}{m - x} \]
	such that
	\[ \frac{f(m) - f(x)}{m - x} \leq a \leq \frac{f(y) - f(m)}{y - m} \]
	for all $x < m < y$. We can rearrange this to give
	\[ f(x) \geq a(x-m) + f(m) = ax + (f(m) - am) \]
	for all $x$.
\end{proof}
\noindent We may now prove Jensen's inequality.
\begin{proof}
	Set $m = \expect{X}$. Then from the lemma above, there exists $a, b \in \mathbb R$ such that
	\begin{equation}
		f(m) = am + b \implies f(\expect{X}) = a\expect{X} + b \tag{$\ast$}
	\end{equation}
	and for all $x$, we have
	\[ f(x) \geq ax + b \]
	We can now apply this inequality to $X$ to get
	\[ f(X) \geq aX + b \]
	Taking the expectation, by $(\ast)$ we get
	\[ \expect{f(X)} \geq a\expect{X} + b = f(\expect{X}) \]
	as required.
\end{proof}
\noindent Like the Cauchy-Schwarz inequality, we would like to consider the cases of equality. Let $X$ be a random variable, and $f$ be a convex function such that if $m = \expect{X}$, then $\exists a, b \in \mathbb R$ such that
\[ f(m) = am + b;\quad \forall x \neq m,\, f(x) > ax + b \]
We know that $f(X) \geq aX + b$, since $f$ is convex. Then $f(X) - (aX+b) \geq 0$ is a non-negative random variable. Taking expectations,
\[ \expect{f(X) - (aX+b)} \geq 0 \]
But $\expect{aX + b} = am + b = f(m) = f(\expect{X})$. We assumed that $\expect{f(X)} = f(\expect{X})$, hence $\expect{aX+b} = \expect{f(X)}$ and $\expect{f(X) - (aX+b)} = 0$. But since $f(X) \geq aX+b$, this forces $f(X) = aX+b$ everywhere. By our assumption, for all $x \neq m$, $f(x) > ax+b$. This forces $X=m$ with probability 1.

\subsection{Arithmetic Mean and Geometric Mean Inequality}
Let $f$ be a convex function. Suppose $x_1, \dots, x_n \in \mathbb R$. Then, from Jensen's inequality,
\[ \frac{1}{n} \sum_{k=1}^n f(x_k) \geq f\left( \frac{1}{n} \sum_{k=1}^n x_k \right) \]
Indeed, we can define a random variable $X$ to take values $x_1, \dots, x_n$ all with equal probability. Then, $\expect{f(X)}$ gives the left hand side, and $f(\expect{X})$ gives the right hand side. Now, let $f(x) = -\log x$. This is a convex function as required. Hence
\begin{align*}
	-\frac{1}{n} \sum_{k=1}^n \log x_k             & \geq -\log\left( \frac{1}{n} \sum_{k=1}^n x_k \right) \\
	\left( \prod_{k=1}^n x_k \right)^{\frac{1}{n}} & \leq \frac{1}{n} \sum_{k=1}^n x_k
\end{align*}
Hence the geometric mean is less than or equal to the arithmetic mean.

\subsection{Conditional Expectation and Law of Total Expectation}
Recall that if $B \in \mathcal F$ with $\prob{B} \geq 0$, we defined
\[ \prob{A \mid B} = \frac{\prob{A \cap B}}{\prob{B}} \]
Now, let $X$ be a random variable, and let $B$ be an event as above with non-zero probability. We can then define
\[ \expect{X \mid B} = \frac{\expect{X \cdot 1(B)}}{\prob{B}} \]
The numerator is notably zero when $1(B) = 0$, so in essence we are excluding the case where $X$ is not $B$.
\begin{theorem}[Law of Total Expectation]
	Suppose $X \geq 0$. Let $(\Omega_n)$ be a partition of $\Omega$ into disjoint events, so $\Omega = \bigcup_n \Omega_n$. Then
	\[ \expect{X} = \sum_n \expect{X \mid \Omega_n} \cdot \prob{\Omega_n} \]
\end{theorem}
\begin{proof}
	We can write $X = X \cdot 1(\Omega)$, where
	\[ 1(\Omega) = \sum_n 1(\Omega_n) \]
	Taking expectations, we get
	\[ \expect{X} = \expect{ \sum_{n} X \cdot 1(\Omega_n) } \]
	By countable additivity of expectation, we have
	\[ \expect{X} = \sum_{n} \expect{ X \cdot 1(\Omega_n) } = \sum_n \expect{X \mid \Omega_n} \cdot \prob{\Omega_n} \]
	as required.
\end{proof}

\section{Joint Distribution, Convolution and Conditional Expectation}
\subsection{Joint Distribution}
\begin{definition}
	Let $X_1, \dots, X_n$ be discrete random variables. Their joint distribution is defined as
	\[ \prob{X_1 = x_1, \dots, X_n = x_n} \]
	for all $x_i \in \Omega_i$.
\end{definition}
\noindent Now, we have
\[ \prob{X_1 = x_1} = \prob{\{ X_1 = x_1\} \cap \bigcup_{i=2}^n \bigcup_{x_i} \{ X_i = x_i \} } = \sum_{x_2, \dots, x_n} \prob{X_1 = x_1, X_2 = x_2, \dots, X_n = x_n} \]
In general,
\[ \prob{X_i = x_i} = \sum_{x_1, x_2, \dots, x_{i-1}, x_{i+1}, \dots, x_n} \prob{X_1 = x_1, X_2 = x_2, \dots, X_n = x_n} \]
We call $(\prob{X_i = x_i})_i$ the marginal distribution of $X_i$. Let $X, Y$ be random variables. The conditional distribution of $X$ given $Y = y$ where $y \in \Omega_y$ is defined to be
\[ \prob{X = x \mid Y = y} = \frac{\prob{X = x, Y = y}}{\prob{Y = y}} \]
We can find
\[ \prob{X = x} = \sum_y \prob{X = x, Y = y} = \sum_y \prob{X = x \mid Y = y} \prob{Y = y} \]
which is the law of total probability.

\subsection{Convolution}
Let $X$ and $Y$ be independent, discrete random variables. We would like to find $\prob{X + Y = z}$. Clearly this is
\begin{align*}
	\prob{X + Y = z} & = \sum_y \prob{X + Y = z, Y = y}           \\
	                 & = \sum_y \prob{X = z-y, Y = y}             \\
	                 & = \sum_y \prob{X = z-y} \cdot \prob{Y = y} \\
\end{align*}
This last sum is called the convolution of the distributions of $X$ and $Y$. Similarly,
\[ \prob{X + Y = z} = \sum_x \prob{X = x} \cdot \prob{Y = z-x} \]
As an example, let $X \sim \mathrm{Poi}(\lambda)$ and $Y \sim \mathrm{Poi}(\mu)$ be independent. Then
\begin{align*}
	\prob{X + Y = n} & = \sum_{r = 0}^n \prob{X = r} \prob{Y = n - r}                                              \\
	                 & = \sum_{r = 0}^n e^{-\lambda} \frac{\lambda^r}{r!} \cdot e^{-\mu} \frac{\mu^{n-r}}{(n-r)!}  \\
	                 & = e^{-(\lambda+\mu)} \sum_{r = 0}^n \frac{\lambda^r\mu^{n-r}}{r!(n-r)!}                     \\
	                 & = \frac{e^{-(\lambda+\mu)}}{n!} \sum_{r = 0}^n \frac{\lambda^r\mu^{n-r} \cdot n!}{r!(n-r)!} \\
	                 & = \frac{e^{-(\lambda+\mu)}}{n!} \sum_{r = 0}^n \binom{n}{r} \lambda^r\mu^{n-r}              \\
	                 & = \frac{e^{-(\lambda+\mu)}}{n!} (\lambda + \mu)^n                                           \\
\end{align*}
which is the probability mass function of a Poisson random variable with parameter $\lambda + \mu$. In other words, $X + Y \sim \mathrm{Poi}(\lambda + \mu)$.

\subsection{Conditional Expectation}
Let $X$ and $Y$ be discrete random variables. Then the conditional expectation of $X$ given that $Y = y$ is
\[ \expect{X \mid Y = y} = \frac{\expect{X \cdot 1(Y = y)}}{\prob{Y = y}} = \frac{1}{\prob{Y = y}} \sum_x x \cdot \prob{X = x, Y = y} = \sum_x x \cdot \prob{X = x \mid Y = y} \]
Observe that for every $y \in \Omega_y$, this expectation is purely a function of $y$. Let $g(y) = \expect{X \mid Y = y}$. Now, we define the conditional expectation of $X$ given $Y$ as $\expect{X \mid Y} = g(Y)$. Note that $\expect{X \mid Y}$ is a random variable, dependent only on $Y$. We have
\begin{align*}
	\expect{X \mid Y} & = g(Y) \cdot 1                                \\
	                  & = g(Y) \sum_y 1(Y = y)                        \\
	                  & = \sum_y g(Y) \cdot 1(Y = y)                  \\
	                  & = \sum_y g(y) \cdot 1(Y = y)                  \\
	                  & = \sum_y \expect{X \mid Y = y} \cdot 1(Y = y)
\end{align*}
This is perhaps a clearer way to see that it depends only on $Y$. As an example, let us consider tossing a $p$-biased coin $n$ times independently. We write $X_i$ for the indicator function that the $i$th toss was a head. Let $Y_n = X_1 + \dots + X_n$. What is $\expect{X_1 \mid Y_n}$? Let $g(y) = \expect{X_1 \mid Y_n = y}$. Then $\expect{X_1 \mid Y_n} = g(Y_n)$. We therefore need to find $g$. Let $y \in \{ 0, \dots, n \}$, then
\begin{align*}
	g(y) & = \expect{X_1 \mid Y_n = y}   \\
	     & = \prob{X_1 = 1 \mid Y_n = y} \\
\end{align*}
Clearly if $y = 0$, then $\prob{X_1 = 1 \mid Y_n = 0} = 0$. Now, suppose $y \neq 0$. We have
\begin{align*}
	\prob{X_1 = 1 \mid Y_n = y} & = \frac{\prob{X_1 = 1, Y_n = y}}{\prob{Y_n = y}}                             \\
	                            & = \frac{\prob{X_1 = 1, X_2 + \dots + X_n = y-1}}{\prob{Y_n = y}}             \\
	                            & = \frac{\prob{X_1 = 1} \cdot \prob{X_2 + \dots + X_n = y-1}}{\prob{Y_n = y}} \\
	                            & = \frac{p \cdot \binom{n-1}{y-1} \cdot p^{y-1}(1-p)^{n-y}}{\prob{Y_n = y}}   \\
	                            & = \frac{\binom{n-1}{y-1} \cdot p^y(1-p)^{n-y}}{\binom{n}{y}p^y (1-p)^{n-y}}  \\
	                            & = \frac{\binom{n-1}{y-1}}{\binom{n}{y}}                                      \\
	                            & = \frac{y}{n}
\end{align*}
Hence
\[ g(y) = \frac{y}{n} \]
We can then find that
\[ \expect{X_1 \mid Y_n} = g(Y_n) = \frac{Y_n}{n} \]
which is indeed a random variable dependent only on $Y_n$.

\subsection{Properties of Conditional Expectation}
The following properties hold.
\begin{itemize}
	\item For all $c \in \mathbb R$, $\expect{cX \mid Y} = c\expect{X \mid Y}$, and $\expect{c \mid Y} = c$.
	\item Let $X_1, \dots, X_n$ be random variables. Then $\expect{\sum_{i=1}^n X_i \mid Y} = \sum_{i=1}^n \expect{X_i \mid Y}$.
	\item $\expect{\expect{X \mid Y}} = \expect{X}$.
\end{itemize}
The last property is not obvious from the definition, so it warrants its own proof. We can see by the standard properties of the expectation that
\begin{align*}
	\expect{X \mid Y}                     & = \sum_y 1(Y = y) \expect{X \mid Y = y}                              \\
	\therefore \expect{\expect{X \mid Y}} & = \sum_y \expect{1(Y = y)} \expect{X \mid Y = y}                     \\
	                                      & = \sum_y \prob{Y = y} \expect{X \mid Y = y}                          \\
	                                      & = \sum_y \prob{Y = y} \frac{\expect{X \cdot 1(Y = y)}}{\prob{Y = y}} \\
	                                      & = \sum_y \expect{X \cdot 1(Y = y)}                                   \\
	                                      & = \expect{\sum_y X \cdot 1(Y = y)}                                   \\
	                                      & = \expect{X \sum_y 1(Y = y)}                                         \\
	                                      & = \expect{X}                                                         \\
\end{align*}
Alternatively, we could expand the inner expectation as a sum:
\[ \sum_y \expect{X \mid Y = y} \cdot \prob{Y = y} = \sum_x \sum_y x \cdot \prob{X = x \mid Y = y} \cdot \prob{Y = y} \]
and the result follows as required. The final property relates conditional probability to independence. Let $X$ and $Y$ be independent. Then $\expect{X \mid Y} = \expect{X}$. Indeed,
\begin{align*}
	\expect{X \mid Y} & = \sum_y 1(Y = y) \expect{X \mid Y = y} \\
	                  & = \sum_y 1(Y = y) \expect{X}            \\
	                  & = \expect{X}                            \\
\end{align*}

\section{Conditional Expectation and Random Walks}
\subsection{More Properties of Conditional Expectation}
\begin{proposition}
	Suppose $Y$ and $Z$ are independent random variables. Then
	\[ \expect{\expect{X \mid Y} \mid Z} = \expect{X} \]
\end{proposition}
\begin{proof}
	Let $\expect{X \mid Y} = g(Y)$ be a random variable that is a function only of $Y$. Since $Y$ and $Z$ are independent, $f(Y)$ is also independent of $Z$ for any function $f$. Then $\expect{g(Y) \mid Z} = \expect{g(Y)} = \expect{X}$.
\end{proof}
\begin{proposition}
	Suppose $h \colon \mathbb R \to \mathbb R$ is a function. Then
	\[ \expect{h(Y) \cdot X \mid Y} = h(Y) \cdot \expect{X \mid Y} \]
	We can `take out what is known', since we know what $Y$ is.
\end{proposition}
\begin{proof}
	Note that $\expect{h(Y) \cdot X \mid Y = y} = \expect{h(y) \cdot X \mid Y = y} = h(y) \expect{X \mid Y = y}$. Then $\expect{h(Y) \cdot X \mid Y} = h(y) \cdot \expect{X \mid Y}$ as required.
\end{proof}
\begin{corollary}
	$\expect{\expect{X \mid Y} \mid Y} = \expect{X \mid Y}$, and $\expect{X \mid X} = X$.
\end{corollary}
\noindent Let $X_i = 1(i\text{th toss is a head})$, and $Y_n = X_1 + \dots + X_n$. We found before that $\expect{X_1 \mid Y_n} = \frac{Y_n}{n}$. By symmetry, for all $i$ we have $\expect{X_i \mid Y_n} = \expect{X_1 \mid Y_n}$. Hence
\[ \expect{Y_n \mid Y_n} = \expect{\sum_{i=1}^n X_i \mid Y_n} = \sum_{i=1}^n \expect{X_i \mid Y_n} = n \cdot \expect{X_1 \mid Y_n} \]
which yields the same result.

\subsection{Random Walks}
A random process, also known as a stochastic process, is a sequence of random variables $X_n$ for $n \in \mathbb N$. A random walk is a random process that can be expressed as
\[ X_n = x + Y_1 + \dots + Y_n \]
where the $Y_i$ are independent and identically distributed, and $x$ is a deterministic number. We will focus on the simple random walk on $\mathbb Z$, which is defined by taking
\[ \prob{Y_i = 1} = p;\quad \prob{Y_i = -1} = 1-p = q \]
This can be thought of as a specific case of a Markov chain; it has the property that the path to $X_i$ does not matter, all that matters is the value that we are at, at any point in time.

\subsection{Gambler's Ruin Estimate}
What is the probability that $X_n$ reaches some value $a$ before it falls to 0? We will write $\mathbb P_x$ for the probability measure $\mathbb P$ with the condition that $X_0 = x$, i.e.
\[ \psub{x}{A} = \prob{A \mid X_0 = x} \]
We define $h(x) = \psub{x}{(X_n) \text{ hits $a$ before hitting 0}}$. We can define a recurrence relation. By the law of total probability, we have, for $0 < x < a$,
\begin{align*}
	h(x) & = \psubx{(X_n) \text{ hits $a$ before hitting 0} \mid Y_1 = 1} \cdot \psubx{Y_1 = 1}   \\
	     & + \psubx{(X_n) \text{ hits $a$ before hitting 0} \mid Y_1 = -1} \cdot \psubx{Y_1 = -1} \\
	     & = p \cdot h(x+1) + q \cdot h(x-1)
\end{align*}
Note that
\[ h(0) = 0;\quad h(a) = 1 \]
There are two cases; $p=q = \frac{1}{2}$ and $p \neq q$. If $p=q=\frac{1}{2}$, then
\[ h(x) - h(x+1) = h(x-1) - h(x) \]
We can then solve this to find
\[ h(x) = \frac{x}{a} \]
If $p \neq q$, then
\[ h(x) = ph(x+1) + qh(x-1) \]
We can try a solution of the form $\lambda^x$. Substituting gives
\[ p\lambda^2 - \lambda + q = 0 \implies \lambda = 1, \frac{q}{p} \]
The general solution can be found by using the boundary conditions.
\[ h(x) = A + B \left( \frac{q}{p} \right)^x \implies h(x) = \frac{\left( \frac{q}{p} \right)^x - 1}{\left( \frac{q}{p} \right)^a - 1} \]
This is known as the `gambler's ruin' estimate, since it determines whether a gambler will reach a target before going bankrupt.

\subsection{Expected Time to Absorption}
Let us define $T$ to be the first time that $x = 0$ or $x = a$. Then $T = \min \{ n \geq 0 \colon X_n \in \{ 0, a \} \}$. We want to find $\esubx{T} = \tau_x$. We can apply a condition on the first step, and use the law of total expectation to give
\[ \tau_x = p \esubx{T \mid Y_1 = 1} + q \esubx{T \mid Y_1 = -1} \]
Hence
\[ \tau_x = p (\tau_{x + 1} + 1) + q (\tau_{x - 1} + 1) \]
We can deduce that, for $0 < x < a$,
\[ \tau_x = 1 + p \tau_{x+1} + q \tau_{x-1} \]
and $\tau_0 = \tau_a = 0$. If $p = q = \frac{1}{2}$, then we can try a solution of the form $Ax^2$.
\[ Ax^2 = 1 + \frac{1}{2}A(x+1)^2 + \frac{1}{2}A(x-1)^2 \]
This gives a general solution of the form
\[ A = -1 \implies \tau_x = -x^2 + Bx + C \implies \tau_x = x(a-x) \]
If $p \neq q$, then we will try a solution of the form $Cx$, giving
\[ C = \frac{1}{q-p} \]
The general solution has the form
\[ \tau_x = \frac{x}{q-p} + A + B\left( \frac{q}{p} \right)^x \implies \tau_x = \frac{x}{q-p} - \frac{q}{q-p} \cdot \frac{\left( \frac{q}{p} \right)^x - 1}{\left( \frac{q}{p} \right)^a - 1}  \]

\section{Probability Generating Functions}
\subsection{Definition}
Let $X$ be a random variable with values in the positive integers, $\mathbb N$. Let $p_r = \prob{X = r}$ be the probability mass function. Then the probability generating function is defined to be
\[ p(z) = \sum_{r=0}^\infty p_r z^r = \expect{z^X} \text{ for } \abs{z} \leq 1 \]
When $\abs{z} \leq 1$, the probability generating function converges absolutely, since $\abs{\sum_{r=0}^\infty p_r z^r} \leq \sum_{r=0}^\infty p_r = 1$. So $p(z)$ is well-defined and has a radius of convergence of at least 1.
\begin{theorem}
	The probability generating function of $X$ uniquely determines the distribution of $X$.
\end{theorem}
\begin{proof}
	Suppose $(p_r)$ and $(q_r)$ are two probability mass functions with
	\[ \sum_{r=0}^\infty p_r z^r = \sum_{r=0}^\infty q_r z^r, \forall \abs{z} \leq 1 \]
	We will show that $p_r = q_r$ for all $r$. First, set $z = 0$, then clearly $p_0 = q_0$. Then by induction, suppose that $p_r = q_r$ for all $r \leq n$. Then we would like to show that $p_{n+1} = q_{n+1}$. We know that
	\[ \sum_{r=n+1}^\infty p_r z^r = \sum_{r=n+1}^\infty q_r z^r \]
	Hence, dividing by $z^{n+1}$, and taking the limit as $z \to 0$, we have $p_{n+1} = q_{n+1}$ as required.
\end{proof}

\subsection{Finding Moments and Probabilities}
\begin{theorem}
	\[ \lim_{z \to 1^-} p'(z) = p'(1^-) = \expect{X} \]
\end{theorem}
\begin{proof}
	We will first assume that $\expect{X}$ is finite; we will then extend the proof to the infinite case. Let $0 < z < 1$, then since the series $p(z)$ is absolutely convergent, we can interchange the sum and the derivative operators, giving
	\[ p'(z) = \sum_{r=0}^\infty r p_r z^{r-1} \]
	We can make an upper bound for this sum:
	\[ \sum_{r=0}^\infty r p_r z^{r-1} \leq \sum_{r=0}^\infty r p_r = \expect{X} \]
	Since $0 < z < 1$, we see that $p'(z)$ is an increasing function of $z$. This implies that there exists a limit of $p'(z)$ as $z \to 1^-$, which is upper bounded by $\expect{X}$. Now, let $\varepsilon > 0$ and let $N$ be an integer large enough such that
	\[ \sum_{r = 0}^N rp_r \geq \expect{X} - \varepsilon \]
	We have further that, since $0 < z < 1$,
	\[ p'(z) \geq \sum_{r=1}^N rp_r z^{r-1} \]
	So
	\[ \lim_{z \to 1^-} p'(z) \geq \sum_{r=1}^N rp_r \geq \expect{X} - \varepsilon \]
	which is true for any $\varepsilon$. Therefore $\lim_{z \to 1^-} p'(z) = \expect{X}$. Now, in the case that $\expect{X}$ is infinite, for any $M$ we can find a sufficiently large $N$ such that
	\[ \sum_{r = 0}^N rp_r \geq M \]
	From above, we know that
	\[ \lim_{z \to 1^-} p'(z) \geq \sum_{r=1}^N rp_r \geq M \]
	Since this is true for any $M$, this limit is equal to $\infty$.
\end{proof}
\noindent In exactly the same way, we can prove that
\[ p''(1^-) = \expect{X(X-1)} \]
and in general,
\[ p^{(k)}(1^-) = \expect{X(X-1)\cdots(X-k+1)} \]
In particular,
\[ \Var{X} = p''(1^-) + p'(1^-) - p'(1^-)^2 \]
Further,
\[ \prob{X = n} = \frac{1}{n!} \eval{\dv[n]{z} p(z)}_{z = 0} \]

\subsection{Sums of Random Variables}
Suppose that $X_1, \dots, X_n$ are independent random variables with probability generating functions $q_1, \dots, q_n$ respectively. Then
\[ p(z) = \expect{z^{X_1 + \dots + X_n}} \]
Recall that if $X$ and $Y$ are independent, then for all functions $f$ and $g$, we have $\expect{f(X)g(Y)} = \expect{f(X)}\expect{g(Y)}$. Therefore,
\[ p(z) = \expect{z^{X_1} z^{X_2} \cdots z^{X_n}} = \expect{z^{X_1}} \cdots \expect{z^{X_n}} = q_1(z) \cdots q_n(z) \]
So the probability generating function factorises into its independent parts. In particular, if all the $X_i$ are independent and identically distributed, then
\[ p(z) = q(z)^n \]

\subsection{Common Probability Generating Functions}
Suppose that $X \sim \mathrm{Bin}(n, p)$. Then
\begin{align*}
	p(z) & = \expect{z^X}                                  \\
	     & = \sum_{r=0}^n z^r \binom{n}{r} p^r (1-p)^{n-r} \\
	     & = \sum_{r=0}^n \binom{n}{r} (pz)^r (1-p)^{n-r}  \\
	     & = (pz + 1 - p)^{n}
\end{align*}
Now, let $X \sim \mathrm{Bin}(n, p)$, $Y \sim \mathrm{Bin}(m, p)$ be independent random variables. Then the probability generating function of $X+Y$ is
\[ (pz + 1 - p)^{n} \cdot (pz + 1 - p)^{m} = (pz + 1 - p)^{n+m} \]
which is the probability generating function of a binomial distribution where the number of trials is $n+m$. Now, suppose that $X \sim \mathrm{Geo}(p)$. Then
\begin{align*}
	p(z) & = \expect{z^X}               \\
	     & = \sum_{r=0}^n z^r (1-p)^r p \\
	     & = \frac{p}{1-z(1-p)}         \\
\end{align*}
Now, suppose that $X \sim \mathrm{Poi}(\lambda)$. Then
\begin{align*}
	p(z) & = \expect{z^X}                                       \\
	     & = \sum_{r=0}^n z^r e^{-\lambda} \frac{\lambda^r}{r!} \\
	     & = e^{\lambda(z-1)}                                   \\
\end{align*}

\subsection{Random Sums of Random Variables}
Consider the sum of a random number of random variables. Let $X_1, \dots$ be independent and identically distributed, and let $N$ be an independent random variable with values in $\mathbb N$. Now, we can define the random variables $S_n$ to be
\[ S_n = X_1 + \dots + X_n \]
Then
\[ S_N = X_1 + \dots + X_N \]
is a random variable dependent on $N$. For all $\omega \in \Omega$,
\begin{align*}
	S_N(\omega) & = X_1(\omega) + \dots + X_{N(\omega)}(\omega) \\
	            & = \sum_{i=1}^{N(\omega)} X_i(\omega)
\end{align*}
Now, let $q$ be the probability generating function of $N$, and $p$ be the probability generating function of $X_1$ (or equivalently, any $X_i$). Then let
\begin{align*}
	r(z) & = \expect{z^{S_N}}                                          \\
	     & = \sum_{n} \expect{z^{X_1 + \dots + X_N} \cdot 1(N = n)}    \\
	     & = \sum_{n} \expect{z^{X_1 + \dots + X_n} \cdot 1(N = n)}    \\
	     & = \sum_{n} \expect{z^{X_1 + \dots + X_n}} \expect{1(N = n)} \\
	     & = \sum_{n} \expect{z^{X_1 + \dots + X_n}} \prob{N = n}      \\
	     & = \sum_{n} \expect{z^{X_1}}^n \prob{N = n}                  \\
	     & = \sum_{n} p(z)^n \prob{N = n}                              \\
	     & = q(p(z))
\end{align*}
Here is an alternative proof using conditional expectation.
\begin{align*}
	r(z) & = \expect{z^{S_N}}                 \\
	     & = \expect{\expect{z^{S_N} \mid N}} \\
\end{align*}
We can see that
\begin{align*}
	\expect{z^{S_N} \mid N = n} & = \expect{z^{S_n} \mid N = n} \\
	                            & = \expect{z^{X_1}}^n          \\
	                            & = p(z)^n
\end{align*}
Therefore,
\begin{align*}
	r(z) & = \expect{p(z)^N} \\
	     & = q(p(z))
\end{align*}
Using this expression for $r$, we can find that
\[ \expect{S_N} = r'(1^-) = q'(p(1^-)) \cdot p'(1^-) = q'(1^-) \cdot p'(1^-) = \expect{N} \expect{X_1} \]
Similarly,
\[ \Var{S_N} = \expect{N} \Var{X_1} + \Var{N} \left( \expect{X_1} \right)^2 \]

\section{Branching Processes}
\subsection{Introduction}
Let $(X_n \colon n \geq 0)$ be a random process, where $X_n$ is the number of individuals in generation $n$, and $X_0 = 1$. The individual in generation 0 produces a random number of offspring with distribution
\[ g_k = \prob{X_1 = k} \]
Then every individual in generation 1 produces an independent number of offspring with the same distribution. This is called a branching process. We can write a recursive formula for $X_n$. First, let $(Y_{k, n} \colon k \geq 1, n \geq 0)$ be an independent and identically distributed sequence with distribution $(g_k)_k$. So $Y_{k, n}$ is the number of offspring of the $k$th individual in generation $n$.
\[ X_{n+1} = \begin{cases}
		Y_{1,n} + \cdots + Y_{X_n,n} & \text{when } X_n \geq 1 \\
		0                            & \text{otherwise}
	\end{cases} \]

\subsection{Expectation of Generation Size}
\begin{theorem}
	\[ \expect{X_n} = \expect{X_1}^n \]
\end{theorem}
\begin{proof}
	Inductively,
	\begin{align*}
		\expect{X_{n+1}}                     & = \expect{\expect{X_{n+1} \mid X_n}}                 \\
		\expect{X_{n+1} \mid X_n = m}        & = \expect{Y_{1,n} + \cdots + Y_{X_n,n} \mid X_n = m} \\
		                                     & = \expect{Y_{1,n} + \cdots + Y_{m,n} \mid X_n = m}   \\
		                                     & = m \expect{Y_{1,n}}                                 \\
		                                     & = m \expect{X_1}                                     \\
		\therefore \expect{X_{n+1} \mid X_n} & = X_n \cdot \expect{X_1}                             \\
		\therefore \expect{X_{n+1}}          & = \expect{X_n \cdot \expect{X_1}}                    \\
		                                     & = \expect{X_n} \cdot \expect{X_1}
	\end{align*}
\end{proof}

\subsection{Probability Generating Functions}
\begin{theorem}
	Let $G(z) = \expect{z^{X_1}}$ be the probability generating function of $X_1$, and $G_n(z) = \expect{z^{X_n}}$ be the probability generating function of $X_n$. Then
	\[ G_{n+1}(z) = G(G_n(z)) = G(G(\cdots G(z)\cdots)) = G_n(G(z)) \]
\end{theorem}
\begin{proof}
	\begin{align*}
		G_{n+1}(z)                                            & = \expect{z^{X_{n+1}}}                                 \\
		                                                      & = \expect{\expect{z^{X_{n+1}} \mid X_n}}               \\
		\expect{z^{X_{n+1}} \mid X_n = m}                     & = \expect{z^{Y_{1, n} + \dots + Y_{m,n}} \mid X_n = m} \\
		                                                      & = \expect{z^{X_1}}^m                                   \\
		                                                      & = G(z)^m                                               \\
		\therefore \expect{\expect{z^{X_{n+1}} \mid X_n = m}} & = \expect{G(z)^m}                                      \\
		                                                      & = G_n(G(z))
	\end{align*}
\end{proof}

\subsection{Probability of Extinction}
We define the extinction probability $q$ as the probability that $X_n = 0$ for some $n \geq 1$, and $q_n = \prob{X_n = 0}$. It is clear that $X_n = 0$ implies that $X_{n+1} = 0$. So the sequence of events $(A_n) = (\{ X_n = 0 \})$ is an increasing sequence of events. So by the continuity of the probability measure, $\prob{A_n}$ converges to $\prob{\bigcup A_n}$ as $n \to \infty$. Note that the event $\bigcup A_n$ is the event that there will be extinction. Therefore, $q_n \to q$ as $n \to \infty$.
\begin{claim}
	$q_{n+1} = G(q_n)$ and $q = G(q)$.
\end{claim}
\begin{proof}
	Using the above theorem on $Q$,
	\begin{align*}
		q_{n+1} & = \prob{X_{n+1} = 0} \\
		        & = G_{n+1}(0)         \\
		        & = G(G_n(0))          \\
		        & = G(q_n)
	\end{align*}
	Since $G$ is continuous, taking the limit as $n \to \infty$ and using that $q_n \to q$ gives $G(q) = q$.
\end{proof}
\noindent We can form another proof for the first part of the above claim.
\begin{proof}
	Instead of conditioning on the previous generation, let us condition on the first generation, i.e. $X_1 = m$. Note that after the first generation, we will have $m$ independent subtrees on the family tree. Each tree is identically distributed to the entire tree as a whole. Hence,
	\[ X_{n+1} = X_n^{(1)} + \dots + X_n^{(m)} \]
	where the $X_i^{(j)}$ are independent and identically distributed random processes each with the same offspring distribution. By the law of total probability,
	\begin{align*}
		q_{n+1} & = \prob{X_{n+1} = 0}                                                     \\
		        & = \sum_m \prob{X_{n+1} = 0 \mid X_1 = m} \cdot \prob{X_1 = m}            \\
		        & = \sum_m \prob{X_n^{(1)} = 0, \dots, X_n^{(m)} = 0} \cdot \prob{X_1 = m} \\
		        & = \sum_m \prob{X_n^{(1)} = 0}^m \cdot \prob{X_1 = m}                     \\
		        & = \sum_m q_n^m \cdot \prob{X_1 = m}                                      \\
		        & = G(q_n)
	\end{align*}
\end{proof}
\begin{theorem}
	The extinction probability $q$ is the minimal non-negative solution to $G(t) = t$. Further, supposing that $\prob{X_1 = 1} < 1$, we have that $q < 1$ if and only if $\expect{X_1} > 1$.
\end{theorem}
\begin{proof}
	First, we will prove the minimality of $q$. Let $t$ be the smallest non-negative solution to $G(t) = t$. We will prove inductively that $q_n \leq t$ for all $n$, and then by taking limits we have that $q \leq t$. Since $q$ is a solution, this will imply that $q=t$. Now, as a base case, $q_0 = 0 = \prob{X_0 = 0} \leq t$. Inductively let us suppose that $q_n \leq t$. We know that $q_{n+1} = G(q_n)$. $G$ is an increasing function on $[0, 1]$, and since $q_n \leq t$ we have $q_{n+1} = G(q_n) \leq G(t) = t$.

	Now, we can take $\prob{X_1 = 1} < 1$. Let us use the notation $g_r = \prob{X_1 = r}$ for simplicity. Consider the function $H(z) = G(z) - z$. Let us assume further that $g_0 + g_1 < 1$, since otherwise we cannot possibly ever increase the amount of individuals in future generations, as $\expect{X_1} = \prob{X_1 = 1} < 1$. In this case, $G(z) = g_0 + g_1 z = 1 - \expect{X_1} + \expect{X_1} \cdot z$, and solving $G(z) = z$ we would get only $z=1$ since $\expect{X_1} < 1$. Now,
	\[ H''(z) = \sum_{r=2}^\infty r(r-1)g_r z^{r-2} > 0\quad \forall z \in (0, 1) \]
	This implies that $H'(z)$ is a strictly increasing function in $(0, 1)$. Hence, $H(z)$ has at most one root different from 1 in $(0, 1)$, which follows from Rolle's theorem; indeed, if it had two roots different from 1, then $H'$ would be zero once in $(z_1, z_2)$ and once in $(z_2, 1)$, which contradicts the fact that $H'$ is strictly increasing.

	Let us first consider the case where $H$ has no other root apart from 1. In this case, $H(1) = 0$ and $H(0) = g_0 \geq 0 \implies H(z) \geq 0$ for all $z \in [0, 1]$. We can find that
	\[ H'(1^-) = \lim_{z \to 1^-} \frac{H(z) - H(1)}{z-1} = \frac{H(z)}{z-1} < 0 \]
	since the numerator is positive, and the denominator is negative. We know that $H'(1^-) = G'(1^-) - 1$, and $H'(1^-) \leq 0 \implies G'(1^-) \leq 1$, and $G'(1^-) = \expect{X_1}$. So when $q=1$, then $\expect{X_1} \leq 1$.

	In the other case, $H$ has one other root $r<1$ as well as 1. We have that $H(r) = 0$ and $H(1) = 0$. By Rolle's theorem, there exists $z \in (r, 1)$ such that $H'(z) = 0$. Further, $H'(x) = G'(x) - 1$ therefore $G'(z) = 1$. Now,
	\[ G'(x) = \sum_{r = 1}^\infty rg_r x^{r-1} \implies H''(x) = G''(x) = \sum_{r = 2}^\infty r(r-1)g_rx^{r-2} \]
	Under the assumption that $g_0 + g_1 < 1$, we have that $G''(x) > 0$ for all $x \in (0, 1)$, hence $G'$ is strictly increasing for all $x \in (0, 1)$. Therefore, $G'(1^-) > G'(z) = 1$ giving $\expect{X_1} > 1$. So if $q < 1$, then $\expect{X_1} > 1$.
\end{proof}

\section{Continuous Random Variables}
\subsection{Probability Distribution Function}
Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Then, as defined before, $X \colon \Omega \to \mathbb R$ is a random variable if
\[ \forall x \in \mathbb R, \{ X \leq x \} = \{ \omega \colon X(\omega) \leq x \} \in \mathcal F \]
We define the probability distribution function $F \colon \mathbb R \to [0, 1]$ as
\[ F(x) = \prob{X \leq x} \]
\begin{theorem}
	The following properties hold.
	\begin{enumerate}[(i)]
		\item If $x \leq y$, then $F(x) \leq F(y)$.
		\item For all $a < b$, $\prob{a < x \leq b} = F(b) - F(a)$.
		\item $F$ is a right continuous function, and left limits always exist. In other words,
		      \[ F(x^+) = \lim_{y \to x^+} F(y) = F(x);\quad F(x^-) = \lim_{y \to x^-} F(y) \leq F(x) \]
		\item For all $x \in\mathbb R$, $F(x^-) = \prob{X < x}$.
		\item We have $\lim_{x \to \infty} F(x) = 1$ and $\lim_{x \to -\infty} F(x) = 0$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	\begin{enumerate}[(i)]
		\item The first statement is immediate from the definition of the probability measure.
		\item We can deduce
		      \begin{align*}
			      \prob{a < X \leq b} & = \prob{\{ a < X \} \cap \{ X \leq b \}}                  \\
			                          & = \prob{X \leq b} - \prob{\{X \leq b\} \cap \{X \leq a\}} \\
			                          & = \prob{X \leq b} - \prob{X \leq a}                       \\
			                          & = F(b) - F(a)
		      \end{align*}
		\item For right continuity, we want to prove $\lim_{n \to \infty} F(x + \frac{1}{n}) = F(x)$. We will define $A_n = \{ x < X \leq x + \frac{1}{n} \}$. Then the $A_n$ are decreasing events, and the intersection of all $A_n$ is the empty set $\varnothing$. Hence, by continuity of the probability measure, $\prob{A_n} \to 0$ as $n \to \infty$. But $\prob{A_n} = \prob{x < X \leq x + \frac{1}{n}} = F(x + \frac{1}{n}) - F(x)$, hence $F(x + \frac{1}{n}) \to F(x)$ as required. Now, we want to show that left limits always exist. This is clear since $F$ is an increasing function, and is always bounded above by 1.
		\item We know $F(x^-) = \lim_{n \to \infty}F(x - \frac{1}{n})$. Consider $B_n = \{ X \leq x - \frac{1}{n} \}$. Then the $B_n$ is an increasing sequence of events, and their union is $\{ X < x \}$. Hence $\prob{B_n}$ converges to $\prob{X < x}$, so $F(x^-) = \prob{X < x}$.
		\item This is evident from the properties of the probability measure.
	\end{enumerate}
\end{proof}

\subsection{Defining a Continuous Random Variable}
For a discrete random variable, $F$ is a step function, which of course is right continuous with left limits.
\begin{definition}
	A random variable $X$ is called \textit{continuous} if $F$ is a continuous function. In this case, clearly left limits and right limits give the same value, and $\prob{X = x} = 0$ for all $x \in\mathbb R$.
\end{definition}
\noindent In this course, we will consider only \textit{absolutely} continuous random variables. A continuous random variable is absolutely continuous if $F$ is differentiable. We will make the convention that $F'(x) = f(x)$, where $f(x)$ is called the probability density function of $X$. The following immediate properties hold.
\begin{enumerate}[(i)]
	\item $f \geq 0$
	\item $\int_{-\infty}^{+\infty} f(x) \dd{x} = 1$
	\item $F(x) = \int_{-\infty}^x f(t) \dd{t}$
	\item For $S \subseteq \mathbb R$, $\prob{X \in S} = \int_S f(x) \dd{x}$
\end{enumerate}
Here is an intuitive explanation of the probability density function. Suppose $\Delta x$ is a small quantity. Then
\[ \prob{x < X \leq x + \Delta x} = \int_x^{x + \Delta x} f(y) \dd{y} \approx f(x) \cdot \Delta x \]
So we can think of $f(x)$ as the continuous analogy to $\prob{X = x}$.

\section{Properties of Continuous Random Variables}
\subsection{Expectation}
Consider a continuous random variable $X \colon \Omega \to \mathbb R$, with probability distribution function $F(x)$ and probability density function $f(x) = F'(x)$. We define the expectation of such a \textit{non-negative} random variable as
\[ \expect{X} = \int_0^\infty x f(x) \dd{x} \]
In this case, the expectation is either non-negative and finite, or positive infinity. Now, let $X$ be a general continuous random variable, that is not necessarily non-negative. Suppose $g \geq 0$. Then,
\[ \expect{g(X)} = \int_{-\infty}^\infty g(x) f(x) \dd{x} \]
We can define $X_+ = \max(X, 0)$ and $X_- = \min(-X, 0)$. If at least one of $\expect{X_+}$ or $\expect{X_-}$ is finite, then clearly
\[ \expect{X} := \expect{X_+} - \expect{X_-} = \int_{-\infty}^\infty xf(x) \dd{x} \]
It is easy to verify that the expectation is a linear function, due to the linearity property of the integral.

\subsection{Computing the Expectation}
\begin{claim}
	Let $X \geq 0$. Then
	\[ \expect{X} = \int_0^\infty \prob{X \geq x} \dd{x} \]
\end{claim}
\begin{proof}
	Using the definition of the expectation,
	\begin{align*}
		\expect{X} & = \int_0^\infty xf(x) \dd{x}                                           \\
		           & = \int_0^\infty \left( \int_0^x \dd{y} \right) f(x) \dd{x}             \\
		           & = \int_0^x \dd{y} \int_y^\infty f(x) \dd{x}                            \\
		           & = \int_0^\infty \dd{y} \left( 1 - \int_{-\infty}^y f(x) \dd{x} \right) \\
		           & = \int_0^\infty \dd{y} \prob{X \geq y}
	\end{align*}
\end{proof}
\noindent Here is an alternative proof.
\begin{proof}
	For every $\omega \in \Omega$, we can write
	\[ X(\omega) = \int_0^\infty 1(X(\omega) \geq x) \dd{x} \]
	Taking expectations, we get
	\[ \expect{X} = \expect{\int_0^\infty 1(X(\omega) \geq x) \dd{x}} \]
	We will interchange the integral and the expectation, although this step is not justified or rigorous.
	\begin{align*}
		\expect{X} & = \int_0^\infty \expect{1(X(\omega) \geq x)} \dd{x} \\
		           & = \int_0^\infty \prob{X \geq x} \dd{x}
	\end{align*}
\end{proof}

\subsection{Variance}
We define the variance of a continuous random variable as
\[ \Var{X} = \expect{(X - \expect{X})^2} = \expect{X^2} - \expect{X}^2 \]

\subsection{Uniform Distribution}
Consider the uniform distribution defined by $a, b \in\mathbb R$.
\[ f(x) = \begin{cases}
		\frac{1}{b-a} & x \in [a, b]     \\
		0             & \text{otherwise}
	\end{cases} \]
We write $X \sim U[a, b]$. For some $x \in [a,b]$, we can write
\[ \prob{X \leq x} = \int_a^x f(y) \dd{y} = \frac{x-a}{b-a} \]
Hence, for $x \in [a,b]$,
\[ F(x) = \begin{cases}
		1               & x > b       \\
		\frac{x-a}{b-a} & x \in [a,b] \\
		0               & x < a
	\end{cases} \]
Then,
\[ \expect{X} = \int_a^b \frac{x}{b-a} \dd{x} = \frac{a+b}{2} \]

\subsection{Exponential Distribution}
The exponential distribution is defined by $f(x) = \lambda e^{-\lambda x}$ for $\lambda > 0$, $x > 0$. We write $X \sim \mathrm{Exp}(\lambda)$.
\[ F(x) = \prob{X \geq x} = \int_0^\infty \lambda e^{-\lambda y} \dd{y} = 1 - e^{-\lambda x} \]
Further,
\[ \expect{X} = \int_0^\infty \lambda x^{-\lambda x} \dd{x} = \frac{1}{\lambda} \]
We can view the exponential distribution as a limit of geometric distributions. Suppose that $T \sim \mathrm{Exp}(\lambda)$, and let $T_n = \floor{nT}$ for all $n \in \mathbb N$. We have
\[ \prob{T_n \geq k} = \prob{T \geq \frac{k}{n}} = e^{-\lambda k / n} = \left( e^{-\lambda/n} \right)^k \]
Hence $T_n$ is a geometric distribution with parameter $p_n = e^{-\lambda/n}$. As $n \to \infty$, $p_n \sim \frac{\lambda}{n}$, and $\frac{T_n}{n} \sim T$. Hence the exponential distribution is the limit of a scaled version of the geometric distribution. A key property of the exponential distribution is that it has no memory. If $T \sim \mathrm{Exp}(\lambda)$, $\prob{T > t + s \mid T > s} = \prob{T > t}$. In fact, the distribution is uniquely characterised by this property.
\begin{proposition}
	Let $T$ be a positive continuous random variable not identically zero or infinity. Then $T$ has the memoryless property $\prob{T > t + s \mid T > s} = \prob{T > t}$ if and only if $T \sim \mathrm{Exp}(\lambda)$ for some $\lambda > 0$.
\end{proposition}
\begin{proof}
	Clearly if $T \sim \mathrm{Exp}(\lambda)$, then $\prob{T > t + s \mid T > s} = e^{-\lambda t} = \prob{T > t}$ as required. Now, given that $T$ has this memoryless property, for all $s$ and $t$, we have $\prob{T > t + s} = \prob{T > t} \prob{T > s}$. Let $g(t) = \prob{T > t}$; we would like to show that $g(t) = e^{-\lambda t}$. Then $g$ satisfies $g(t+s) = g(t)g(s)$. Then for all $m \in \mathbb N$, $g(mt) = (g(t))^m$. Setting $t=1$, $g(m) = g(1)^m$. Now, $g(m/n)^n = g(mn/n) = g(m)$ hence $g(m/n) = g(1)^{m/n}$. So for all rational numbers $q \in \mathbb Q$, $g(q) = g(1)^q$.

	Now, $g(1) = \prob{T > 1} \in (0, 1)$. Indeed, $g(1) \neq 0$ since in this case, for any rational number $q$ we would have $g(q) = 0$ contradicting the assumption that $T$ was not identically zero, and $g(1) \neq \infty$ because in this case $T$ would be identically infinity. Now, let $\lambda = -\log\prob{T > 1} > 0$. We have now proven that $g(t) = e^{-\lambda t}$ for all $t\in\mathbb Q$.

	Let $t \in \mathbb R_+$. Then for all $\varepsilon > 0$, there exist $r, s \in \mathbb Q$ such that $r \leq t \leq s$ and $\abs{r - s} \leq \varepsilon$. In this case, $e^{-\lambda s} = \prob{T > s} \leq \prob{T > t} \leq \prob{T > r} = e^{-\lambda r}$. Sending $\varepsilon \to 0$ finishes the proof, showing that $g(t) = e^{-\lambda t}$ for all positive reals.
\end{proof}

\subsection{Functions of Continuous Random Variables}
\begin{theorem}
	Suppose that $X$ is a continuous random variable with density $f$. Let $g$ be a monotonic continuous function (either strictly increasing or strictly decreasing), such that $g^{-1}$ is differentiable. Then $g(X)$ is a continuous random variable with density $fg^{-1}(x) \abs{\dv{x} g^{-1}(x)}$.
\end{theorem}
\begin{proof}
	Suppose that $g$ is strictly increasing. We have
	\[ \prob{g(X) \leq x} = \prob{X \leq g^{-1}(x)} = F(g^{-1}(x)) \]
	Hence,
	\[ \dv{x} \prob{g(X) \leq x} = F'(g^{-1}(x)) \cdot \dv{x} g^{-1}(x) = f(g^{-1}(x)) \dv{x}g^{-1}(x) \]
	Note that since $g$ is strictly increasing, so is $g^{-1}$. Now, suppose the $g$ is strictly decreasing. Since the random variable is continuous,
	\[ \prob{g(X) \leq x} = \prob{X \geq g^{-1}(x)} = 1 - F(g^{-1}(x)) \]
	Hence,
	\[ \dv{x} \prob{g(X) \leq x} = -F'(g^{-1}(x)) \cdot \dv{x} g^{-1}(x) = f(g^{-1}(x)) \abs{\dv{x}g^{-1}(x)} \]
	Likewise, in this case, $g$ is strictly decreasing.
\end{proof}

\subsection{Normal Distribution}
The normal distribution is characterised by $\mu \in \mathbb R$ and $\sigma > 0$. We define
\[ f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\qty{-\frac{(x-\mu)^2}{2\sigma^2}} \]
$f(x)$ is indeed a probability density function:
\[ I = \int_{-\infty}^\infty f(x) \dd{x} = \int_{-\infty}^\infty \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\qty{-\frac{(x-\mu)^2}{2\sigma^2}} \dd{x} \]
Applying the substitution $x \mapsto \frac{x-\mu}{\sigma}$, we have
\[ I = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^\infty \exp\qty{-\frac{x^2}{2}} \dd{x} \]
We can evaluate this integral by considering $I^2$.
\[ I^2 = \frac{2}{\pi} \int_0^\infty \int_0^\infty e^{\frac{-(u^2 - v^2)}{2}} \dd{u}\dd{v} \]
Using polar coordinates $u = r\cos\theta$ and $v = r\sin\theta$, we have
\[ I^2 = \frac{2}{\pi} \int_0^\infty \dd{r} \int_0^{\frac{\pi}{2}} \dd{\theta} re^{-\frac{r^2}{2}} = 1 \implies I = \pm 1 \]
But clearly $I > 0$, so $I=1$. Hence $f$ really is a probability density function. Now, if $X \sim \mathrm{N}(\mu, \sigma^2)$,
\begin{align*}
	\expect{X} & = \int_{-\infty}^{\infty} \frac{x}{\sqrt{2\pi\sigma^2}} \exp\qty{-\frac{(x-\mu)^2}{2\sigma^2}} \dd{x}                                                                                                                                                                                                                      \\
	           & = \underbrace{\int_{-\infty}^{\infty} \frac{x - \mu}{\sqrt{2\pi\sigma^2}} \exp\qty{-\frac{(x-\mu)^2}{2\sigma^2}} \dd{x}}_{\text{odd function around } \mu \text{ hence } 0} + \mu\underbrace{\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\qty{-\frac{(x-\mu)^2}{2\sigma^2}} \dd{x}}_{I = 1 \text{ by above}} \\
	           & = \mu                                                                                                                                                                                                                                                                                                                      \\
\end{align*}
We can also compute the variance, using the substitution $u = \frac{x - \mu}{\sigma}$, giving
\begin{align*}
	\Var{X} & = \int_{-\infty}^{\infty} \frac{(x - \mu)^2}{\sqrt{2\pi\sigma^2}} \exp\qty{-\frac{(x-\mu)^2}{2\sigma^2}} \dd{x} \\
	        & = \sigma^2 \int_{-\infty}^{\infty} \frac{u^2}{\sqrt{2\pi}} \exp\qty{-\frac{u^2}{2}} \dd{u}                      \\
	        & = \sigma^2
\end{align*}
In particular, when $\mu = 0$ and $\sigma^2 = 1$, we call the distribution $\mathrm{N}(\mu, \sigma^2) = \mathrm{N}(0, 1)$ the standard normal distribution. We define
\[ \Phi(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-\frac{u^2}{2}} \dd{u};\quad \phi(x) = \Phi'(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \]
Hence $\Phi(x) = \prob{X \leq x}$ if $X$ has the standard normal distribution. Since $\phi(x) = \phi(-x)$, we have $\Phi(x) + \Phi(-x) = 1$, hence $\prob{X \leq x} = 1 - \prob{X \leq -x}$.

\section{Multivariate Density Functions}
\subsection{Standardising Normal Distributions}
Suppose $X \sim \mathrm{N}(\mu, \sigma^2)$. Let $a \neq 0, b \in \mathbb R$, and let $g(x) = ax+b$. We define $Y = g(X) = aX+b$. We can find the density $f_Y$ of $Y$, by noting that $g$ is a monotonic function and the inverse has a derivative. We can then use the theorem in the last lecture to show that
\begin{align*}
	f_Y(y) & = f_X(g^{-1}(y)) \cdot \abs{\dv{y} g^{-1}(y)}                                                       \\
	       & = \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{(\frac{y-b}{a} - \mu)^2}{2\sigma^2}) \cdot \frac{1}{2a} \\
	       & =\frac{1}{\sqrt{2\pi a^2\sigma^2}} \exp(-\frac{(y - a\mu + b)^2}{2 a^2 \sigma^2})
\end{align*}
Hence $Y \sim \mathrm{N}(a \mu + b, a^2 \sigma^2)$. In particular, $\frac{X-\mu}{\sigma}$ is exactly the standard normal distribution.
\begin{definition}
	Suppose $X$ is a continuous random variable. Then the median of $X$, denoted by $m$, is the number satisfying
	\[ \prob{X \leq m} = \prob{X \geq m} = \frac{1}{2} \]
\end{definition}
\noindent If $X \sim \mathrm{N}(\mu, \sigma^2)$, then $\prob{X \leq \mu} = \Phi(0) = \frac{1}{2}$ hence $\mu$ is the median of the normal distribution.

\subsection{Multivariate Density Functions}
Suppose $X = (X_1, \dots, X_n) \in \mathbb R^n$ is a random variable. We say that $X$ has density $f$ is
\[ \prob{X_1 \leq x_1, \dots, X_n \leq x_n} = \int_{-\infty}^{x_1} \dots \int_{-\infty}^{x_n}  f(y_1, \dots, y_n) \dd{y_1} \dots \dd{y_n} \]
Then,
\[ f(x_1, \dots, x_n) = \frac{\partial^n}{\partial x_1 \dots \partial x_n} F(x_1, \dots, x_n) \]
This generalises the fact that for all (reasonable) $B \subseteq \mathbb R^n$,
\[ \prob{(X_1, \dots, X_n) \in B} = \int_B f(y_1, \dots, y_n) \dd{y_1}\dots\dd{y_n} \]

\subsection{Independence of Events}
In the continuous case, we can no longer use the definition $\prob{X = a, Y = b} = \prob{X = a}\prob{Y = b}$, since the probability of a random variable being a specific value is always zero. Instead, we define that $X_1, \dots X_n$ are independent if for all $x_1, \dots, x_n \in \mathbb R$,
\[ \prob{X_1 \leq x_1, \dots, X_n \leq x_n} = \prob{X_1 \leq x_1}\cdots\prob{X_n \leq x_n} \]
\begin{theorem}
	Suppose $X = (X_1, \dots, X_n)$ has density $f$.
	\begin{enumerate}[(a)]
		\item Suppose $X_1, \dots, X_n$ are independent with densities $f_1, \dots, f_n$. Then $f(x_1, \dots, x_n) = f_1(x_1)\cdots f_n(x_n)$.
		\item Suppose that $f$ factorises as $f(x_1, \dots, x_n) = f_1(x_1)\cdots f_n(x_n)$ for some non-negative functions $f_1, \dots, f_n$. Then $X_1, \dots, X_n$ are independent with densities proportional to $f_1, \dots, f_n$. (In order to have a density function, we require that it integrates to 1, so we choose a scaling factor such that this requirement holds.)
	\end{enumerate}
	In other words, $f$ factorises if and only if it is comprised of independent events.
\end{theorem}
\begin{proof}
	\begin{enumerate}[(a)]
		\item We know that
		      \begin{align*}
			      \prob{X_1 \leq x_1, \dots, X_n \leq x_n} & = \prob{X_1 \leq x_1}\cdots\prob{X_n \leq x_n}                                       \\
			                                               & = \int_{-\infty}^{x_1} f_1(y_1)\dd{y_1} \cdots \int_{-\infty}^{x_n} f_n(y_n)\dd{y_n} \\
			                                               & = \int_{-\infty}^{x_1} \dots \int_{-\infty}^{x_n} \prod_{i=1}^n f_i(y_i) \dd{y_i}
		      \end{align*}
		      So the density of $(X_1, \dots, X_n)$ is the product of the $(f_i)$.
		\item Suppose $f$ factorises. Let $B_1, \dots, B_n \subseteq \mathbb R$. Then
		      \[ \prob{X_1 \in B_1, \dots, X_n \in B_n} = \int_{B_1} \cdots \int_{B_n} f_1(x_1)\cdots f_n(x_n) \dd{y_1} \cdots \dd{y_n} \]
		      Now, let $B_j = \mathbb R$ for all $j \neq i$. Then
		      \[ \prob{X_i \in B_i} = \prob{X_i \in B_i, X_j \in B_j \;\forall j \neq i} = \int_{B_i} f_i(y_i) \dd{y_i} \cdot \prod_{j \neq 1} \int_{B_j} f_j(x_j)\dd{y_j} \]
		      Since $f$ is a density function,
		      \[ \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty f(x_1, \dots, x_n) \dd{x_1} \cdots \dd{x_n} = 1 \]
		      But $f$ is the product of the $f_i$, so
		      \[ \prod_j \int_{-\infty}^\infty f_j(y) \dd{y} = 1 \implies \prod_{j \neq i} \int_{-\infty}^\infty f_j(y) \dd{y} = \frac{1}{\int_{-\infty}^\infty f_i(y) \dd{y}} \]
		      Hence,
		      \[ \prob{X_i \in B_i} = \frac{\int_{B_i} f_i(y) \dd{y}}{\int_{-\infty}^\infty f_i(y) \dd{y}} \]
		      This shows that the density of $X_i$ is
		      \[ \frac{f_i}{\int_{-\infty}^\infty f_i(y) \dd{y}} \]
		      The $X_i$ are independent, since
		      \begin{align*}
			      \prob{X_1 \leq x_1, \dots, X_n \leq x_n} & = \frac{\int_{-\infty}^{x_1} f_1(y_1)\dd{y_1} \cdots \int_{-\infty}^{x_n} f_n(y_n) \dd{y_n}}{\int_{-\infty}^\infty f_1(y_1)\dd{y_1} \cdots \int_{-\infty}^\infty f_n(y_n) \dd{y_n}} \\
			                                               & = \prob{X_1 \leq x_1}\cdots\prob{X_n \leq x_n}
		      \end{align*}
	\end{enumerate}
\end{proof}

\subsection{Marginal Density}
Suppose that $(X_1, \dots, X_n)$ has density $f$. Now we can compute the marginal density as follows.
\begin{align*}
	\prob{X_1 \leq x} & = \prob{X_1 \leq x, X_2 \in \mathbb R, \dots, X_n \in \mathbb R}                                                                                                                        \\
	                  & = \int_{-\infty}^x \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty f(x_1, \dots, x_n) \dd{x_1}\cdots \dd{x_n}                                                                        \\
	                  & = \int_{-\infty}^x \dd{x_1} \underbrace{\left( \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty f(x_1, \dots, x_n) \dd{x_2}\cdots \dd{x_n} \right)}_{\text{marginal density of } X_1} \\
\end{align*}

\subsection{Sum of Random Variables}
Recall that in the discrete case, for independent random variables $X$ and $Y$ we have
\[ \prob{X+Y = z} = \sum_y \prob{X+Y = z, Y=y} = \sum_y \prob{X = z-y} \prob{Y = y} = \sum_y p_x(z-y) p_y(y) \]
which was called the convolution. In the continuous case,
\begin{align*}
	\prob{X+Y \leq z} & = \iint_{\{ x+y \leq z \}} f_{X, Y}(x, y) \dd{x}\dd{y}                                                                   \\
	                  & = \int_{-\infty}^\infty \int_{-\infty}^{z-x} f_X(x)f_Y(y) \dd{x}\dd{y}                                                   \\
	                  & = \int_{-\infty}^\infty \left( \int_{-\infty}^{z} f_X(x)f_Y(y-x) \dd{y} \right) \dd{x}\; (\text{using } y \mapsto y + x) \\
	                  & = \int_{-\infty}^z \dd{y} \underbrace{\left( \int_{-\infty}^\infty f_Y(y-x) f_X(x) \dd{x} \right)}_{g(y)}
\end{align*}
Hence the density of $X+Y$ is $g(y)$, where
\[ g(y) = \int_{-\infty}^\infty f_Y(y-x) f_X(x) \dd{x} \]
\begin{definition}
	Let $f, g$ be density functions. Then the convolution of $f$ and $g$ is
	\[ (f \star g)(y) = \int_{-\infty}^\infty f_Y(y-x) f_X(x) \dd{x} \]
\end{definition}
\noindent Here is a non-rigorous argument, which can be used as a heuristic.
\begin{align*}
	\prob{X + Y \leq z}        & = \int_{-\infty}^\infty \prob{X + Y \leq z, Y \in \dd{y}}      \\
	                           & = \int_{-\infty}^\infty \prob{X + Y \leq z, Y \in \dd{y}}      \\
	                           & = \int_{-\infty}^\infty \prob{X \leq z - y}\prob{Y \in \dd{y}} \\
	                           & = \int_{-\infty}^\infty \prob{X \leq z - y}f_Y(y)\dd{y}        \\
	                           & = \int_{-\infty}^\infty F_X(z - y)f_Y(y)\dd{y}                 \\
	\dv{z} \prob{X + Y \leq z} & = \int_{-\infty}^\infty \dv{z} F_X(z - y)f_Y(y)\dd{y}          \\
	                           & = \int_{-\infty}^\infty f_X(z - y)f_Y(y)\dd{y}                 \\
\end{align*}

\section{Conditions, Transformations and Ordered Statistics}
\subsection{Conditional Density}
We will now define the conditional density of a continuous random variable, given the value of another continuous random variable. Let $X$ and $Y$ be continuous random variables with joint density $f_{X, Y}$ and marginal densities $f_X$ and $f_Y$. Then we define the conditional density of $X$ given that $Y = y$ is defined as
\[ f_{X \mid Y}(x \mid y) = \frac{f_{X, Y}(x, y)}{f_Y(y)} \]
Then we can find the law of total probability in the continuous case.
\begin{align*}
	f_X(x) & = \int_{-\infty}^\infty f_{XY}(x, y) \dd{y}                 \\
	       & = \int_{-\infty}^\infty f_{X \mid Y}(x \mid y)f_Y(y) \dd{y}
\end{align*}

\subsection{Conditional Expectation}
We want to define $\expect{X \mid Y}$ to be some function $g(Y)$ for some function $g$. We will define
\[ g(y) = \int_{-\infty}^\infty xf_{X \mid Y}(x \mid y) \dd{x} \]
which is the analogous expression to $\expect{X \mid Y = y}$ from the discrete case. Then we just set $\expect{X \mid Y} = g(Y)$ to be the conditional expectation.

\subsection{Transformations of Multidimensional Random Variables}
\begin{theorem}
	Let $X$ be a continuous random variable with values in $D \subseteq \mathbb R^d$, with density $f_X$. Now, let $g$ be a bijection $D$ to $g(D)$ which has a continuous derivative, and $\det g'(x) \neq 0$ for all $x \in D$. Then the random variable $Y = g(X)$ has density
	\[ f_Y(y) = f_X(x) \cdot \abs{J} \text{ where } x = g^{-1}(y) \]
	where $J$ is the Jacobian
	\[ J = \det \left( \left( \pdv{x_i}{y_j} \right)_{i, j = 1}^d \right) \]
\end{theorem}
\noindent No proof will be given for this theorem. As an example, let $X$ and $Y$ be independent continuous random variables with the standard normal distribution. The point $(X, Y)$ in $\mathbb R^2$ has polar coordinates $(R, \Theta)$. What are the densities of $R$ and $\Theta$? We have $X = R\cos\Theta$ and $Y = R\sin\Theta$. The Jacobian is
\[ J = \det\begin{pmatrix}
		\cos\theta & -r\sin\theta \\
		\sin\theta & r\cos\theta
	\end{pmatrix} = r \]
Hence,
\begin{align*}
	f_{R, \Theta}(r, \theta) & = f_{X, Y}(r\cos\theta, r\sin\theta) \abs{J}                                                                            \\
	                         & = f_{X, Y}(r\cos\theta, r\sin\theta) r                                                                                  \\
	                         & = f_X(r\cos\theta) f_Y(r\sin\theta) r                                                                                   \\
	                         & = \frac{1}{\sqrt{2\pi}}e^{-\frac{r^2\cos^2\theta}{2}} \cdot \frac{1}{\sqrt{2\pi}}e^{-\frac{r^2\sin^2\theta}{2}} \cdot r \\
	                         & = \frac{1}{2\pi}e^{-\frac{r^2}{2}} \cdot r
\end{align*}
for all $r > 0$ and $\theta \in [0, 2\pi]$. Note that the joint density factorises into marginal densities:
\[ f_{R, \Theta}(r, \theta) = \underbrace{\frac{1}{2\pi}}_{f_\Theta} \underbrace{re^{-\frac{r^2}{2}}}_{f_R} \]
so the random variables $R$ and $\Theta$ are independent, where $\Theta \sim U[0, 2\pi]$ and $R$ has density $re^{\frac{-r^2}{2}}$ on $(0, \infty)$.

\subsection{Ordered Statistics of a Random Sample}
Let $X_1, \dots, X_n$ be independent and identically distributed random variables with distribution function $F$ and density function $f$.  We can put them in increasing order:
\[ X_{(1)} \leq X_{(2)} \leq \dots \leq X_{(n)} \]
and let $Y_i = X_{(i)}$. The $(Y_i)$ are the order statistics.
\begin{align*}
	\prob{Y_1 \leq x} & = \prob{\min(X_1, \dots, X_n) \leq x}    \\
	                  & = 1 - \prob{\min(X_1, \dots, X_n) > x}   \\
	                  & = 1 - \prob{X_1 > x}\cdots\prob{X_n > x} \\
	                  & = 1 - (1 - F(x))^n
\end{align*}
Further,
\begin{align*}
	f_{Y_1}(x) & = \dv{x}\left( 1 - (1 - F(x))^n \right) \\
	           & = n (1 - F(x))^{n-1} f(x)
\end{align*}
We can compute an analogous result for the maximum.
\begin{align*}
	\prob{Y_n \leq x} & = (F(x))^n           \\
	f_{Y_n}(x)        & = n(F(x))^{n-1} f(x)
\end{align*}
What are the densities of the other random variables? First, let $x_1 < x_2 < \dots < x_n$. Then, we can first find the joint distribution $\prob{Y_1 \leq x_1, \dots, Y_n \leq x_n}$. Note that this is simply the sum over all possible permutations of the $(X_i)$ of $\prob{X_1 \leq x_1, \dots, X_n \leq x_n}$. But since the variables are independent and identically distributed, these probabilities are the same. Hence,
\begin{align*}
	\prob{Y_1 \leq x_1, \dots, Y_n \leq x_n}        & = n! \cdot \prob{X_1 \leq x_1, \dots, X_n \leq x_n, X_1 < \dots < X_n}                                               \\
	                                                & = n! \int_{-\infty}^{x_1} \int_{u_1}^{x_2} \cdots \int_{u_{n-1}}^{x_n} f(u_1) \cdots f(u_n) \dd{u_1} \cdots \dd{u_n} \\
	\therefore f_{Y_1, \dots, Y_n}(x_1, \dots, x_n) & = n! f(x_1) \cdots f(x_n)
\end{align*}
when $x_1 < x_2 < \dots < x_n$, and the joint density is zero otherwise. Note that this joint density does not factorise as a product of densities, since we must always consider the indicator function that $x_1 < x_2 < \dots < x_n$.

\subsection{Ordered Statistics on Exponential Distribution}
Let $X \sim \mathrm{Exp}(\lambda)$, $Y \sim \mathrm{Exp}(\mu)$ be independent continuous random variables. Let $Z = \min(X, Y)$.
\[ \prob{Z \geq z} = \prob{X \geq z, Y \geq z} = \prob{X \geq z} \prob{Y \geq z} = e^{-\lambda z} \cdot e^{-\mu z} = e^{-(\lambda + \mu)z} \]
Hence $Z$ has the exponential distribution with parameter $\lambda+\mu$. More generally, if $X_1, \dots, X_n$ are independent continuous random variables with $X_i \sim \mathrm{Exp}(\lambda_i)$, then $Z = \min(X_1, \dots, X_n)$ has distribution $\mathrm{Exp}\left( \sum_{i=1}^n \lambda_i \right)$. Now, let $X_1, \dots, X_n$ be independent identically distributed random variables with distribution $\mathrm{Exp}(\lambda)$, and let $Y_i$ be their order statistics. Then
\[ Z_1 = Y_1;\quad Z_2 = Y_2 - Y_1;\quad Z_i = Y_i - Y_{i-1} \]
So the $Z_i$ are the `durations between consecutive results' from the $X_i$. What is the density of these $Z_i$? First, note that
\[ Z = \begin{pmatrix}
		Z_1 \\ \vdots \\ Z_n
	\end{pmatrix} = A \begin{pmatrix}
		Y_1 \\ \vdots \\ Y_n
	\end{pmatrix};\quad A = \begin{pmatrix}
		1      & 0      & 0      & \cdots & 0      \\
		-1     & 1      & 0      & \cdots & 0      \\
		0      & -1     & 1      & \cdots & 0      \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		0      & 0      & 0      & \cdots & 1
	\end{pmatrix} \]
Note that $\det A = 1$, and $Z = AY$, and note further that
\[ y_j = \sum_{i=1}^j z_i \]
Now,
\begin{align*}
	f_{(Z_1, \dots, Z_n)}(z_1, \dots, z_n) & = f_{(Y_1, \dots, Y_n)}(y_1, \dots, y_n) \underbrace{\abs{A}}_{=1} \\
	                                       & = n! f(y_1) \cdots f(y_n)                                          \\
	                                       & = n! (\lambda e^{-\lambda y_1}) \cdots (\lambda e^{-\lambda y_n})  \\
	                                       & = n! \lambda^n e^{-\lambda(nz_1 + (n-1)z_2 + \dots + z_n)}         \\
	                                       & = \prod_{i=1}^n (n-i+1) \lambda e^{-\lambda (n-i+1)z_i}
\end{align*}
The density function of the vector $Z$ factorises into functions of the $z_i$, so $Z_1, \dots, Z_n$ are independent and $Z_i \sim \mathrm{Exp}(\lambda(n-i+1))$.

\section{Moment Generating Functions}
\subsection{Moment Generating Functions}
Consider a continuous random variable $X$ with density $f$. Then the moment generating function of $X$ is defined as
\[ m(\theta) = \expect{e^{\theta X}} = \int_{-\infty}^\infty e^{\theta x} f(x) \dd{x} \]
whenever this integral is finite. Note that $m(0) = 1$.
\begin{theorem}
	The moment generating function uniquely determines the distribution of a continuous random variable, provided that it is defined on some open interval $(a, b)$ of values of $\theta$.
\end{theorem}
\noindent No proof will be given.
\begin{theorem}
	Suppose the moment generating function is defined on an open interval of values of $\theta$. Then
	\[ \eval{\dv[r]{\theta}m(\theta)}_{\theta = 0} = \expect{X^r} \]
\end{theorem}
\begin{theorem}
	Suppose $X_1, \dots, X_n$ are independent random variables. Then
	\[ m(\theta) = \expect{e^{\theta(X_1 + \dots + X_n)}} = \prod_{i=1}^n \expect{e^{\theta X_i}} \]
\end{theorem}
\begin{proof}
	Since the $X_i$ are independent, we can move the product outside of the expectation.
\end{proof}

\subsection{Gamma Distribution}
Let $X$ be a random variable with density
\[ f(x) = e^{-\lambda x}\frac{\lambda^n x^{n-1}}{(n-1)!} \]
where $\lambda > 0$, $n \in \mathbb N$, $x \geq 0$. We can say that $X \sim \Gamma(n, \lambda)$. First, we check that $f$ is indeed a density.
\begin{align*}
	I_n & = \int_0^\infty f(x) \dd{x}                                                     \\
	    & = \int_0^\infty \lambda e^{-\lambda x} \frac{\lambda^n x^{n-1}}{(n-1)!} \dd{x}  \\
	    & = \int_0^\infty \frac{e^{-\lambda x} \lambda^{n-1} (n-1) x^{n-2}}{(n-1)!}\dd{x} \\
	    & = \int_0^\infty \frac{e^{-\lambda x} \lambda^{n-1} x^{n-2}}{(n-2)!}\dd{x}       \\
	    & = I_{n-1} = \dots = I_1
\end{align*}
Note that for $n=1$, $f(x) = \lambda e^{-\lambda x}$ which is the density of the exponential distribution. Therefore, $I_n = 1$ as required, so $f$ really is a density. Now,
\[ m(\theta) = \int_0^\infty \frac{e^{\theta x} e^{-\lambda x} \lambda^n x^{n-1}}{(n-1)!} \dd{x} \]
If $\lambda > \theta$, then we have a finite integral. If $\lambda \leq \theta$, then the exponential term $e^{\theta x}$ will dominate and we will have an infinite integral. So, let $\lambda > \theta$.
\begin{align*}
	m(\theta) & = \int_0^\infty \frac{e^{\theta x} e^{-\lambda x} \lambda^n x^{n-1}}{(n-1)!} \dd{x}                                                            \\
	          & = \left( \frac{\lambda}{\lambda - \theta} \right)^n \int_0^\infty \frac{e^{-(\lambda - \theta) x} (\lambda - \theta)^n x^{n-1}}{(n-1)!} \dd{x}
\end{align*}
The integral on the right hand side is the probability distribution function of a random variable $Y \sim \Gamma(n, \lambda - \theta)$, which gives 1 since the integral is taken over the entire domain. Hence,
\[ m(\theta) = \left( \frac{\lambda}{\lambda - \theta} \right)^n \]
Now, let $X \sim \Gamma(n, \lambda)$ and $Y \sim \Gamma(m, \lambda)$ be independent continuous random variables. Then
\[ m(\theta) = \expect{e^{\theta(X + Y)}} = \expect{e^{\theta X}} \expect{e^{\theta Y}} = \qty(\frac{\lambda}{\lambda - \theta})^{n+m} \]
So by the uniqueness property we saw earlier, we get that $X+Y \sim \Gamma(n+m, \lambda)$. In particular, this implies that if $X_1, \dots, X_n$ are independent and identically distributed with the distribution $\mathrm{Exp}(\lambda) = \Gamma(1, \lambda)$, then
\[ X_1 + \dots + X_n \sim \Gamma(n, \lambda) \]
We could alternatively consider $\Gamma(\alpha, \lambda)$ for $\alpha > 0$ by replacing $(n-1)!$ with
\[ \Gamma(\alpha) = \int_0^\infty e^{-x} x^{\alpha - 1} \dd{x} \]
which agrees with this factorial function for integer values of $\alpha$.

\subsection{Moment Generating Function of the Normal Distribution}
Recall that
\[ f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp(-\frac{(x-\mu)^2}{2\sigma^2}) \]
Now,
\[ m(\theta) = \int_0^\infty e^{\theta x} \frac{1}{\sqrt{2\pi \sigma^2}} \exp(-\frac{(x-\mu)^2}{2\sigma^2}) \dd{x} = \int_0^\infty \frac{1}{\sqrt{2\pi \sigma^2}} \exp(\theta x-\frac{(x-\mu)^2}{2\sigma^2}) \dd{x} \]
Note that
\[ \theta x - \frac{(x-\mu)^2}{2\sigma^2} = \theta \mu + \frac{\theta^2 \sigma^2}{2} - \frac{\left( x - (\mu + \theta\sigma^2) \right)^2}{2\sigma^2} \]
Hence,
\begin{align*}
	m(\theta) & = \int_0^\infty \frac{1}{\sqrt{2\pi \sigma^2}} \exp(\theta \mu + \frac{\theta^2 \sigma^2}{2} - \frac{\left( x - (\mu + \theta\sigma^2) \right)^2}{2\sigma^2}) \dd{x}       \\
	          & = \exp(\theta \mu + \frac{\theta^2 \sigma^2}{2}) \int_0^\infty \frac{1}{\sqrt{2\pi \sigma^2}} \exp(- \frac{\left( x - (\mu + \theta\sigma^2) \right)^2}{2\sigma^2}) \dd{x}
\end{align*}
Note that the integral on the right hand side has the form of the probability distribution function of a variable $Y \sim \mathrm{N}(\mu + \theta \sigma^2, \sigma^2)$, hence it integrates to 1.
\[ m(\theta) = \exp(\theta \mu + \frac{\theta^2 \sigma^2}{2}) \]
Recall that if $X \sim \mathrm{N}(\mu, \sigma^2)$, then $aX + b \sim \mathrm{N}(a\mu + b, a^2\sigma^2)$. We can then deduce that
\[ \expect{e^{\theta(aX + b)}} = \exp(\theta(a \mu + b) + \frac{\theta^2 a^2\sigma^2}{2}) \]
Now, suppose that $X \sim \mathrm{N}(\mu, \sigma^2)$ and $Y \sim \mathrm{N}(\nu, \tau^2)$ are independent. Then
\[ \expect{e^{\theta(X + Y)}} = \expect{e^{\theta X}} \expect{e^{\theta Y}} = \exp(\theta \mu + \frac{\theta^2 \sigma^2}{2}) \exp(\theta \nu + \frac{\theta^2 \tau^2}{2}) = \exp(\theta(\mu + \nu) + \frac{\theta^2 (\sigma^2 + \tau^2)}{2}) \]
Hence $X + Y \sim \mathrm{N}(\mu + \nu, \sigma^2 + \tau^2)$.

\subsection{Cauchy Distribution}
Suppose that a continuous random variable $X$ has density
\[ f(x) = \frac{1}{\pi(1 + x^2)} \]
where $x \in \mathbb R$. Now,
\[ m(\theta) = \expect{e^{\theta X}} = \int_{-\infty}^\infty \frac{e^{\theta x}}{\pi(1 + x^2)} = \begin{cases}
		\infty & \theta \neq 0 \\
		1      & \theta = 0
	\end{cases} \]
Suppose $X \sim f$. Then $X, 2X, 3X, \dots$ have the same moment generating function, but they do not have the same distribution. This is because $m(\theta)$ is not finite on an open interval.

\subsection{Multivariate Moment Generating Functions}
Let $X = (X_1, \dots, X_n)$ be a random variable with values in $\mathbb R^n$. Then the moment generating function of $X$ is defined as
\[ m(\theta) = \expect{e^{\theta^T X}} = \expect{e^{\theta_1 X_1 + \dots + \theta_n X_n}};\quad \theta = \begin{pmatrix}
		\theta_1 \\ \vdots \\ \theta_n
	\end{pmatrix} \]
\begin{theorem}
	If the moment generating function is finite for a range of values of $\theta$, it uniquely determines the distribution of $X$. Also,
	\[ \eval{\pdv[r]{m}{\theta_i}}_{\theta = 0} = \expect{X_i^r} \]
	and
	\[ \eval{\frac{\partial^{r+s}m}{\partial \theta_i^r \partial \theta_j^s}}_{\theta = 0} = \expect{X_i^r X_j^s} \]
	Further,
	\[ m(\theta) = \prod_{i = 1}^n \expect{e^{\theta_i X_i}} \]
	if and only if $X_1, \dots, X_n$ are independent.
\end{theorem}
\noindent No proof is provided.

\subsection{Convergence in Distribution}
\begin{definition}
	Let $(X_n \colon n \in \mathbb N)$ be a sequence of random variables and let $X$ be another random variable. We say that $X_n$ converges to $X$ in distribution, written $X_n \convdist X$, if
	\[ F_{X_n}(x) \to F_X(x) \]
	for all $x \in \mathbb R$ that are continuity points of $F_X$.
\end{definition}
\begin{theorem}[Continuity property for moment generating functions]
	Let $X$ be a continuous random variable with $m(\theta) < \infty$ for some $\theta \neq 0$. Suppose that $m_n(\theta) \to m(\theta)$ for all $\theta \in \mathbb R$, where $m_n(\theta) = \expect{e^{\theta X_n}}$, and $m(\theta) = \expect{e^{\theta X}}$. Then $X_n \convdist X$.
\end{theorem}

\subsection{Weak Law of Large Numbers}
\begin{theorem}
	Let $(X_n \colon n \in \mathbb N)$ be a sequence of independent and identically distributed random variables, with $\mu = \expect{X_1} < \infty$. Let $S_n = X_1 + \dots + X_n$. Then for all $\varepsilon > 0$,
	\[ \prob{\abs{\frac{S_n}{n} - \mu} > \varepsilon} \to 0 \]
	as $n \to \infty$.
\end{theorem}
\noindent We will give a proof assuming that the variance of $X_1$ is finite.
\begin{proof}
	By Chebyshev's inequality,
	\begin{align*}
		\prob{\abs{\frac{S_n}{n} - \mu} > \varepsilon} & = \prob{\abs{S_n - n\mu} > \varepsilon n} \\
		                                               & \leq \frac{\Var{S_n}}{\varepsilon^2 n^2}  \\
		                                               & = \frac{n\sigma^2}{\varepsilon^2 n^2}     \\
		                                               & \to 0
	\end{align*}
\end{proof}

\section{Limit Theorems}
\subsection{Types of Convergence}
\begin{definition}
	A sequence $(X_n)$ converges to $X$ \textit{in probability}, written $X_n \convprob X$ as $n \to \infty$ if for all $\varepsilon > 0$,
	\[ \prob{\abs{X_n - X} > \varepsilon} \to 0;\quad n \to \infty \]
\end{definition}
\begin{definition}
	A sequence $(X_n)$ converges to $X$ \textit{almost surely} (with probability 1), if
	\[ \prob{\lim_{n \to \infty} X_n = X } = 1 \]
\end{definition}
\noindent This second definition is a stronger form of convergence. If a sequence $(X_n)$ converges to zero almost surely, then $X_n \convprob 0$ as $n \to \infty$.
\begin{proof}
	We want to show that given any $\varepsilon > 0$, $\prob{\abs{X_n} > \varepsilon} \to 0$ as $n \to \infty$, or equivalently, $\prob{\abs{X_n} \leq \varepsilon} \to 1$.
	\[ \prob{\abs{X_n} \leq \varepsilon} \geq \prob{\underbrace{\bigcap_{m = n}^\infty  \qty{\abs{X_m} \leq \varepsilon}}_{A_n}} \]
	Note that $A_n$ is an increasing sequence of events, and
	\[ \bigcup_n A_n = \qty{\abs{X_m} \leq \varepsilon \text{ for all } m \text{ sufficiently large}} \]
	Hence, as $n \to \infty$,
	\[ \prob{A_n} \to \prob{\bigcup A_n} \]
	Therefore,
	\[ \lim_{n \to \infty} \prob{\abs{X_n} \leq \varepsilon} \geq \lim_{n \to \infty} \prob{A_n} = \prob{\bigcup A_n} \geq \prob{\lim_{n \to \infty} X_n = 0} \]
	Since $X_n$ converges to zero almost surely, this event on the right hand side has probability 1, so in particular the limit on the left has probability 1, as required.
\end{proof}

\subsection{Strong Law of Large Numbers}
\begin{theorem}
	Let $(X_n)_{n \in \mathbb N}$ be an independent and identically distributed sequence of random variables, with $\mu = \expect{X_1}$ finite. Let $S_n = X_1 + \dots + X_n$. Then
	\[ \frac{S_n}{n} \to \mu \text{ as } n \to \infty \text{ almost surely} \]
	In other words,
	\[ \prob{\lim_{n \to \infty}\frac{S_n}{n}\to \mu} = 1 \]
\end{theorem}
\noindent The following proof, made under the assumption of a finite fourth moment, is non-examinable. A proof can be formulated without this assumption, but it is more complicated.
\begin{proof}
	Let $Y_i = X_i - \mu$. Then $\expect{Y_i} = 0$, and $\expect{Y_i^4} \leq 2^4(\expect{X_i^4} + \mu^4) < \infty$. It then suffices to show that
	\[ \frac{S_n}{n} \to 0 \text{ a.s.} \]
	where $S_n = \sum_1^n X_i$ and $\expect{X_i} = 0$, $\expect{X_i^4} < \infty$. First,
	\[ S_n^4 = \left( \sum_{i=1}^n X_i \right)^4 = \sum_{i=1}^n X_i^4 + \binom{4}{2}\sum_{i=1}^n X_i^2 X_j^2 + R \]
	where $R$ is a sum of terms of the form $X_i^2 X_j X_k$ or $X_i^3 X_j$ or $X_i X_j X_k X_\ell$ for $i, j, k, l$ distinct. Once we take expectations, each term in $R$ will have no contribution to the result, since they all contain an $\expect{X_i} = 0$ term.
	\begin{align*}
		\expect{S_n^4} & = n\expect{X_i^4} + \binom{4}{2}\frac{n(n-1)}{2}\expect{X_i^2 X_j^2} + \expect{R} \\
		               & = n\expect{X_1^4} + 3n(n-1)\expect{X_1^2}\expect{X_1^2}                           \\
		               & \leq n\expect{X_1^4} + 3n(n-1)\expect{X_1^4}                                      \\
		               & = 3n^2\expect{X_1^4}
	\end{align*}
	by Jensen's inequality. Now,
	\[ \expect{\sum_{n=1}^\infty \qty(\frac{S_n}{n})^4} \leq \sum_{n=1}^\infty \frac{3}{n^2}\expect{X_1^4} < \infty \]
	Hence,
	\[ \sum_{n=1}^\infty \qty(\frac{S_n}{n})^4 < \infty \text{ with probability 1} \]
	Then since the sum of infinitely many positive terms is finite, the terms must converge to zero.
	\[ \lim_{n\to\infty}\frac{S_n}{n} \to 0 \text{ a.s.} \]
\end{proof}

\subsection{Central Limit Theorem}
Suppose, like before, that we have a sequence of independent and identically distributed random variables $X_n$, and suppose further that $\expect{X_1} = \mu$, and $\Var{X_1} = \sigma^2 < \infty$.
\[ \Var{\frac{S_n}{n} - \mu} = \frac{\sigma^2}{n} \]
We can normalise this new random variable $\frac{S_n}{n} - \mu$ by dividing by its standard deviation.
\[ \frac{\frac{S_n}{n} - \mu}{\sqrt{\Var{\frac{S_n}{n} - \mu}}} = \frac{\frac{S_n}{n} - \mu}{\frac{\sigma}{\sqrt{n}}} = \frac{Sn - n\mu}{\sigma \sqrt{n}} \]
\begin{theorem}
	For all $x \in \mathbb R$,
	\[ \prob{\frac{S_n - n\mu}{\sigma \sqrt{n}} \leq x} \to \Phi(x) = \int_{-\infty}^x \frac{e^{-\frac{y^2}{2}}}{\sqrt{2\pi}} \dd{y} \]
	In other words,
	\[ \frac{S_n - n\mu}{\sigma \sqrt{n}} \convdist Z \]
	where $Z$ is the standard normal distribution.
\end{theorem}
\noindent Less formally, we might say that the central limit theorem shows that, for a large $n$,
\[ S_n \approx n\mu + \sigma\sqrt{n}Z \sim N(n\mu, n\sigma^2) \]
\begin{proof}
	Consider $Y_i = \frac{X_i - \mu}{\sigma}$. Then the $Y_i$ have zero expectation and unit variance. It then suffices to prove the central limit theorem when the $X_i$ have zero expectation and unit variance. We assume further that there exists $\delta > 0$ such that
	\[ \expect{e^{\delta X_1}} < \infty;\quad \expect{e^{-\delta X_1}} < \infty \]
	We will show that
	\[ \frac{S_n}{n} \convdist \mathrm{N}(0, 1) \]
	By the continuity property of moment generating functions, it is sufficient to show that for all $\theta \in \mathbb R$,
	\[ \lim_{n \to \infty}\expect{e^{\frac{\theta S_n}{n}}} = \expect{e^{\theta Z}} = e^{\frac{\theta^2}{2}} \]
	Let $m(\theta) = \expect{e^{\theta X_1}}$. Then
	\[ \expect{e^{\frac{\theta S_n}{n}}} = \expect{e^{\frac{\theta}{\sqrt{n}} X_1}}^n = \qty(m\qty(\frac{\theta}{\sqrt{n}}))^n \]
	We now need to show that
	\[ \lim_{n \to \infty}\qty(m\qty(\frac{\theta}{\sqrt{n}}))^n = e^{\frac{\theta^2}{2}} \]
	Now, let $\abs{\theta} < \frac{\delta}{2}$. In this case,
	\begin{align*}
		m(\theta) & = \expect{e^{\theta X_1}}                                                                                                    \\
		          & = \expect{1 + \theta X_1 + \frac{\theta^2}{2} X_1^2 + \sum_{k=3}^\infty \frac{\theta^k}{k!}X_1^k}                            \\
		          & = \expect{1} + \expect{\theta X_1} + \expect{\frac{\theta^2}{2} X_1^2} + \expect{\sum_{k=3}^\infty \frac{\theta^k}{k!}X_1^k} \\
		          & = 1 + \frac{\theta^2}{2} + \expect{\sum_{k=3}^\infty \frac{\theta^k}{k!}X_1^k}
	\end{align*}
	Now, it suffices to prove that $\abs{\expect{\sum_{k=3}^\infty \frac{\theta^k}{k!}X_1^k}} = o(\theta^2)$ as $\theta \to 0$. Indeed, if we have this bound, then $m\qty(\frac{\theta}{\sqrt{n}}) = 1 + \frac{\theta^2}{2n} + o\qty(\frac{\theta^2}{n})$, and hence $\lim_{n \to \infty}\qty(m\qty(\frac{\theta}{\sqrt{n}}))^n = e^{\frac{\theta^2}{2}}$. To find this bound, we know that
	\begin{align*}
		\abs{\expect{\sum_{k=3}^\infty \frac{\theta^k}{k!}X_1^k}} & \leq \expect{\sum_{k=3}^\infty \frac{\abs{\theta}^k \abs{X_1}^k}{k!}}             \\
		                                                          & = \expect{\abs{\theta X_1}^3 \sum_{k=0}^\infty \frac{\abs{\theta X_1}^k}{(k+3)!}} \\
		                                                          & \leq \expect{\abs{\theta X_1}^3 \sum_{k=0}^\infty \frac{\abs{\theta X_1}^k}{k!}}
	\end{align*}
	Since $\abs{\theta} \leq \frac{\delta}{2}$,
	\[ \expect{\abs{\theta X_1}^3 \sum_{k=0}^\infty \frac{\abs{\theta X_1}^k}{k!}} \leq \expect{\abs{\theta X_1}^3 e^{\frac{\delta}{2}\abs{X_1}}} \]
	Now,
	\[ \abs{\theta X_1}^3 e^{\frac{\delta}{2}\abs{X_1}} = \abs{\theta}^3 \frac{\qty(\frac{\delta}{2}\abs{X_1})^3}{3!} \cdot \frac{3!}{\qty(\frac{\delta}{2})^3} \cdot e^{\frac{\delta}{2}\abs{X_1}} \]
	Note that
	\[ \frac{\qty(\frac{\delta}{2}\abs{X_1})^3}{3!} \leq \sum_{k=0}^\infty \frac{\qty(\frac{\delta}{2}\abs{X_1})^k}{k!} = e^{\frac{\delta}{2}\abs{X_1}} \]
	Hence,
	\[ \abs{\theta X_1}^3 e^{\frac{\delta}{2}\abs{X_1}} \leq \abs{\theta}^3 e^{\frac{\delta}{2}\abs{X_1}} \cdot \frac{3!}{\qty(\frac{\delta}{2})^3} \cdot e^{\frac{\delta}{2}\abs{X_1}} = \frac{3!\abs{\theta}^3}{\qty(\frac{\delta}{2})^3}e^{\delta\abs{X_1}} = 3!\qty(\frac{2\abs{\theta}}{\delta})^3 e^{\delta \abs{X_1}} \]
	Therefore,
	\[ e^{\delta \abs{X_1}} \leq e^{\delta X_1} + e^{-\delta X_1} \]
	So finally,
	\[ \expect{\abs{\theta X_1}^3 \sum_{k=0}^\infty \frac{\abs{\theta X_1}^k}{k!}} \leq 3!\qty(\frac{2\abs{\theta}}{\delta})^3 \expect{e^{\delta X_1} + e^{-\delta X_1}} = o(\abs{\theta}^2) \]
	as $\theta \to 0$.
\end{proof}

\subsection{Applications of Central Limit Theorem}
We can use the central limit theorem to approximate the binomial distribution using the normal distribution. Suppose that $S_n \sim \mathrm{Bin}(n, p)$. Then $S_n = \sum_{i=1}^n X_i$, where the $X_i$ have the Bernoulli distribution with parameter $p$. We know that $\expect{S_n} = np$, and $\Var{S_n} = np(1-p)$. Therefore, in particular,
\[ S_n \approx \mathrm{N}(np, np(1-p)) \]
for $n$ large. Note that we showed before that
\[ \mathrm{Bin}\qty(n, \frac{\lambda}{n}) \to \mathrm{Po}(\lambda) \]
Note that with this approximation to the binomial, we let the parameter $p$ depend on $n$. Since this is the case, we can no longer apply the central limit theorem, and we get a Poisson distributed approximation.

We can, however, use the central limit theorem to find a normal approximation for a Poisson random variable $S_n \sim \mathrm{Po}(n)$, since $S_n$ can be written as $\sum_{i=1}^n X_i$ where the $X_i \sim \mathrm{Po}(1)$. Then
\[ S_n \approx \mathrm{N}(n, n) \]

\section{???}
\subsection{Sampling Error via Central Limit Theorem}
Suppose individuals independently vote `yes' (with probability $p$) or `no' (with probability $1-p$). We can sample the population to find an approximation for $p$. Pick $N$ individuals at random, and let $\hat{p}_N = \frac{S_N}{N}$, where $S_n$ is the number of individuals who voted `yes'. We would like to find the minimum $N$ such that $\abs{\hat{p}_N - p} \leq 4\%$ with probability at least $99\%$. We have
\[ S_N \sim \mathrm{Bin}(N, p) \approx Np + \sqrt{Np(1-p)}Z;\quad Z \sim \mathrm{N}(0, 1) \]
Hence,
\[ \frac{S_N}{N} \approx p + \sqrt{\frac{p(1-p)}{N}}Z \implies \abs{\hat{p}_N - p} \approx \sqrt{\frac{p(1-p)}{N}}\abs{Z} \]
We then want to find $N$ such that
\[ \prob{\sqrt{\frac{p(1-p)}{N}}\abs{Z} \leq 0.04} \geq 0.99 \]
We can compute this from the tables of the standard normal distribution. If $z = 2.58$, then $\prob{\abs{Z} \geq 2.58} = 0.01$, hence we need an $N$ such that
\[ 0.04 \sqrt{\frac{N}{p(1-p)}} \geq 2.58 \]
In the worst case scenario, $p = \frac{1}{2}$ would give the largest $N$. So we need $N \geq 1040$ to get a good result for all $p$.

\subsection{Buffon's Needle}
Consider a set of parallel lines on a plane, all a distance $L$ apart. Imagine dropping a needle of length $\ell \leq L$ onto this plane at random. What is the probability that it intersects at least one line?

We will interpret a random drop to be represented by independent values $x$ and $\theta$, where $x$ is the perpendicular distance from the lower end of the needle to the nearest line above it, and $\theta$ is the angle between the horizontal and the needle, where a value of $\theta = 0$ means that the needle is horizontal, and higher values of $\theta$ mean that the needle has been rotated $\theta$ radians anticlockwise. We assume that $\Theta \sim \mathrm{U}[0, \pi]$, and $X \sim \mathrm{U}[0, L]$, and that they are independent. The needle intersects a line if and only if $\ell\sin\theta \geq x$. We have
\begin{align*}
	\prob{\text{intersection}} & = \prob{X \leq \ell\sin\Theta}                                                  \\
	                           & = \int_0^L \int_0^\pi \frac{1}{\pi L} 1(x \leq \ell\sin\theta)\dd{x}\dd{\theta} \\
	                           & = \frac{2\ell}{\pi L}
\end{align*}
Let this probability be denoted by $p$. So we can compute an approximation to $\pi$ by finding
\[ \pi = \frac{2\ell}{pL} \]
We can use the sampling error calculation above to find the amount of needles required to get a good approximation to $\pi$ (within $0.1\%$) with probability ast least $99\%$, so we want
\[ \prob{\abs{\hat{\pi}_n - \pi} \leq 0.001} \geq 0.99 \]
Let $S_n$ be the number of needles intersecting a line. Then $S_n \sim \mathrm{Bin}(n, p)$. So by the central limit theorem,
\[ S_n \approx np + \sqrt{np(1-p)} Z \implies \hat{p}_n = \frac{S_n}{n} = p + \sqrt{\frac{p(1-p)}{n}}Z \]
Hence,
\[ \hat{p}_n - p \approx \sqrt{\frac{p(1-p)}{n}}Z \]
Now, let $f(x) = 2\ell/xL$. Then $f(p) = \pi$, $f'(p) = -\frac{\pi}{p}$, and $\hat{\pi}_n = f(\hat{p}_n)$. We can then use a Taylor expansion to find
\[ \hat{\pi}_n = f(\hat{p}_n) \approx f(p) + (\hat{p}_n - p)f'(p) \implies \hat{\pi}_n \approx \pi - (\hat{p}_n - p) \frac{\pi}{p} \]
Hence,
\[ \hat{\pi}_n - \pi \approx - \frac{\pi}{p} \sqrt{\frac{p(1-p)}{n}} = -\pi \sqrt{\frac{1-p}{pn}}Z \]
We want
\[ \prob{\pi \sqrt{\frac{1-p}{pn}}\abs{Z} \leq 0.001} \geq 0.99 \]
So using tables, we find in the worst case scenario that $n \approx \num{3.75e7}$. So this approximation becomes good very slowly.

\subsection{Bertrand's Paradox}
Consider a circle of radius $r$, and draw a random chord on the circle. What is the probability that its length $C$ is less than $r$? There are two interpretations of the words `random chord', that give different results. This is Bertrand's paradox.
\begin{enumerate}[(i)]
	\item First, let us interpret `random chord' as follows. Let $X \sim \mathrm{U}[0, r]$, and then we draw a chord perpendicular to a radius, such that it intersects the radius at a distance of $X$ from the origin. Then we have formed a triangle between this intersection point, one end of the chord, and the circle's centre. By Pythagoras' theorem, the length of the chord is then twice the height of this triangle, so $C = 2\sqrt{r^2 - X^2}$. Hence,
	      \[ \prob{C \leq r} = \prob{2\sqrt{r^2 - X^2} \leq r} = \prob{4(r^2 - X^2) \leq r^2} = \prob{X \geq \frac{\sqrt{3}}{2}r} = 1 - \frac{sqrt 3}{2} \approx 0.134 \]
	\item Instead, let us fix one end point of the chord $A$, and let $\Theta \sim \mathrm{U}[0, 2\pi]$. Let the other end point $B$ be such that the angle between the radii $OA$ and $OB$ is $\Theta$. Then if $\Theta \in [0, \pi]$, the length of the chord can be found by splitting this triangle in two by dropping a perpendicular from the centre, giving
	      \[ C = 2r\sin\frac{\Theta}{2} \]
	      If $\Theta \in [\pi, 2\pi]$, then
	      \[ C = 2r\sin\frac{2\pi - \Theta}{2} = 2r\sin\frac{\Theta}{2} \]
	      as before.
	      Now,
	      \[ \prob{C \leq r} = \prob{2r\sin\frac{\Theta}{2} \leq r} = \prob{\sin\frac{\Theta}{2} \leq \frac{1}{2}} = \prob{\Theta \leq \frac{\pi}{3}} + \prob{\Theta \geq \frac{5\pi}{3}} = \frac{1}{6} + \frac{1}{6} = \frac{1}{3} \approx 0.333 \]
\end{enumerate}
Clearly, the two probabilities do not match.

\subsection{Multidimensional Gaussian Random Variables}
Recall that a random variable $X$ with values in $\mathbb R$ is called Gaussian (or normal) if
\[ X = \mu + \sigma Z;\quad \mu \in \mathbb R, \sigma \geq 0, Z \sim \mathrm{N}(0, 1) \]
The density function of $X$ is
\[ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{(x-\mu)^2}{2\sigma^2}) \]
Now, let $X = (X_1, \dots, X_n)^\transpose$ with values in $\mathbb R^n$. Then we define that $X$ is a Gaussian vector (also called Gaussian) if
\[ \forall u = \begin{pmatrix}
		u_1 \\ \vdots \\ u_n
	\end{pmatrix} \in \mathbb R^n,\, u^\transpose X = \sum_{i=1}^n u_i X_i = \mu + \sigma Z \]
so any linear combination of the $X_i$ is Gaussian. This does not require that the $X_i$ are independent, just that their sum is always Gaussian.

Let $X$ be Gaussian in $\mathbb R^n$. Suppose that $A$ is an $m \times n$ matrix, and $b \in \mathbb R^m$. Then $AX + b$ is also Gaussian. Indeed, let $u \in \mathbb R^m$, and let $v = A^\transpose u$. Then
\[ u^\transpose(AX + b) = u^\transpose AX + u^\transpose b = v^\transpose X + u^\transpose b \]
Since $X$ is Gaussian, $v^\transpose X$ is also Gaussian. An additive constant preserves this property, so the entire expression is Gaussian.

\section{Properties of Gaussian Vectors}
\subsection{Expectation and Variance}
We define the mean of a Gaussian vector $X$ as
\[ \mu = \expect{X} = \begin{pmatrix}
	\expect{X_1} \\ \vdots \\ \expect{X_n}
\end{pmatrix};\quad \mu_i = \expect{X_i} \]
We further define
\begin{align*}
	V &= \Var{X} = \expect{(X - \mu) (X - \mu)^\transpose} \\
	&= \begin{pmatrix}
		\expect{(X_1 - \mu_1)^2} & \expect{(X_1 - \mu_1)(X_2 - \mu_2)} & \cdots & \expect{(X_1 - \mu_1)(X_n - \mu_n)} \\
		\expect{(X_2 - \mu_2)(X_1 - \mu_1)} & \expect{(X_2 - \mu_2)^2} & \cdots & \expect{(X_2 - \mu_2)(X_n - \mu_n)} \\
		\vdots & \vdots & \ddots & \vdots \\
		\expect{(X_n - \mu_n)(X_1 - \mu_1)} & \expect{(X_n - \mu_1)(X_n - \mu_2)} & \cdots & \expect{(X_n - \mu_n)^2}
	\end{pmatrix}
\end{align*}
Hence the components of $V$ are
\[ V_{ij} = \Cov{X_i, X_j} \]
In particular, $V$ is a symmetric matrix, and
\[ \expect{u^\transpose X} = \expect{\sum_{i=1}^n u_i X_i} = \sum_{i=1}^n u_i \mu_i = u^\transpose \mu \]
and
\[ \var{u^\transpose X} = \Var{\sum_{i=1}^n u_i X_i} = \sum_{i,j = 1}^n u_i \Cov{X_i, X_j} u_j = u^\transpose V u \]
Hence $u^\transpose X \sim \mathrm{N}(u^\transpose \mu, u^\transpose V u)$. Further, $V$ is a non-negative definite matrix. Indeed, let $u \in \mathbb R^n$. Then $\Var{u^\transpose X} = u^\transpose V u$. Since $\Var{u^\transpose X} \geq 0$, we have $u^\transpose V u \geq 0$.

\subsection{Moment Generating Function}
We define the moment generating function of $X$ by
\[ m(\lambda) = \expect{e^{\lambda^\transpose X}} \]
where $\lambda \in \mathbb R^n$. Then, we know that $\lambda^\transpose X \sim \mathrm{N}(\lambda^\transpose \mu, \lambda^\transpose V \lambda)$. Hence $m(\lambda)$ is the moment generating function of a normal random variable with the above mean and variance, applied to the parameter $\theta = 1$.
\[ m(\lambda) = \exp(\lambda^\transpose \mu + \frac{\lambda^\transpose V\lambda}{2}) \]
Since the moment generating function uniquely characterises the distribution, it is clear that a Gaussian vector is uniquely characterised by its mean vector $\mu$ and variance matrix $V$. In this case, we write $X \sim \mathrm{N}(\mu, V)$.

\subsection{Constructing Gaussian Vectors}
Given a $\mu$ and a $V$ matrix, we might like to create a Gaussian vector that has this mean and variance. Let $Z_1, \dots, Z_n$ be a list of independent and identically distributed standard normal random variables. Let $Z = (Z_1, \dots, Z_n)^\transpose$. Then $Z$ is a Gaussian vector.
\begin{proof}
	For any vector $u \in \mathbb R^n$, we have
	\[ u^\transpose Z = \sum_{i=1}^n u_i Z_i \]
	Because the $Z_i$ are independent, it is easy to take the moment generating function to get
	\[ \expect{\exp(\lambda \sum_{i=1}^n u_i z_i)} = \expect{\prod_{i=1}^n \exp(\lambda u_i Z_i)} = \prod_{i=1}^n \expect{\exp(\lambda u_i Z_i)} = \prod_{i=1}^n \exp(\frac{(\lambda u_i)^2}{2}) = \exp(\frac{\lambda^2 \abs{u}^2}{2}) \]
	So $u^\transpose Z \sim \mathrm{N}(0, \abs{u}^2)$, which is normal as required.
\end{proof}
\noindent Now, $\expect{Z} = 0$, and $\Var{Z} = I$, the identity matrix. We then write $Z \sim \mathrm{N}(0, I)$. Now, let $\mu \in \mathbb R^n$, and $V$ be a non-negative definite matrix. We want to construct a Gaussian vector $X$ such that its mean is $\mu$ and its expectation is $V$, by using $Z$. In the one-dimensional case, this is easy, since $\mu$ is a single value, and $V$ contains only one element, $\sigma^2$. In this case therefore, $Z \sim \mathrm{N}(0, 1)$ so $\mu + \sigma Z \sim \mathrm{N}(\mu, \sigma^2)$. In the general case, since $V$ is non-negative definite, we can write
\[ V = U^\transpose DU \]
where $U^{-1} = U^\transpose$, and $D$ is a diagonal matrix with diagonal entries $\lambda_i \geq 0$. We define the square root of the matrix $V$ to be
\[ \sigma = U^\transpose \sqrt{D} U \]
where $\sqrt{D}$ is the diagonal matrix with diagonal entries $\sqrt{\lambda_i}$. Then clearly,
\[ \sigma^2 = U^\transpose \sqrt{D} U U^\transpose \sqrt{D} U = U^\transpose \sqrt{D} \sqrt{D} U = U^\transpose DU = V \]
Now, let $X = \mu + \sigma Z$. We now want to show that $X \sim \mathrm{N}(\mu, V)$.
\begin{proof}
	$X$ is certainly Gaussian, since it is generated by a linear multiple of the Gaussian vector $Z$, with an added constant. By linearity,
	\[ \expect{X} = \mu \]
	and
	\begin{align*}
		\Var{X} &= \expect{(X - \mu)(X - \mu)^\transpose} \\
		&= \expect{(\sigma Z)(\sigma Z)^\transpose} \\
		&= \expect{\sigma Z Z^\transpose \sigma^\transpose} \\
		&= \sigma \expect{Z Z^\transpose} \sigma^\transpose \\
		&= \sigma \sigma^\transpose \\
		&= \sigma \sigma \\
		&= V
	\end{align*}
\end{proof}

\subsection{Density}
We can calculate the density of such a Gaussian vector $X \sim \mathrm{N}(\mu, V)$. First, consider the case where $V$ is positive definite. Recall that in the one-dimensional case,
\[ f_X(x) = f_Z(z) \abs{J};\quad x = \mu + \sigma z \]
In general, since $V$ is positive definite, $\sigma$ is invertible. So $x = \mu + \sigma z$ gives $z = \sigma^{-1}(x-\mu)$. Hence,
\begin{align*}
	f_X(x) &= f_Z(z) \abs{J} \\
	&= \prod_{i=1}^n \frac{\exp(-\frac{z_i^2}{2})}{\sqrt{2\pi}} \abs{\det \sigma^{-1}} \\
	&= \frac{1}{(2\pi)^{n/2}}\exp(-\frac{\abs{z}^2}{2}) \cdot \frac{1}{\sqrt{\det V}} \\
	&= \frac{1}{\sqrt{(2\pi)^n \det V}}\exp(-\frac{z^\transpose z}{2})
\end{align*}
Now,
\begin{align*}
	z^\transpose z &= (\sigma^{-1} (x-\mu))^\transpose (\sigma^{-1} (x-\mu)) \\
	&= (x-\mu)^\transpose (\sigma^{-1})^\transpose \sigma^{-1} (x-\mu) \\
	&= (x-\mu)^\transpose \sigma^{-2} (x-\mu) \\
	&= (x-\mu)^\transpose V^{-1} (x-\mu)
\end{align*}
Hence,
\[ f_X(x) = \frac{1}{\sqrt{(2\pi)^n \det V}}\exp(-\frac{(x-\mu)^\transpose V^{-1} (x-\mu)}{2}) \]
In the case where $V$ is just non-negative definite (so it could have some zero eigenvalues), we can make an orthogonal change of basis, and assume that
\[ V = \begin{pmatrix}
	U & 0 \\
	0 & 0
\end{pmatrix};\quad \mu = \begin{pmatrix}
	\lambda \\ \nu
\end{pmatrix} \]
where $U$ is an $m \times m$ positive definite matrix, where $m < n$, and where $\lambda \in \mathbb R^m$, $\nu \in \mathbb R^{n-m}$. For $U$, we can then apply the result above. We can write
\[ X = \begin{pmatrix}
	Y \\ \nu
\end{pmatrix} \]
where $Y$ has density
\[ f_Y(y) = \frac{1}{\sqrt{(2\pi)^n \det U}}\exp(-\frac{(y-\lambda)^\transpose U^{-1} (y-\lambda)}{2}) \]

\end{document}
