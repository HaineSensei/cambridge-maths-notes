\subsection{Sequences and limits}
How can we ascribe meaning to expressions like this?
\[
	1 + \frac{1}{2} + \frac{1}{4} + \frac{1}{8} + \cdots
\]
Certainly, we have a concept of addition, and we can keep adding as many terms as we like, but there is no implicit definition of an infinite sum from the aforementioned axioms.

A definition that makes sense would involve partial sums \(x_n\) of this infinite series.
However, we could not just say that the partial sums get progressively closer to a value, because then trivially something like \(\frac{1}{2}, \frac{2}{3}, \frac{3}{4}, \frac{4}{5}, \cdots\) tends to 107, even though they're clearly getting closer.

A more accurate definition would be to state that we can get arbitrarily close (within some given \(\varepsilon\)) to a `limit value' \(c\) by taking some amount of terms \(n\) of this series: \(c - \varepsilon < x_n < c + \varepsilon\).
But this is still wrong: the sequence \(\frac{1}{2}, 10, \frac{2}{3}, 10, \frac{3}{4}, 10, \frac{4}{5}, 10, \cdots\) could then tend to 1 even though every other term is 10.

The best definition would state that the sequence of partial sums would \textit{stay} within \(\varepsilon\) of \(c\) for all \(x_k\) where \(k \geq n\) for some \(n \in \mathbb N\).
In less formal words, for any \(\varepsilon > 0\), \(x_n\) will eventually stay within \(\varepsilon\) of \(c\).
Equivalently, \(\forall \varepsilon > 0, \exists N \in \mathbb N\) such that \(\forall n > N\) we have \(\abs{x_n - c} < \varepsilon\).

\subsection{Examples}
\begin{enumerate}
	\item Consider the sequence \(\frac{1}{2},\; \frac{1}{2} + \frac{1}{4},\; \frac{1}{2} + \frac{1}{4} + \frac{1}{8}, \cdots\).
	      This is \(x_1, x_2, x_3, \cdots\) where \(x_n = 1 - \frac{1}{2^n}\) (inductively on \(n\)).
	      We want to show that \(x_n\) tends to 1.
	      Given some \(\varepsilon > 0\), we choose some \(N \in \mathbb N\) with \(N > \frac{1}{\varepsilon}\).
	      Then, for every \(n \geq N\), \(\abs{x_n - 1} = \frac{1}{2^n} \leq \frac{1}{n} \leq \frac{1}{N} < \varepsilon\).
	\item Consider the constant sequence \(c, c, c, c, \cdots\).
	      We want to show that \(x_n \to c\).
	      Given some \(\varepsilon > 0\), we have \(\abs*{x_n - c} < \varepsilon\) for all \(n\); \(N=1\) is the time after which the sequence stays within \(\varepsilon\) of \(c\).
	\item Consider now \(x_n = (-1)^n\), i.e.\ \(-1, 1, -1, 1, \cdots\).
	      We want to show that this does not tend to a limit.
	      Suppose \(x_n \to c\) as \(n \to \infty\).
	      We may choose some \(\varepsilon\) that acts as a counterexample --- for example, \(\varepsilon = 1\).
	      So \(\exists N \in \mathbb N\) such that \(\forall n \geq n\) we have \(\abs{x_n - c} < 1\).
	      In particular, \(\abs{1 - c} < 1\) and \(\abs{-1 - c} < 1\) so \(\abs{1 - (-1)} < 2\), by the triangle inequality.
	      This is a contradiction.
	\item The sequence \(x_n\) given by
	      \[
		      x_n = \begin{cases}
			      \frac{1}{n} & n \text{ odd}  \\
			      0           & n \text{ even}
		      \end{cases}
	      \]
	      should tend to zero.
	      Given some \(\varepsilon > 0\), we will choose \(N \in \mathbb N\) with \(\frac{1}{N} < \varepsilon\).
	      Then for all \(n \geq N\), either \(x_n = \frac{1}{n}\) or 0.
	      In either case, \(\abs{x_n - 0} \leq \frac{1}{n} \leq \frac{1}{N} < \varepsilon\).
\end{enumerate}
We can denote the entirety of a sequence \(x_1, x_2, \cdots\) as \((x_n)\) or \((x_n)_{n=1}^\infty\).
For example, \(\left( (-1)^n \right)_{n=1}^{\infty}\) is divergent.
This isn't saying that it goes to infinity, just that it doesn't converge.
Note also that if \(x_n \to c\) and \(x_n \to d\), then \(c=d\).
Suppose that \(c \neq d\).
Then pick \(\varepsilon = \frac{\abs{c-d}}{2}\).
Then \(\exists N \in \mathbb N\) with \(\abs{x_n - c} < \varepsilon\), and \(\exists M \in \mathbb N\) with \(\abs{x_n - d} < \varepsilon\).
After the point \(\max(N, M)\), the points must be within \(\varepsilon\) of both \(c\) and \(d\), but as \(c\) and \(d\) are \(2\varepsilon\) apart this is a contradiction (by the triangle inequality).

\subsection{Series}
A sequence given in the form \(x_1,\; x_1 + x_2,\; x_1 + x_2 + x_3, \cdots\) is called a series.
They are often written \(\sum_{n=1}^\infty x_n\).
The \(k\)th term of the sequence, given by \(\sum_{n=1}^k x_n\), is called the \(k\)th partial sum.
If the series converges to some value \(c\), then we can write \(\sum_{n=1}^\infty x_n = c\).
Note that we cannot use this notation to denote the limit until we know that the limit actually exists.
This is just the same as with sequences, where we cannot write \(\lim_{n\to\infty} x_n\) until we know that the limit exists.

Limits behave as we would expect.
For example, if \(x_n \leq d\) for all \(n\), and \(x_n \to c\), then \(c \leq d\).
Suppose \(c > d\).
Thwn we will choose \(\varepsilon = \frac{\abs{c - d}}{2}\).
Then there are no points \(x_n\) within this bound of \(c\) \contradiction.

\begin{proposition}
	If \(x_n \to c\) and \(y_n \to d\), then \(x_n + y_n \to c + d\).
\end{proposition}
\begin{proof}
	Given some \(\varepsilon > 0\), let \(\zeta = \frac{1}{2}\varepsilon\).
	Then, after some term \(x_N\), \(\abs{x_n - c} < \zeta\), and after some term \(y_M\), \(\abs{y_m - d} < \zeta\).
	So for every \(n \geq \max(M, N)\), by the triangle inequality, \(\abs{(x_n + y_n) - (c + d)} < 2\zeta = \varepsilon\) as required.
\end{proof}
This is commonly known as an \(\varepsilon/2\) argument.
Also, if we had instead not taken any \(\zeta\) value and just stuck with \(\varepsilon\), it would still be a good proof because we could just have divided \(\varepsilon\) at the beginning --- it's not expected that you completely rewrite the proof to add in this division.

\subsection{Testing convergence of a sequence}
A sequence \(x_1, x_2, \cdots\) is called `increasing' if \(x_{n+1} \geq x_n\) for all \(n\).
\begin{theorem}
	If \(x_1, x_2, \cdots\) is increasing and bounded above, it converges to a limit.
\end{theorem}
This is a very important theorem that we will refer back to time and time again.
\begin{note}
	If we were in \(\mathbb Q\), this would not necessarily hold.
	For example, \(1, 1.4, 1.41, 1.414, 1.4142, \cdots\) (the decimal expansion of \(\sqrt{2}\)).
	They don't converge to a limit in \(\mathbb Q\).
	So our proof will have to be more rigorous than just `they have to tend to somewhere below the upper bound'; we must use a property that \(\mathbb R\) has that \(\mathbb Q\) does not have, i.e.\ the least upper bound axiom.
\end{note}
\begin{proof}
	Let \(c = \sup \{ x_1, x_2, \cdots \}\).
	We want to prove that \(x_n \to c\).
	Given some \(\varepsilon > 0\), there exists some \(n\) such that \(x_n > c - \varepsilon\) (else, \(c - \varepsilon\) would be a smaller upper bound \contradiction).
	As the sequence is increasing, all \(x_k\) where \(k > n\) are at least \(x_n\).
	So \(\abs{x_k - c} < \varepsilon\) as required.
\end{proof}
Of course, a decreasing sequence works in an identical way; if it is bounded below then it converges.
More compactly, a bounded monotone sequence is convergent (where monotone means either increasing or decreasing).

\subsection{Examples of series convergence}
\begin{proposition}
	The harmonic series
	\[
		\sum_{n=1}^\infty \frac 1 n
	\]
	diverges; the solution to the Basel problem
	\[
		\sum_{n=1}^\infty \frac 1 {n^2}
	\]
	converges.
\end{proposition}
There is no closed form for the \(n\)th term of either of these sequences, which is one reason that series are often more challenging to work with than regular sequences.
\begin{proof}
	Since the harmonic series is difficult to deal with, we will compare it to a sequence that we understand easier.
	Therefore, we show that the first sequence diverges using a comparison test with powers of 2, one of the simplest series.
	\begin{align*}
		       & 1 + \frac 1 2 + \frac 1 3 + \frac 1 4 + \frac 1 5 + \frac 1 6 + \frac 1 7 + \frac 1 8 + \frac 1 9 + \cdots                                                      \\
		\geq\  & 1 + \frac 1 2 + \underbrace{\frac 1 4 + \frac 1 4}_{\frac 1 2} + \underbrace{\frac 1 8 + \frac 1 8 + \frac 1 8 + \frac 1 8}_{\frac 1 2} + \frac 1 {16} + \cdots
	\end{align*}
	By inspection, we can see that the harmonic series is larger than the sum of an infinite amount of \(\frac 1 2\), so surely it must diverge.
	More rigorously:
	\begin{align*}
		\frac 1 3 + \frac 1 4                                              & \geq \frac 1 2                         \\
		\frac 1 5 + \frac 1 6 + \frac 1 7 + \frac 1 8                      & \geq \frac 1 2                         \\
		\frac{1}{2^n + 1} + \frac{1}{2^n + 2} + \cdots + \frac{1}{2^{n+1}} & \geq \frac{2^n}{2^{n+1}} = \frac{1}{2}
	\end{align*}
	So the partial sums of the series are unbounded, so the series diverges.
	For the sum of reciprocals of squares, we want to do a similar thing because again the only simple sequence we have to work with is the powers of 2.
	\begin{align*}
		       & 1 + \frac 1 {2^2} + \frac 1 {3^2} + \frac 1 {4^2} + \frac 1 {5^2} + \frac 1 {6^2} + \frac 1 {7^2} + \frac 1 {8^2} + \frac 1 {9^2} + \cdots                                                           \\
		\leq\  & 1 + \underbrace{\frac 1 {2^2} + \frac 1 {2^2}}_{\frac 2 {2^2}} + \underbrace{\frac 1 {4^2} + \frac 1 {4^2} + \frac 1 {4^2} + \frac 1 {4^2}}_{\frac 4 {4^2}} + \frac 1 {8^2} + \frac 1 {8^2} + \cdots
	\end{align*}
	The bottom sequence simplifies to just the sequence \(1 + \frac{1}{2} + \frac{1}{4} + \frac{1}{8} + \cdots \to 2\), and the upper sequence is bounded above by the lower sequence.
	More rigorously:
	\begin{align*}
		\frac{1}{2^2} + \frac{1}{3^2}                                                & \leq \frac{2}{2^2} = \frac{1}{2}         \\
		\frac{1}{4^2} + \frac{1}{5^2} + \frac{1}{6^2} + \frac{1}{7^2}                & \leq \frac{4}{4^2} = \frac{1}{4}         \\
		\frac{1}{(2^n)^2} + \frac{1}{(2^n + 1)^2} + \cdots + \frac{1}{(2^{n+1}-1)^2} & \leq \frac{2^n}{(2^n)^2} = \frac{1}{2^n}
	\end{align*}
	So the partial sums are bounded, and hence the series converges by the above theorem.
\end{proof}
In fact, \(\sum_{n=1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}\).
This is proved in the Linear Analysis course in Part II.\@

\subsection{Decimal expansions}
What should \(0.a_1a_2a_3\cdots\) mean (where each \(a\) is a digit from 0 to 9)?
It should be the limit of \(0.a_1\), \(0.a_1a_2\), \(0.a_1a_2a_3\) and so on.
We will define it by
\[
	0.a_1a_2a_3\cdots := \sum_{n=1}^\infty \frac{a_n}{10}
\]
This clearly converges as the partial sums are increasing and bounded above by 1, so infinite decimal expansions are valid.
Conversely, given some \(x \in \mathbb R\) with \(0 < x < 1\), we can certainly write it as a (potentially infinite) decimal.
We will start by choosing the greatest \(a_1\) from 0 to 9 such that \(\frac{a_1}{10} \leq x\).
Thus \(0 < x - \frac{a_1}{10} < \frac{1}{10}\).
Now, we can pick the greatest \(a_2\) in the set such that \(\frac{a_1}{10} + \frac{a_2}{100} \leq x\).
Therefore, \(0 \leq x - \frac{a_1}{10} - \frac{a_2}{100} < \frac{1}{100}\).
Continue inductively, and then we obtain a decimal expansion \(0.a_1a_2a_3\cdots\) such that \(0 \leq x - \sum_{n=1}^k \frac{a_n}{10^n} < \frac{1}{10^k}\) for any given \(k\).
By the definition of convergence, the sequence given for \(a\) tends to \(x\) as required.

Note, if \(0.a_1a_2\cdots\) and \(0.b_1b_2\cdots\) are different decimal expansions of the same number, then there exists some \(N \in \mathbb N\) such that \(a_n = b_n\) for all \(n < N\) and \(a_N = b_N - 1\) and \(a_n = 9, b_n = 0\) for all \(n > N\) (or vice versa).
For example, \(0.99999\dots\) is equivalent to \(1.00000\dots\)
