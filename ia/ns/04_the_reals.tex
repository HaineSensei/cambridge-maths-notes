\subsection{Motivation for the reals}
Why do we need the real numbers in the first place?
Well, we introduce new sets of numbers when there are equations that we cannot solve using our current number system.
For example, the equation \(x+2=0\) is not solvable in \(\mathbb N\), so we constructed \(\mathbb Z\).
Then we could not solve equations like \(2x = 3\), so we created the rationals, \(\mathbb Q\).
Now, we cannot solve equations such as \(x^2 = 2\), so we must create a new set of numbers that contains this solution.

\begin{proposition}
	There does not exist a \(q \in \mathbb Q\) such that \(q^2 = 2\).
	Note that in this proposition we make no assumption that \(q^2 = 2\) is solvable, or that a solution if one exists does not lie within \(\mathbb Q\); we simply state that confined to the realm of \(\mathbb Q\) the equation is unsolvable.
\end{proposition}
\begin{proof}[Proof 1]
	Suppose that such a \(q \in \mathbb Q\) exists, such that \(q^2 = 2\).
	Without loss of generality, we will assume that \(q>0\) because \((-q)^2 = q^2\).
	So let \(q\) be written as \(a/b\) where \(a, b \in \mathbb N\).
	Then \(a^2/b^2 = 2\), so \(a^2 = 2b^2\).
	If we factorise each side as a product of primes, the exponent of the prime 2 on the left hand side must be even, but on the right hand side it must be odd.
	This contradicts the unique factorisation of natural numbers.
	So such a \(q\) does not exist.
\end{proof}
\begin{proof}[Proof 2]
	Suppose that there exists some \(q \in \mathbb Q\) written similarly to above as \(a/b\).
	Note that for any \(c, d \in \mathbb Z\), \(cq + d\) is of the form \(e/b\) for some integer \(e\).
	Therefore, if \(cq+d>0\) then \(cq+d \geq 1/b\).

	Now, note that \(0 < (q - 1) < 1\), so for a suitably large \(n\), we have \(0 < (q - 1)^n < 1/b\).
	However, \((q-1)^n\) is of the form \(cq+d\) because \(q^2 = 1\) so we can eliminate all exponents.
	This is a contradiction so such a \(q\) does not exist.
\end{proof}
We can see from the proofs above that \(\mathbb Q\) has a `gap' at \(\sqrt 2\).
How can we express this fact without mentioning \(\mathbb R\)?
We can't just say plainly that \(\sqrt 2 \notin \mathbb Q\) because as far as we know from \(\mathbb Q\), there is no reason to assume that such a number called \(\sqrt 2\) even exists!
We need to find a way to express the concept of \(\sqrt 2\) in the language of \(\mathbb Q\).
One way to do this is by creating sone set \(S = \{ q \in \mathbb Q: q^2 < 2 \}\).
Then we can write down some upper bounds for this set.
For example, 2 is a trivial upper bound, as is \(1.5\), and as is \(1.42\).
In fact, we can continue making smaller and smaller upper bounds.
We can see therefore that there exists no least upper bound in \(\mathbb Q\).

\subsection{Axioms of the reals}
We define the reals as follows: the reals are a set written \(\mathbb R\) with elements 0 and 1 with \(0 \neq 1\); with operations \(+\) and \(\cdot\); and an ordering \(<\); such that:
\begin{enumerate}
	\item \(+\) is commutative, associative, has identity 0, and there are inverses for all elements;
	\item \(\cdot\) is commutative, associative, has identity 1, and there are inverses for all nonzero elements;
	\item \(\cdot\) is distributive over \(+\);
	\item for all \(a\) and \(b\) in \(\mathbb R\), exactly one of \(a<b\), \(a=b\) and \(a>b\) are true, and that \(a<b\) and \(b<c\) implies \(a<c\);
	\item for all \(a, b, c \in \mathbb R\), \(a<b\) implies \(a + c < b + c\), and \(a<b\) implies \(ac < bc\) when \(c > 0\); and
	\item for any set \(S\) of reals that is non-empty and bounded above, \(S\) has a least upper bound.
\end{enumerate}
There are some notable immediate remarks about the definitions of the reals.
\begin{itemize}
	\item We can contain the rationals inside the reals: \(\mathbb Q \subset \mathbb R\)
	\item The least upper bound axiom is false in \(\mathbb Q\), which is why it's so important in \(\mathbb R\).
	\item Why did we specify `non-empty' and `bounded above' in the least upper bound axiom?
	      Of course, if a set is not bounded above, then it has no upper bound, so clearly it can have no least upper bound.
	      If a set is empty, then every real is an upper bound for this set, and as there is no least real number, there is no least upper bound.
	\item It is possible to construct \(\mathbb R\) out of \(\mathbb Q\), and check that the above axioms hold.
	      However, this is a rare example where the construction of \(\mathbb R\) is complicated and irrelevant, so it is not covered here.
\end{itemize}
The reals do not contain infinitely big or infinitesimally small elements.
\begin{proposition}[the axiom of Archimedes]
	\(\mathbb N\) is not bounded above in \(\mathbb R\).
\end{proposition}
\begin{proof}
	If there were some upper bound \(c = \sup \mathbb N\), then \(c-1\) is clearly not an upper bound for \(\mathbb N\).
	So there exists some natural number \(n\) such that \(n > c-1\).
	But then clearly \(n+1 \in \mathbb N > c\) contradicting the existence of this upper bound.
\end{proof}
\begin{corollary}
	For each \(t \in \mathbb R > 0\), \(\exists n \in \mathbb N\) such that \(\frac{1}{n} < t\).
\end{corollary}
\begin{proof}
	We have some \(n \in \mathbb N\) with \(n > \frac{1}{t}\) by the above proposition.
	So \(\frac{1}{n} < t\).
\end{proof}

\subsection{Examples of sets and least upper bounds}
Note that a common way to write `least upper bound' is the word supremum, denoted \(\sup S\).
\begin{enumerate}
	\item Let \(S = \{ x \in \mathbb R: 0 \leq x \leq 1 \} = [0, 1]\).
	      The least upper bound of \(S\) is 1, because:
	      \begin{itemize}
		      \item 1 is an upper bound for \(S\); \(\forall x \in S, x\leq1 \); and
		      \item Every upper bound \(y\) must have \(y \geq 1\) because \(1 \in S\).
	      \end{itemize}
	\item Let \(S = \{ x \in \mathbb R: 0 < x < 1 \} = (0, 1)\).
	      \(\sup S = 1\) because:
	      \begin{itemize}
		      \item 1 is an upper bound for \(S\); \(\forall x \in S, x \leq 1\); and
		      \item No upper bound \(c\) has \(c<1\).
		            Indeed, certainly \(c>0\) (\(c > \frac{1}{2}\) since \(\frac{1}{2} \in S\)).
		            So if \(c<1\), then \(0<c<1\), so the number \(\frac{1+c}{2} \in S\) and is larger than \(c\), so it is not an upper bound.
	      \end{itemize}
	\item Let \(S = \qty{ 1 - \frac{1}{n}: n \in \mathbb N }\).
	      \(\sup S = 1\) because:
	      \begin{itemize}
		      \item 1 is clearly an upper bound.
		      \item Let us suppose \(c < 1\) is an upper bound.
		            Then \(\forall n \in \mathbb N, 1 - \frac{1}{n} < c\) so \(1 - c < \frac{1}{n}\).
		            From the corollary of the Axiom of Archimedes above, this is a contradiction.
	      \end{itemize}
\end{enumerate}
\begin{remark}
	If \(S\) has a greatest element, then this element is the supremum of the set: \(\sup S \in S\).
	But if \(S\) does not have a greatest element, then \(\sup S \notin S\).
	Also, we do not need any kind of `greatest lower bound' axiom---if \(S\) is a non-empty, bounded below set of reals, then the set \(\{ -x: x \in S \}\) is non-empty and bounded above, and so has a least upper bound, so \(S\) has a greatest lower bound equivalent to its additive inverse.
	This is commonly called the `infimum', or \(\inf S\).
\end{remark}
\begin{theorem}
	\(\exists x \in \mathbb R\) with \(x^2 = 2\).
\end{theorem}
\begin{proof}
	Let \(S\) be the set of all real numbers such that \(x^2 < 2\).
	Of course, it is non-empty (try \(x=0\)) and bounded above (try \(x=2\)).
	So let \(c = \sup S\); we want to show that \(c^2 = 2\).
	We prove this by eliminating all alternatives; clearly either \(c^2 < 2\), \(c^2 = 2\) or \(c^2 > 2\).
	\begin{itemize}
		\item (\(c^2 < 2\)) We want to prove that \((c+t)^2 < 2\) for some small \(t\).
		      For \(0<t<1\), we have \((c+t)^2 = c^2 + 2ct + t^2 \leq c^2 + 5t\), since \(c\) is at most 2, and \(t^2\) is at most \(t\).
		      So this value is less than 2 for some suitably small \(t\), contradicting the least upper bound---we have just shown that \((c+t) \in S\).
		\item (\(c^2 > 2\)) We want to prove that \((c-t)^2 > 2\) for some small \(t\).
		      For \(0<t<1\), we have \((c-t)^2 = c^2 - 2ct + t^2 \geq c^2 - 4t\), since \(c\) is at most 2, and \(t^2\) is at least zero.
		      So this value is greater than 2 for some suitably small \(t\), contradicting the least upper bound---we have just created a lower upper bound.
	\end{itemize}
	So \(c^2 = 2\).
\end{proof}
This same kind of proof works for a lot of real values, for example \(\sqrt[n]{x}\) for \(n \in \mathbb N\), \(x\in \mathbb R, x < 0\).
Reals that are not rational are called irrational.
This is a negative statement however, so it is better in proofs to suppose that something is rational, and then show a contradiction.

Also, the rationals are `dense'; for any \(a, b \in \mathbb R\), there is another rational between them.
We may assume without loss of generality that they are both non-negative and that \(a<b\).
Then pick some \(n \in \mathbb N\) with \(\frac{1}{n} < b-a\).
Among the list \(\frac{0}{n}, \frac{1}{n}, \frac{2}{n}, \dots\), there is a final one that is less than or equal to \(a\), which we will denote \(\frac{q}{n}\) (otherwise \(a\) is an upper bound to this list, contradicting the axiom of Archimedes).
So \(a < \frac{q + 1}{n} < b\) as required.

The irrationals are also dense; for any reals \(a\) and \(b\) with the same conditions above, these exists some irrational \(c\) with \(a<c<b\).
We know that there exists a rational \(c\) with \(a\sqrt{2} < c < b\sqrt{2}\), so \(a < \frac{c}{\sqrt{2}} < b\).

\subsection{Sequences and limits}
How can we ascribe meaning to expressions like this?
\[
	1 + \frac{1}{2} + \frac{1}{4} + \frac{1}{8} + \dots
\]
Certainly, we have a concept of addition, and we can keep adding as many terms as we like, but there is no implicit definition of an infinite sum from the aforementioned axioms.

A definition that makes sense would involve partial sums \(x_n\) of this infinite series.
However, we could not just say that the partial sums get progressively closer to a value, because then trivially something like \(\frac{1}{2}, \frac{2}{3}, \frac{3}{4}, \frac{4}{5}, \dots\) tends to 107, even though they're clearly getting closer.

A more accurate definition would be to state that we can get arbitrarily close (within some given \(\varepsilon\)) to a `limit value' \(c\) by taking some amount of terms \(n\) of this series: \(c - \varepsilon < x_n < c + \varepsilon\).
But this is still wrong: the sequence \(\frac{1}{2}, 10, \frac{2}{3}, 10, \frac{3}{4}, 10, \frac{4}{5}, 10, \dots\) could then tend to 1 even though every other term is 10.

The best definition would state that the sequence of partial sums would \textit{stay} within \(\varepsilon\) of \(c\) for all \(x_k\) where \(k \geq n\) for some \(n \in \mathbb N\).
In less formal words, for any \(\varepsilon > 0\), \(x_n\) will eventually stay within \(\varepsilon\) of \(c\).
Equivalently, \(\forall \varepsilon > 0, \exists N \in \mathbb N\) such that \(\forall n > N\) we have \(\abs{x_n - c} < \varepsilon\).
\begin{enumerate}
	\item Consider the sequence \(\frac{1}{2},\; \frac{1}{2} + \frac{1}{4},\; \frac{1}{2} + \frac{1}{4} + \frac{1}{8}, \dots\).
	      This is \(x_1, x_2, x_3, \dots\) where \(x_n = 1 - \frac{1}{2^n}\) (inductively on \(n\)).
	      We want to show that \(x_n\) tends to 1.
	      Given some \(\varepsilon > 0\), we choose some \(N \in \mathbb N\) with \(N > \frac{1}{\varepsilon}\).
	      Then, for every \(n \geq N\), \(\abs{x_n - 1} = \frac{1}{2^n} \leq \frac{1}{n} \leq \frac{1}{N} < \varepsilon\).
	\item Consider the constant sequence \(c, c, c, c, \dots\).
	      We want to show that \(x_n \to c\).
	      Given some \(\varepsilon > 0\), we have \(\abs*{x_n - c} < \varepsilon\) for all \(n\); \(N=1\) is the time after which the sequence stays within \(\varepsilon\) of \(c\).
	\item Consider now \(x_n = (-1)^n\), i.e.\ \(-1, 1, -1, 1, \dots\).
	      We want to show that this does not tend to a limit.
	      Suppose \(x_n \to c\) as \(n \to \infty\).
	      We may choose some \(\varepsilon\) that acts as a counterexample---for example, \(\varepsilon = 1\).
	      So \(\exists N \in \mathbb N\) such that \(\forall n \geq n\) we have \(\abs{x_n - c} < 1\).
	      In particular, \(\abs{1 - c} < 1\) and \(\abs{-1 - c} < 1\) so \(\abs{1 - (-1)} < 2\), by the triangle inequality.
	      This is a contradiction.
	\item The sequence \(x_n\) given by
	      \[
		      x_n = \begin{cases}
			      \frac{1}{n} & n \text{ odd}  \\
			      0           & n \text{ even}
		      \end{cases}
	      \]
	      should tend to zero.
	      Given some \(\varepsilon > 0\), we will choose \(N \in \mathbb N\) with \(\frac{1}{N} < \varepsilon\).
	      Then for all \(n \geq N\), either \(x_n = \frac{1}{n}\) or 0.
	      In either case, \(\abs{x_n - 0} \leq \frac{1}{n} \leq \frac{1}{N} < \varepsilon\).
\end{enumerate}
We can denote the entirety of a sequence \(x_1, x_2, \dots\) as
\[(x_n) \quad \text{or} \quad (x_n)_{n=1}^\infty\]
For example, \(\left( (-1)^n \right)_{n=1}^{\infty}\) is divergent.
This isn't saying that it goes to infinity, just that it doesn't converge.
Note also that if \(x_n \to c\) and \(x_n \to d\), then \(c=d\).
Suppose that \(c \neq d\).
Then pick \(\varepsilon = \frac{\abs{c-d}}{2}\).
Then \(\exists N \in \mathbb N\) with \(\abs{x_n - c} < \varepsilon\), and \(\exists M \in \mathbb N\) with \(\abs{x_n - d} < \varepsilon\).
After the point \(\max(N, M)\), the points must be within \(\varepsilon\) of both \(c\) and \(d\), but as \(c\) and \(d\) are \(2\varepsilon\) apart this is a contradiction (by the triangle inequality).

\subsection{Series}
A sequence given in the form \(x_1,\; x_1 + x_2,\; x_1 + x_2 + x_3, \dots\) is called a series.
They are often written \(\sum_{n=1}^\infty x_n\).
The \(k\)th term of the sequence, given by \(\sum_{n=1}^k x_n\), is called the \(k\)th partial sum.
If the series converges to some value \(c\), then we can write \(\sum_{n=1}^\infty x_n = c\).
Note that we cannot use this notation to denote the limit until we know that the limit actually exists.
This is just the same as with sequences, where we cannot write \(\lim_{n\to\infty} x_n\) until we know that the limit exists.

Limits behave as we would expect.
For example, if \(x_n \leq d\) for all \(n\), and \(x_n \to c\), then \(c \leq d\).
Suppose \(c > d\).
Then we will choose \(\varepsilon = \frac{\abs{c - d}}{2}\).
Then there are no points \(x_n\) within this bound of \(c\) \contradiction.

\begin{proposition}
	If \(x_n \to c\) and \(y_n \to d\), then \(x_n + y_n \to c + d\).
\end{proposition}
\begin{proof}
	Given some \(\varepsilon > 0\), let \(\zeta = \frac{1}{2}\varepsilon\).
	Then, after some term \(x_N\), \(\abs{x_n - c} < \zeta\), and after some term \(y_M\), \(\abs{y_m - d} < \zeta\).
	So for every \(n \geq \max(M, N)\), by the triangle inequality, \(\abs{(x_n + y_n) - (c + d)} < 2\zeta = \varepsilon\) as required.
\end{proof}
This is commonly known as an \(\varepsilon/2\) argument.
Also, if we had instead not taken any \(\zeta\) value and just stuck with \(\varepsilon\), it would still be a good proof because we could just have divided \(\varepsilon\) at the beginning---it's not expected that you completely rewrite the proof to add in this division.

\subsection{Testing convergence of a sequence}
A sequence \(x_1, x_2, \dots\) is called `increasing' if \(x_{n+1} \geq x_n\) for all \(n\).
\begin{theorem}
	If \(x_1, x_2, \dots\) is increasing and bounded above, it converges to a limit.
\end{theorem}
This is a very important theorem that we will refer back to time and time again.
\begin{note}
	If we were in \(\mathbb Q\), this would not necessarily hold.
	For example, consider the decimal expansion of \(\sqrt{2}\).
	\[1, 1.4, 1.41, 1.414, 1.4142, \dots\]
	They don't converge to a limit in \(\mathbb Q\).
	So our proof will have to be more rigorous than just `they have to tend to somewhere below the upper bound'; we must use a property that \(\mathbb R\) has that \(\mathbb Q\) does not have, i.e.\ the least upper bound axiom.
\end{note}
\begin{proof}
	Let \(c = \sup \{ x_1, x_2, \dots \}\).
	We want to prove that \(x_n \to c\).
	Given some \(\varepsilon > 0\), there exists some \(n\) such that \(x_n > c - \varepsilon\) (else, \(c - \varepsilon\) would be a smaller upper bound \contradiction).
	As the sequence is increasing, all \(x_k\) where \(k > n\) are at least \(x_n\).
	So \(\abs{x_k - c} < \varepsilon\) as required.
\end{proof}
Of course, a decreasing sequence works in an identical way; if it is bounded below then it converges.
More compactly, a bounded monotone sequence is convergent (where monotone means either increasing or decreasing).
\begin{proposition}
	The harmonic series
	\[
		\sum_{n=1}^\infty \frac 1 n
	\]
	diverges; the solution to the Basel problem
	\[
		\sum_{n=1}^\infty \frac 1 {n^2}
	\]
	converges.
\end{proposition}
There is no closed form for the \(n\)th term of either of these sequences, which is one reason that series are often more challenging to work with than regular sequences.
\begin{proof}
	Since the harmonic series is difficult to deal with, we will compare it to a sequence that we understand easier.
	Therefore, we show that the first sequence diverges using a comparison test with powers of 2, one of the simplest series.
	\begin{align*}
		       & 1 + \frac 1 2 + \frac 1 3 + \frac 1 4 + \frac 1 5 + \frac 1 6 + \frac 1 7 + \frac 1 8 + \frac 1 9 + \cdots                                                      \\
		\geq\  & 1 + \frac 1 2 + \underbrace{\frac 1 4 + \frac 1 4}_{\frac 1 2} + \underbrace{\frac 1 8 + \frac 1 8 + \frac 1 8 + \frac 1 8}_{\frac 1 2} + \frac 1 {16} + \cdots
	\end{align*}
	By inspection, we can see that the harmonic series is larger than the sum of an infinite amount of \(\frac 1 2\), so surely it must diverge.
	More rigorously:
	\begin{align*}
		\frac 1 3 + \frac 1 4                                              & \geq \frac 1 2                         \\
		\frac 1 5 + \frac 1 6 + \frac 1 7 + \frac 1 8                      & \geq \frac 1 2                         \\
		\frac{1}{2^n + 1} + \frac{1}{2^n + 2} + \dots + \frac{1}{2^{n+1}} & \geq \frac{2^n}{2^{n+1}} = \frac{1}{2}
	\end{align*}
	So the partial sums of the series are unbounded, so the series diverges.
	For the sum of reciprocals of squares, we want to do a similar thing because again the only simple sequence we have to work with is the powers of 2.
	\begin{align*}
		       & 1 + \frac 1 {2^2} + \frac 1 {3^2} + \frac 1 {4^2} + \frac 1 {5^2} + \frac 1 {6^2} + \frac 1 {7^2} + \frac 1 {8^2} + \frac 1 {9^2} + \cdots                                                           \\
		\leq\  & 1 + \underbrace{\frac 1 {2^2} + \frac 1 {2^2}}_{\frac 2 {2^2}} + \underbrace{\frac 1 {4^2} + \frac 1 {4^2} + \frac 1 {4^2} + \frac 1 {4^2}}_{\frac 4 {4^2}} + \frac 1 {8^2} + \frac 1 {8^2} + \cdots
	\end{align*}
	The bottom sequence simplifies to just the sequence \(1 + \frac{1}{2} + \frac{1}{4} + \frac{1}{8} + \cdots \to 2\), and the upper sequence is bounded above by the lower sequence.
	More rigorously:
	\begin{align*}
		\frac{1}{2^2} + \frac{1}{3^2}                                                & \leq \frac{2}{2^2} = \frac{1}{2}         \\
		\frac{1}{4^2} + \frac{1}{5^2} + \frac{1}{6^2} + \frac{1}{7^2}                & \leq \frac{4}{4^2} = \frac{1}{4}         \\
		\frac{1}{(2^n)^2} + \frac{1}{(2^n + 1)^2} + \dots + \frac{1}{(2^{n+1}-1)^2} & \leq \frac{2^n}{(2^n)^2} = \frac{1}{2^n}
	\end{align*}
	So the partial sums are bounded, and hence the series converges by the above theorem.
\end{proof}
In fact, \(\sum_{n=1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}\).
This is proved in the Linear Analysis course in Part II.\@

\subsection{Decimal expansions}
What should \(0.a_1a_2a_3\dots\) mean (where each \(a\) is a digit from 0 to 9)?
It should be the limit of \(0.a_1\), \(0.a_1a_2\), \(0.a_1a_2a_3\) and so on.
We will define it by
\[
	0.a_1a_2a_3\dots := \sum_{n=1}^\infty \frac{a_n}{10}
\]
This clearly converges as the partial sums are increasing and bounded above by 1, so infinite decimal expansions are valid.
Conversely, given some \(x \in \mathbb R\) with \(0 < x < 1\), we can certainly write it as a (potentially infinite) decimal.
We will start by choosing the greatest \(a_1\) from 0 to 9 such that \(\frac{a_1}{10} \leq x\).
Thus \(0 < x - \frac{a_1}{10} < \frac{1}{10}\).
Now, we can pick the greatest \(a_2\) in the set such that \(\frac{a_1}{10} + \frac{a_2}{100} \leq x\).
Therefore, \(0 \leq x - \frac{a_1}{10} - \frac{a_2}{100} < \frac{1}{100}\).
Continue inductively, and then we obtain a decimal expansion \(0.a_1a_2a_3\dots\) such that \(0 \leq x - \sum_{n=1}^k \frac{a_n}{10^n} < \frac{1}{10^k}\) for any given \(k\).
By the definition of convergence, the sequence given for \(a\) tends to \(x\) as required.

Note, if \(0.a_1a_2\dots\) and \(0.b_1b_2\dots\) are different decimal expansions of the same number, then there exists some \(N \in \mathbb N\) such that \(a_n = b_n\) for all \(n < N\) and \(a_N = b_N - 1\) and \(a_n = 9, b_n = 0\) for all \(n > N\) (or vice versa).
For example, \(0.99999\dots\) is equivalent to \(1.00000\dots\)

\subsection{The number \(e\)}
We define
\[
	e = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \frac{1}{4!} + \dots
\]
The partial sums are increasing and bounded above by the powers of two after the first term, so it converges.

\subsection{Algebraic and transcendental numbers}
A real \(x\) is called algebraic if it is a root of a nonzero polynomial with integer coefficients.
Otherwise, it is called transcendental.
For example, any rational \(\frac{p}{q}\) is algebraic as it is the root of \(qx-p=0\).
As another example, \(\sqrt 2 + 1\) is algebraic as it is a root of the equation \(x^2 - 2x - 1 = 0\).
The logical next question to ask is whether all reals are algebraic.

\begin{proposition}
	\(e\) is not rational.
\end{proposition}
\begin{proof}
	Suppose that \(e\) is rational, let it be written \(\frac{p}{q}\), where \(q > 1\) (if \(q=1\), rewrite it as \(\frac{2p}{2q}\)).
	Multiplying up by \(q!
	\) (easier than just \(q\) because then we can compare factorials) gives
	\[
		\sum_{n=0}^\infty \frac{q!}{n!} \in \mathbb Z
	\]
	We know that \(\sum_{n=0}^q \frac{q!}{n!} \in \mathbb Z\).
	The next terms are:
	\begin{align*}
		\frac{q!}{(q+1)!} & = \frac{1}{q+1}                                    \\
		\frac{q!}{(q+2)!} & = \frac{1}{(q+1)(q+2)} \leq \frac{1}{(q+1)^2}      \\
		\frac{q!}{(q+3)!} & = \frac{1}{(q+1)(q+2)(q+3)} \leq \frac{1}{(q+1)^3} \\
		\frac{q!}{(q+n)!} & \leq \frac{1}{(q+1)^n}                             \\
	\end{align*}
	So the next partial sums are bounded above by the geometric series.
	\[
		\sum_{n=q+1}^\infty \frac{q!}{n!} \leq \frac{1}{q} < 1
	\]
	So the whole series multiplied by \(q!
	\) is a whole number plus a fractional part, which is not an integer \contradiction.
\end{proof}
Ideally now we'd have a proof that \(e\) is transcendental.
However, even though the terms of \(e\) tend to zero quickly, they don't tend to zero quite quickly enough for us to be able to prove it using what we know now.
We instead prove that there exists some transcendental number using a different example, one whose terms tend to zero very quickly indeed.
\begin{theorem}
	Liouville's constant \(c = \sum_{n=1}^\infty \frac{1}{10^{n!}}\) is transcendental.
	As a decimal expansion:
	\[
		c = 0.1100010000000000000000010\dots
	\]
\end{theorem}
This is a long proof, the hardest in this course.
We will cherry-pick some important results about polynomials in order to make this proof, without a proper introduction to features of polynomials.
\begin{itemize}
	\item For any polynomial \(P\), \(\exists k \in \mathbb R\) such that \(\abs{P(x) - P(y)} \leq k\abs{x-y}\) for all \(0 \leq x, y \leq 1\).
	      Indeed, say \(P(x) = a_d x^d + \dots + a_0\), then
	      \begin{align*}
		      P(x) - P(y)       & = a_d(x^d - y^d) + a_{d-1}(x^{d-1} - y^{d-1}) + \dots + a_1(x-y)     \\
		                        & = (x-y) [ a_d(x^{d-1} + x^{d-2}y + \dots + y^{d-1}) + \dots + a_1 ] \\
		      \abs{P(x) - P(y)} & \leq \abs{x-y} [ (\abs{a_d} + \abs{a_{d-1}} + \dots + \abs{a_1})d ]
	      \end{align*}
	      because \(x\) and \(y\) are between 0 and 1.
	\item A nonzero polynomial of degree \(d\) has at most \(d\) roots.
	      Given some polynomial \(P\) of degree \(d\):
	      \begin{itemize}
		      \item If \(P\) has no roots, we are trivially done.
		      \item If \(P\) has some root \(a\), then \(P\) can be written as \((x-a)Q(x)\).
		            Inductively, \(Q(x)\) has at most \(d-1\) roots, so \(P\) has at most \(d\) roots.
	      \end{itemize}
\end{itemize}
Now we can prove the above theorem.
\begin{proof}
	We will write \(c_n = \sum_{k=0}^n \frac{1}{10^{k!}}\), such that \(c_n \to c\).
	Suppose that some polynomial \(P\) has \(c\) as a root.
	Then \(\exists k\) such that \(\abs{P(x) - P(y)} \leq k\abs{x-y}\) when \(0 \leq x, y \leq 1\).
	Let \(P\) have degree \(d\), such that
	\[
		P(x) = a_d x^d + \dots + a_0
	\]
	Now, \(\abs{c - c_n} = \sum_{k=n+1}^\infty \frac{1}{10^{k!}} \leq \frac{2}{10^{(n+1)!}}\).
	This is a trivial upper bound, of course better upper bounds exist.

	Also, \(c_n = \frac{a}{10^{n!}}\) for some \(a \in \mathbb Z\).
	So \(P(c_n) = \frac{b}{10^{dn!}}\) for some \(b \in \mathbb Z\) (since \(P(\frac{s}{t}) = \frac{q}{t^d}\) for some integer \(q\), where \(\frac{s}{t} \in \mathbb Q\)).

	For \(n\) large enough, \(c_n\) is not a root, because \(P\) only has finitely many roots.
	So
	\[
		\abs{P(c) - P(c_n)} = \abs{P(c_n)} \leq \frac{1}{10^{dn!}}
	\]
	Therefore
	\[
		\frac{1}{10^{dn!}} \leq k\frac{2}{10^{(n+1)!}}
	\]
	which is a contradiction if \(n\) is large enough.
\end{proof}
Here are some remarks about this proof.
\begin{itemize}
	\item This same proof shows that any real \(x\) such that \(\forall n \exists \frac{p}{q}\in \mathbb Q\) with \(0 < \abs{x - \frac{p}{q}} < \frac{1}{q^n}\) is transcendental.
	      Informally, \(x\) has very good rational approximations.
	\item Such \(x\) are often called Liouville numbers; the proof works for all Liouville numbers.
	\item This proof does not show that \(e\) is transcendental (even though it is), because the terms do not go to zero fast enough.
	\item We now know that there exist some transcendental numbers.
	      Another proof of existence of transcendental numbers will be seen in a later lecture.
\end{itemize}

\subsection{Complex numbers}
Some polynomials have no real roots, for example \(x^2 + 1\).
We'll try to `force' an \(x\) with the property \(x^2 = -1\).
Note that for example we could not force an \(x\) into existence with the property \(x^2=2, x^3=3\); how do we know introducing \(i\) will not lead to a contradiction?
We will define \(\mathbb C\) to consist of the plane \(\mathbb R^2\), i.e.\ pairs of real numbers, with operations \(+\) and \(\cdot\) which satisfy:
\begin{itemize}
	\item \((a,b)+(c,d) = (a+c, b+d)\)
	\item \((a,b)\cdot(c,d) = (ac-bd, ad+bc)\)
\end{itemize}
We can view \(\mathbb R\) as being contained within \(\mathbb C\) by identifying the real number \(a\) with \((a, 0)\).
Note that the rules of arithmetic of the reals are inherited inside the first element of the complex plane, so there is no contradiction here.
Then let \(i=(0,1)\).
Trivially then, any point \((a, b)\) in the complex numbers may be written as \(a+bi\) where \(a, b \in \mathbb R\).
And, of course, \(i^2 = -1\).

All of the basic rules like associativity and distributivity work in the complex plane.
There are multiplicative inverses: given \(a+bi\), we know that \((a+bi)(a-bi) = a^2 + b^2\) so \(\frac{a-bi}{a^2 + b^2}\) is the inverse (provided the point is nonzero).
This kind of structure with familiar properties is known as a field, for example \(\mathbb C\), \(\mathbb R\), \(\mathbb Q\), \(\mathbb Z_p\) where \(p\) is prime.
The fundamental theorem of algebra states that any nonzero polynomial with complex coefficients has a complex root; this is proven in the IB course Complex Analysis.
