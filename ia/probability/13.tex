\subsection{Definition}
Let \(X\) be a random variable with values in the positive integers, \(\mathbb N\).
Let \(p_r = \prob{X = r}\) be the probability mass function.
Then the probability generating function is defined to be
\[
	p(z) = \sum_{r=0}^\infty p_r z^r = \expect{z^X} \text{ for } \abs{z} \leq 1
\]
When \(\abs{z} \leq 1\), the probability generating function converges absolutely, since \(\abs{\sum_{r=0}^\infty p_r z^r} \leq \sum_{r=0}^\infty p_r = 1\).
So \(p(z)\) is well-defined and has a radius of convergence of at least 1.
\begin{theorem}
	The probability generating function of \(X\) uniquely determines the distribution of \(X\).
\end{theorem}
\begin{proof}
	Suppose \((p_r)\) and \((q_r)\) are two probability mass functions with
	\[
		\sum_{r=0}^\infty p_r z^r = \sum_{r=0}^\infty q_r z^r, \forall \abs{z} \leq 1
	\]
	We will show that \(p_r = q_r\) for all \(r\).
	First, set \(z = 0\), then clearly \(p_0 = q_0\).
	Then by induction, suppose that \(p_r = q_r\) for all \(r \leq n\).
	Then we would like to show that \(p_{n+1} = q_{n+1}\).
	We know that
	\[
		\sum_{r=n+1}^\infty p_r z^r = \sum_{r=n+1}^\infty q_r z^r
	\]
	Hence, dividing by \(z^{n+1}\), and taking the limit as \(z \to 0\), we have \(p_{n+1} = q_{n+1}\) as required.
\end{proof}

\subsection{Finding Moments and Probabilities}
\begin{theorem}
	\[
		\lim_{z \to 1^-} p'(z) = p'(1^-) = \expect{X}
	\]
\end{theorem}
\begin{proof}
	We will first assume that \(\expect{X}\) is finite; we will then extend the proof to the infinite case.
	Let \(0 < z < 1\), then since the series \(p(z)\) is absolutely convergent, we can interchange the sum and the derivative operators, giving
	\[
		p'(z) = \sum_{r=0}^\infty r p_r z^{r-1}
	\]
	We can make an upper bound for this sum:
	\[
		\sum_{r=0}^\infty r p_r z^{r-1} \leq \sum_{r=0}^\infty r p_r = \expect{X}
	\]
	Since \(0 < z < 1\), we see that \(p'(z)\) is an increasing function of \(z\).
	This implies that there exists a limit of \(p'(z)\) as \(z \to 1^-\), which is upper bounded by \(\expect{X}\).
	Now, let \(\varepsilon > 0\) and let \(N\) be an integer large enough such that
	\[
		\sum_{r = 0}^N rp_r \geq \expect{X} - \varepsilon
	\]
	We have further that, since \(0 < z < 1\),
	\[
		p'(z) \geq \sum_{r=1}^N rp_r z^{r-1}
	\]
	So
	\[
		\lim_{z \to 1^-} p'(z) \geq \sum_{r=1}^N rp_r \geq \expect{X} - \varepsilon
	\]
	which is true for any \(\varepsilon\).
	Therefore \(\lim_{z \to 1^-} p'(z) = \expect{X}\).
	Now, in the case that \(\expect{X}\) is infinite, for any \(M\) we can find a sufficiently large \(N\) such that
	\[
		\sum_{r = 0}^N rp_r \geq M
	\]
	From above, we know that
	\[
		\lim_{z \to 1^-} p'(z) \geq \sum_{r=1}^N rp_r \geq M
	\]
	Since this is true for any \(M\), this limit is equal to \(\infty\).
\end{proof}
\noindent In exactly the same way, we can prove that
\[
	p''(1^-) = \expect{X(X-1)}
\]
and in general,
\[
	p^{(k)}(1^-) = \expect{X(X-1)\cdots(X-k+1)}
\]
In particular,
\[
	\Var{X} = p''(1^-) + p'(1^-) - p'(1^-)^2
\]
Further,
\[
	\prob{X = n} = \frac{1}{n!} \eval{\dv[n]{z} p(z)}_{z = 0}
\]

\subsection{Sums of Random Variables}
Suppose that \(X_1, \dots, X_n\) are independent random variables with probability generating functions \(q_1, \dots, q_n\) respectively.
Then
\[
	p(z) = \expect{z^{X_1 + \dots + X_n}}
\]
Recall that if \(X\) and \(Y\) are independent, then for all functions \(f\) and \(g\), we have \(\expect{f(X)g(Y)} = \expect{f(X)}\expect{g(Y)}\).
Therefore,
\[
	p(z) = \expect{z^{X_1} z^{X_2} \cdots z^{X_n}} = \expect{z^{X_1}} \cdots \expect{z^{X_n}} = q_1(z) \cdots q_n(z)
\]
So the probability generating function factorises into its independent parts.
In particular, if all the \(X_i\) are independent and identically distributed, then
\[
	p(z) = q(z)^n
\]

\subsection{Common Probability Generating Functions}
Suppose that \(X \sim \mathrm{Bin}(n, p)\).
Then
\begin{align*}
	p(z) & = \expect{z^X}                                  \\
	     & = \sum_{r=0}^n z^r \binom{n}{r} p^r (1-p)^{n-r} \\
	     & = \sum_{r=0}^n \binom{n}{r} (pz)^r (1-p)^{n-r}  \\
	     & = (pz + 1 - p)^{n}
\end{align*}
Now, let \(X \sim \mathrm{Bin}(n, p)\), \(Y \sim \mathrm{Bin}(m, p)\) be independent random variables.
Then the probability generating function of \(X+Y\) is
\[
	(pz + 1 - p)^{n} \cdot (pz + 1 - p)^{m} = (pz + 1 - p)^{n+m}
\]
which is the probability generating function of a binomial distribution where the number of trials is \(n+m\).
Now, suppose that \(X \sim \mathrm{Geo}(p)\).
Then
\begin{align*}
	p(z) & = \expect{z^X}               \\
	     & = \sum_{r=0}^n z^r (1-p)^r p \\
	     & = \frac{p}{1-z(1-p)}         \\
\end{align*}
Now, suppose that \(X \sim \mathrm{Poi}(\lambda)\).
Then
\begin{align*}
	p(z) & = \expect{z^X}                                       \\
	     & = \sum_{r=0}^n z^r e^{-\lambda} \frac{\lambda^r}{r!} \\
	     & = e^{\lambda(z-1)}                                   \\
\end{align*}

\subsection{Random Sums of Random Variables}
Consider the sum of a random number of random variables.
Let \(X_1, \dots\) be independent and identically distributed, and let \(N\) be an independent random variable with values in \(\mathbb N\).
Now, we can define the random variables \(S_n\) to be
\[
	S_n = X_1 + \dots + X_n
\]
Then
\[
	S_N = X_1 + \dots + X_N
\]
is a random variable dependent on \(N\).
For all \(\omega \in \Omega\),
\begin{align*}
	S_N(\omega) & = X_1(\omega) + \dots + X_{N(\omega)}(\omega) \\
	            & = \sum_{i=1}^{N(\omega)} X_i(\omega)
\end{align*}
Now, let \(q\) be the probability generating function of \(N\), and \(p\) be the probability generating function of \(X_1\) (or equivalently, any \(X_i\)).
Then let
\begin{align*}
	r(z) & = \expect{z^{S_N}}                                          \\
	     & = \sum_{n} \expect{z^{X_1 + \dots + X_N} \cdot 1(N = n)}    \\
	     & = \sum_{n} \expect{z^{X_1 + \dots + X_n} \cdot 1(N = n)}    \\
	     & = \sum_{n} \expect{z^{X_1 + \dots + X_n}} \expect{1(N = n)} \\
	     & = \sum_{n} \expect{z^{X_1 + \dots + X_n}} \prob{N = n}      \\
	     & = \sum_{n} \expect{z^{X_1}}^n \prob{N = n}                  \\
	     & = \sum_{n} p(z)^n \prob{N = n}                              \\
	     & = q(p(z))
\end{align*}
Here is an alternative proof using conditional expectation.
\begin{align*}
	r(z) & = \expect{z^{S_N}}                 \\
	     & = \expect{\expect{z^{S_N} \mid N}} \\
\end{align*}
We can see that
\begin{align*}
	\expect{z^{S_N} \mid N = n} & = \expect{z^{S_n} \mid N = n} \\
	                            & = \expect{z^{X_1}}^n          \\
	                            & = p(z)^n
\end{align*}
Therefore,
\begin{align*}
	r(z) & = \expect{p(z)^N} \\
	     & = q(p(z))
\end{align*}
Using this expression for \(r\), we can find that
\[
	\expect{S_N} = r'(1^-) = q'(p(1^-)) \cdot p'(1^-) = q'(1^-) \cdot p'(1^-) = \expect{N} \expect{X_1}
\]
Similarly,
\[
	\Var{S_N} = \expect{N} \Var{X_1} + \Var{N} \left( \expect{X_1} \right)^2
\]
