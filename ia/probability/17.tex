\subsection{Standardising Normal Distributions}
Suppose \(X \sim \mathrm{N}(\mu, \sigma^2)\).
Let \(a \neq 0, b \in \mathbb R\), and let \(g(x) = ax+b\).
We define \(Y = g(X) = aX+b\).
We can find the density \(f_Y\) of \(Y\), by noting that \(g\) is a monotonic function and the inverse has a derivative.
We can then use the theorem in the last lecture to show that
\begin{align*}
	f_Y(y) & = f_X(g^{-1}(y)) \cdot \abs{\dv{y} g^{-1}(y)}                                                       \\
	       & = \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{(\frac{y-b}{a} - \mu)^2}{2\sigma^2}) \cdot \frac{1}{2a} \\
	       & =\frac{1}{\sqrt{2\pi a^2\sigma^2}} \exp(-\frac{(y - a\mu + b)^2}{2 a^2 \sigma^2})
\end{align*}
Hence \(Y \sim \mathrm{N}(a \mu + b, a^2 \sigma^2)\).
In particular, \(\frac{X-\mu}{\sigma}\) is exactly the standard normal distribution.
\begin{definition}
	Suppose \(X\) is a continuous random variable.
	Then the median of \(X\), denoted by \(m\), is the number satisfying
	\[
		\prob{X \leq m} = \prob{X \geq m} = \frac{1}{2}
	\]
\end{definition}
\noindent If \(X \sim \mathrm{N}(\mu, \sigma^2)\), then \(\prob{X \leq \mu} = \Phi(0) = \frac{1}{2}\) hence \(\mu\) is the median of the normal distribution.

\subsection{Multivariate Density Functions}
Suppose \(X = (X_1, \dots, X_n) \in \mathbb R^n\) is a random variable.
We say that \(X\) has density \(f\) if
\[
	\prob{X_1 \leq x_1, \dots, X_n \leq x_n} = \int_{-\infty}^{x_1} \dots \int_{-\infty}^{x_n}  f(y_1, \dots, y_n) \dd{y_1} \dots \dd{y_n}
\]
Then,
\[
	f(x_1, \dots, x_n) = \frac{\partial^n}{\partial x_1 \dots \partial x_n} F(x_1, \dots, x_n)
\]
This generalises the fact that for all (reasonable) \(B \subseteq \mathbb R^n\),
\[
	\prob{(X_1, \dots, X_n) \in B} = \int_B f(y_1, \dots, y_n) \dd{y_1}\dots\dd{y_n}
\]

\subsection{Independence of Events}
In the continuous case, we can no longer use the definition \(\prob{X = a, Y = b} = \prob{X = a}\prob{Y = b}\), since the probability of a random variable being a specific value is always zero.
Instead, we define that \(X_1, \dots X_n\) are independent if for all \(x_1, \dots, x_n \in \mathbb R\),
\[
	\prob{X_1 \leq x_1, \dots, X_n \leq x_n} = \prob{X_1 \leq x_1}\cdots\prob{X_n \leq x_n}
\]
\begin{theorem}
	Suppose \(X = (X_1, \dots, X_n)\) has density \(f\).
	\begin{enumerate}[(a)]
		\item Suppose \(X_1, \dots, X_n\) are independent with densities \(f_1, \dots, f_n\).
		      Then \(f(x_1, \dots, x_n) = f_1(x_1)\cdots f_n(x_n)\).
		\item Suppose that \(f\) factorises as \(f(x_1, \dots, x_n) = f_1(x_1)\cdots f_n(x_n)\) for some non-negative functions \(f_1, \dots, f_n\).
		      Then \(X_1, \dots, X_n\) are independent with densities proportional to \(f_1, \dots, f_n\).
		      (In order to have a density function, we require that it integrates to 1, so we choose a scaling factor such that this requirement holds.)
	\end{enumerate}
	In other words, \(f\) factorises if and only if it is comprised of independent events.
\end{theorem}
\begin{proof}
	\begin{enumerate}[(a)]
		\item We know that
		      \begin{align*}
			      \prob{X_1 \leq x_1, \dots, X_n \leq x_n} & = \prob{X_1 \leq x_1}\cdots\prob{X_n \leq x_n}                                       \\
			                                               & = \int_{-\infty}^{x_1} f_1(y_1)\dd{y_1} \cdots \int_{-\infty}^{x_n} f_n(y_n)\dd{y_n} \\
			                                               & = \int_{-\infty}^{x_1} \dots \int_{-\infty}^{x_n} \prod_{i=1}^n f_i(y_i) \dd{y_i}
		      \end{align*}
		      So the density of \((X_1, \dots, X_n)\) is the product of the \((f_i)\).
		\item Suppose \(f\) factorises.
		      Let \(B_1, \dots, B_n \subseteq \mathbb R\).
		      Then
		      \[
			      \prob{X_1 \in B_1, \dots, X_n \in B_n} = \int_{B_1} \cdots \int_{B_n} f_1(x_1)\cdots f_n(x_n) \dd{y_1} \cdots \dd{y_n}
		      \]
		      Now, let \(B_j = \mathbb R\) for all \(j \neq i\).
		      Then
		      \[
			      \prob{X_i \in B_i} = \prob{X_i \in B_i, X_j \in B_j \;\forall j \neq i} = \int_{B_i} f_i(y_i) \dd{y_i} \cdot \prod_{j \neq 1} \int_{B_j} f_j(x_j)\dd{y_j}
		      \]
		      Since \(f\) is a density function,
		      \[
			      \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty f(x_1, \dots, x_n) \dd{x_1} \cdots \dd{x_n} = 1
		      \]
		      But \(f\) is the product of the \(f_i\), so
		      \[
			      \prod_j \int_{-\infty}^\infty f_j(y) \dd{y} = 1 \implies \prod_{j \neq i} \int_{-\infty}^\infty f_j(y) \dd{y} = \frac{1}{\int_{-\infty}^\infty f_i(y) \dd{y}}
		      \]
		      Hence,
		      \[
			      \prob{X_i \in B_i} = \frac{\int_{B_i} f_i(y) \dd{y}}{\int_{-\infty}^\infty f_i(y) \dd{y}}
		      \]
		      This shows that the density of \(X_i\) is
		      \[
			      \frac{f_i}{\int_{-\infty}^\infty f_i(y) \dd{y}}
		      \]
		      The \(X_i\) are independent, since
		      \begin{align*}
			      \prob{X_1 \leq x_1, \dots, X_n \leq x_n} & = \frac{\int_{-\infty}^{x_1} f_1(y_1)\dd{y_1} \cdots \int_{-\infty}^{x_n} f_n(y_n) \dd{y_n}}{\int_{-\infty}^\infty f_1(y_1)\dd{y_1} \cdots \int_{-\infty}^\infty f_n(y_n) \dd{y_n}} \\
			                                               & = \prob{X_1 \leq x_1}\cdots\prob{X_n \leq x_n}
		      \end{align*}
	\end{enumerate}
\end{proof}

\subsection{Marginal Density}
Suppose that \((X_1, \dots, X_n)\) has density \(f\).
Now we can compute the marginal density as follows.
\begin{align*}
	\prob{X_1 \leq x} & = \prob{X_1 \leq x, X_2 \in \mathbb R, \dots, X_n \in \mathbb R}                                                                                                                        \\
	                  & = \int_{-\infty}^x \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty f(x_1, \dots, x_n) \dd{x_1}\cdots \dd{x_n}                                                                        \\
	                  & = \int_{-\infty}^x \dd{x_1} \underbrace{\left( \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty f(x_1, \dots, x_n) \dd{x_2}\cdots \dd{x_n} \right)}_{\text{marginal density of } X_1} \\
\end{align*}

\subsection{Sum of Random Variables}
Recall that in the discrete case, for independent random variables \(X\) and \(Y\) we have
\[
	\prob{X+Y = z} = \sum_y \prob{X+Y = z, Y=y} = \sum_y \prob{X = z-y} \prob{Y = y} = \sum_y p_x(z-y) p_y(y)
\]
which was called the convolution.
In the continuous case,
\begin{align*}
	\prob{X+Y \leq z} & = \iint_{\{ x+y \leq z \}} f_{X, Y}(x, y) \dd{x}\dd{y}                                                                   \\
	                  & = \int_{-\infty}^\infty \int_{-\infty}^{z-x} f_X(x)f_Y(y) \dd{x}\dd{y}                                                   \\
	                  & = \int_{-\infty}^\infty \left( \int_{-\infty}^{z} f_X(x)f_Y(y-x) \dd{y} \right) \dd{x}\; (\text{using } y \mapsto y + x) \\
	                  & = \int_{-\infty}^z \dd{y} \underbrace{\left( \int_{-\infty}^\infty f_Y(y-x) f_X(x) \dd{x} \right)}_{g(y)}
\end{align*}
Hence the density of \(X+Y\) is \(g(y)\), where
\[
	g(y) = \int_{-\infty}^\infty f_Y(y-x) f_X(x) \dd{x}
\]
\begin{definition}
	Let \(f, g\) be density functions.
	Then the convolution of \(f\) and \(g\) is
	\[
		(f \star g)(y) = \int_{-\infty}^\infty f_Y(y-x) f_X(x) \dd{x}
	\]
\end{definition}
\noindent Here is a non-rigorous argument, which can be used as a heuristic.
\begin{align*}
	\prob{X + Y \leq z}        & = \int_{-\infty}^\infty \prob{X + Y \leq z, Y \in \dd{y}}      \\
	                           & = \int_{-\infty}^\infty \prob{X + Y \leq z, Y \in \dd{y}}      \\
	                           & = \int_{-\infty}^\infty \prob{X \leq z - y}\prob{Y \in \dd{y}} \\
	                           & = \int_{-\infty}^\infty \prob{X \leq z - y}f_Y(y)\dd{y}        \\
	                           & = \int_{-\infty}^\infty F_X(z - y)f_Y(y)\dd{y}                 \\
	\dv{z} \prob{X + Y \leq z} & = \int_{-\infty}^\infty \dv{z} F_X(z - y)f_Y(y)\dd{y}          \\
	                           & = \int_{-\infty}^\infty f_X(z - y)f_Y(y)\dd{y}                 \\
\end{align*}
