\subsection{Introduction}
Let \((X_n \colon n \geq 0)\) be a random process, where \(X_n\) is the number of individuals in generation \(n\), and \(X_0 = 1\).
The individual in generation 0 produces a random number of offspring with distribution
\[
	g_k = \prob{X_1 = k}
\]
Then every individual in generation 1 produces an independent number of offspring with the same distribution.
This is called a branching process.
We can write a recursive formula for \(X_n\).
First, let \((Y_{k, n} \colon k \geq 1, n \geq 0)\) be an independent and identically distributed sequence with distribution \((g_k)_k\).
So \(Y_{k, n}\) is the number of offspring of the \(k\)th individual in generation \(n\).
\[
	X_{n+1} = \begin{cases}
		Y_{1,n} + \cdots + Y_{X_n,n} & \text{when } X_n \geq 1 \\
		0                            & \text{otherwise}
	\end{cases}
\]

\subsection{Expectation of Generation Size}
\begin{theorem}
	\[
		\expect{X_n} = \expect{X_1}^n
	\]
\end{theorem}
\begin{proof}
	Inductively,
	\begin{align*}
		\expect{X_{n+1}}                     & = \expect{\expect{X_{n+1} \mid X_n}}                 \\
		\expect{X_{n+1} \mid X_n = m}        & = \expect{Y_{1,n} + \cdots + Y_{X_n,n} \mid X_n = m} \\
		                                     & = \expect{Y_{1,n} + \cdots + Y_{m,n} \mid X_n = m}   \\
		                                     & = m \expect{Y_{1,n}}                                 \\
		                                     & = m \expect{X_1}                                     \\
		\therefore \expect{X_{n+1} \mid X_n} & = X_n \cdot \expect{X_1}                             \\
		\therefore \expect{X_{n+1}}          & = \expect{X_n \cdot \expect{X_1}}                    \\
		                                     & = \expect{X_n} \cdot \expect{X_1}
	\end{align*}
\end{proof}

\subsection{Probability Generating Functions}
\begin{theorem}
	Let \(G(z) = \expect{z^{X_1}}\) be the probability generating function of \(X_1\), and \(G_n(z) = \expect{z^{X_n}}\) be the probability generating function of \(X_n\).
	Then
	\[
		G_{n+1}(z) = G(G_n(z)) = G(G(\cdots G(z)\cdots)) = G_n(G(z))
	\]
\end{theorem}
\begin{proof}
	\begin{align*}
		G_{n+1}(z)                                        & = \expect{z^{X_{n+1}}}                                 \\
		                                                  & = \expect{\expect{z^{X_{n+1}} \mid X_n}}               \\
		\expect{z^{X_{n+1}} \mid X_n = m}                 & = \expect{z^{Y_{1, n} + \dots + Y_{m,n}} \mid X_n = m} \\
		                                                  & = \expect{z^{X_1}}^m                                   \\
		                                                  & = G(z)^m                                               \\
		\therefore \expect{\expect{z^{X_{n+1}} \mid X_n}} & = \expect{G(z)^{X_n}}                                  \\
		                                                  & = G_n(G(z))
	\end{align*}
\end{proof}

\subsection{Probability of Extinction}
We define the extinction probability \(q\) as the probability that \(X_n = 0\) for some \(n \geq 1\), and \(q_n = \prob{X_n = 0}\).
It is clear that \(X_n = 0\) implies that \(X_{n+1} = 0\).
So the sequence of events \((A_n) = (\{ X_n = 0 \})\) is an increasing sequence of events.
So by the continuity of the probability measure, \(\prob{A_n}\) converges to \(\prob{\bigcup A_n}\) as \(n \to \infty\).
Note that the event \(\bigcup A_n\) is the event that there will be extinction.
Therefore, \(q_n \to q\) as \(n \to \infty\).
\begin{claim}
	\(q_{n+1} = G(q_n)\) and \(q = G(q)\).
\end{claim}
\begin{proof}
	Using the above theorem on \(q\),
	\begin{align*}
		q_{n+1} & = \prob{X_{n+1} = 0} \\
		        & = G_{n+1}(0)         \\
		        & = G(G_n(0))          \\
		        & = G(q_n)
	\end{align*}
	Since \(G\) is continuous, taking the limit as \(n \to \infty\) and using that \(q_n \to q\) gives \(G(q) = q\).
\end{proof}
\noindent We can form another proof for the first part of the above claim.
\begin{proof}
	Instead of conditioning on the previous generation, let us condition on the first generation, i.e.\ \(X_1 = m\).
	Note that after the first generation, we will have \(m\) independent subtrees on the family tree.
	Each tree is identically distributed to the entire tree as a whole.
	Hence,
	\[
		X_{n+1} = X_n^{(1)} + \dots + X_n^{(m)}
	\]
	where the \(X_i^{(j)}\) are independent and identically distributed random processes each with the same offspring distribution.
	By the law of total probability,
	\begin{align*}
		q_{n+1} & = \prob{X_{n+1} = 0}                                                     \\
		        & = \sum_m \prob{X_{n+1} = 0 \mid X_1 = m} \cdot \prob{X_1 = m}            \\
		        & = \sum_m \prob{X_n^{(1)} = 0, \dots, X_n^{(m)} = 0} \cdot \prob{X_1 = m} \\
		        & = \sum_m \prob{X_n^{(1)} = 0}^m \cdot \prob{X_1 = m}                     \\
		        & = \sum_m q_n^m \cdot \prob{X_1 = m}                                      \\
		        & = G(q_n)
	\end{align*}
\end{proof}
\begin{theorem}
	The extinction probability \(q\) is the minimal non-negative solution to \(G(t) = t\).
	Further, supposing that \(\prob{X_1 = 1} < 1\), we have that \(q < 1\) if and only if \(\expect{X_1} > 1\).
\end{theorem}
\begin{proof}
	First, we will prove the minimality of \(q\).
	Let \(t\) be the smallest non-negative solution to \(G(t) = t\).
	We will prove inductively that \(q_n \leq t\) for all \(n\), and then by taking limits we have that \(q \leq t\).
	Since \(q\) is a solution, this will imply that \(q=t\).
	Now, as a base case, \(q_0 = 0 = \prob{X_0 = 0} \leq t\).
	Inductively let us suppose that \(q_n \leq t\).
	We know that \(q_{n+1} = G(q_n)\).
	\(G\) is an increasing function on \([0, 1]\), and since \(q_n \leq t\) we have \(q_{n+1} = G(q_n) \leq G(t) = t\).

	Now, we can take \(\prob{X_1 = 1} < 1\).
	Let us use the notation \(g_r = \prob{X_1 = r}\) for simplicity.
	Consider the function \(H(z) = G(z) - z\).
	Let us assume further that \(g_0 + g_1 < 1\), since otherwise we cannot possibly ever increase the amount of individuals in future generations, as \(\expect{X_1} = \prob{X_1 = 1} < 1\).
	In this case, \(G(z) = g_0 + g_1 z = 1 - \expect{X_1} + \expect{X_1} \cdot z\), and solving \(G(z) = z\) we would get only \(z=1\) since \(\expect{X_1} < 1\).
	Now,
	\[
		H''(z) = \sum_{r=2}^\infty r(r-1)g_r z^{r-2} > 0\quad \forall z \in (0, 1)
	\]
	This implies that \(H'(z)\) is a strictly increasing function in \((0, 1)\).
	Hence, \(H(z)\) has at most one root different from 1 in \((0, 1)\), which follows from Rolle's theorem; indeed, if it had two roots different from 1, then \(H'\) would be zero once in \((z_1, z_2)\) and once in \((z_2, 1)\), which contradicts the fact that \(H'\) is strictly increasing.

	Let us first consider the case where \(H\) has no other root apart from 1.
	In this case, \(H(1) = 0\) and \(H(0) = g_0 \geq 0 \implies H(z) \geq 0\) for all \(z \in [0, 1]\).
	We can find that
	\[
		H'(1^-) = \lim_{z \to 1^-} \frac{H(z) - H(1)}{z-1} = \frac{H(z)}{z-1} < 0
	\]
	since the numerator is positive, and the denominator is negative.
	We know that \(H'(1^-) = G'(1^-) - 1\), and \(H'(1^-) \leq 0 \implies G'(1^-) \leq 1\), and \(G'(1^-) = \expect{X_1}\).
	So when \(q=1\), then \(\expect{X_1} \leq 1\).

	In the other case, \(H\) has one other root \(r<1\) as well as 1.
	We have that \(H(r) = 0\) and \(H(1) = 0\).
	By Rolle's theorem, there exists \(z \in (r, 1)\) such that \(H'(z) = 0\).
	Further, \(H'(x) = G'(x) - 1\) therefore \(G'(z) = 1\).
	Now,
	\[
		G'(x) = \sum_{r = 1}^\infty rg_r x^{r-1} \implies H''(x) = G''(x) = \sum_{r = 2}^\infty r(r-1)g_r x^{r-2}
	\]
	Under the assumption that \(g_0 + g_1 < 1\), we have that \(G''(x) > 0\) for all \(x \in (0, 1)\), hence \(G'\) is strictly increasing for all \(x \in (0, 1)\).
	Therefore, \(G'(1^-) > G'(z) = 1\) giving \(\expect{X_1} > 1\).
	So if \(q < 1\), then \(\expect{X_1} > 1\).
\end{proof}
