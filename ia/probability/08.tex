\subsection{Countable Additivity for the Expectation}
Suppose \(X_1, X_2, \dots\) are non-negative random variables.
Then
\[
	\expect{\sum_n X_n} = \sum_n \expect{X_n}
\]
The non-negativity constraint allows us to guarantee that the sums are well-defined; they could be infinite, but at least their values are well-defined.
We will construct a proof assuming that \(\Omega\) is countable, however the result holds regardless of the choice of \(\Omega\).
\begin{proof}
	\begin{align*}
		\expect{\sum_n X_n} & = \sum_\omega \sum_n X_n(\omega) \prob{\{ \omega \}} \\
		                    & = \sum_n \sum_\omega X_n(\omega) \prob{\{ \omega \}} \\
		                    & = \sum_n \expect{X_n}
	\end{align*}
\end{proof}
\noindent We are allowed to rearrange the sums since all relevant terms are non-negative.

\subsection{Expectation of Indicator Function}
If \(X = 1(A)\) where \(A \in \mathcal F\), then \(\expect{X} = \prob{A}\).
This is obvious from the second definition of the expectation.

\subsection{Expectation under Function Application}
If \(g \colon \mathbb R \to \mathbb R\), we can define \(g(X)\) to be the random variable given by
\[
	g(X)(\omega) = g(X(\omega))
\]
Then
\[
	\expect{g(X)} = \sum_{x \in \Omega_X} g(x) \cdot \prob{X = x}
\]
\begin{proof}
	Let \(Y = g(X)\).
	Then
	\[
		\expect{Y} = \sum_{y \in \Omega_Y} y \cdot \prob{Y = y}
	\]
	Note that
	\begin{align*}
		\{ Y = y \} & = \{ \omega \colon Y(\omega) = y \}              \\
		            & = \{ \omega \colon g(X(\omega)) = y \}           \\
		            & = \{ \omega \colon X(\omega) \in g^{-1}(\{y\})\} \\
		            & = \{ X \in g^{-1} (\{ y \}) \}
	\end{align*}
	where \(g^{-1}(\{ y \})\) is the set of all \(x\) such that \(g(x) \in \{ y \}\).
	So
	\begin{align*}
		\expect{Y} & = \sum_{y \in \Omega_y} y \cdot \prob{X \in g^{-1}(\{ y \})}              \\
		           & = \sum_{y \in \Omega_Y} y \cdot \sum_{x \in g^{-1}(\{ y \})} \prob{X = x} \\
		           & = \sum_{y \in \Omega_Y} \sum_{x \in g^{-1}(\{y\})} g(x) \prob{X = x}      \\
		           & = \sum_{x \in \Omega_X} g(x) \prob{X = x}
	\end{align*}
\end{proof}

\subsection{Calculating Expectation with Cumulative Probabilities}
If \(X \geq 0\) and takes integer values, then
\[
	\expect{X} = \sum_{k=1}^\infty \prob{X \geq k} = \sum_{k=0}^\infty \prob{X > k}
\]
\begin{proof}
	Since \(X\) takes non-negative integer values,
	\[
		X = \sum_{k=1}^\infty 1(X \geq k) = \sum_{k=0}^\infty 1(X > k)
	\]
	This represents the fact that any integer is the sum of that many ones, e.g.
	\(4 = 1 + 1 + 1 + 1 + 0 + 0 + \dots\) to infinity.
	Taking the expectation of the above formula, using that \(\expect{1(A)} = \prob{A}\) and countable additivity, we have the result as claimed.
\end{proof}

\subsection{Inclusion-Exclusion Formula with Indicators}
We can provide another proof of the inclusion-exclusion formula, using some basic properties of indicator functions.
\begin{itemize}
	\item \(1(\stcomp{A}) = 1 - 1(A)\)
	\item \(1(A \cap B) = 1(A) \cdot 1(B)\)
	\item Following from the above, \(1(A \cup B) = 1-(1-1(A))(1-1(B))\).
\end{itemize}
More generally,
\[
	1(A_1 \cup \dots \cup A_n) = 1-\prod_{i=1}^n(1-1(A_i))
\]
which gives the inclusion-exclusion formula.
Taking the expectation of both sides, we can see that
\[
	\prob{A_1 \cup \dots \cup A_n} = \sum_{i=1}^n \prob{A_i} - \sum_{i_1 < i_2} \prob{A_{i_1} \cap A_{i_2}} + \dots + (-1)^{n+1}\prob{A_1 \cap \dots \cap A_n}
\]
which is the result as previously found.

\subsection{Variance}
Let \(X\) be a random variable, and \(r \in \mathbb N\).
If it is well-defined, we call \(\expect{X^r}\) the \(r\)th moment of \(X\).
We define the variance of \(X\) by
\[
	\Var{X} = \expect{(X - \expect{X})^2}
\]
If the variance is small, \(X\) is highly concentrated around \(\expect{X}\).
If the variance is large, \(X\) has a wide distribution including values not necessarily near \(\expect{X}\).
We call \(\sqrt{\Var{X}}\) the standard deviation of \(X\), denoted with \(\sigma\).
The variance has the following basic properties:
\begin{itemize}
	\item \(\Var{X} \geq 0\), and if \(\Var{X} = 0\), \(\prob{X = \expect{X}} = 1\).
	\item If \(c \in \mathbb R\), then \(\Var{cX} = c^2\Var{X}\), and \(\Var{X + c} = \Var{X}\).
	\item \(\Var{X} = \expect{X^2} - \expect{X}^2\).
	      This follows since \(\expect{(X - \expect{X})^2} = \expect{X^2 - 2X\expect{X} + \expect{X}^2} = \expect{X^2} - 2\expect{X} \expect{X} + \expect{X}^2 = \expect{X^2} - \expect{X}^2\).
	\item \(\Var{X} = \min_{c \in \mathbb R} \expect{(X - c)^2}\), and this minimum is achieved at \(c = \expect{X}\).
	      Indeed, if we let \(f(c) = \expect{(X - c)^2}\), then \(f(c) = \expect{X^2} - 2c\expect{X} + c^2\).
	      Minimising \(f\), we get \(f(\expect{X}) = \Var{X}\) as required.
\end{itemize}
As an example, consider \(X \sim \text{Bin}(n, p)\).
Then \(\expect{X} = np\), as we found before.
Note that we can also represent this binomial distribution as the sum of \(n\) Bernoulli distributions of parameter \(p\) to get the same result.
The variance of \(X\) is
\[
	\Var{X} = \expect{X^2} - \expect{X}^2
\]
In fact, in order to compute \(\expect{X^2}\) it is easier to find \(\expect{X(X-1)}\).
\begin{align*}
	\expect{X(X-1)} & = \sum_{k=2}^n k \cdot (k-1) \cdot \binom{n}{k} \cdot p^k \cdot (1-p)^{n-k} \\
	                & = \sum_{k=2}^n \frac{k(k-1)n! p^k (1-p)^{n-k}}{(n-k)!k!}                    \\
	                & = \sum_{k=2}^n \frac{n! p^k (1-p)^{n-k}}{((n-2)-(k-2))!(k-2)!}              \\
	                & = n(n-1)p^2 \sum_{k=2}^n \binom{n-2}{k-2} p^{k-2} (1-p)^{n-k}               \\
	                & = n(n-1)p^2
\end{align*}
Hence,
\[
	\Var{X} = \expect{X(X-1)} + \expect{X} - \expect{X}^2 = n(n-1)p^2 + np - (np)^2 = np(1-p)
\]
As a second example, if \(X \sim \text{Poi}(\lambda)\), we have \(\expect{X} = \lambda\).
Because of the factorial term, it is easier to use \(X(X-1)\) than \(X^2\).
\begin{align*}
	\expect{X(X-1)} & = \sum_{k=2}^\infty k(k-1) e^{-\lambda} \frac{\lambda^k}{k!}                  \\
	                & = e^{-\lambda} \sum_{k=2}^\infty \frac{\lambda_{k-2}}{(k-2)!} \cdot \lambda^2 \\
	                & = \lambda^2
\end{align*}
Hence,
\[
	\Var{X} = \lambda^2 + \lambda - \lambda^2 = \lambda
\]
