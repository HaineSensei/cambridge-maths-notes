\documentclass{article}

\input{../util.tex}

\title{Vectors and Matrices}
\author{Cambridge University Mathematical Tripos: Part IA}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Complex Numbers}
\subsection{Definitions}
We construct the complex numbers from $\mathbb R$ by adding an element $i$ such that $i^2 = -1$. By definition, any complex number $z \in \mathbb C = x + iy$ where $x, y \in \mathbb R$. We use the notation $x = \Re z$ and $y = \Im z$ to query the components of a complex number. The complex numbers contains the set of real numbers, due to the fact that $x = x + i0$. We define the operations of addition and multiplication in familiar ways, which lets us state that $\mathbb C$ is a field.

We also define the complex conjugate $\overline{z}$ as negating the imaginary part of $z$. Trivially we can see facts such as $\overline{\left( \overline{z}\right) } = z$; $\overline{z + w} = \overline z + \overline w$ and $\overline{zw} = \overline z \cdot \overline w$.

\subsection{The Fundamental Theorem of Algebra}
The Fundamental Theorem of Algebra states that a polynomial of degree $n$ can be written as a product of $n$ linear factors:
\[ c_nz^n + \cdots + c_1z^1 + c_0z^0 = c_n(z-\alpha_1)(z-\alpha_2) \cdots (z-\alpha_n)\quad (\text{where } c_i, \alpha_i \in \mathbb C) \]

We can reformulate this statement as follows: a polynomial of degree $n$ has $n$ solutions $\alpha_i$, counting repeats. This theorem is not proved in this course.

\subsection{Properties of Modulus}
The modulus of complex numbers $z_1, z_2$ satisfies:
\begin{itemize}
	\item (composition) $\abs{z_1 z_2} = \abs{z_1} \abs{z_2}$, and
	\item (triangle inequality) $\abs{z_1 + z_2} \leq \abs{z_1} + \abs{z_2}$
\end{itemize}
\begin{proof}
	The composition property is trivial. To prove the triangle inequality, we square both sides and compare.
	\begin{align*}
		\text{LHS} & = \abs{z_1 + z_2}^2                                                 \\
		           & = (z_1 + z_2)\overline{(z_1 + z_2)}                                 \\
		           & = \abs{z_1}^2 + \overline{z_1}z_2 + z_1\overline{z_2} + \abs{z_2}^2 \\
		\text{RHS} & = \abs{z_1}^2 + 2 \abs{z_1}\abs{z_2} + \abs{z_2}^2
	\end{align*}
	Note that
	\begin{align*}
		\overline{z_1}z_2 + z_1\overline{z_2}                                           & \leq 2 \abs{z_1}\abs{z_2}     \\
		\iff \frac{1}{2}\left( \overline{z_1}z_2 + \overline{\overline{z_1}z_2} \right) & \leq \abs{z_1}\abs{z_2}       \\
		\iff \Re (\overline{z_1} z_2)                                                   & \leq \abs{\overline{z_1} z_2}
	\end{align*}
	which is true.
\end{proof}

We can alternatively use the map $z_2 \to z_2 - z_1$ to write the triangle inequality as
\begin{align*}
	\abs{z_2 - z_1}            & \geq \abs{z_2} - \abs{z_1}       \\
	\text{or } \abs{z_2 - z_1} & \geq \abs{z_1} - \abs{z_2}       \\
	\therefore \abs{z_2 - z_1} & \geq \abs{\abs{z_2} - \abs{z_1}} \\
\end{align*}

\subsection{De Moivre's Theorem}
De Moivre's Theorem states that
\[ (\cos \theta + i \sin \theta)^n = \cos n \theta + i \sin n \theta \quad(\forall n \in \mathbb Z) \]
We can prove this using induction for $n \geq 0$. To show the negative case, simply use the positive result and raise it to the power of $-1$.

\section{Complex Valued Functions}
\subsection{Complex Function Definitions}
For $z \in \mathbb C$, we can define:
\begin{align*}
	\exp z & = \sum_{n=0}^{\infty} \frac{1}{n!}z^n          \\
	\cos z & = \frac{1}{2} \left( e^{iz} + e^{-iz} \right)  \\
	\sin z & = \frac{1}{2i} \left( e^{iz} - e^{-iz} \right)
\end{align*}
By defining $\log z = w \st e^w = z$, we have a complex logarithm function. By expanding the definition, we get that $\log z = \log r + i\theta$ where $r = \abs{z}$ and $\theta = \arg{z}$. Note that because the argument of a complex number is multi-valued, so is the logarithm.

We can define exponentiation in the general case by defining $z^\alpha = e^{\alpha \log z}$. Depending on the choice of $\alpha$, we have three cases:
\begin{itemize}
	\item If $\alpha = p \in \mathbb Z$ then the result of $z^p$ is unambiguous because
	      \[ z^p = e^{p \log z} = e^{p (\log r + i \theta + 2 \pi i n)} \]
	      which has a factor of $e^{2 \pi i p n}$ which is 1.
	\item For a similar reason, a rational exponent has finitely many values.
	\item But in the general case, there are infinitely many values.
\end{itemize}
We can calculate results such as the square root of a complex number, which have two results as you might expect.

\begin{note}
	We can't use facts like $z^\alpha z^\beta = z^{\alpha + \beta}$ in the complex case because the left and right hand sides both have infinite sets of answers, which may not be the same.
\end{note}

\subsection{Transformations and Primitives}
We can represent a line passing through $x_0\in \mathbb C$ parallel to $w \in \mathbb C$ using the formula:
\[ z = z_0 + \lambda w\quad(\lambda \in \mathbb R) \]
We can eliminate the dependency on $\lambda$ by computing the conjugate of both sides:
\begin{align*}
	\overline{z}                  & = \overline{z_0} + \lambda \overline{w} \\
	\overline{w}z - w\overline{z} & = \overline{w}z_0 - w\overline{z_0}
\end{align*} % TODO prove this probably
We can also write the equation for a circle with centre $c \in \mathbb C$ and radius $\rho \in \mathbb R$:
\[ z = c + \rho e^{i\alpha} \]
or equivalently:
\[ \abs{z - c} = \abs{\rho e^{i\alpha}} = \rho \]
or by squaring both sides:
\[ \abs{z}^2 - c\overline{z} - \overline{c}z = \rho^2 - \abs{c}^2 \]

\section{Vectors in Three Dimensions}
We use the normal Euclidean notions of points, lines, planes, length, angles and so on. By choosing an (arbitrary) origin point $O$, we may write positions as position vectors with respect to that origin point.

\subsection{Vector Addition and Scalar Multiplication}
We define vector addition using the shape of a parallelogram with points $\bm 0, \bm a, \bm a + \bm b, \bm b$. We define scalar multiplication of a vector using the line $\overrightarrow{OA}$ and setting the length to be multiplied by the constant. Note that this vector space is an abelian group under addition.
\begin{definition}
	$\bm a$ and $\bm b$ are defined to be parallel if and only if $\bm a = \lambda \bm b$ or $\bm b = \lambda \bm a$ for some $\lambda \in \mathbb R$. This is denoted $a \parallel b$. Note that the vectors may be zero, in particular the zero vector is parallel to all vectors.
\end{definition}
\begin{definition}
	The span of a set of vectors is defined as $\vecspan \{\bm a, \bm b, \cdots, \bm c\} = \{ \alpha \bm a + \beta \bm b + \cdots + \gamma \bm c: \alpha, \beta, \gamma \in \mathbb R \}$. This is the line/plane/volume etc. containing the vectors. The span has an amount of dimensions at most equal to the amount of vectors in the input set. For example, the span of a set of two vectors may be a point, line or plane containing the vectors.
\end{definition}

\subsection{Scalar Product}
\begin{definition}
	Given two vectors $\bm a, \bm b$, let $\theta$ be the angle between the two vectors. Then, we define
	\[ \bm a \cdot \bm b = \abs{\bm a} \abs{\bm b} \cos \theta \]
	Note that if either of the vectors is zero, $\theta$ is undefined. However, the dot product is zero anyway here, so this is irrelevant.
\end{definition}
\begin{definition}
	Two vectors $\bm a$ and $\bm b$ are defined to be parallel (or orthogonal) if and only if $\bm a \cdot \bm b = 0$. This is denoted $\bm a \perp \bm b$. This is true in two cases:
	\begin{enumerate}
		\item $\cos \theta = 0 \iff \theta = \frac{\pi}{2} \mod \pi$, or
		\item $\bm a = 0$ or $\bm b = 0$.
	\end{enumerate}
	Therefore, the zero vector is perpendicular to all vectors.
\end{definition}
\begin{definition}
	We can decompose a vector $\bm b$ into components relative to $\bm a$:
	\[ \bm b = \bm b_\parallel + \bm b_\perp \]
	where $\bm b_\parallel$ is the component of $\bm b$ parallel to $\bm a$, and $\bm b_\perp$ is the component of $\bm b$ perpendicular to $\bm a$. In particular, we have that
	\[ \bm a \cdot \bm b = \bm a \cdot \bm b_\parallel \]
\end{definition}

\subsection{Vector Product}
\begin{definition}
	Given two vectors $\bm a, \bm b$, let $\theta$ be the angle between the two vectors measured with respect to an arbitrary normal $\bm{\hat n}$. Then, we define
	\[ \bm a \wedge \bm b = \bm a \times \bm b = \abs{\bm a} \abs{\bm b} \bm{\hat n} \sin \theta \]
	Note that by swapping the sign of $\bm{\hat n}$, $\theta$ changes to $2 \pi - \theta$, leaving the result unchanged. There are two degenerate cases:
	\begin{itemize}
		\item $\theta$ is undefined if $\bm a$ or $\bm b$ is the zero vector, but the result is zero anyway because we multiply by the magnitudes of both vectors.
		\item $\bm{\hat n}$ is undefined if $\bm a \parallel \bm b$, but here $\sin \theta = 0$ so the result is zero anyway.
	\end{itemize}
\end{definition}
We can provide several useful interpretations of the cross product:
\begin{itemize}
	\item The magnitude of $\bm a \times \bm b$ is the vector area of the parallelogram defined by the points $\bm 0, \bm a, \bm a + \bm b, \bm b$.
	\item By fixing a vector $\bm a$, we can consider the plane perpendicular to it. If $\bm x$ is another vector in the plane, $\bm x \mapsto \bm a \times \bm x$ rotates $\bm x$ by $\frac{\pi}{2}$ in the plane, scaling it by the magnitude of $\bm a$.
\end{itemize}
Note that by resolving a vector $\bm b$ perpendicular to another vector $\bm a$, we have that
\[ \bm a \times \bm b = \bm a \times \bm b_\perp \]
A final useful property of the cross product is that since the result is perpendicular to both input vectors, we have
\[ \bm a \cdot (\bm a \times \bm b) = \bm b \cdot(\bm a \times \bm b) = 0 \]

\section{Orthonormal Bases}
\subsection{Basis Vectors}
To represent vectors as some collection of numbers, we can choose some basis vectors $\bm e_1, \bm e_2, \bm e_3$ which are `orthonormal', i.e. they are unit vectors and pairwise orthogonal. Note that
\[ \bm e_i \cdot \bm e_j = \begin{cases}
		1 & \text{if } i = j \\
		0 & \text{otherwise}
	\end{cases} \]
The set $\{ \bm e_1, \bm e_2, \bm e_3 \}$ is called a basis because any vector can be written uniquely as a linear combination of the basis vectors. Because we have orthonormal basis vectors, we can reduce this to
\[ \bm a = \sum_i \bm a_i \bm e_i \implies \bm a_i = \bm e_i \cdot \bm a \]
By representing a vector as a linear combination of basis vectors, it is very easy to evaluate the scalar product algebraically. To calculate the vector product, we first need to define whether $\bm e_1 \times \bm e_2 = \bm e_3$ or $-\bm e_3$. By convention, we assume that the basis vectors are right-handed, i.e. $\bm e_1 \times \bm e_2 = \bm e_3$. Then we can calculate the formula for the cross product in terms of the vectors' components.

\subsection{Scalar Triple Product}
The scalar triple product is the scalar product of one vector with the cross product of two more.
\[ \bm a \cdot (\bm b \times \bm c) = \bm b \cdot (\bm c \times \bm a) = \bm c \cdot (\bm a \times \bm b) = [\bm a, \bm b, \bm c] \]
The result of the scalar triple product is the signed volume of the parallelepiped starting at the origin with axes $\bm a$, $\bm b$, $\bm c$. We can represent this triple product as the determinant of a matrix:
\[
	\bm a \cdot (\bm b \times \bm c) =
	\begin{vmatrix}
		\bm a_1 & \bm a_2 & \bm a_3 \\
		\bm b_1 & \bm b_2 & \bm b_3 \\
		\bm c_1 & \bm c_2 & \bm c_3
	\end{vmatrix}
\]
If the scalar triple product is greater than zero, then $\bm a, \bm b, \bm c$ is called a right handed set. If it is equal to zero, then the vectors are all coplanar: $\bm c \in \vecspan \{ \bm a, \bm b \}$.

\subsection{Vector Triple Product}
The vector triple product is the cross product of three vectors. Note that this is non-associative. The proof is covered in the subsequent lecture.
\[ \bm a \times (\bm b \times \bm c) = (\bm a \cdot \bm c) \bm b - (\bm a\cdot \bm b) \bm c \]
\[ (\bm a \times \bm b) \times \bm c = (\bm a \cdot \bm c) \bm b - (\bm b\cdot \bm c) \bm a \]

\subsection{Lines}
A line through $\bm a$ parallel to $\bm u$ is defined by
\[ \bm r = \bm a + \lambda \bm u \]
where $\lambda$ is some real parameter. We can eliminate lambda by using the cross product with $\bm u$. This will allow us to get a $\bm u \times \bm u$ term which will cancel to zero.
\[ \bm u \times \bm r = \bm u \times \bm a \]
Informally, this is saying that $\bm r$ and $\bm a$ have the same components perpendicular to $\bm u$. Note that we can also reverse this process. Consider the equation
\[ \bm u \times \bm r = \bm c \]
By using the dot product with $\bm u$ we can say
\[ \bm u \cdot (\bm u \times \bm r) = \bm u \cdot \bm c \]
If $\bm u \cdot \bm c \neq 0$ then the equation is inconsistent. Otherwise, we can suppose that maybe $\bm r = \bm u \times \bm c$ and use the formula for the vector product to get the left hand side to be $\bm u \times (\bm u \times \bm c) = -\abs{\bm u}^2 \bm c$. Therefore, by inspection, $\bm a = -\frac{1}{\abs{\bm u}^2}(\bm u \times \bm c)$ is a solution. Now, note that we can add any multiple of $\bm u$ to $\bm a$ and it remains a solution. So the general solution is $\bm r = \bm a + \lambda\bm u$.

\subsection{Planes}
The general point on a plane that passes through $\bm a$ and has directions $\bm u$ and $\bm v$ is
\[ \bm r = \bm a + \lambda \bm u + \mu \bm v \]
where $\bm u$ and $\bm v$ are not parallel, and $\lambda$ and $\mu$ are real parameters. We can do a dot product with $\bm n = (\bm u \times \bm v)$ to eliminate both parameters.
\[ \bm n \cdot \bm r = \kappa \]
where $\kappa = \bm n \cdot \bm a$. Note that $\abs{\kappa}/\abs{\bm n}$ is the perpendicular distance from the origin to the plane.

\subsection{Other Vector Equations}
The equation of a sphere is given by a quadratic vector equation in $\bm r$.
\[ \bm r^2 + \bm r \cdot \bm a = k \]
We can complete the square to give
\[ \left(\bm r + \frac 1 2 \bm a \right)^2 = \frac 1 4 \bm a^2 + k \]
which is clearly a sphere with centre $-\frac 1 2 \bm a$ and radius $\left( \frac 1 4 \bm a^2 + k \right)^{1/2}$.

Another example of a vector equation is
\[ \bm r + \bm a \times (\bm b \times \bm r) = \bm c \tag{1} \]
where $\bm a, \bm b, \bm c$ are fixed. We can dot with $\bm a$ to eliminate the second term:
\[ \bm a \cdot \bm r = \bm a \cdot \bm c \tag{2} \]
Note that using the dot product loses information --- this is simply a tool to make deductions; (2) does not contain the full information of (1). Combining (1) and (2), and using the formula for the vector triple product, we get
\begin{align*}
	\bm r + (\bm a \cdot \bm r) \bm b - (\bm a \cdot \bm b) \bm r          & = \bm c \tag{3} \\
	\implies \bm r + (\bm a \cdot \bm c) \bm b - (\bm a \cdot \bm b) \bm r & = \bm c
\end{align*}
This eliminates the dependency on $\bm r$ inside the dot product. Now, we can factorise, leaving
\[ (1 - \bm a \cdot \bm b) \bm r = \bm c - (\bm a \cdot \bm c) \bm b \tag{4} \]
If $1 - \bm a \cdot \bm b \neq 0$ then $\bm r$ has a single solution, a point. Otherwise, the right hand side must also be zero (otherwise the equation is inconsistent). Therefore, $\bm c - (\bm a \cdot \bm c)\bm b = \bm 0$. We can now combine this expression for $\bm c$ into (3), eliminating the $(1- \bm a \cdot \bm b)$ term, to get
\[ (\bm a \cdot \bm r - \bm a \cdot \bm c) \bm b = \bm 0 \]
This shows us that (given that $\bm b$ is non-zero) the solutions to the equation are given by (2), which is the equation of a plane.

\section{Index Notation and the Summation Convention}
\subsection{Kronecker $\delta$}
The Kronecker $\delta$ is defined by
\[ \delta_{ij} = \begin{cases}
		1 & \text{if } i = j    \\
		0 & \text{if } i \neq j
	\end{cases} \]
Then $\bm e_i \bm e_j = \delta_{ij}$. We can also use $\delta$ to rewrite indices: $\sum_i \delta_{ij} \bm a_i = \bm a_j$. So
\begin{align*}
	\bm a \cdot \bm b & = \left( \sum_i \bm a_i \bm e_i \right) \cdot \left( \sum_j \bm b_j \bm e_j \right) \\
	                  & = \sum_{ij} \bm a_i \bm b_j (\bm e_i \cdot \bm e_j)                                 \\
	                  & = \sum_{ij} \bm a_i \bm b_j \delta_{ij}                                             \\
	                  & = \sum_i \bm a_i \bm b_i
\end{align*}

\subsection{Levi-Civita $\varepsilon$}
The Levi-Civita $\varepsilon$ is defined by
\[
	\varepsilon_{ijk} = \begin{cases}
		+1 & \text{if } ijk \text{ is an even permutation of } [1, 2, 3] \\
		-1 & \text{if } ijk \text{ is an odd permutation of } [1, 2, 3]  \\
		0  & \text{otherwise}
	\end{cases}
\]
Then
\begin{align*}
	\varepsilon_{123} = \varepsilon_{231} = \varepsilon_{312} & = +1 \\
	\varepsilon_{132} = \varepsilon_{321} = \varepsilon_{213} & = -1
\end{align*}
and all other permutations of $[1, 2, 3]$ yield 0. This shows that $\varepsilon$ is totally antisymmetric; exchanging any pair of indices changes the sign. We now have:
\begin{align*}
	\bm e_i \times \bm e_j & = \sum_k \varepsilon_{ijk} \bm e_k                                                   \\
	\intertext{And:}
	\bm a \times \bm b     & = \left( \sum_i \bm a_i \bm e_i \right) \times \left( \sum_j \bm b_j \bm e_j \right) \\
	\bm a \times \bm b     & = \sum_{ij} \bm a_i \bm b_j \left( \bm e_i \times \bm e_j \right)                    \\
	\bm a \times \bm b     & = \sum_{ijk} \bm a_i \bm b_j \varepsilon_{ijk} \bm e_k
\end{align*}
So the individual terms of the cross product can be written
\[ (\bm a \times \bm b)_k = \sum_{ij} \bm a_i \bm b_j \varepsilon_{ijk} \]

\subsection{Summation Convention}
We use the `summation convention' to abbreviate the many summation symbols used throughout linear algebra.
\begin{enumerate}
	\item An index which occurs exactly once in some term, denoted a `free index', must appear once in every term in that equation.
	\item An index which occurs exactly twice in a given term, denoted a `repeated/contracted/dummy index', is implicitly summed over.
	\item No index can occur more than twice in a given term.
\end{enumerate}

\subsection{$\varepsilon\varepsilon$ Identities}
The most general $\varepsilon\varepsilon$ identity is as follows:
\begin{align*}
	\varepsilon_{ijk} \varepsilon_{pqr}
	 & = \delta_{ip}\delta_{jq}\delta_{kr} - \delta_{jp}\delta_{iq}\delta_{kr} \\
	 & + \delta_{jp}\delta_{kq}\delta_{ir} - \delta_{kp}\delta_{jq}\delta_{ir} \\
	 & = \delta_{kp}\delta_{iq}\delta_{jr} - \delta_{ip}\delta_{kq}\delta_{jr}
\end{align*}
This is, however, very verbose and not used often throughout the course. It is provable by noting the total antisymmetry in $i,j,k$ and $p,q,r$ on both sides of the equation implies that both sides agree up to a constant factor. We can check that this factor is 1 by substituting in values such as $i=p=1$, $j=q=2$ and $k=r=3$.

The next most generic form is a very useful identity.
\[ \varepsilon_{ijk}\varepsilon_{pqk} = \delta_{ip}\delta_{jq} - \delta_{iq}\delta_{jp} \]
This is essentially the first line of the above identity, noting that $k=r$. We can prove this is true by observing the antisymmetry, and that both sides vanish under $i=j$ or $p=q$. So it suffices to check two cases: $i=p, j=q$ and $i=q, j=p$.

We can now continue making more indices equal to each other to get even more specific identities:
\[ \varepsilon_{ijk}\varepsilon_{pjk} = 2\delta_{ip} \]
This is easy to prove by noting that $\delta_{jj} = \sum_j \delta_{jj} = 3$, and using the $\delta$ rewrite rule.

Finally, we have
\[ \varepsilon_{ijk}\varepsilon_{ijk} = 6 \]
No indices are free here, so the values of $i, j, k$ themselves are predetermined by the fact that we are in three-dimensional space.

\subsection{Vector Triple Product Identity}
Using the summation convention (as will now be implied for the remainder of the course), we can prove
\begin{align*}
	\left[ \bm a \times (\bm b \times \bm c) \right]_i
	 & = \varepsilon_{ijk} \bm a_j (\bm b \times \bm c)_k                                                  \\
	 & = \varepsilon_{ijk} \bm a_j \varepsilon_{pqk} \bm b_p \bm c_q                                       \\
	 & = \varepsilon_{ijk}\varepsilon_{pqk} \bm a_j \bm b_p \bm c_q                                        \\
	 & = (\delta_{ip}\delta_{jq})\bm a_j \bm b_p \bm c_q - (\delta_{iq}\delta_{jp})\bm a_j \bm b_p \bm c_q \\
	 & = (\bm a \cdot \bm c) \bm b_i - (\bm a \cdot \bm b) \bm c_i
\end{align*}

\section{Vectors in $\mathbb R^n$}
\subsection{Definitions}
We define multidimensional real space as follows:
\[ \mathbb R^n = \{ \bm x = (x_1, x_2, \cdots, x_n) : x_i \in \mathbb R \} \]
We can define addition and scalar multiplication by mapping these operations over each term in the tuple. Therefore, we have a notion of linear combinations of vectors and hence a concept of parallel vectors. We can say, like before in $\mathbb R^3$, that $\bm x \parallel \bm y$ if and only if $\bm x = \lambda \bm y$ or $\bm y = \lambda \bm x$.

\subsection{Inner Product}
We define an operator analogous to the scalar product in $\mathbb R^3$. The inner product is defined as $x \cdot y = x_i y_i$. Directly from this definition, we can deduce some properties:
\begin{itemize}
	\item (symmetric) $\bm x \cdot \bm y = \bm y \cdot \bm x$
	\item (bilinear) $(\lambda \bm x + \lambda'\bm x')\cdot \bm y = \lambda \bm x\cdot \bm y + \lambda' \bm x' \cdot \bm y$
	\item (positive definite) $\bm x \cdot \bm x \geq 0$, and the equality holds if and only if $\bm x = \bm 0$.
\end{itemize}

\subsection{Norm}
We can define the norm of a vector (similar to the concept of length in three-dimension space), denoted $\abs {\bm x}$, by $\abs{\bm x}^2 = \bm x \cdot \bm x$. We can now define orthogonality as follows: $\bm x \perp \bm y \iff \bm x \cdot \bm y = 0$.

\subsection{Basis Vectors}
We define the standard basis vectors $\bm e_1, \bm e_2, \cdots \bm e_n$ by setting each element of the tuple $\bm e_i$ to zero apart from the $i$th element, which is set to one. Also, we redefine the Kronecker $\delta$ to be valid in higher-dimensional space. Note that under this definition, the standard basis vectors are orthonormal because $\bm e_i \cdot \bm e_j = \delta_{ij}$.

\subsection{Cauchy-Schwarz Inequality}
\begin{proposition}
	For vectors $\bm x, \bm y$ in $\mathbb R^n$, $\abs{\bm x \cdot \bm y} \leq \abs{\bm x} \abs{\bm y}$, where the equality is true if and only if $\bm x \parallel \bm y$.
\end{proposition}
\begin{proof}
	If $\bm y = \bm 0$, then the result is immediate. So suppose that $\bm y \neq 0$, then for some $\lambda \in \mathbb R$, we have
	\begin{align*}
		\abs{\bm x - \lambda \bm y}^2 & =
		(\bm x - \lambda \bm y) \cdot (\bm x - \lambda \bm y)                                                          \\
		                              & = \abs{\bm x}^2 - 2 \lambda \bm x \cdot \bm y + \lambda^2 \abs{\bm y}^2 \geq 0
	\end{align*}
	As this is a positive real quadratic in $\lambda$ that is always greater than zero, it has at most one real root. Therefore the discriminant is less than or equal to zero.
	\[ (-2 \bm x \cdot \bm y)^2 - 4 \abs{\bm x}^2\abs{\bm y}^2 \leq 0
		\implies \abs{\bm x \cdot \bm y} \leq \abs{\bm x}\abs{\bm y} \]
	where the equality only holds if $\bm x$ and $\bm y$ are parallel (i.e. when $\bm x - \lambda \bm y$ equals zero for some $\lambda$).
\end{proof}

\subsection{Triangle Inequality}
Following from the Cauchy-Schwarz inequality,
\begin{align*}
	\abs{\bm x + \bm y}^2
	 & = \abs{\bm x}^2 + 2(\bm x \cdot \bm y) + \abs{\bm y}^2        \\
	 & \leq \abs{\bm x}^2 + 2 \abs{\bm x}\abs{\bm y} + \abs{\bm y}^2 \\
	 & = \left(\abs{\bm x} + \abs{\bm y}\right)^2
\end{align*}
where the equality holds under the same conditions as above.

\subsection{Levi-Civita $\varepsilon$ in $\mathbb R^n$}
Note that the Levi-Civita $\varepsilon$ has three indices in $\mathbb R^3$. We can extend this $\varepsilon$ to higher and lower dimensions by increasing or reducing the amount of indices. It does not make logical sense to use the same $\varepsilon$ without changing the amount of indices to define, for example, a vector product in four-dimensional space, since we would have unused indices. The expression $(\bm x \times \bm y)_k = \varepsilon_{ijk} \bm a_i \bm b_j$ works because there is one free index, $k$, on the right hand side, so we can use this to calculate the values of each element of the result.

We can, however, use this $\varepsilon$ to extend the notion of a scalar triple product to other dimensions, for example two-dimensional space, with $[\bm a, \bm b] := \varepsilon_{ij} \bm a_i \bm b_j$. This is the signed area of the parallelogram spanning $\bm a$ and $\bm b$.

\subsection{Vector Spaces}
Vector spaces are not studied axiomatically in this course, but the axioms are given here for completeness. A real (as in, $\mathbb R$) vector space $V$ is a set of objects with two operators $+: V \times V \to V$ and $\cdot: \mathbb R \times V \to V$ such that
\begin{itemize}
	\item $(V, +)$ is an abelian group
	\item $\lambda(v + w) = \lambda v + \lambda w$
	\item $(\lambda + \mu)v = \lambda v + \mu v$
	\item $\lambda(\mu v) = (\lambda \mu) v$
	\item $1v = v$ (to exclude trivial cases for example $\lambda v = 0$ for all $v$)
\end{itemize}

\section{Subspaces}
\subsection{Definitions}
A subspace of a real vector space $V$ is a subset $U \subseteq V$ that is a vector space. Equivalently, if all pairs of vectors $v, w \in U$ satisfy $\lambda v + \mu w \in U$. then $U$ is a subspace of $V$. Note that the span generated from a set of vectors is a subspace, as it is characterised by this equivalent definition. Also, note that the origin must be part of any subspace, because multiplying a vector by zero must yield the origin.

\subsection{Linear Dependence}
In some real vector space $V$, let $\bm v_1, \bm v_2 \cdots \bm v_r$ be vectors in $V$. Now consider the linear relation
\[ \lambda_1 \bm v_1 + \lambda_2 \bm v_2 + \cdots + \lambda_r \bm v_r = 0 \]
Then we call the set of vectors a linearly independent set if the only solution is where all $\lambda$ values are zero. Otherwise, it is a linearly dependent set.

\subsection{Inner Product Spaces}
An inner product is an extra structure that we can have on a real vector space $V$, which is often denoted by angle brackets or parentheses. It can also be characterised by axioms (specifically the ones in Section 6.2). Features like the norm of a vector, and theorems like the Cauchy-Schwarz inequality, follow from these axioms.

For example, let us consider the vector space
\[ V = \{ f: [0, 1] \to \mathbb R: f \text{ smooth}; f(0) = f(1) = 0 \} \]
We can define the inner product to be
\[ f \cdot g = \langle f, g \rangle = \int_0^1 f(x)g(x)\dd x \]
Then by the Cauchy-Schwarz inequality, we have
\begin{align*}
	\abs{\langle f, g \rangle}              & \leq \norm{f} \cdot \norm{g}                                  \\
	\therefore \abs{\int_0^1 f(x)g(x)\dd x} & \leq \sqrt{\int_0^1 f(x)^2 \dd x}\sqrt{\int_0^1 g(x)^2 \dd x}
\end{align*}

\begin{lemma}
	In any real inner product space $V$, if $\bm v_1 \cdots v_r \neq \bm 0$ are orthogonal, they are linearly independent.
\end{lemma}
\begin{proof}
	If $\sum_i \alpha_i \bm v_i = 0$, then
	\begin{align*}
		\left\langle \bm v_j, \sum_i \alpha_i \bm v_i \right\rangle     & = 0 \\
		\intertext{And because each vector that is not $\bm v_j$ is orthogonal to it, those terms cancel, leaving}
		\therefore \left\langle \bm v_j, \alpha_j \bm v_j \right\rangle & = 0 \\
		\alpha_j \left\langle \bm v_j, \bm v_j \right\rangle            & = 0 \\
		\alpha_j = 0
	\end{align*}
	So they are linearly independent.
\end{proof}

\subsection{Bases and Dimensions}
In a vector space $V$, a basis is a set $\mathcal B = \{ \bm e_1 \cdots \bm e_n \}$ such that
\begin{itemize}
	\item $\mathcal B$ spans $V$; and
	\item $\mathcal B$ is linearly independent, which implies that the coefficients on these basis vectors are unique for any vector in $V$, since it is impossible to write one vector in terms of the others
\end{itemize}

\begin{theorem}
	If $\{\bm e_1 \cdots \bm e_n \}$ and $\{ \bm f_1 \cdots \bm f_m \}$ are bases for a real vector space $V$, then $n=m$, which we call the dimension of $V$.
\end{theorem}
\begin{proof}
	This proof is non-examinable (without prompts). We can write each basis vector in terms of the others, since they all span the same vector space. Thus:
	\[ \bm f_a = \sum_i A_{ai} \bm e_i;\quad \bm e_i = \sum_a B_{ia} \bm f_a \]
	Note that indices $i,j$ span from 1 to $n$, while $a,b$ span from 1 to $m$. We can substitute one expression into the other, forming:
	\begin{align*}
		\bm f_a & = \sum_i A_{ai} \left( \sum_b B_{ib}\bm f_b \right)  \\
		\bm f_a & = \sum_b \left( \sum_i A_{ai} B_{ib} \right) \bm f_b
	\end{align*}
	Note that we have now written $\bm f_a$ as a linear combination of $\bm f_b$ for all valid $b$. But since they are linearly independent, the coefficient of $\bm f_b$ must be zero if $a \neq b$, and one of $a = b$. Therefore, we have
	\[ \delta_{ab} = \sum_i A_{ai} B_{ib} \]
	We can make a similar statement about $\bm e_i$:
	\[ \delta_{ij} = \sum_a B_{ia} A_{aj} = \sum_a A_{aj} B_{ia} \]
	Now, assigning $a=b$ and $i=j$, summing over both, and substituting into our two previous expressions for $\delta$, we have:
	\begin{alignat}{2}
		\sum_{ia} A_{ai} B_{ia} & = \sum_a \delta_{aa} &  & = \sum_i \delta_{ii} \\
		                        & = m                  &  & = n
	\end{alignat}
\end{proof}

\section{Choosing Bases and $\mathbb C^n$}
\subsection{Choosing Bases}
Note that $\{ \bm 0 \}$ is a trivial subspace of all vector spaces, and it has dimension zero since it requires a linear combination of no vectors.

\begin{proposition}
	Let $V$ be a vector space with finite subsets $Y = \{ \bm w_1, \cdots, \bm w_m \}$ that spans $V$, and $X = \{ \bm u_1, \cdots, \bm u_k \}$ that is linearly independent. Let $n = \dim V$. Then:
	\begin{enumerate}[(i)]
		\item A basis can be found as a subset of $Y$ by discarding vectors in $Y$ as necessary, and that $n \leq m$.
		\item $X$ can be extended to a basis by adding in additional vectors from $Y$ as necessary, and that $k \leq n$.
	\end{enumerate}
\end{proposition}
\begin{proof}
	This proof is non-examinable (without prompts).
	\begin{enumerate}[(i)]
		\item If $Y$ is linearly independent, then $Y$ is a basis and $m = n$. Otherwise, $Y$ is not linearly independent. So there exists some linear relation
		      \[ \sum_{i=1}^{m} \lambda_i \bm w_i = \bm 0 \]
		      where there is some $i$ such that $\lambda_i \neq 0$. Without loss of generality (because the order of elements in $Y$ does not matter) we will reorder $Y$ such that $\bm w_m \neq 0$. So we have
		      \[ \bm w_m = \frac{-1}{\lambda_m} \sum_{i=1}^{m-1} \lambda_i \bm w_i \]
		      So $\vecspan Y = \vecspan (Y \setminus \{ \bm w_m \})$. We can repeat this process of eliminating vectors from $Y$ until linear independence is achieved. We know that this process will end because $Y$ is a finite set. Clearly, in this case, $n < m$. So for all cases, $n \leq m$.

		\item If $X$ spans $V$, then $X$ is a basis and $k=n$. Else, there exists some $u_{k+1} \in V$ that is not in the span of $X$. Then, we will construct an arbitrary linear relation
		      \[ \sum_{i=1}^{k+1} \mu_i \bm u_i = \bm 0 \]
		      Note that this implies that $\mu_{k+1} = \bm 0$ because it is not in the span of $X$, and that $\mu_i = 0$ for all $i \leq k$ because the original $X$ was linearly independent. So we know that all the coefficients are zero, and therefore $X \cup \{ u_{k+1} \}$ is linearly independent.

		      Note that we can always choose this $u_{k+1}$ to be an element of $Y$ because we just need to ensure that $u_{k+1} \notin \vecspan X$. Suppose we cannot choose such a vector in $Y$. Then $Y \subseteq \vecspan X \implies \vecspan Y \subseteq \vecspan X \implies \vecspan X = V$, which is clearly false because $X$ does not span $V$. This is a contradiction, so we can always choose such a vector from $Y$. We can repeat this process of taking vectors from $Y$ and adding them to $X$ until we have a basis. This process will always terminate in a finite amount of steps because we are taking new vectors from a finite set $Y$. Therefore $k \leq n$, as we are adding vectors (increasing $k$) until $k=n$.
	\end{enumerate}
\end{proof}

\subsection{Infinite Dimensions}
It is perfectly possible to have a vector space that has infinite dimensionality. However, they will be rarely touched upon in this course apart from specific examples, like the following example. Let $V = \{ f: [0, 1] \to \mathbb R: f \text{ smooth}, f(0) = f(1) = 0\}$. Then let $S_n(x) = \sqrt 2 \sin(n \pi x)$ where $n$ is a natural number $1, 2, \cdots$. Clearly, $S_n \in V$ for all $n$. The inner product of two of these $S$ functions is given by
\begin{align*}
	\langle S_n, S_m \rangle & = 2 \int_0^1 \sin(n \pi x) \sin(m \pi x) \dd x \\
	                         & = \delta_{mn}
\end{align*}
So $S_n$ are orthonormal and therefore linearly independent. So we can continue adding more vectors until it becomes a basis. However, the set of all $S_n$ is already infinite --- so $V$ must have infinite dimensionality.

\subsection{Vectors in $\mathbb C^n$}
We define $\mathbb C^n$ by
\[ \mathbb C^n := \{ \bm z = (z_1, z_2, \cdots, z_n): \forall i, z_i \in \mathbb C \} \]
We define addition and scalar multiplication in obvious ways. Note that we have a choice over what the scalars are allowed to be. If we only allow scalars that are real numbers, $\mathbb C^n$ can be considered a real vector space with bases $(0, \cdots, 1, \cdots, 0)$ and $(0, \cdots, i, \cdots, 0)$ and dimension $2n$. Alternatively, if we let the scalars be any complex numbers, we don't need to have imaginary bases, thus giving us a complex vector space with bases $(0, \cdots, 1, \cdots, 0)$ and dimension $n$. We can say that $\mathbb C^n$ has dimension $2n$ over $\mathbb R$, and dimension $n$ over $\mathbb C$. From here on, unless stated otherwise, we treat $\mathbb C^n$ to be a complex vector space.

\subsection{Inner Product in $\mathbb C^n$}
We can define the inner product by
\[ \langle \bm z, \bm w \rangle := \sum_j \overline{z_j} w_j \]
The conjugate over the $z$ terms ensures that the inner product is positive definite. It has these properties, analogous to the properties of the inner product in the real vector space $\mathbb R^n$:
\begin{itemize}
	\item (Hermitian) $\langle \bm z, \bm w \rangle = \overline{\langle \bm w, \bm z \rangle}$
	\item (linear/antilinear) $\langle \bm z, \lambda \bm w + \lambda' \bm w' \rangle = \lambda \langle \bm z, \bm w \rangle + \lambda' \langle \bm z, \bm w' \rangle$ and $\langle \lambda \bm z + \lambda' \bm z', w \rangle = \overline{\lambda} \langle \bm z, \bm w \rangle + \overline{\lambda'} \langle \bm z', \bm w \rangle$
	\item (positive definite) $\langle \bm z, \bm z \rangle = \sum_j \abs{z_j}^2$ which is real and greater than or equal to zero, where the equality holds if and only if $\bm z = \bm 0$.
\end{itemize}
We can also define the norm of $\bm z$ to satisfy $\abs{\bm z} \geq 0$ and $\abs{\bm z}^2 = \langle \bm z, \bm z \rangle$. Note that the standard basis for $\mathbb C^n$ is orthonormal, since the inner product of any two basis vectors $\bm e_j$ and $\bm e_k$ is given by $\delta_{jk}$.

\subsection{Inner Product in Complex Plane}
Here is an example of the use of the complex inner product on $\mathbb C^1 = \mathbb C$. Note first that $\langle z, w \rangle = \overline z w$. Let $z = a_1 + ia_2$ and $w = b_1 + ib_2$ where $a_1, a_2, b_1, b_2 \in \mathbb R$. Then
\begin{align*}
	\langle z, w \rangle & = \overline z w                              \\
	                     & = (a_1 b_1 + a_2 b_2) + i(a_1 b_2 - a_2 b_1) \\
	                     & = (z \cdot w) + i[z, w]
\end{align*}
We can therefore use the inner product to compute two different scalar products at the same time.

\section{Linear Maps}
\subsection{Introduction}
A linear map (or linear transformation) is some operation $T: V \to W$ between vector spaces $V$ and $W$ preserving the core vector space structure (specifically, the linearity). It is defined such that
\[ T\left(\lambda \bm x + \mu \bm y\right) = \lambda T(\bm x) + \mu T(\bm y) \]
for all $\bm x, \bm y \in V$ where the scalars $\lambda$ and $\mu$ match up with the scalar field that $V$ and $W$ use (so this could be $\mathbb R$ or $\mathbb C$ in our examples). Much of the language used for linear maps between vector spaces is analogous to the language used for homomorphisms between groups.

Note that a linear map is completely determined by its action on a basis $\{ \bm e_1, \cdots, \bm e_n \}$ where $n = \dim V$, since
\[ T\left(\sum_i x_i \bm e_i \right) = \sum_i x_i T(\bm e_i) \]
We denote $\bm x' = T(\bm x) \in W$, and define $\bm x'$ as the image of $x$ under $T$. Further, we define
\[ \Im (T) = \{ \bm x' \in W : \bm x' =T(\bm x) \text{ for some } \bm x \in V \} \]
to be the image of $T$, and we define
\[ \ker (T) = \{ \bm x \in V : T(\bm x) = \bm 0 \} \]
to be the kernel of $T$.

\begin{lemma}
	$\ker T$ is a subspace of $V$, and $\Im T$ is a subspace of $W$.
\end{lemma}
\begin{proof}
	To verify that some subset is a subspace, it suffices to check that it is non-empty, and that it is closed under linear combinations.

	$\ker T$ is non-empty because $\bm 0 \in \ker T$. For $\bm x, \bm y \in \ker T$, we have $T(\lambda \bm x + \mu \bm y) = \lambda T(\bm x) + \mu T(\bm y) = \bm 0 \in \ker T$ as required.

	$\Im T$ is non-empty because $\bm 0 \in \Im T$. For $\bm x, \bm y \in V$, let $\bm x' = T(\bm x)$ and $\bm y' = T(\bm y)$, therefore $\bm x', \bm y' \in \Im T$. Now, $\lambda \bm x' + \mu \bm y' = T(\lambda \bm x + \mu \bm y)$ so it is closed under linear combinations as required.
\end{proof}
Here are some examples of images and kernels.
\begin{enumerate}[(i)]
	\item The zero linear map $\bm x \mapsto \bm 0$ has:
	      \begin{align*}
		      \Im T  & = \{ \bm 0 \} \\
		      \ker T & = V
	      \end{align*}
	\item The identity linear map $\bm x \mapsto \bm x$ has:
	      \begin{align*}
		      \Im T  & = V           \\
		      \ker T & = \{ \bm 0 \}
	      \end{align*}
	\item Let $T: \mathbb R^3 \to \mathbb R^3$, such that
	      \begin{align*}
		      x_1' & = 3x_1 - x_2 + 5x_3 \\
		      x_2' & = -x_1 - 2x_3       \\
		      x_3' & = 2x_1 + x_2 + 3x+3
	      \end{align*}
	      This map has
	      \begin{align*}
		      \Im T  & = \left\{ \lambda \begin{pmatrix} 3 \\ -1 \\ 2 \end{pmatrix} + \mu \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix} : \lambda, \mu \in \mathbb R \right\} \\
		      \ker T & = \left\{ \lambda \begin{pmatrix} 2 \\ -1 \\ -1 \end{pmatrix} : \lambda \in \mathbb R \right\}
	      \end{align*}
\end{enumerate}

\subsection{Rank and Nullity}
We define the rank of a linear map to be the dimension of its image, and the nullity of a linear map to be the dimension of its kernel.
\[ \rank T = \dim \Im T; \quad \nullity T = \dim \ker T \]
Note that therefore for $T: V \to W$, we have $\rank T \leq \dim W$ and $\ker T \leq \dim V$.
\begin{theorem}
	For some linear map $T: V \to W$,
	\[ \rank T + \nullity T = \dim V \]
\end{theorem}
\begin{proof}
	This proof is non-examinable (without prompts). Let $\bm e_1, \cdots, \bm e_k$ be a basis for $\ker T$, so $T(\bm e_i) = \bm 0$ for all valid $i$. We may extend this basis by adding more vectors $\bm e_i$ where $k < i \leq n$ until we have a basis for $V$, where $n=\dim V$. We claim that the set $\mathcal B = \{ T(\bm e_{k+1}), \cdots, T(\bm e_n) \}$ is a basis for $\Im T$. If this is true, then clearly the result follows because $k = \dim \ker T = \nullity T$ and $n-k = \dim \Im T = \rank T$.

	To prove the claim we need to show that $\mathcal B$ spans $\Im T$ and that it is a linearly independent set.
	\begin{itemize}
		\item $\mathcal B$ spans $\Im T$ because for any $\bm x = \sum_{i=1}^n x_i \bm e_i$, we have
		      \[ T(\bm x) = \sum_{i=k+1}^n x_i T(\bm e_i) \in \vecspan \mathcal B \]
		\item $\mathcal B$ is linearly independent. Consider a general linear combination of basis vectors:
		      \[ \sum_{i=k+1}^n \lambda_i T(\bm e_i) = 0 \implies T\left( \sum_{i=k+1}^n \lambda_i \bm e_i \right) = 0 \]
		      so
		      \[ \sum_{i=k+1}^n \lambda_i \bm e_i \in \ker T \]
		      Because this is in the kernel, it may be written in terms of the basis vectors of the kernel. So, we have
		      \[ \sum_{i=k+1}^n \lambda_i \bm e_i = \sum_{i=1}^k \mu_i \bm e_i \]
		      This is a linear relation in terms of all basis vectors of $V$. So all coefficients are zero.
	\end{itemize}
\end{proof}

\subsection{Rotations}
Linear maps are often used to describe geometrical transformations, such as rotations, reflections, projections, dilations and shears. A convenient way to express these maps is by describing where the basis vectors are mapped to. In $\mathbb R^2$, we may describe a rotation anticlockwise around the origin by angle $\theta$ with
\begin{align*}
	\bm e_1 & \mapsto \cos \theta \bm e_1 + \sin \theta \bm e_2  \\
	\bm e_2 & \mapsto -\sin \theta \bm e_1 + \cos \theta \bm e_2
\end{align*}
In $\mathbb R^3$ we can construct a similar transformation for a rotation around the $\bm e_3$ axis with
\begin{align*}
	\bm e_1 & \mapsto \cos \theta \bm e_1 + \sin \theta \bm e_2  \\
	\bm e_2 & \mapsto -\sin \theta \bm e_1 + \cos \theta \bm e_2 \\
	\bm e_3 & \mapsto \bm e_3
\end{align*}
We can extend this to a general rotation in $\mathbb R^3$ about an axis given by a unit normal vector $\hat {\bm n}$. For any vector $\bm x \in \mathbb R^3$ we can resolve parallel and perpendicular to $\nhat$ as follows.
\[ \bm x = \bm x_\parallel + \bm x_\perp;\quad \bm x_\parallel = (\bm x \cdot \nhat) \nhat;\quad \bm x_\perp = x - (\bm x \cdot \nhat) \nhat \]
Note that $\nhat$ resembles the $\bm e_3$ axis here, and $\bm x_\perp$ resembles the $\bm e_1$ axis. So we can compute the equivalent of $\bm e_2$ using the cross product, $\nhat \times \bm x_\perp = \nhat \times \bm x$. Now we may define the map with
\begin{align*}
	\bm x_\parallel & \mapsto \bm x_\parallel                                               \\
	\bm x_\perp     & \mapsto (\cos \theta) \bm x_\perp + (\sin \theta)(\nhat \times \bm x)
\end{align*}
So all together, we have
\[ \bm x \mapsto (\cos \theta) \bm x + (1 - \cos \theta) (\nhat \cdot \bm x)\nhat + (\sin \theta)(\nhat \times \bm x) \]

\subsection{Reflections and Projections}
For a plane with normal $\nhat$, we define a projection to be
\begin{align*}
	\bm x_\parallel & \mapsto \bm 0                                           \\
	\bm x_\perp     & \mapsto \bm x_\perp                                     \\
	\bm x           & \mapsto \bm x_\perp = \bm x - (\bm x \cdot \nhat) \nhat
\end{align*}
and a reflection to be
\begin{align*}
	\bm x_\parallel & \mapsto -\bm x_\parallel                                                   \\
	\bm x_\perp     & \mapsto \bm x_\perp                                                        \\
	\bm x           & \mapsto \bm x_\perp - \bm x_\parallel = \bm x - 2(\bm x \cdot \nhat) \nhat
\end{align*}
The same expressions also apply in $\mathbb R^2$, where we replace the plane with a line.

\subsection{Dilations}
Given scale factors $\alpha, \beta, \gamma > 0$, we define a dilation along the axes by
\begin{align*}
	\bm e_1 & \mapsto \alpha \bm e_1 \\
	\bm e_2 & \mapsto \beta \bm e_2  \\
	\bm e_3 & \mapsto \gamma \bm e_3
\end{align*}

\subsection{Shears}
Let $\bm a, \bm b$ be orthogonal unit vectors in $\mathbb R^3$, i.e. $\abs{\bm a} = \abs{\bm b} = \bm 0$ and $\bm a \cdot \bm b = 0$, and we define a real parameter $\lambda$. A shear is defined as
\begin{align*}
	\bm x & \mapsto \bm x' = \bm x + \lambda \bm a (\bm x \cdot \bm b) \\
	\bm a & \mapsto \bm a                                              \\
	\bm b & \mapsto \bm b + \lambda \bm a
\end{align*}
This definition holds equivalently in $\mathbb R^2$.

\section{Matrices as Linear Maps}
\subsection{Definitions}
Consider a linear map $T: \mathbb R^n \to \mathbb R^m$, with standard bases $\{ \bm e_i \} \in \mathbb R^n$, $\{ \bm f_a \}, \in \mathbb R^m$, and with $T(\bm x) = \bm x'$.
Let further

\[ \bm x = \sum_i x_i \bm e_i = \begin{pmatrix}
		x_1 \\ x_2 \\ \vdots \\ x_n
	\end{pmatrix};\quad x' = \sum_a x_a' \bm f_a = \begin{pmatrix}
		x_1' \\ x_2' \\ \vdots \\ x_m'
	\end{pmatrix} \]

Linearity implies that $T$ is fixed by specifying
\[ T(\bm e_i) = \bm e_i' = \bm C_i \in \mathbb R^m \]
We take these $\bm C$ as columns of an $m \times n$ array or matrix $M$, with rows denoted as $\bm R_a \in \mathbb R^n$.

\[ \begin{pmatrix}
		\uparrow   &        & \uparrow   \\
		\bm C_1    & \cdots & \bm C_n    \\
		\downarrow &        & \downarrow
	\end{pmatrix} = M = \begin{pmatrix}
		\leftarrow & \bm R_1 & \rightarrow \\
		           & \vdots  &             \\
		\leftarrow & \bm R_m & \rightarrow
	\end{pmatrix} \]

$M$ has entries $M_{ai} \in \mathbb R$, where $a$ labels rows and $i$ labels columns, so
\[ (\bm C_i)_a = M_{ai} = (\bm R_a)_i \]
The action of $T$ is then given by the matrix $M$ multiplying the vector $\bm x$ in the following way:
\[ \bm x' = M \bm x \]
defined by
\[ x_a' = M_{ai}x_i \]
or explicitly:
\[
	\begin{pmatrix}
		x_1' \\ x_2' \\ \vdots \\ x_m'
	\end{pmatrix}
	=
	\begin{pmatrix}
		M_{11} & M_{12} & \cdots & M_{1n} \\
		M_{21} & M_{22} & \cdots & M_{2n} \\
		\vdots & \vdots & \ddots & \vdots \\
		M_{m1} & M_{m2} & \cdots & M_{mn}
	\end{pmatrix}
	\begin{pmatrix}
		x_1 \\ x_2 \\ \vdots \\ x_n
	\end{pmatrix}
	=
	\begin{pmatrix}
		M_{11} x_1 + M_{12} x_2 + \cdots + M_{1n} x_n \\
		M_{21} x_1 + M_{22} x_2 + \cdots + M_{2n} x_n \\
		\vdots                                        \\
		M_{m1} x_1 + M_{m2} x_2 + \cdots + M_{mn} x_n
	\end{pmatrix}
\]
To check that the matrix multiplication above gives the action of $T$, we can plug in a generic value $\bm x$, and we get
\[ \bm x' = T\left(\sum_i x_i \bm e_i\right) = \sum_i x_i T(\bm e_i) = \sum_i x_i \bm C_i \]
and by taking component $a$ of the vector, we have
\[ x_a' = \sum_i x_i (\bm C_i)_a = \sum_i x_i M_{ai} \]
as required. Note also that
\[ x_a' = M_{ai}x_i = (\bm R_a)_i x_i = \bm R_a \cdot \bm x \]
We can now regard the properties of $T$ as properties of $M$ (suitably interpreted). For example:
\begin{itemize}
	\item $\Im(T) = \Im(M) = \vecspan \{ \bm C_1, \cdots, \bm C_n \}$. In words, the image of a matrix is the span of its columns.
	\item $\ker(T) = \ker(M) = \{ \bm x: \forall a, \bm R_a \cdot \bm x = 0 \}$. In some sense, the kernel of $M$ is the subspace perpendicular to all of its rows.
\end{itemize}

\subsection{Examples}
\begin{enumerate}[(i)]
	\item The zero map $\mathbb R^n \to \mathbb R^m$ corresponds to the zero matrix
	      \[ M = 0 \text{ with } M_{ai} = 0 \]
	\item The identity map $\mathbb R^n \to \mathbb R^n$ corresponds to the identity (or unit) matrix
	      \[ M = I \text{ with } I_{ij} = \delta_{ij} \]
	\item The map $\mathbb R^3 \to \mathbb R^3$ given by $\bm x' = T(\bm x) = M\bm x$ with
	      \[ M = \begin{pmatrix}
			      3  & 1 & 5  \\
			      -1 & 0 & -2 \\
			      2  & 1 & 3
		      \end{pmatrix} \]
	      gives
	      \[
		      \begin{pmatrix}
			      x_1' \\ x_2' \\ x_3'
		      \end{pmatrix}
		      =
		      \begin{pmatrix}
			      3x_1 + x_2 + 5x_3 \\
			      -x_1 - 2x_3       \\
			      2x_1 + x_2 + 3x_3
		      \end{pmatrix}
	      \]
	      In this case, we may read off the column vectors $\bm C_a$ from the matrix. Note that since they form a linearly dependent set, we have
	      \[ \Im(T) = \Im(M) = \vecspan \{ \bm C_1, \bm C_2, \bm C_3 \} = \vecspan \{ \bm C_1, \bm C_2 \} \]
	      Here, $\bm R_2 \times \bm R_3 = \begin{pmatrix}
			      2 & -1 & -1
		      \end{pmatrix} = \bm u$ is actually perpendicular to all rows as they form a linearly dependent set. So
	      \[ \ker(T) = \ker(M) = \{ \lambda \bm u \} \]
	\item A rotation through $\theta$ in $\mathbb R^2$ is given by (building from the images of the basis vectors):
	      \[ \begin{pmatrix}
			      \cos \theta & -\sin \theta \\
			      \sin \theta & \cos \theta
		      \end{pmatrix} \]
	\item A dilation $\bm x' = M \bm x$ with scale factors $\alpha, \beta, \gamma$ along axes in $\mathbb R^3$ is given by
	      \[ \begin{pmatrix}
			      \alpha & 0     & 0      \\
			      0      & \beta & 0      \\
			      0      & 0     & \gamma
		      \end{pmatrix} \]
	\item A reflection in a plane perpendicular to a unit vector $\nhat$ is given by a matrix $H$ that must have the property that
	      \begin{align*}
		      \bm x' & = H \bm x = \bm x - 2(\bm x - \nhat) \nhat \\
		      x_i'   & = x_i - 2x_jn_jn_i = H_{ij}x_j             \\
		      \intertext{And by comparing coefficients of $x_j$, and using $\delta$ to rewrite $x_i$ using the $j$ index, we have}
		      H_{ij} & = \delta_ij - 2n_in_j
	      \end{align*}
	      For example, with $\nhat = \frac{1}{\sqrt 3}\begin{pmatrix}
			      1 & 1 & 1
		      \end{pmatrix}$, then $n_in_j = \frac{1}{3}$ for all $i, j$, so
	      \[ H = \frac{1}{3}\begin{pmatrix}
			      1  & -2 & -2 \\
			      -2 & 1  & -2 \\
			      -2 & -2 & 1
		      \end{pmatrix} \]
	\item A shear is defined by a matrix $S$ such that
	      \[ \bm x' = S\bm x = \bm x + \lambda(\bm b \cdot \bm x)\bm a \]
	      where $\bm a$, $\bm b$ are unit vectors with $\bm a \perp \bm b$, and where $\lambda$ is a real scale factor. Therefore:
	      \begin{align*}
		      x_i'              & = x_i + \lambda b_j x_j a_i = S_{ij}x_j \\
		      \therefore S_{ij} & = \delta_{ij} + \lambda a_i b_j
	      \end{align*}
	      For example in $\mathbb R^2$ with $\bm a = \begin{pmatrix}
			      1 \\ 0
		      \end{pmatrix}$ and $\bm b = \begin{pmatrix}
			      0 \\ 1
		      \end{pmatrix}$, we have
	      \[ S = \begin{pmatrix}
			      1 & \lambda \\ 0 & 1
		      \end{pmatrix} \]
	\item A rotation matrix $R$ in $\mathbb R^3$ with axis $\nhat$ and angle $\theta$ must satisfy
	      \begin{align*}
		      \bm x'            & = R\bm x = (\cos \theta)\bm x + (1 - \cos \theta)(\nhat \cdot \bm x)\nhat + (\sin \theta)(\nhat \times \bm x) \\
		      x_i'              & = (\cos \theta)x_i + (1 - \cos \theta)n_j x_j n_i - (\sin \theta) \varepsilon_{ijk}x_j n_k = R_{ij} x_j       \\
		      \therefore R_{ij} & = \delta_{ij}(\cos \theta) - (1 - \cos \theta)n_in_j - (\sin \theta)\varepsilon_{ijk} n_k
	      \end{align*}
\end{enumerate}

\subsection{Matrix of a General Linear Map}
Consider a linear map $T: V \to W$ between general real or complex vector spaces of dimension $n, m$ respectively. We will choose bases $\{ \bm e_i \}$ for $V$ and $\{ \bm f_a \}$ for $W$. The matrix representing the linear map $T$ with respect to these bases is an $m \times n$ array with entries $M_{ai} \in \mathbb R$ or $\mathbb C$ as appropriate, defined by
\[ T(\bm e_i) = \sum_a \bm f_a M_{ai} \]
Then
\[ \bm x' = T(\bm x) \iff x_a' = \sum_i M_{ai}x_i = M_{ai}x_i \]
where
\[ \bm x = \sum_i x_i \bm e_i;\quad \bm x' = \sum_a x_a \bm f_a \]
Note therefore that (in real vector spaces) given choices of bases $\{ \bm e_i \}$ and $\{ \bm f_a \}$, $V$ is identified with $\mathbb R_n$ in the sense that any vector has $n$ real components, and that $W$ is identified with $R_m$ analogously, and that therefore $T$ is identified with an $m\times n$ real matrix $M$. Note further that entries in column $i$ of $M$ are components of $T(\bm e_i)$ with respect to basis $\{ \bm f_a \}$.

\section{Matrix Algebra}
\subsection{Linear Combinations}
If $T: V \to W$ and $S: V \to W$, between real or complex vector spaces $V, W$ of dimension $n, m$ respectively, are linear, then
\[ \alpha T + \beta S: V \to W \]
is also a linear map, where
\[ (\alpha T + \beta S)(\bm x) = \alpha T(\bm x) + \beta S(\bm x) \]
for any $\bm x \in V$. So the set of linear maps is a vector space. If $M$ and $N$ are the $m\times N$ matrices for $T, S$ then $\alpha M + \beta N$ is the $m\times n$ matrix for the linear combination above, where
\[ (\alpha M + \beta N)_{ai} + \alpha M_{ai} + \beta N_{ai};\quad a = 1, \cdots, m;\quad i = 1, \cdots, n \]
with respect to the same bases.

\subsection{Matrix Multiplication}
If $A$ is an $m\times n$ matrix with entries $A_{ai}$, and $B$ is an $n \times p$ matrix with entries $B_{ir}$, then we define $AB$ to be an $m \times p$ matrix with entries
\[ (AB)_{ar} = A_{ai}B_{ir};\quad a = 1, \cdots, m;\quad i = 1, \cdots, n;\quad r = 1, \cdots, p \]
The product is not defined unless the amount of columns of $A$ matches the number of rows of $B$.

Matrix multiplication corresponds to composition of linear maps. Consider linear maps:
\begin{align*}
	S: \mathbb R^p \to \mathbb R^n                  & ;\; S(\bm x) = B \bm x,\, \bm x \in \mathbb R^p \\
	T: \mathbb R^n \to \mathbb R^m                  & ;\; T(\bm x) = A \bm x,\, \bm x \in \mathbb R^n \\
	\implies T \circ S: \mathbb R^p \to \mathbb R^m & ;\; (T\circ S)(\bm x) = (AB)x
\end{align*}
since
\[ \left[ (AB)\bm x \right]_a = (AB)_{ar}x_r \]
and
\[ A(B(\bm x)) = A_{ai} (B\bm x)_i = A_{ai} B_{ir} x_r = (AB)_{ar}x_r \]
as required. The definition of matrix multiplication ensures that these answers agree. Of course, this proof works for complex or general vector spaces.

\subsection{Properties of Matrix Product}
Whenever the products are defined, then for any scalars $\lambda$ and $\mu$:
\begin{itemize}
	\item $(\lambda M + \mu N)P = \lambda MP + \mu NP$
	\item $P(\lambda M + \mu N) = \lambda PM + \mu PN$
	\item $(MN)P = M(NP)$
	\item $IM = MI = M$ where $I_{ij} = \delta_{ij}$
\end{itemize}
We may view matrix multiplication in the following ways.
\begin{enumerate}[(i)]
	\item Regarding a vector $\bm x \in \mathbb R^n$ as a column vector (an $n \times 1$ matrix), then the matrix-vector and matrix-matrix multiplication rules agree.
	\item Consider the product $AB$ where $A$ is an $m \times n$ matrix and $B$ is an $n \times p$, with columns $\bm C_r(B) \in \mathbb R^n$ and columns $\bm C_r(AB) \in \mathbb R^m$, where $1 \leq r \leq p$. The columns are related by $\bm C_r(AB) = A \bm C_r(B)$. Less formally, eavh column in the right matrix is acted on by the left matrix as if it were a vector, then the resultant vectors are combined into the output matrix.
	\item In terms of rows and columns,
	      \[ AB = \begin{pmatrix}
			                 & \vdots     &             \\
			      \leftarrow & \bm R_n(A) & \rightarrow \\
			                 & \vdots     &
		      \end{pmatrix} \begin{pmatrix}
			             & \uparrow   &        \\
			      \cdots & \bm C_r(B) & \cdots \\
			             & \downarrow &
		      \end{pmatrix} \]
	      gives
	      \begin{align*}
		      (AB)_{ar} & = \left[ \bm R_a(A) \right]_i \left[ \bm C_r(B) \right]_i                                              \\
		                & = \bm R_a(A) \cdot \bm C_r(B) \text{ for real matrices, where the $\cdot$ is the dot product in $R^n$}
	      \end{align*}
\end{enumerate}

\subsection{Matrix Inverses}
If $A$ is an $m \times n$ then $B$, an $n \times m$ matrix, is a left inverse of $A$ if $BA = I$ (the $n \times n$ identity matrix). $C$ is a right inverse of $A$ if $AC = I$ (the $m \times m$ identity matrix). If $m = n$ ($A$ is square), then one of these implies the other; there is no distinction between left and right inverses. We say that $B = C = A^{-1}$, \textit{the} inverse of the matrix $A$, such that $AA^{-1} = A^{-1}A = I$. Not every matrix has an inverse. If such an inverse exists, $A$ is called invertible, or non-singular.

Consider $\bm x, \bm x' \in \mathbb R^n$ or $\mathbb C^n$, and $M$ is an $n \times n$ matrix. If $M^{-1}$ exists, we can solve the equation $\bm x' = M \bm x$ for $\bm x$, given $\bm x'$, because we can apply the matrix inverse on the left. For example, where $n=2$, we have
\[ M = \begin{pmatrix}
		M_{11} & M_{12} \\
		M_{21} & M_{22}
	\end{pmatrix} \]
and
\begin{align*}
	x_1' & = M_{11}x_1 + M_{12}x_2 \\
	x_2' & = M_{21}x_1 + M_{22}x_2
\end{align*}
We can solve these simultaneous equations to construct the general matrix inverse.
\begin{align*}
	M_{22} x_1' - M_{12}x_2'  & = (\det M)x_1 \\
	-M_{21} x_1' + M_{11}x_2' & = (\det M)x_2
\end{align*}
where $\det M = M_{11} M_{22} - M_{12} M_{21}$, called the determinant of the matrix. Where the determinant is nonzero, the matrix inverse
\[ M^{-1} = \frac{1}{\det M}\begin{pmatrix}
		M_{22}  & -M_{12} \\
		-M_{21} & M_{11}
	\end{pmatrix} \]
exists. Note that
\begin{align*}
	\bm C_1     & = M \bm e_1 = \begin{pmatrix} M_{11} \\ M_{21} \end{pmatrix}                            \\
	\bm C_2     & = M \bm e_2 = \begin{pmatrix} M_{12} \\ M_{22} \end{pmatrix}                            \\
	\iff \det M & = [\bm C_1, \bm C_2] = [M\bm e_1, M\bm e_2] \text{ in } \mathbb R^2
\end{align*}
So the determinant gives the signed factor by which areas are scaled under the action of $M$. $\det M$ is nonzero if and only if $M\bm e_1$ and $M\bm e_2$ are linearly independent, which is true if and only if the image of $M$ has dimension 2, i.e. $M$ has maximal rank. For example, a shear
\[ S(\lambda) = \begin{pmatrix}
		1 & \lambda \\ 0 & 1
	\end{pmatrix} \]
has determinant 1, so areas are preserved. In particular, in this case,
\[ S^{-1}(\lambda) = \begin{pmatrix}
		1 & -\lambda \\ 0 & 1
	\end{pmatrix} = S(-\lambda) \]
As another example, we know that a matrix $R(\theta)$ for a rotation about a fixed axis $\nhat$ through angle $\theta$ has formula
\begin{align*}
	R(\theta)_{ij} R(-\theta)_{jk} & = (\delta_{ij}\cos \theta + (1 - \cos \theta) n_i n_j - \varepsilon_{ijp}n_p \sin \theta) \times (\delta_{jk}\cos \theta + (1 - \cos \theta) n_j n_k + \varepsilon_{jkq}n_q \sin \theta) \\
	\intertext{Expanding out, noting that $n_in_i = 1$ as $\nhat$ is a unit vector, and cancelling:}
	                               & = \delta_{ik} \cos^2 \theta + 2\cos \theta(1 - \cos \theta) n_in_k + (1 - \cos \theta)^2n_in_k - \varepsilon_{ijp}\varepsilon_{jkq} n_p n_q \sin^2 \theta                                \\
	\intertext{By using an $\varepsilon\varepsilon$ identity:}
	                               & = \delta_{ik}\cos^2\theta + (1 - \cos^2 \theta)n_in_k + \delta_{ik}n_pn_p \sin^2 \theta - (\sin^2 \theta)n_in_k                                                                          \\
	                               & = \delta_{ik}\cos^2\theta + \delta_{ik}n_pn_p \sin^2 \theta                                                                                                                              \\
	                               & = \delta_{ik}\cos^2\theta + \delta_{ik} \sin^2 \theta                                                                                                                                    \\
	                               & = \delta_{ik}
\end{align*}
as required.

\section{Real and Complex Transposes and Conjugates}
\subsection{Transpose}
If $M$ is an $m \times n$ (real or complex) matrix, the transpose $M^\transpose$ is an $n \times m$ matrix defined by
\[ (M^\transpose)_{ia} = M_{ai} \]
which essentially exchanges rows and columns. Here are some key properties.
\begin{itemize}
	\item $(\alpha A + \beta B))^\transpose = \alpha A^\transpose + \beta B^\transpose$ for $\alpha, \beta$ scalars, and $A, B$ both $m \times n$ matrices.
	\item $(AB)^\transpose = B^\transpose A^\transpose$, where $A$ is $m \times n$ and $B$ is $n \times p$. This is because
	      \begin{align*}
		      [(AB)^\transpose]_{ra} & = (AB)_{ar}                               \\
		                             & = A_{ai} B_{ir}                           \\
		                             & = (A^\transpose)_{ia} (B^\transpose)_{ri} \\
		                             & = (B^\transpose)_{ri} (A^\transpose)_{ia} \\
		                             & = (B^\transpose A^\transpose)_{ra}
	      \end{align*}
	\item If $\bm x$ is a column vector (or an $n \times 1$ matrix), $\bm x^\transpose$ is the equivalent row vector (a $1 \times n$ matrix).
	\item The inner product in $\mathbb R^n$ can therefore be written $\bm x \cdot \bm y = \bm x^\transpose \bm y$. Note that this is not equivalent to $\bm x \bm y^\transpose$, which is known as the outer product, which results in a matrix not a scalar.
	\item If $M$ is $n \times n$ (square) then $M$ is:
	      \begin{itemize}
		      \item symmetric iff $M^\transpose = M$, or $M_{ij} = M_{ji}$
		      \item antisymmetric iff $M^\transpose + -M$, or $M_{ij} = -M_{ji}$
	      \end{itemize}
	\item Any $M$ which is square can be written as a sum of a symmetric and and an antisymmetric part
	      \[ M = S + A\quad\text{where } S = \frac{1}{2}(M + M^\transpose);\quad A = \frac{1}{2}(M - M^\transpose) \]
	      as $S$ is symmetric and $A$ is antisymmetric by construction.
	\item If $A$ is $3 \times 3$ and antisymmetric, then we can write
	      \[ A_{ij} = \varepsilon_{ijk}a_k\text{ where } A = \begin{pmatrix}
			      0    & a_3  & -a_2 \\
			      -a_3 & 0    & a_1  \\
			      a_2  & -a_1 & 0
		      \end{pmatrix} \]
	      Then, we have
	      \[
		      (A \bm x)_i = \varepsilon_{ijk}a_k x_j = (\bm x \times \bm a)_i
	      \]
\end{itemize}

\subsection{Hermitian Conjugate}
Let $M$ be an $m \times n$ matrix. Then the Hermitian conjugate (also known as the conjugate transpose) $M^\dagger$ is an $n \times m$ matrix defined by
\[
	(M^\dagger)_{ia} = \overline{M_{ai}}
\]
If $M$ is square, then $M$ is Hermitian if and only if $M^\dagger = M$, or alternatively $M_{ia} = \overline{M_{ai}}$; $M$ is anti-Hermitian if $M^\dagger = -M$, or alternatively $M_{ia} = -\overline{M_{ai}}$. Similarly to above, if $\bm z$ is a column vector in $\mathbb C^n$ (an $n \times 1$ matrix), then the complex inner product is given by $\bm z \cdot \bm w = \bm z^\dagger \bm w$.

\subsection{Trace}
For a complex $n \times n$ (square) matrix $M$, the trace of the matrix, denoted $\tr(M)$, is defined by
\[ \tr(M) = M_{ii} = M_{11} + M_{22} + \cdots + M_{nn} \]
It has a number of key properties.
\begin{itemize}
	\item $\tr(\alpha M + \beta N) = \alpha \tr M + \beta \tr N$ where $\alpha$ and $\beta$ are scalars, and $M$ and $N$ are $n \times n$ matrices.
	\item $\tr(MN) = \tr(NM)$ where $M$ is $m \times n$ and $N$ is $n \times m$. $MN$ and $NM$ need not have the same dimension, but their traces are identical. We can check this as follows: $\tr(MN) = (MN)_{aa} = M_{ai} N_{ia} = N_{ia} M_{ai} = (NM)_{ii} = \tr(NM)$.
	\item $\tr(M^\transpose) = \tr(M)$
	\item $\tr(I) = \delta_{ii} = n$ where $n$ is the dimensionality of the vector space.
	\item If $S$ is $n \times n$ and symmetric, let
	      \begin{align*}
		      T                             & = S - \frac{1}{n}\tr(S) I                \\
		      \text{or } T_{ij}             & = S_{ij} - \frac{1}{n}\tr(S) \delta_{ij} \\
		      \text{then } \tr (T) = T_{ii} & = S_{ii} = \frac{1}{n}\tr(S) \delta_{ii} \\
		                                    & = \tr(S) - \frac{1}{n}\tr(S) = 0
	      \end{align*}
	      Then $S = T + \frac{1}{n}\tr(S)I$ where $T$ is traceless and the right hand term $\frac{1}{n}\tr(S)I$ is `pure trace'.
	\item If $A$ is $n \times n$ antisymmetric, $\tr(A) = A_{ii} = 0$.
\end{itemize}

\subsection{Orthogonal Matrices}
A real $n \times n$ matrix $U$ is orthogonal if and only if its transpose is its inverse.
\[ U^\transpose U = UU^\transpose = I \]
These conditions can be written
\[ U_{ki}U_{kj} = U_{ik}U_{jk} = \delta_{ij} \]
In words, the left hand side says that the columns of $U$ are orthonormal, and the middle part of the equation says that the rows of $U$ are orthonormal.
\[
	U^\transpose U = \begin{pmatrix}
		           & \vdots  &             \\
		\leftarrow & \bm C_i & \rightarrow \\
		           & \vdots  &
	\end{pmatrix}
	\begin{pmatrix}
		       & \uparrow   &        \\
		\cdots & \bm C_j    & \cdots \\
		       & \downarrow &
	\end{pmatrix}
	= \begin{pmatrix}
		1      & \cdots & 0      \\
		\vdots & \ddots & \vdots \\
		0      & \cdots & 1
	\end{pmatrix}
\]
For example, if $U = R(\theta)$ is a rotation through $\theta$ around an axis $\nhat$, then $U^\transpose = R(\theta)^\transpose = R(-\theta) = R(\theta)^{-1} = U^{-1}$. An equivalent definition for orthogonality is: $U$ is orthogonal if and only if it preserves the inner product on $\mathbb R^n$.
\[ (U\bm x)\cdot(U \bm y) = \bm x \cdot \bm y\quad \forall \bm x, \bm y \in \mathbb R^n \]
To check equivalence:
\begin{align*}
	(U\bm x)\cdot(U \bm y) & = (U\bm x)^\transpose (U\bm y)             \\
	                       & = (\bm x^\transpose U^\transpose) (U\bm y) \\
	                       & = \bm x^\transpose (U^\transpose U) \bm y  \\
	                       & = \bm x^\transpose \bm y                   \\
	                       & = \bm x \cdot \bm y
\end{align*}
which is true if and only if $U^\transpose U = I$. Note that in $\mathbb R^n$, the columns of $U$ are $U\bm e_i, \cdots, U\bm e_n$ so the inner product is preserved when $U$ acts on the standard basis vectors if and only if
\[ (U\bm e_i)\cdot(U\bm e_j) = \bm e_i \cdot \bm e_j = \delta_{ij} \]
i.e. the columns of $U$ are orthonormal.

Let us now try to find a general $2 \times 2$ orthogonal matrix. We begin by transforming the basis vectors. $\bm e_i = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$ must be transformed to a unit vector. Therefore, in the most general sense:
\[ U \begin{pmatrix}
		1 \\0
	\end{pmatrix} = \begin{pmatrix}
		\cos \theta & \sin \theta
	\end{pmatrix} \]
for some parameter $\theta$. Now, the other basis vector $\bm e_2$ must be orthogonal to it, and so it must be
\[
	U \begin{pmatrix}
		0 \\ 1
	\end{pmatrix} = \pm\begin{pmatrix}
		-\sin \theta \\
		\cos \theta
	\end{pmatrix}
\]
So we have two cases:
\[ U = R = \begin{pmatrix}
		\cos \theta & -\sin\theta \\ \sin \theta & \cos \theta
	\end{pmatrix};\quad U = H = \begin{pmatrix}
		\cos \theta & \sin \theta \\ \sin \theta & -\cos \theta
	\end{pmatrix} \]
where $R$ is a rotation by $\theta$ and $H$ is a reflection in $\mathbb R^2$, where
\[ \nhat = \begin{pmatrix}
		-\sin \frac{\theta}{2} \\ \cos \frac{\theta}{2}
	\end{pmatrix} \]
because
\[ H_{ij} = \delta_{ij} - 2n_in_j \therefore H = \begin{pmatrix}
		1 - 2 \sin^2 \frac{\theta}{2}              & 2\sin\frac{\theta}{2}\cos\frac{\theta}{2} \\
		2\sin\frac{\theta}{2} \cos\frac{\theta}{2} & 1-2\cos^2\frac{\theta}{2}
	\end{pmatrix} \]
which simplifies as required. Note that $\det R = +1$, but $\det H = -1$.

\subsection{Unitary Matrices}
A complex $n \times n$ matrix $U$ is called unitary if and only if
\[ U^\dagger U = U U^\dagger = I \]
Equivalently, $U$ is unitary if and only if it preserves the complex inner product on $\mathbb C_n$:
\[ \langle U \bm z, U \bm w \rangle = \langle \bm z, \bm w \rangle\quad \forall \bm z, \bm w \in \mathbb C^n \]
To check equivalence:
\begin{align*}
	\langle U \bm z, U \bm w \rangle & = (U \bm z)^\dagger (U \bm w)         \\
	                                 & = (\bm z^\dagger U^\dagger) (U \bm w) \\
	                                 & = \bm z^\dagger (U^\dagger U) \bm w   \\
	                                 & = \bm z^\dagger \bm w
\end{align*}
which of course matches if and only if $U^\dagger U = I$.

\section{Adjugates and Alternating Forms}
\subsection{Inverses in Two Dimensions}
Consider a linear map $T\colon \mathbb R^n \to \mathbb R^n$. If $T$ is invertible (i.e. bijective), then $\ker T = \{ \bm 0 \}$ as $T$ is injective, and $\Im T = \mathbb R^n$ as $T$ is surjective. These conditions are actually equivalent due to the rank-nullity theorem. Conversely, if the conditions hold, then $T(\bm e_1), T(\bm e_2), \cdots, T(\bm e_n)$ must be a basis of the image, so we can just define $T^{-1}$ by defining its actions on the basis vectors $T(\bm e_1), T(bm e_2) \cdots T(\bm e_n)$, specifically mapping them to the standard basis.

How can we test whether the conditions above hold for a matrix $M$ representing $T$, and how can we find $M^{-1}$ from $M$ explicitly? For any $n \times n$ matrix $M$ (not necessarily invertible), we will define the adjugate matrix $\adjugate M$ and the determinant $\det M$ such that
\[ \adjugate M M = (\det M) I \tag{$\ast$} \]
Then if $\det M \neq 0$, $M$ is invertible, where
\[ M^{-1} = \frac{1}{\det M}\adjugate M \]
From $n=2$, recall that ($\ast$) holds with
\[ M = \begin{pmatrix}
		M_{11} & M_{21} \\
		M_{12} & M_{22}
	\end{pmatrix};\quad \adjugate M = \begin{pmatrix}
		M_{22}  & -M_{21} \\
		-M_{12} & M_{11}
	\end{pmatrix};\quad \det M = [M\bm e_1, M\bm e_2] = \varepsilon_{ij}M_{i1}M_{j2} \]
The determinant in this case is the factor by which areas scale under $M$. $\det M \neq 0$ if and only if $M\bm e_1, M\bm e_2$ are linearly independent.

\subsection{Three Dimensions}
For $n=3$, we will define similarly
\[ \det M = [M\bm e_1, M\bm e_2, M\bm e_3] = \varepsilon{ijk}M_{i1}M_{j2}M_{k3} \]
We define it like this because this is the factor by which volumes scale under $M$ in three dimensions. So
\[ \det M \neq 0 \iff \{ M \bm e_1, M \bm e_2, M \bm e_3 \} \text{ linearly independent, or } \Im M = \mathbb R^3 \]
Now we define $\adjugate M$ from $M$ using row/column notation.
\begin{align*}
	\bm R_1(\adjugate M) & = \bm C_2(M) \times \bm C_3(M) \\
	\bm R_2(\adjugate M) & = \bm C_3(M) \times \bm C_1(M) \\
	\bm R_3(\adjugate M) & = \bm C_1(M) \times \bm C_2(M)
\end{align*}
Note that therefore,
\[ (\adjugate M M)_{ij} = \bm R_i(\adjugate M) \cdot \bm C_j(M) = \underbrace{(\bm C_1(M) \times \bm C_2(M) \cdot \bm C_3(M))}_{\det M}\delta_{ij} \]
as claimed. For example, let us invert the following matrix.
\begin{align*}
	M                      & = \begin{pmatrix}
		1 & 3 & 0 \\ 0 & -1 & -2 \\ 4 & 1 & -1
	\end{pmatrix}                                                                  \\
	\bm C_2 \times \bm C_3 & = \begin{pmatrix} 3 \\ -1 \\ 1 \end{pmatrix} \times \begin{pmatrix} 0 \\ 2 \\ -1 \end{pmatrix} = \begin{pmatrix} -1 \\ 3 \\ 6 \end{pmatrix} \\
	\bm C_3 \times \bm C_1 & = \begin{pmatrix} 0 \\ 2 \\ -1 \end{pmatrix} \times \begin{pmatrix} 1 \\ 0 \\ 4 \end{pmatrix} = \begin{pmatrix} 8 \\ -1 \\ -2 \end{pmatrix} \\
	\bm C_1 \times \bm C_2 & = \begin{pmatrix} 1 \\ 0 \\ 4 \end{pmatrix} \times \begin{pmatrix} 3 \\ -1 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 11 \\ -1 \end{pmatrix} \\
	\adjugate M            & = \begin{pmatrix}
		-1 & 3 & 6 \\ 8 & -1 & -2 \\ 4 & 11 & -1
	\end{pmatrix}                                                                  \\
	\det M                 & = \bm C_1 \cdot \bm C_2 \times \bm C_3 = 23                                                    \\
	\adjugate M M          & = 23 I
\end{align*}

\subsection{$\varepsilon$ in Higher Dimensions}
Recall (from IA Groups):
\begin{itemize}
	\item A permutation $\sigma$ on the set $\{ 1, 2, \cdots, n \}$ is a bijection from the set to itself, specified by an ordered list $\sigma(1), \sigma(2), \cdots, \sigma(n)$.
	\item Permutations form a group $S_n$, called the symmetric group of order $n!$
	\item A transposition $\tau = (p, q)$ where $p \neq q$ is a permutation that swaps $p$ and $q$.
	\item Any permutation is a product of of $k$ transpositions, where $k$ is unique modulo 2 for a given $\sigma$. In this course, we will write $\varepsilon(\sigma)$ to mean the sign (or signature) of the permutation, $(-1)^k$. $\sigma$ is even if the sign is 1, and odd if the sign is $-1$.
\end{itemize}
The alternating symbol $\varepsilon$ in $\mathbb R^n$ or $\mathbb C^n$ is an $n$-index object (tensor) defined by
\[ \varepsilon_{\underbrace{ij\cdots l}_{\mathclap{n \text{ indices}}}} = \begin{cases}
		+1 & \text{if } i, j \cdots, l \text{ is an even permutation of } 1, 2, \cdots, n \\
		-1 & \text{if } i, j \cdots, l \text{ is an odd permutation of } 1, 2, \cdots, n  \\
		0  & \text{otherwise, i.e. if any indices take the same value}
	\end{cases} \]
Thus if $\sigma$ is any permutation, then
\[ \varepsilon_{\sigma(1)\cdots\sigma(n)} = \varepsilon(\sigma) \]
So $\varepsilon_{ij\cdots l}$ is totally antisymmetric and changes sign whenever a pair of indices are exchanged.
\begin{definition}
	Given vectors $\bm v_1, \cdots \bm v_n \in \mathbb R^n$ or $\mathbb C^n$, the alternating form combines them to give the scalar
	\begin{align*}
		[\bm v_1, \bm v_2, \cdots, \bm v_n ] & = \varepsilon_{ij\cdots l} (\bm v_1)_i (\bm v_2)_j \cdots (\bm v_n)_l                                                            \\
		                                     & = \sum_{\sigma \in S_n} \varepsilon(\sigma) \cdot (\bm v_1)_{\sigma(1)} \cdot (\bm v_2)_{\sigma(2)} \cdots (\bm v_n)_{\sigma(n)}
	\end{align*}
\end{definition}

\subsection{Properties}
\begin{enumerate}[(i)]
	\item The alternating form is multilinear.
	      \[ [ \bm v_1, \cdots, \bm v_{p-1}, \alpha \bm u + \beta \bm w, \bm v_{p+1} \cdots, \bm v_n ] = \alpha [ \bm v_1, \cdots, \bm v_{p-1}, \bm u, \bm v_{p+1} \cdots, \bm v_n ] + \beta [ \bm v_1, \cdots, \bm v_{p-1}, \bm w, \bm v_{p+1} \cdots, \bm v_n ] \]
	\item It is totally antisymmetric. $[ \bm v_{\sigma(1)}, \bm v_{\sigma(2)}, \cdots, \bm v_{\sigma(n)} ] = \varepsilon(\sigma) [ \bm v_1, \cdots, \bm v_n ]$
	\item Standard basis vectors give a positive result: $[\bm e_i, \cdots, \bm e_n] = 1$.
\end{enumerate}
These three properties fix the alternating form completely, and they also imply
\begin{enumerate}[(i)]
	\setcounter{enumi}{3}
	\item If $\bm v_p = \bm v_q$ where $p \neq q$, then
	      \[ [\bm v_1, \cdots, \bm v_p, \cdots, \bm v_q, \cdots, \bm v_n ] = 0 \]
	\item If $v_p$ can be written as a non-trivial linear combination of the other vectors, then
	      \[ [\bm v_1, \cdots, \bm v_p, \cdots, \bm v_n ] = 0 \]
\end{enumerate}
Property (iv) follows from property (ii), where we swap $\bm v_p$ and $\bm v_q$. Property (v) follows from substituting the linear combination representation of $\bm v_p$ into the alternating form expression, the using properties (i) and (iv).

\section{Investigation into Alternating Forms}
\subsection{Checking Properties}
To justify (ii) above, it suffices to check a transposition $\tau = (p\ q)$ where (without loss of generality) $p < q$, then since transpositions generate all permutations the result follows.
\begin{align*}
	 & [\bm v_1, \cdots, \bm v_{p-1}, \bm v_q, \bm v_{p+1}, \cdots, \bm v_{q-1}, \bm v_p, \bm v_{q+1}, \cdots, \bm v_n]                                                                                                                          \\
	 & = \sum_\sigma \varepsilon(\sigma) (\bm v_1)_{\sigma(1)} \cdots (\bm v_{p-1})_{\sigma(p-1)}(\bm v_q)_{\sigma(p)}(\bm v_{p+1})_{\sigma(p+1)} \cdots (\bm v_{q-1})_{\sigma(q-1)}(\bm v_p)_{\sigma(q)}(\bm v_{q+1})_{\sigma(q+1)}             \\
	 & = \sum_\sigma \varepsilon(\sigma) (\bm v_1)_{\sigma'(1)} \cdots (\bm v_{p-1})_{\sigma'(p-1)}(\bm v_q)_{\sigma'(q)}(\bm v_{p+1})_{\sigma'(p+1)} \cdots (\bm v_{q-1})_{\sigma'(q-1)}(\bm v_p)_{\sigma'(p)}(\bm v_{q+1})_{\sigma'(q+1)}      \\
	\intertext{where $\sigma' = \sigma\tau$}
	 & = -\sum_{\sigma'} \varepsilon(\sigma') (\bm v_1)_{\sigma'(1)} \cdots (\bm v_{p-1})_{\sigma'(p-1)}(\bm v_p)_{\sigma'(p)}(\bm v_{p+1})_{\sigma'(p+1)} \cdots (\bm v_{q-1})_{\sigma'(q-1)}(\bm v_q)_{\sigma'(q)}(\bm v_{q+1})_{\sigma'(q+1)} \\
	 & = -[\bm v_1, \cdots, \bm v_{p-1}, \bm v_p, \bm v_{p+1}, \cdots, \bm v_{q-1}, \bm v_q, \bm v_{q+1}, \cdots, \bm v_n]
\end{align*}
as required.

\begin{proposition}
	$[ \bm v_1, \bm v_2, \cdots, \bm v_n] \neq 0$ if and only if $\bm v_1, \bm v_2, \cdots, \bm v_n$ are linearly independent.
\end{proposition}
\begin{proof}
	To show the forward implication, let us suppose that they are not linearly independent and use property (v). Then we can express some $\bm v_p$ as a linear combination of the others. Then $[\bm v_1, \bm v_2, \cdots, \bm v_n] = 0$.

	To show the other direction, note that $\bm v_1, \bm v_2, \cdots, \bm v_3$ means that they span, and if they span then each of the standard basis vectors $\bm e_i$ can be written as a linear combination of the $\bm v$ vectors, i.e. $\bm e_i = U_{ai} \bm v_a$. Then
	\begin{align*}
		[\bm e_1, \bm e_2, \cdots, \bm e_n] & = [U_{a1}\bm v_a, U_{b2}\bm v_b, \cdots, U_{cn}\bm v_c]                                 \\
		                                    & = U_{a1}U_{b2}\cdots U_{cn}[\bm v_a, \bm v_b, \cdots, \bm v_c]                          \\
		                                    & = U_{a1}U_{b2}\cdots U_{cn} \varepsilon_{ab\cdots c}[\bm v_1, \bm v_2, \cdots, \bm v_n]
	\end{align*}
	By definition, the left hand side is $+1$, so $[\bm v_1, \bm v_2, \cdots, \bm v_n]$ is nonzero.
\end{proof}
As an example of these ideas, let
\[ \bm v_1 = \begin{pmatrix} i \\ 0 \\ 0 \\ 2 \end{pmatrix};\quad\bm v_2 = \begin{pmatrix} 0 \\ 0 \\ 5i \\ 0 \end{pmatrix};\quad\bm v_3 = \begin{pmatrix} 3 \\ 2i \\ 0 \\ 0 \end{pmatrix};\quad\bm v_4 = \begin{pmatrix} 0 \\ 0 \\ i \\ 1 \end{pmatrix};\quad \text{where }\bm v_j \in \mathbb C_4 \]
Then
\begin{align*}
	[\bm v_1, \bm v_2, \bm v_3, \bm v_4]
	 & = 5i[\bm v_1, \bm e_3, \bm v_3, \bm v_4]                                      \\
	 & = 5i[i\bm e_1 + 2\bm e_4, \bm e_3, 3\bm e_1 + 2i\bm e_2, -i\bm e_3 + \bm e_4] \\
	\intertext{By multilinearity, we can eliminate all $\bm e_3$ terms not in the second position because they will cancel with it, giving}
	 & = 5i[i\bm e_1 + 2\bm e_4, \bm e_3, 3\bm e_1 + 2i\bm e_2, \bm e_4]             \\
	\intertext{And likewise with $\bm e_4$:}
	 & = 5i[i\bm e_1, \bm e_3, 3\bm e_1 + 2i\bm e_2, \bm e_4]                        \\
	\intertext{And again with $\bm e_1$:}
	 & = 5i[i\bm e_1, \bm e_3, 2i\bm e_2, \bm e_4]                                   \\
	 & = 5i\cdot 2i \cdot i[\bm e_1, \bm e_3, \bm e_2, \bm e_4]                      \\
	 & = 10i[\bm e_1, \bm e_2, \bm e_3, \bm e_4]                                     \\
	 & = 10i
\end{align*}

\subsection{Defining the Determinant}
For an $n \times n$ matrix $M$ with columns $\bm C_a = M\bm e_a$, then the determinant $\det(M) = \abs{M} \in \mathbb R$ or $\mathbb C$ is given by any of the following equivalent definitions.
\begin{align*}
	\det M
	 & = [\bm C_1, \bm C_2, \cdots, \bm C_n]                                                \\
	 & = [M\bm e_1, M\bm e_2, \cdots, M\bm e_n]                                             \\
	 & = \varepsilon_{ij\cdots l}M_{i1}M_{j2} \cdots M_{ln}                                 \\
	 & = \sum_\sigma \varepsilon(\sigma) M_{\sigma(1)1}M_{\sigma(2)2} \cdots M_{\sigma(n)n}
\end{align*}
Here are some examples.
\begin{enumerate}[(i)]
	\item $n=2$
	      \[ \det M = \sum_\sigma M_{\sigma(1)1}M_{\sigma(2)2} = \begin{vmatrix}
			      M_{11} & M_{21} \\ M_{12} & M_{22}
		      \end{vmatrix} = M_{11}M_{22} - M_{12}M_{21} \]
	\item $M$ diagonal, i.e. $M_{ij} = 0$ for $i \neq j$
	      \[ M = \begin{pmatrix}
			      M_{11} & 0      & \cdots & 0      \\
			      0      & M_{22} & \cdots & 0      \\
			      \vdots & \vdots & \ddots & \vdots \\
			      0      & 0      & \cdots & M_{nn}
		      \end{pmatrix} \implies \det M = M_{11}M_{22}\cdots M_{nn} \]
	\item Let $M$ be $n\times n$, $A$ be $(n-1) \times (n-1)$, where
	      \[ M = \left( \begin{array}{c|c}
				      A & 0 \\\hline
				      0 & 1
			      \end{array} \right) \]
	      We call $M$ a matrix `in block form'. So $M_{ni} = M_{in} = 0$ if $i \neq n$. So we can restrict the permutation $\sigma$ to only transmuting the first $(n-1)$ terms, i.e. $\sigma(n) = n$. So $\det M = \det A$.
\end{enumerate}

\begin{proposition}
	If $\bm R_a$ are the rows of $M$, $\det M$ is given by
	\begin{align*}
		\det M
		 & = [\bm R_1, \bm R_2, \cdots, \bm R_n]                                                \\
		 & = \varepsilon_{ij\cdots l}M_{1i}M_{2j} \cdots M_{nl}                                 \\
		 & = \sum_\sigma \varepsilon(\sigma) M_{1\sigma(1)}M_{2\sigma(2)} \cdots M_{n\sigma(n)}
	\end{align*}
	i.e. $\det M = \det M^\transpose$.
\end{proposition}
\begin{proof}
	Recall that $(\bm C_a)_i = M_{ia} = (\bm R_i)_a$. We need to show that one of these definitions is equivalent to one of the previous definitions, then all other equivalent definitions follow. We use the $\Sigma$ definition by considering the product $M_{1\sigma(1)}M_{2\sigma(2)} \cdots M_{n\sigma(n)}$. We may rewrite this product in a different order: $M_{\rho(1)1}M_{\rho(2)2} \cdots M_{\rho(n)n}$. Then $\rho = \sigma^{-1}$. But then $\varepsilon(\sigma) = \varepsilon(\rho)$, and a sum over $\sigma$ is equivalent to a sum over $\rho$.
\end{proof}

\subsection{Evaluating Determinants: Expanding by Rows or Columns}
For an $n \times n$ matrix $M$ with entries $M_{ia}$, we define the minor $M^{ia}$ to be the $(n-1)\times(n-1)$ determinant of the matrix obtained by deleting row $i$ and column $a$ from $M$.
\begin{proposition}
	The determinant of a generic $n \times n$ matrix $M$ is given by
	\begin{align*}
		\det M
		 & = \sum_i (-1)^{i+a} M_{ia} M^{ia} \text{ for a fixed $a$} \\
		 & = \sum_a (-1)^{i+a} M_{ia} M^{ia} \text{ for a fixed $i$}
	\end{align*}
\end{proposition}
This process is known as expanding by row $i$ or by column $a$. As an example, let us take the following $4 \times 4$ complex matrix
\[ M = \begin{pmatrix}
		i & 0  & 3  & 0  \\
		0 & 0  & 2i & 0  \\
		0 & 5i & 0  & -i \\
		2 & 0  & 0  & 1
	\end{pmatrix} \]
Then, the determinant is given by (expanding by row 3)
\begin{align*}
	\det M
	 & = -5i\begin{vmatrix}
		i & 3  & 0 \\
		0 & 2i & 0 \\
		2 & 0  & 1
	\end{vmatrix} + i\begin{vmatrix}
		i & 0 & 3  \\
		0 & 0 & 2i \\
		2 & 0 & 0
	\end{vmatrix}                                                               \\
	 & = -5i\left[i\begin{vmatrix}
			2i & 0 \\
			0  & 1
		\end{vmatrix} - 3 \begin{vmatrix}
			0 & 0 \\
			2 & 1
		\end{vmatrix}\right] + i\left[-2i\begin{vmatrix}
			i & 0 \\
			2 & 0
		\end{vmatrix}\right] \\
	 & = -5i[i \cdot 2i - 3 \cdot 0] + i[-2i \cdot 0]                                                                                \\
	 & = -5i[-2] + i[0]                                                                                                              \\
	 & = 10i
\end{align*}

\section{Constructing General Determinants and Inverses}
\subsection{Simplifying Determinants: Row and Column Operations}
Consider the following consequences of the properties of the determinant:
\begin{itemize}
	\item (row and column scaling) If $\bm R_i \mapsto \lambda \bm R_i$ for a fixed $i$, or $\bm C_a \mapsto \lambda \bm C_a$, then $\det M \mapsto \lambda \det M$ by multilinearity. If we scale all rows or columns, then $M \mapsto \lambda M$, so $\det M \mapsto \lambda^n \det M$ where $M$ is an $n \times n$ matrix.
	\item (row and column operations) If $\bm R_i \mapsto \bm R_i + \lambda \bm R_j$ where $i \neq j$ (or the corresponding conversion with columns), then $\det M \mapsto \det M$.
	\item (row and column exchanges) If we swap $\bm R_i$ and $\bm R_j$ (or two columns), then $\det M \mapsto -\det M$.
\end{itemize}
For example, let us find the terminant of matrix $A$, where
\[ A = \begin{pmatrix}
		1 & 1 & a \\ a & 1 & 1 \\ 1 & a & 1
	\end{pmatrix};\quad a \in \mathbb C \]
Then:
\begin{align*}
	\det A                                                    & = \begin{vmatrix} 1 & 1 & a \\ a & 1 & 1 \\ 1 & a & 1 \end{vmatrix}                            \\
	\bm C_1 \mapsto \bm C_1 - \bm C_3:\quad \det A            & = \begin{vmatrix} 1-a & 1 & a \\ a-1 & 1 & 1 \\ 0 & a & 1 \end{vmatrix}                            \\
	\det A                                                    & = (1-a)\begin{vmatrix} 1 & 1 & a \\ -1 & 1 & 1 \\ 0 & a & 1 \end{vmatrix}                       \\
	\bm C_2 \mapsto \bm C_2 - \bm C_3:\quad \det A            & = (1-a)\begin{vmatrix} 1 & 1-a & a \\ -1 & 0 & 1 \\ 0 & a-1 & 1 \end{vmatrix}                       \\
	\det A                                                    & = (1-a)^2\begin{vmatrix} 1 & 1 & a \\ -1 & 0 & 1 \\ 0 & -1 & 1 \end{vmatrix}                     \\
	\bm R_1 \mapsto \bm R_1 + \bm R_2 + \bm R_3 :\quad \det A & = (1-a)^2\begin{vmatrix} 0 & 0 & a+2 \\ -1 & 0 & 1 \\ 0 & -1 & 1 \end{vmatrix}                     \\
	\det A                                                    & = (1-a)^2(a+2)\begin{vmatrix}-1&0\\0&-1\end{vmatrix} = (1-a)^2(a+2)
\end{align*}

\subsection{Multiplicative Property of Determinants}
\begin{theorem}
	For $n\times n$ matrices $M, N$, $\det (MN) = \det M \cdot \det N$.
\end{theorem}
\noindent We can prove this using the following elaboration on the definition of the determinant:
\begin{lemma}
	\[ \varepsilon_{i_1 i_2 \cdots i_n} M_{i_1 a_1} M_{i_2 a_2} \cdots M_{i_n a_n} = (\det M) \varepsilon_{a_1 a_2 \cdots a_n} \]
\end{lemma}
\begin{proof}
	The left hand side and right hand side are each totally antisymmetric (alternating) in $a_1, a_2, \cdots, a_n$, so they must be related by a constant of proportionality. To fix the constant, we can simply consider taking $a_i = i$ and the result follows.
\end{proof}
\noindent Now, we prove the above theorem.
\begin{proof}
	Using the lemma above:
	\begin{align*}
		\det MN & = \varepsilon_{i_1 i_2 \cdots i_n} (MN)_{i_1 1} (MN)_{i_2 2} \cdots (MN)_{i_n n}                                                    \\
		        & = \varepsilon_{i_1 i_2 \cdots i_n} {M_{i_1 k_1} \atop N_{k_1 1}} {M_{i_2 k_2} \atop N_{k_2 2}} \cdots {M_{i_n k_n} \atop N_{k_n n}} \\
		        & = (\det M) \varepsilon_{a_1 a_2 \cdots a_n} N_{k_1 1} N_{k_2 2} \cdots N_{k_n n}                                                    \\
		        & = (\det M)(\det N)
	\end{align*}
	as required.
\end{proof}

\subsection{Consequences of Multiplicative Property}
\begin{enumerate}[(i)]
	\item $M^{-1}M = I \implies \det(M^{-1}) \det(M) = \det I = 1$. Therefore, $\det (M^{-1}) = (\det M)^{-1}$, so $\det M$ must be nonzero for $M$ to be invertible.
	\item For $R$ real and orthogonal, $R^\transpose R = I \implies \det(R^\transpose) \det(R) = 1$. But $\det (R^\transpose) = \det R$, so $(\det R)^2 = 1$, so $\det R = \pm 1$.
	\item For $U$ complex and unitary, $U^\dagger U = I \implies \det(U^\dagger) \det(U) = 1$. But since $U^\dagger = \overline{U^\transpose}$, we have $\overline{\det U} \det U = 1$, so $\abs{(\det U)^2} = 1$, so $\abs{\det U} = 1$.
\end{enumerate}

\subsection{Cofactors and Determinants}
Consider a column of some $n \times n$ matrix $M$, written in the form
\[ \bm C_a = \sum_i M_{ia} \bm e_i \]
\begin{align*}
	\implies \det M & = [ \bm C_1, \cdots, \bm C_a, \cdots, \bm C_n ]                                         \\
	                & = [ \bm C_1, \cdots, \bm C_{a-1}, \sum_i M_{ia} \bm e_i, \bm C_{a+1}, \cdots, \bm C_n ] \\
	                & = \sum_i M_{ia} \Delta_{ia}
\end{align*}
where
\begin{align*}
	\Delta_{ia} & = [ \bm C_1, \cdots, \bm C_{a-1}, \bm e_i, \bm C_{a+1}, \cdots, \bm C_n ]                                                                                          \\
	            & = \begin{vmatrix}
		\mathhuge A                 & \begin{matrix}
			0 \\ \vdots \\ 0
		\end{matrix} & \mathhuge B                 \\
		\begin{matrix}
			0 & \cdots & 0
		\end{matrix} & 1                           & \begin{matrix}
			0 & \cdots & 0
		\end{matrix} \\
		\mathhuge C                 & \begin{matrix}
			0 \\ \vdots \\ 0
		\end{matrix} & \mathhuge D
	\end{vmatrix}
	\intertext{where the zero entries in the rows arise from antisymmetry, giving}
	            & = \underbrace{(-1)^{n-a}}_{\text{amount of column transpositions}} \cdot \underbrace{(-1)^{n-i}}_{\text{amount of row transpositions}} \begin{vmatrix}
		\mathhuge A & \mathhuge B \\
		\mathhuge C & \mathhuge D
	\end{vmatrix} \\
	            & = (-1)^{i+a}M^{ia}
\end{align*}
where $M^{ia}$ is the minor in this position; the determinant of the matrix with this particular row and column removed. We call $\Delta_{ia}$ the cofactor.
\[ \det M = \sum_i M_{ia} \Delta_{ia} = \sum_i(-1)^{i+a}M_{ia}M^{ia} \]
Similarly, by considering rows,
\[ \det M = \sum_a M_{ia} \Delta_{ia} = \sum_a(-1)^{i+a}M_{ia}M^{ia} \]

\subsection{Adjugates and Inverses}
Reasoning as above, consider $\bm C_b = \sum_i M_{ib} \bm e_i$. Then,
\[ [\bm C_1, \cdots, \bm C_{a-1}, \bm C_b, \bm C_{a+1}, \cdots, \bm C_n ] = \sum_i M_{ib} \Delta_{ia} \]
If $a=b$ then clearly this is $\det M$. Otherwise, $\bm C_b$ is equal to one of the other columns, so $\sum_i M_{ib} \Delta_{ia} = 0$.
\[ \sum_i M_{ib} \Delta_{ia} = (\det M)\delta_{ab} \]
Similarly,
\[ \sum_a M_{ja} \Delta_{ia} = (\det M)\delta_{ij} \]
Now, let $\Delta$ be the matrix of cofactors (i.e. entries $\Delta_{ia}$), and we define the adjugate $\adjugate M = \Delta^\transpose$. Then
\[ \Delta_{ia}M_{ib} = \adjugate M_{ai}M_{ib} = (\adjugate M M)_{ab} = (\det M)\delta_{ab} \]
Therefore,
\[ \adjugate M M = (\det M) I \]
We can reach this result similarly considering the other index. Hence, if $\det M \neq 0$ then $M^{-1} = \frac{1}{\det M}\adjugate M$.

\section{Systems of Linear Equations}
\subsection{Introduction and Nature of Solutions}
Consider a system of $n$ linear equations in $n$ unknowns $x_i$ written in matrix-vector form:
\[ A\bm x = \bm b,\quad \bm x, \bm b \in \mathbb R^n, \]
where $A$ is an $n \times n$ matrix. There are three possibilities:
\begin{enumerate}[(i)]
	\item $\det A \neq 0 \implies A^{-1}$ exists so there is a unique solution $\bm x = \bm A^{-1} \bm b$
	\item $\det A = 0$ and $b \notin \Im A$ means that there is no solution
	\item $\det A = 0$ and $b \in \Im A$ means that there are infinitely many solutions of the form
	      \[ \bm x = \bm x_0 + \bm u \]
	      where $\bm u \in \ker A$ and $\bm x_0$ is a particular solution
\end{enumerate}
A solution therefore exists if and only if $A\bm x_0 = \bm b$ for some $\bm x_0$, which is true if and only if $\bm b \in \Im A$. Then $\bm x$ is also a solution if and only if $\bm u = \bm x - \bm x_0$ satisfies
\[ A\bm u = \bm 0 \]
This equation is known as the equivalent homogeneous problem. Now, $\det A \neq 0 \iff \Im A = \mathbb R^n \iff \ker A = \{ \bm 0 \}$. So in case (i), there is always a unique solution for any $\bm b$. But $\det A = 0 \iff \rank(A) < n \iff \nullity A > 0$. Then either $b \notin \Im A$ as in case (ii), or $b \in \Im A$ as in case (iii).

If $\bm u_1, \dots, \bm u_k$ is a basis for $\ker A$, then the general solution to the homogeneous problem is some linear combination of these basis vectors, i.e.
\[ \bm u = \sum_{i=1}^k \lambda_i \bm u_i,\quad k = \nullity A \]
This is similar to the complementary function and particular integral technique used to solve linear differential equations.

For example, in $A\bm x = \bm b$, let
\[ A = \begin{pmatrix}
		1 & 1 & a \\ a & 1 & 1 \\ 1 & a & 1
	\end{pmatrix};\quad \bm b = \begin{pmatrix}
		1 \\ c \\ 1
	\end{pmatrix};\quad a, c \in \mathbb R \]
We have previously found that $\det A = (a-1)^2(a+2)$. So the cases are:
\begin{itemize}
	\item ($a \neq 1, a \neq -2$) $\det A \neq 0$ and $A^{-1}$ exists; we previously found this to be
	      \[ A^{-1} = \frac{1}{(1-a)(2+a)}\begin{pmatrix}
			      1 & 1+a & 1 \\ 1 & 1 & -1-a \\ -1-a & 1 & 1
		      \end{pmatrix} \]
	      For these values of $a$, there is a unique solution for any $c$, demonstrating case (i) above:
	      \[ \bm x = A^{-1} \bm b = \frac{1}{(1-a)(2+a)}\begin{pmatrix}
			      2-c-ca \\ c-a \\ c-a
		      \end{pmatrix} \]
	      Geometrically, this solution is simply a point.
	\item ($a = 1$) In this case, the matrix is simply
	      \[ A = \begin{pmatrix}
			      1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1
		      \end{pmatrix} \implies \Im A = \vecspan \left\{ \begin{pmatrix}
			      1 \\ 1 \\ 1
		      \end{pmatrix} \right\} = \left\{ \lambda\begin{pmatrix}
			      1 \\ 1 \\ 1
		      \end{pmatrix} \right\};\quad \ker A = \vecspan\left\{ \begin{pmatrix}
			      -1 \\ 1 \\ 0
		      \end{pmatrix}, \begin{pmatrix}
			      -1 \\ 0 \\ 1
		      \end{pmatrix} \right\} \]
	      Note that $\bm b \in \Im A$ if and only if $c=1$, where a particular solution is
	      \[ \bm x_0 = \begin{pmatrix}
			      1 \\ 0 \\ 0
		      \end{pmatrix} \]
	      So the general solution is given by
	      \[ \bm x = \bm x_0 + \bm u = \begin{pmatrix}
			      1 - \lambda - \mu \\ \lambda \\ \mu
		      \end{pmatrix} \]
	      In summary, for $a=1$, $c=1$ we have case (iii). Geometrically this is a plane. For $a=1$, $c \neq 1$, we have case (ii) where there are no solutions.

	\item ($a=-2$) The matrix becomes
	      \[ A = \begin{pmatrix}
			      1 & 1 & -2 \\ -2 & 1 & 1 \\ 1 & -2 & 1
		      \end{pmatrix} \implies \Im A = \vecspan \left\{ \begin{pmatrix}
			      1 \\ -2 \\ 1
		      \end{pmatrix}, \begin{pmatrix}
			      1 \\ 1 \\ -2
		      \end{pmatrix} \right\};\quad \ker A = \left\{ \lambda\begin{pmatrix}
			      1 \\ 1 \\ 1
		      \end{pmatrix} \right\} \]
	      Now, $\bm b \in \Im A$ if and only if $c = -2$, the particular solution is
	      \[ \bm x_0 = \begin{pmatrix}
			      1 \\ 0 \\ 0
		      \end{pmatrix} \]
	      The general solution is therefore
	      \[ \bm x = \bm x_0 + \bm u = \begin{pmatrix}
			      1 + \lambda \\ \lambda \\ \lambda
		      \end{pmatrix} \]
	      In summary, for $a=-2$ and $c=-2$ we have case (iii). Geometrically this is a line. For $a=-2$, $c \neq -2$, we have case (ii) where there are no solutions.
\end{itemize}

\subsection{Geometrical Interpretation in $\mathbb R^3$}
Let $\bm R_1, \bm R_2, \bm R_3$ be the rows of the $3 \times 3$ matrix $A$. Then the rows represent the normals of planes. This is clear by expanding the matrix multiplication of the homogeneous form:
\begin{align*}
	A\bm u = \bm 0 \iff & \bm R_1 \cdot \bm u = 0 \\
	                    & \bm R_2 \cdot \bm u = 0 \\
	                    & \bm R_3 \cdot \bm u = 0
\end{align*}
So the solution of the homogeneous problem (i.e. finding the general solution) amounts to determining where the planes intersect.
\begin{itemize}
	\item ($\rank A = 3$) The rows are linearly independent, so the three planes' normals are linearly independent and the planes intersect at $\bm 0$ only.
	\item ($\rank A = 2$) The normals span a plane, so the planes intersect in a line.
	\item ($\rank A = 1$) The normals are parallel and therefore the planes coincide.
	\item ($\rank A = 0$) The normals are all zero, so any vector in $\mathbb R^3$ solves the equation.
\end{itemize}
Now, let us consider instead the original problem $A \bm x = \bm b$:
\begin{align*}
	A\bm b = \bm 0 \iff & \bm R_1 \cdot \bm u = b_1 \\
	                    & \bm R_2 \cdot \bm u = b_2 \\
	                    & \bm R_3 \cdot \bm u = b_3
\end{align*}
The planes still have normals $\bm R_i$ as before, but they do not necessarily pass through the origin.
\begin{itemize}
	\item ($\rank A = 3$) The planes' normals are linearly independent and the planes intersect at a point; this is the unique solution.
	\item ($\rank A < 3$) The existence of a solution depends on the value of $\bm b$.
	      \begin{itemize}
		      \item ($\rank A = 2$) The planes may intersect in a line as before, but they may instead form a sheaf (the planes pairwise intersect in lines but they do not as a triple), or two planes could be parallel and not intersect each other at all.
		      \item ($\rank A = 1$) The normals are parallel, so the planes may coincide or they might be parallel. There is no solution unless all three planes coincide.
	      \end{itemize}
\end{itemize}

\section{Eigenvalues and Eigenvectors}
\subsection{Definitions}
For a linear map $T\colon V \to V$, a vector $\bm v \in V$ with $\bm v \neq 0$ is called an eigenvector of $T$ with eigenvalue $\lambda$ if $T(\bm v) = \lambda \bm v$. If $V = \mathbb R^n$ or $\mathbb C^n$, and $T$ is given by an $n \times n$ matrix $A$, then
\[ A\bm v = \lambda v \iff (A - \lambda I)\bm v = \bm 0 \]
and for a given $\lambda$, this holds for some $\bm v \neq 0$ if and only if
\[ \det(A - \lambda I) = 0 \]
This is called the characteristic equation for $A$. So $\lambda$ is an eigenvalue if and only if it is a root of the characteristic polynomial
\[ \chi_A(t) = \det(A - tI) = \begin{vmatrix}
		A_{11} - t & A_{12}     & \cdots & A_{1n}     \\
		A_{21}     & A_{22} - t & \cdots & A_{2n}     \\
		\vdots     & \vdots     & \ddots & \vdots     \\
		A_{n1}     & A_{n2}     & \cdots & A_{nn} - t
	\end{vmatrix} \]
We can look for eigenvalues as roots of the characteristic polynomial or characteristic equation, and then determine the corresponding eigenvectors once we've deduced what the possibilities are.

\subsection{Examples of Eigenvalues and Eigenvectors}
\begin{enumerate}[(i)]
	\item $V = \mathbb C^2$:
	      \[ A = \begin{pmatrix}
			      2 & i \\ -i & 2
		      \end{pmatrix} \implies \det(A - \lambda I) = (2-\lambda)^2 - 1 = 0 \]
	      So we have $(2 - \lambda)^2 = 1$ so $\lambda = 1$ or 3.
	      \begin{itemize}
		      \item ($\lambda = 1$)
		            \[ (A - I)\bm v = \begin{pmatrix}
				            1 & i \\ -i & 1
			            \end{pmatrix}\begin{pmatrix}
				            v_1 \\ v_2
			            \end{pmatrix} = \bm 0 \implies \bm v = \alpha\begin{pmatrix}
				            1 \\ i
			            \end{pmatrix} \]
		            for any $\alpha \neq 0$.
		      \item ($\lambda = 3$)
		            \[ (A - 3I)\bm v = \begin{pmatrix}
				            -1 & i \\ -i & -1
			            \end{pmatrix}\begin{pmatrix}
				            v_1 \\ v_2
			            \end{pmatrix} = \bm 0 \implies \bm v = \beta\begin{pmatrix}
				            1 \\ -i
			            \end{pmatrix} \]
		            for any $\beta \neq 0$.
	      \end{itemize}
	\item $V = \mathbb R^2$:
	      \[ A = \begin{pmatrix}
			      1 & 1 \\ 0 & 1
		      \end{pmatrix} \implies \det(A - \lambda I) = (1-\lambda)^2 = 0 \]
	      So $\lambda = 1$ only, a repeated root.
	      \[ (A - I)\bm v = \begin{pmatrix}
			      0 & 1 \\ 0 & 0
		      \end{pmatrix}\begin{pmatrix}
			      v_1 \\ v_2
		      \end{pmatrix} = \bm 0 \implies \bm v = \alpha\begin{pmatrix}
			      1 \\ 0
		      \end{pmatrix} \]
	      for any $\alpha \neq 0$. There is only one (linearly independent) eigenvector here.
	\item $V = \mathbb R^2$ or $\mathbb C^2$:
	      \[ U = \begin{pmatrix}
			      \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta
		      \end{pmatrix} \implies \chi_U(t) = \det(U - tI) = t^2 - 2t\cos\theta + 1 \]
	      The eigenvalues $\lambda$ are $e^{\pm i \theta}$. The eigenvectors are
	      \[ \bm v = \alpha \begin{pmatrix}
			      1 \\ \mp i
		      \end{pmatrix};\quad \alpha \neq 0 \]
	      So there are no real eigenvalues or eigenvectors except when $\theta = n \pi$.
	\item $V = \mathbb C^n$:
	      \[ A = \begin{pmatrix}
			      \lambda_1 & 0         & \cdots & 0         \\
			      0         & \lambda_2 & \cdots & 0         \\
			      \vdots    & \vdots    & \ddots & \vdots    \\
			      0         & 0         & \cdots & \lambda_n
		      \end{pmatrix} \implies \chi_A(t) = \det(A - tI) = (\lambda_1 - t)(\lambda_2 - t)(\lambda_3 - t)\dots(\lambda_n - t) \]
	      So the eigenvalues are all the $\lambda_i$, and the eigenvectors are $\bm v = \alpha \bm e_i$ ($\alpha \neq 0$) for each $i$.
\end{enumerate}

\subsection{Deductions Involving $\chi_A(t)$}
For an $n \times n$ matrix $A$, the characteristic polynomial $\chi_A(t)$ has degree $n$:
\[ \chi_A(t) = \sum_{j = 0}^n c_j t^j = (-1)^n(t-\lambda_1)\dots(t-\lambda_n) \]
\begin{enumerate}[(i)]
	\item There exists at least one eigenvalue (solution to $\chi_A$), due to the fundamental theorem of algebra, or $n$ roots counted with multiplicity.
	\item $\tr(A) = A_{ii} = \sum_{i=1}^n \lambda_i$, the sum of the eigenvalues. Compare terms of degree $n-1$ in $t$, and from the determinant we get
	      \[ (-t)^{n-1}A_{11} + (-t)^{n-1}A_{22} + \dots + (-t)^{n-1}A_{nn} \]
	      The overall sign matches with the expansion of $(-1)^n(t-\lambda_1)(t-\lambda_2)\dots(t-\lambda_n)$.
	\item $\det(A) = \chi_A(0) = \prod_{i=1}^n \lambda_i$, the product of the eigenvalues.
	\item If $A$ is real, then the coefficients $c_i$ in the characteristic polynomial are real, so $\chi_A(\lambda) = 0 \iff \chi_A(\overline\lambda) = 0$. So the non-real roots occur in conjugate pairs if $A$ is real.
\end{enumerate}

\subsection{Eigenspaces and Multiplicities}
For an eigenvalue $\lambda$ of a matrix $A$, we define the eigenspace
\[ E_\lambda = \{ \bm v : A \bm v = \lambda \bm v \} = \ker (A - \lambda I) \]
All nonzero vectors in this space are eigenvectors. The geometric multiplicity is
\[ m_\lambda = \dim E_\lambda = \nullity (A - \lambda I) \]
equivalent to the number of linearly independent eigenvectors with the given eigenvalue $\lambda$. The algebraic multiplicity is
\[ M_\lambda = \text{the multiplicity of } \lambda \text{ as a root of } \chi_A(t) \]
i.e. $\chi_A(t) = (t - \lambda)^{M_t} f(t)$, where $f(\lambda) \neq 0$.

\begin{proposition}
	$M_\lambda \geq m_\lambda$ (and $m_\lambda \geq 1$ since $\lambda$ is an eigenvalue). The proof of this proposition is delayed until the next section where we will then have the tools to prove it.
\end{proposition}

\subsection{Examples of Eigenspaces}
\begin{enumerate}[(i)]
	\item
	      \[ A = \begin{pmatrix}
			      -2 & 2 & -3 \\ 2 & 1 & -6 \\ -1 & -2 & 0
		      \end{pmatrix} \implies \chi_A(t) = \det(A - tI) = (5-t)(t+3)^2 \]
	      So $\lambda = 5, -3$. $M_5 = 1$, $M_{-3} = 2$. We will now find the eigenspaces.
	      \begin{itemize}
		      \item ($\lambda = 5$)
		            \[ E_5 = \left\{ \alpha\begin{pmatrix}
				            1 \\ 2 \\ -1
			            \end{pmatrix} \right\} \]
		      \item ($\lambda = -3$)
		            \[ E_{-3} = \left\{ \alpha\begin{pmatrix}
				            -2 \\ 1 \\ 0
			            \end{pmatrix} + \beta\begin{pmatrix}
				            3 \\ 0 \\ 1
			            \end{pmatrix} \right\} \]
	      \end{itemize}
	      Note that to compute the eigenvectors, we just need to solve the equation $(A - \lambda I)\bm x = \bm 0$. In the case of $\lambda = -3$, for example, we then have
	      \[ \begin{pmatrix}
			      1 & 2 & -3 \\ 2 & 4 & -6 \\ -1 & -2 & 3
		      \end{pmatrix} \begin{pmatrix}
			      x_1 \\ x_2 \\ x_3
		      \end{pmatrix} = \bm 0 \]
	      We can use the first line of the matrix to get a linear combination for $x_1, x_2, x_3$, specifically $x_1 + 2x_2 = 3x_3 = 0$, so we can eliminate one of the variables (here, $x_1$) to get
	      \[ \bm x = \begin{pmatrix}
			      -2x_2 + 3x_3 \\ x_2 \\ x_3
		      \end{pmatrix} = \bm 0 \]
	      Now, $\dim E_5 = m_5 = 1 = M_5$. Similarly, $\dim E_{-3} = m_{-3} = 2 = M_{-3}$.

	\item
	      \[ A = \begin{pmatrix}
			      -3 & -1 & 1 \\ -1 & -3 & 1 \\ -2 & -2 & 0
		      \end{pmatrix} \implies \chi_A(t) = \det(A - tI) = -(t + 2)^3 \]
	      We have a root $\lambda = -2$ with $M_{-2} = 3$. To find the eigenspace, we will look for solutions of:
	      \[ (A + 2I)\bm x = \begin{pmatrix}
			      -1 & -1 & 1 \\ -1 & -1 & 1 \\ -2 & -2 & 2
		      \end{pmatrix} \begin{pmatrix}
			      x_1 \\ x_2 \\ x_3
		      \end{pmatrix} = \bm 0 \implies \bm x = \begin{pmatrix}
			      -x_2 + x_3 \\ x_2 \\ x_3
		      \end{pmatrix} \]
	      So
	      \[ E_{-2} = \left\{ \alpha\begin{pmatrix}
			      -1 \\ 1 \\ 0
		      \end{pmatrix} + \beta\begin{pmatrix}
			      1 \\ 0 \\ 1
		      \end{pmatrix} \right\} \]
	      Further, $m_{-2} = 2 < 3 = M_{-2}$.

	\item A reflection in a plane through the origin with unit normal $\nhat$ satisfies
	      \[ H\nhat = -\nhat;\quad \forall \bm u \perp \nhat, H \bm u = \bm u \]
	      The eigenvalues are therefore $\pm 1$ and $E_{-1} = \{ \alpha \nhat \}$, and $E_1 = \{ \bm x: \bm x \cdot \nhat = 0 \}$. The multiplicities are given by $M_{-1} = m_{-1} = 1, M_1 = m_1 = 2$.

	\item A rotation about an axis $\nhat$ through angle $\theta$ in $\mathbb R^3$ satisfies
	      \[ R\nhat = \nhat \]
	      So the axis of rotation is the eigenvector with eigenvalue 1. There are no other real eigenvalues unless $\theta = n\pi$. The rotation restricted to the plane perpendicular to $\nhat$ has eigenvalues $e^{\pm i \theta}$ as shown above.
\end{enumerate}

\section{Diagonalisable Matrices}
\subsection{Linear Independence of Eigenvectors}
\begin{proposition}
	Let $\bm v_1, \bm v_2, \dots, \bm v_r$ be eigenvectors of an $n\times n$ matrix $A$ with eigenvalues $\lambda_1, \lambda_2,\dots,\lambda_r$. If the eigenvalues are distinct, then the eigenvectors are linearly independent.
\end{proposition}
\begin{proof}
	Note that if we take some linear combination $\bm w = \sum_{j=1}^r \alpha_j\bm v_j$, then $(A - \lambda I)\bm w = \sum_{j=1}^r \alpha_j(\lambda_j - \lambda)\bm v_j$. Here are two methods for getting this proof.
	\begin{enumerate}[(i)]
		\item Suppose the eigenvectors are linearly dependent, so there exist linear combinations $\bm w = \bm 0$ where some $\alpha$ are nonzero. Let $p$ be the amount of nonzero $\alpha$ values. So, $2 \leq p \leq r$. Now, pick such a $\bm w$ for which $p$ is least. Without loss of generality, let $\alpha_1$ be one of the nonzero coefficients. Then
		      \[ (A - \lambda_1 I)\bm w = \sum_{j=2}^r \alpha_j(\lambda_j - \lambda_1)\bm v_j = \bm 0 \]
		      This is a linear relation with $p-1$ nonzero coefficients \contradiction.
		\item Alternatively, given a linear relation $\bm w=\bm 0$,
		      \[ \prod_{j \neq k} (A - \lambda_j I) \bm w = \alpha_k \prod_{j \neq k} (\lambda_k - \lambda_j) \bm v_k = \bm 0 \]
		      for some fixed $k$. So $\alpha_k = 0$. So the eigenvectors are linearly independent as claimed.
	\end{enumerate}
\end{proof}
\begin{corollary}
	With conditions as in the proposition above, let $\mathcal B_{\lambda_i}$ be a basis for the eigenspace $E_{\lambda_i}$. Then $\mathcal B = \mathcal B_{\lambda_1} \cup \mathcal B_{\lambda_2} \cup \dots \cup \mathcal B_{\lambda_r}$ is linearly independent.
\end{corollary}
\begin{proof}
	Consider a general linear combination of all these vectors, it has the form
	\[ \bm w = \bm w_1 + \bm w_2 + \dots + \bm w_r \]
	where each $\bm w_i \in E_i$. Applying the same arguments as in the proposition, we find that
	\[ \bm w = 0 \implies \forall i\,\bm w_i = 0 \]
	So each $\bm w_i$ is the trivial linear combination of elements of $\mathcal B_{\lambda_i}$ and the result follows.
\end{proof}

\subsection{Diagonalisability and Similarity}
\begin{proposition}
	For an $n \times n$ matrix $A$ acting on $V = \mathbb R^n$ or $\mathbb C^n$, the following conditions are equivalent:
	\begin{enumerate}[(i)]
		\item there exists a basis of eigenvectors of $A$ for $V$, named $\bm v_1, \bm v_2, \dots, \bm v_n$ which $A\bm v_i = \lambda_i\bm v_i$ for each $i$; and
		\item there exists an $n \times n$ invertible matrix $P$ with the property that
		      \[ P^{-1}AP = D = \begin{pmatrix}
				      \lambda_1 & 0         & \cdots & 0         \\
				      0         & \lambda_2 & \cdots & 0         \\
				      \vdots    & \vdots    & \ddots & \vdots    \\
				      0         & 0         & \cdots & \lambda_n
			      \end{pmatrix} \]
	\end{enumerate}
	If either of these conditions hold, then $A$ is diagonalisable.
\end{proposition}
\begin{proof}
	Note that for any matrix $P$, $AP$ has columns $A\bm C_i(P)$, and $PD$ has columns $\lambda_i \bm C_i(P)$. Then (i) and (ii) are related by choosing $\bm v_i = \bm C_i(P)$. Then $P^{-1}AP = D \iff AP = PD \iff A\bm v_i = \lambda_i\bm v_i$.

	In essence, given a basis of eigenvectors as in (i), the relation above defines $P$, and if the eigenvectors are linearly independent then $P$ is invertible. Conversely, given a matrix $P$ as in (ii), its columns are a basis of eigenvectors.
\end{proof}
Let's try some examples.
\begin{enumerate}[(i)]
	\item Let
	      \[ A = \begin{pmatrix}
			      1 & 1 \\ 0 & 1
		      \end{pmatrix} \implies E_1 = \left\{ \alpha\begin{pmatrix}
			      1 \\ 0
		      \end{pmatrix} \right\} \]
	      This is a single eigenvalue $\lambda = 1$ with one linearly independent eigenvector. So there is no basis of eigenvectors for $\mathbb R^2$ or $\mathbb C^2$, so $A$ is not diagonalisable.
	\item Let
	      \[ U = \begin{pmatrix}
			      \cos \theta & -\sin \theta \\
			      \sin \theta & \cos \theta
		      \end{pmatrix} \implies E_{e^{i\theta}} = \left\{ \alpha\begin{pmatrix}
			      1 \\ -i
		      \end{pmatrix} \right\};\quad E_{e^{-i\theta}} = \left\{ \beta\begin{pmatrix}
			      1 \\ i
		      \end{pmatrix} \right\} \]
	      which are two linearly independent complex eigenvectors. So,
	      \[ P = \begin{pmatrix}
			      1 & 1 \\ -i & i
		      \end{pmatrix};\quad P^{-1} = \frac{1}{2}\begin{pmatrix}
			      1 & i \\ 1 & -i
		      \end{pmatrix};\quad P^{-1}UP = \begin{pmatrix}
			      e^{i\theta} & 0 \\ 0 & e^{i\theta}
		      \end{pmatrix} \]
	      So $U$ is diagonalisable over $\mathbb C^2$ but not over $\mathbb R^2$.
\end{enumerate}

\subsection{Criteria for Diagonalisability}
\begin{proposition}
	Consider an $n \times n$ matrix $A$.
	\begin{enumerate}[(i)]
		\item $A$ is diagonalisable if it has $n$ distinct eigenvalues (sufficient condition).
		\item $A$ is diagonalisable if and only if for every eigenvalue $\lambda$, $M_\lambda = m_\lambda$ (necessary and sufficient condition).
	\end{enumerate}
\end{proposition}
\begin{proof}
	Use the proposition and corollary above.
	\begin{enumerate}[(i)]
		\item If we have $n$ distinct eigenvalues, then we have $n$ linearly independent eigenvectors. Hence they form a basis.
		\item If $\lambda_i$ are all the distinct eigenvalues, then $\mathcal B_{\lambda_1} \cup \dots \cup \mathcal B_{\lambda_r}$ are linearly independent. The number of elements in this new basis is $\sum_{i} m_{\lambda_i} = \sum_{i} M_{\lambda_i} = n$ which is the degree of the characteristic polynomial. So we have a basis.
	\end{enumerate}
	Note that case (i) is just a specialisation of case (ii) where both multiplicities are 1.
\end{proof}
Let us consider some examples.
\begin{enumerate}[(i)]
	\item Let
	      \[ A = \begin{pmatrix}
			      -2 & 2 & -3 \\ 2 & 1 & -6 \\ -1 & -2 & 0
		      \end{pmatrix} \implies \lambda = 5, -3;\quad M_5=m_5=1;\quad M_{-3}=m_{-3}=2 \]
	      So $A$ is diagonalisable by case (ii) above, and moreover
	      \[ P = \begin{pmatrix}
			      1  & -2 & 3 \\
			      2  & 1  & 0 \\
			      -1 & 0  & 1
		      \end{pmatrix};\quad P^{-1} = \frac{1}{8}\begin{pmatrix}
			      1  & 2 & -3 \\
			      -2 & 4 & 6  \\
			      1  & 2 & 5
		      \end{pmatrix} \implies P^{-1}AP = \begin{pmatrix}
			      5 & 0  & 0  \\
			      0 & -3 & 0  \\
			      0 & 0  & -3
		      \end{pmatrix} \]
	\item Let
	      \[ A = \begin{pmatrix}
			      -3 & -1 & 1 \\
			      -1 & -3 & 1 \\
			      -2 & 2  & 0
		      \end{pmatrix} \implies \lambda = -2;\quad M_{-2}=3 > m_{-2} = 2 \]
	      So $A$ is not diagonalisable. As a check, if it were diagonalisable, then there would be some matrix $P$ such that $P^{-1}AP = -2I \implies A = P(-2I)P^{-1} = -2I$ \contradiction.
\end{enumerate}

\subsection{Similarity}
Matrices $A$ and $B$ (both $n \times n$) are similar if $B = P^{-1}AP$ for some invertible $n\times n$ matrix $P$. This is an equivalence relation.
\begin{proposition}
	If $A$ and $B$ are similar, then
	\begin{enumerate}[(i)]
		\item $\tr B = \tr A$
		\item $\det B = \det A$
		\item $\chi_B = \chi_A$
	\end{enumerate}
\end{proposition}
\begin{proof}
	\begin{enumerate}[(i)]
		\item \begin{align*}
			      \tr B & = \tr (P^{-1}AP) \\&= \tr(APP^{-1}) \\&= \tr A
		      \end{align*}
		\item \begin{align*}
			      \det B & = \det (P^{-1}AP) \\&= \det P^{-1} \det A \det P \\&= \det A
		      \end{align*}
		\item \begin{align*}
			      \det(B - tI) & = \det(P^{-1}AP - tI) \\&= \det(P^{-1}AP - tP^{-1}P) \\&= \det(P^{-1}(A - tI)P) \\&= \det P^{-1} \det(A - tI) \det P \\&= \det(A - tI)
		      \end{align*}
	\end{enumerate}
\end{proof}

\section{Hermitian and Symmetric Matrices}
\subsection{Real Eigenvalues and Orthogonal Eigenvectors}
Recall that an $n\times n$ matrix $A$ is hermitian if and only if $A^\dagger = \overline{A}^\transpose = A$, or $\overline{A_{ij}} = A_{ji}$. If $A$ is real, then it is hermitian if and only if it is symmetric. The complex inner product for $\bm v, \bm w \in \mathbb C^n$ is $\bm v^\dagger \bm w = \sum_i \overline{v_i}w_i$, and for $\bm v, \bm w \in \mathbb R^n$, this reduces to the dot product in $\mathbb R^n$, $\bm v^\transpose \bm w$.

Here is a key observation. If $A$ is hermitian, then
\[ (A\bm v)^\dagger \bm w = \bm v^\dagger (A \bm w) \]
\begin{theorem}
	For an $n \times n$ matrix $A$ that is hermitian:
	\begin{enumerate}[(i)]
		\item Every eigenvalue $\lambda$ is real;
		\item Eigenvectors $\bm v, \bm w$ with different eigenvalues $\lambda, \mu$ respectively, are orthogonal, i.e. $\bm v^\dagger \bm w = 0$; and
		\item If $A$ is real and symmetric, then for each eigenvalue $\lambda$ we can choose a real eigenvector, and part (ii) becomes $\bm v \cdot \bm w = 0$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	\begin{enumerate}[(i)]
		\item Using the observation above with $\bm v = \bm w$ where $\bm v$ is any eigenvector with eigenvalue $\lambda$, we get
		      \begin{align*}
			      \bm v^\dagger (A\bm v)        & = (A\bm v)^\dagger \bm v                   \\
			      \bm v^\dagger (\lambda\bm v)  & = (\lambda\bm v)^\dagger \bm v             \\
			      \lambda \bm v^\dagger (\bm v) & = \overline{\lambda} (\bm v)^\dagger \bm v \\
			      \intertext{As $\bm v$ is an eigenvector, it is nonzero, so $\bm v^\dagger \bm v \neq 0$, so}
			      \lambda                       & = \overline \lambda
		      \end{align*}
		\item Using the same observation,
		      \begin{align*}
			      \bm v^\dagger (A \bm w)   & = (A \bm v)^\dagger \bm w       \\
			      \bm v^\dagger (\mu \bm w) & = (\lambda \bm v)^\dagger \bm w \\
			      \mu \bm v^\dagger \bm w   & = \lambda bm v^\dagger \bm w
		      \end{align*}
		      Since $\lambda \neq \mu$, $\bm v^\dagger \bm w = 0$, so the eigenvectors are orthogonal.
		\item Given $A\bm v = \lambda \bm v$ with $\bm v \in \mathbb C^n$ but $A$ is real, let
		      \[ \bm v = \bm u + i\bm u';\quad \bm u, \bm u' \in \mathbb R^n \]
		      Since $\bm v$ is an eigenvector, and this is a linear equation, we have
		      \[ A\bm u = \lambda \bm u;\quad A\bm u' = \lambda \bm u' \]
		      So $\bm u$ and $\bm u'$ are eigenvectors. $\bm v \neq 0$ implies that at least one of $\bm u$ and $\bm u'$ are nonzero, so there is at least one real eigenvector with this eigenvalue.
	\end{enumerate}
\end{proof}
Case (ii) is a stronger claim for hermitian matrices than just showing that eigenvectors are linearly independent. Furthermore, previously we considered bases $\mathcal B_\lambda$ for each eigenspace $E_\lambda$, and it is now natural to choose bases $\mathcal B_\lambda$ to be orthonormal when we are considering hermitian matrices. Here are some examples.
\begin{enumerate}[(i)]
	\item Let
	      \[ A = \begin{pmatrix}
			      2 & i \\ -i & 2
		      \end{pmatrix};\quad A^\dagger = A;\quad \lambda = 1, 3;\quad\bm u_1 = \frac{1}{\sqrt{2}} \begin{pmatrix}
			      1 \\i
		      \end{pmatrix};\quad\bm u_2 = \frac{1}{\sqrt{2}} \begin{pmatrix}
			      1 \\-i
		      \end{pmatrix} \]
	      We have chosen coefficients for the vectors $\bm u_1$ and $\bm u_2$ such that they are unit vectors. As shown above, they are then orthonormal. We know that having distinct eigenvalues means that a matrix is diagonalisable. So let us set
	      \[ P =  \frac{1}{\sqrt{2}} \begin{pmatrix}
			      1 & 1 \\ i & -i
		      \end{pmatrix} \implies P^{-1}AP = D = \begin{pmatrix}
			      1 & 0 \\ 0 & 3
		      \end{pmatrix} \]
	      Since the eigenvectors are orthonormal, so are the columns of $P$, so $P^{-1} = P^\dagger$ (i.e. $P$ is unitary).
	\item Let
	      \[ A = \begin{pmatrix}
			      0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0
		      \end{pmatrix} \]
	      $A$ is real and symmetric, with eigenvalues $\lambda = -1, 2$ with $M_{-1} = 2$, $M_2 = 1$. Further,
	      \[ E_{-1} = \vecspan \{ \bm w_1, \bm w_2 \};\quad \bm w_1 = \begin{pmatrix}
			      1 \\ -1 \\ 0
		      \end{pmatrix};\quad \bm w_2 = \begin{pmatrix}
			      1 \\ 0 \\ -1
		      \end{pmatrix} \]
	      So $m_{-1} = 2$, and the matrix is diagonalisable. Let us choose an orthonormal basis for $E_{-1}$ by taking
	      \[ \bm u_1 = \frac{1}{\abs{\bm w_1}}\bm w_1 = \frac{1}{\sqrt 2}\begin{pmatrix}
			      1 \\ -1 \\ 0
		      \end{pmatrix} \]
	      and we can consider
	      \[ \bm w_2' = \bm w_2 - (\bm u_1 \cdot \bm w_2)\bm u_1 = \begin{pmatrix}
			      1/2 \\ 1/2 \\ -1
		      \end{pmatrix} \]
	      so that $\bm w_2'$ is orthogonal to $\bm u_1$ by construction. We can then normalise this vector to get
	      \[ \bm u_2 = \frac{1}{\abs{\bm w_2'}}\bm w_2' = \frac{1}{\sqrt 6} \begin{pmatrix}
			      1 \\ 1 \\ -2
		      \end{pmatrix} \]
	      and therefore
	      \[ \mathcal B_{-1} = \{ \bm u_1, \bm u_2 \} \]
	      is an orthonormal basis. For $E_2$, let us choose $\mathcal B_2 = \{ \bm u_3 \}$ where
	      \[ \bm u_3 = \frac{1}{\sqrt 3}\begin{pmatrix}
			      1 \\ 1 \\ 1
		      \end{pmatrix} \]
	      Together,
	      \[ \mathcal B = \left\{ \frac{1}{\sqrt 2}\begin{pmatrix}
			      1 \\ -1 \\ 0
		      \end{pmatrix}, \frac{1}{\sqrt 6} \begin{pmatrix}
			      1 \\ 1 \\ -2
		      \end{pmatrix}, \frac{1}{\sqrt 3}\begin{pmatrix}
			      1 \\ 1 \\ 1
		      \end{pmatrix} \right\} \]
	      is an orthonormal basis for $\mathbb R^3$. Let $P$ be the matrix with columns $\bm u_1, \bm u_2, \bm u_3$, then $P^{-1}AP = D$ as required. Since we have chosen an orthonormal basis, $P$ is orthogonal, so $P^\transpose AP = D$.
\end{enumerate}

\subsection{Unitary and Orthogonal Diagonalisation}
\begin{theorem}
	Any $n\times n$ hermitian matrix $A$ is diagonalisable.
	\begin{enumerate}[(i)]
		\item There exists a basis of eigenvectors $\bm u_1, \dots, \bm u_n \in \mathbb C^n$ with $A\bm u_i = \lambda \bm u_i$; equivalently
		\item There exists an $n \times n$ invertible matrix $P$ with $P^{-1}AP = D$ where $D$ is the matrix with eigenvalues on the diagonal, where the columns of $P$ are the eigenvectors $\bm u_i$.
	\end{enumerate}
	In addition, the eigenvectors $\bm u_i$ can be chosen to be orthonormal, so
	\[ \bm u^\dagger_i \bm u_j = \delta_{ij} \]
	or equivalently, the matrix $P$ can be chosen to be unitary,
	\[ P^\dagger = P^{-1} \implies P^\dagger AP = D \]
	In the special case that the matrix $A$ is real, the eigenvectors can be chosen to be real, and so
	\[ \bm u^\transpose \bm u_j = \bm u_i \cdot \bm u_j = \delta_{ij} \]
	so $P$ is orthogonal, so
	\[ P^\transpose = P^{-1} \implies P^\transpose AP = D \]
\end{theorem}

\section{Quadratic Forms}
\subsection{Simple Example}
Consider a function $\mathcal F\colon \mathbb R^2 \to \mathbb R$ defined by
\[ \mathcal F(\bm x) = 2x_1^2 - 4x_1x_2 + 5x_2^2 \]
This can be simplified by writing
\[ \mathcal F(\bm x) = x_1'^2 + 6x_2'^2 \]
where
\[ x_1' = \frac{1}{\sqrt 5}(2x_1 + x_2);\quad x_2' = \frac{1}{\sqrt 5}(-x_1 + 2x_2) \]
This can be found by writing $\mathcal F(\bm x) = \bm x^\transpose A\bm x$ where
\[ A = \begin{pmatrix}
		2 & -2 \\ -2 & 5
	\end{pmatrix} \]
by inspection from the original equation, and then diagonalising $A$. We find the eigenvalues to be $\lambda = 1, 6$, with eigenvectors
\[ \frac{1}{\sqrt 5} \begin{pmatrix}
		2 \\ 1
	\end{pmatrix};\quad \frac{1}{\sqrt 5}\begin{pmatrix}
		-1 \\ 2
	\end{pmatrix} \]

\subsection{Diagonalising Quadratic Forms}
In general, a quadratic form is a function $\mathcal F\colon \mathbb R^n \to \mathbb R$ given by
\[ \mathcal F(\bm x) = \bm x^\transpose A \bm x \implies \mathcal F(\bm x)_{ij} = x_i A_{ij} x_j \]
where $A$ is a real symmetric $n \times n$ matrix. Any antisymmetric part of $A$ would not contribute to the result, so there is no loss of generality under this restriction. From the section above, we know we can write $P^\transpose A P = D$ where $D$ is a diagonal matrix containing the eigenvalues, and $P$ is constructed from the eigenvectors, with orthonormal columns $\bm u_i$. Setting $\bm x' = P^\transpose \bm x$, or equivalently $\bm x = P \bm x'$, we have
\begin{align*}
	\mathcal F(\bm x) & = \bm x^\transpose A \bm x                                              \\
	                  & = (P \bm x')^\transpose A (P \bm x')                                    \\
	                  & = (\bm x')^\transpose P^\transpose A P \bm x'                           \\
	                  & = (\bm x')^\transpose D \bm x'                                          \\
	                  & = \sum_i \lambda_i x_i'^2 = \lambda_1 x_1'^2 + \lambda_2 x_2'^2 + \dots
\end{align*}
We say that $\mathcal F$ has been diagonalised. Now, note that
\begin{align*}
	\bm x' & = x_1'\bm e_1 + \dots + x_n'\bm e_n \\
	\bm x  & = x_1\bm e_1 + \dots + x_n\bm e_n   \\
	       & = x_1'\bm u_1 + \dots + x_n'\bm u_n
\end{align*}
where the $\bm e_i$ are the standard basis vectors, since
\[ \bm x_i' = \bm u_i \cdot \bm x \iff \bm x' = P^\transpose \bm x \]
Hence the $\bm x_i'$ can be regarded as coordinates with respect to a new set of axes defined by the orthonormal eigenvector basis, known as the principal axes of the quadratic form. They are related to the standard axes (given by basis vectors $\bm e_i$) by the orthogonal transformation $P$.

\subsection{Example in $\mathbb R^2$}
Consider $\mathcal F(\bm x) = \bm x^\transpose A \bm x$ with
\[ A = \begin{pmatrix}
		\alpha & \beta \\ \beta & \alpha
	\end{pmatrix} \]
The eigenvalues are $\lambda = \alpha + \beta, \alpha - \beta$ and
\[ \bm u_1 = \frac{1}{\sqrt 2}\begin{pmatrix}
		1 \\ 1
	\end{pmatrix};\quad \bm u_2 = \frac{1}{\sqrt 2}\begin{pmatrix}
		-1 \\ 1
	\end{pmatrix} \]
So in terms of the standard basis vectors,
\[ \mathcal F(\bm x) = \alpha x_1^2 + 2\beta x_1x_2 + \alpha x_2^2 \]
And in terms of our new basis vectors,
\[ \mathcal F(\bm x) = (\alpha + \beta) x_1'^2 + (\alpha - \beta) x_2'^2 \]
where
\begin{align*}
	\bm x_1' & = \bm u_1 \cdot \bm x = \frac{1}{\sqrt 2}(x_1 + x_2)  \\
	\bm x_2' & = \bm u_2 \cdot \bm x = \frac{1}{\sqrt 2}(-x_1 + x_2) \\
\end{align*}
Taking for example $\alpha = \frac{3}{2}, \beta = \frac{-1}{2}$, we have $\lambda_1 = 1, \lambda_2 = 2$. If we choose $\mathcal F = 1$, this represents an ellipse in our new coordinate system:
\[ x_1'^2 + 2x_2'^2 = 1 \]
If instead we chose $\alpha = \frac{-1}{2}, \beta = \frac{3}{2}$. We now have $\lambda_1 = 1, \lambda_2 = -2$. The locus at $\mathcal F = 1$ gives a hyperbola:
\[ x_1'^2 - 2x_2'^2 = 1 \]

\subsection{Example in $\mathbb R^3$}
In $\mathbb R^3$, note that if $\lambda_1, \lambda_2, \lambda_3$ are all strictly positive, then $\mathcal F = 1$ gives an ellipsoid. This is analogous to the $\mathbb R^2$ case above.

Let us consider an example. Earlier, we found that the eigenvalues of the matrix $A$ where
\[ A = \begin{pmatrix}
		0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0
	\end{pmatrix} \]
are $\lambda_1 = \lambda_2 = -1, \lambda_3 = 2$, where
\[ \bm u_1 = \frac{1}{\sqrt 2} \begin{pmatrix}
		1 \\ -1 \\ 0
	\end{pmatrix};\quad \bm u_2 = \frac{1}{\sqrt 6}\begin{pmatrix}
		1 \\ 1 \\ -2
	\end{pmatrix};\quad \bm u_3 = \frac{1}{\sqrt 3}\begin{pmatrix}
		1 \\ 1 \\ 1
	\end{pmatrix} \]
Then
\begin{align*}
	\mathcal F(\bm x) & = 2x_1x_2 + 2x_2x_3 + 2x_3x_1 \\
	                  & = -x_1'^2 -x_2'^2 + 2x_3'^2
\end{align*}
Now, $\mathcal F = 1$ corresponds to
\[ 2x_3'^2 = 1 + x_1'^2 + x_2'^2 \]
So we can more clearly see that this is a hyperboloid of two sheets in $\mathbb R^3$ with rotational symmetry between the $x_1$ and $x_2$ axes. Further, $\mathcal F = -1$ corresponds to
\[ 1 + 2x_3'^2 = x_1'^2 + x_2'^2 \]
Here, this is a hyperboloid of one sheet since for any fixed $x_3$ coordinate, it defines a circle in the $x_1$ and $x_2$ axes.

\subsection{Hessian Matrix as a Quadratic Form}
Consider a smooth function $f\colon \mathbb R^n \to \mathbb R$ with a stationary point at $\bm x = \bm a$, i.e. $\frac{\partial f}{\partial x_i} = 0$ at $\bm x = \bm a$. By Taylor's theorem,
\[ f(\bm a + \bm h) + f(\bm a) + \mathcal F(\bm h) + O(\abs{\bm h}^3) \]
where $\mathcal F$ is a quadratic form with
\[ A_{ij} = \frac{1}{2}\frac{\partial^2 f}{\partial x_i\partial x_j} \]
all evaluated at $\bm x = \bm a$. Note that this $A$ is the Hessian matrix, and that the linear term vanishes since we are at a stationary point. Rewriting this $\bm h$ in terms of the eigenvectors of $A$ (the principal axes), we have
\[ \mathcal F = \lambda_1 h_1'^2 + \lambda_2 h_2'^2 + \dots + \lambda_n h_n'^2 \]
So clearly if $\lambda_i > 0$ for all $i$, then $f$ has a minimum at $\bm x = \bm a$. If $\lambda_i < 0$ for all $I$, then $f$ has a maximum at $\bm x = \bm a$. Otherwise, it has a saddle point. Note that it is often sufficient to consider the trace and determinant of $A$, since $\trace A = \lambda_1 + \lambda_2$ and $\det A = \lambda_1\lambda_2$.

\section{Cayley-Hamilton Theorem and Changing Bases}
\subsection{Matrix Polynomials}
If $A$ is an $n \times n$ complex matrix and
\[ p(t) = c_0 + c_1t + c_2^2 + \dots + c_kt^k \]
is a polynomial, then
\[ p(A) = c_0I + c_1A + c_2A^2 + \dots + c_kA^k \]
We can also define power series on matrices (subject to convergence). For example, the exponential series which always converges:
\[ \exp(A) = I + A + \frac{1}{2}A^2 + \dots + \frac{1}{r!}A^r + \dots \]
For a diagonal matrix, polynomials and power series can be computed easily since the power of a diagonal matrix just involves raising its diagonal elements to said power. Therefore,
\[ D = \begin{pmatrix}
		\lambda_1 & 0         & \cdots & 0         \\
		0         & \lambda_2 & \cdots & 0         \\
		\vdots    & \vdots    & \ddots & \vdots    \\
		0         & 0         & \cdots & \lambda_n
	\end{pmatrix} \implies p(D) = \begin{pmatrix}
		p(\lambda_1) & 0            & \cdots & 0            \\
		0            & p(\lambda_2) & \cdots & 0            \\
		\vdots       & \vdots       & \ddots & \vdots       \\
		0            & 0            & \cdots & p(\lambda_n)
	\end{pmatrix} \]
Therefore,
\[ \exp(D) = \begin{pmatrix}
		e^{\lambda_1} & 0             & \cdots & 0             \\
		0             & e^{\lambda_2} & \cdots & 0             \\
		\vdots        & \vdots        & \ddots & \vdots        \\
		0             & 0             & \cdots & e^{\lambda_n}
	\end{pmatrix} \]
If $B = P^{-1}AP$ (similar to $A$) where $P$ is an $n \times n$ invertible matrix, then
\[ B^r = P^{-1}A^rP \]
Therefore,
\[ p(B) = p(P^{-1}AP) = P^{-1}p(A)P \]
Of special interest is the characteristic polynomial,
\[ \chi_A(t) = \det(A - tI) = c_0 + c_1t + c_2t^2 + \dots + c_nt^n \]
where $c_0 = \det A$, and $c_n = (-1)^n$.
\begin{theorem}[Cayley-Hamilton Theorem]
	\[ \chi_A(A) = c_0I + c_1A + c_2A^2 + \dots + c_nA^n = 0 \]
	Less formally, a matrix satisfies its own characteristic equation.
\end{theorem}
\begin{remark}
	We can find an expression for the matrix inverse.
	\[ -c_0I = A(c_1 + c_2A + \dots + c_nA^{n-1}) \]
	If $c_0 = \det A \neq 0$, then
	\[ A^{-1} = \frac{-1}{c_0}(c_1 + c_2A + \dots + c_nA^{n-1}) \]
\end{remark}

\subsection{Proofs of Special Cases of Cayley-Hamilton Theorem}
\begin{proof}[Proof for a $2\times 2$ matrix]
	Let $A$ be a general $2\times 2$ matrix.
	\[ A = \begin{pmatrix}
			a & b \\ c & d
		\end{pmatrix} \implies \chi_A(t) = t^2 - (a+d)t + (ad-bc) \]
	We can check the theorem by substitution.
	\[ \chi_A(A) = A^2 - (a+d)A - (ad-bc)I \]
	This is shown on the last example sheet.
\end{proof}
\begin{proof}[Proof for diagonalisable $n \times n$ matrices]
	Consider $A$ with eigenvalues $\lambda_i$, and an invertible matrix $P$ such that $P^{-1}AP = D$, where $D$ is diagonal.
	\[ \chi_A(D) = \begin{pmatrix}
			\chi_A(\lambda_1) & 0                 & \cdots & 0                 \\
			0                 & \chi_A(\lambda_2) & \cdots & 0                 \\
			\vdots            & \vdots            & \ddots & \vdots            \\
			0                 & 0                 & \cdots & \chi_A(\lambda_n)
		\end{pmatrix} = 0 \]
	since the $\lambda_i$ are eigenvalues. Then
	\[ \chi_A(A) = \chi_A(PDP^{-1}) = P\chi_A(D)P^{-1} = 0 \]
\end{proof}

\subsection{Proof in General Case (non-examinable)}
\begin{proof}
	Let $M = A - tI$. Then $\det M = \det(A - tI) = \chi_A(t) = \sum_{r=0}c_rt^r$. We can construct the adjugate matrix.
	\[ \adjugate M = \sum_{r=0}^{n-1} B_rt^r \]
	Therefore,
	\begin{align*}
		\adjugate M M = (\det M) I & = \left(\sum_{r=0}^{n-1} B_rt^r\right)(A-tI)                                              \\
		                           & = B_0A + (B_1A - B_0)t + (B_2A - B_1)t^2 + \dots + (B_{n-1}A - B_{n-2})t^{n-1} - B_{n-1}t
	\end{align*}
	Now by comparing coefficients,
	\begin{align*}
		C_0I     & = B_0A               \\
		C_1I     & = B_1A - B_0         \\
		\vdots                          \\
		C_{n-1}I & = B_{n-1}A - B_{n-2} \\
		C_nI     & = -B_{n-1}
	\end{align*}
	Summing all of these coefficients, multiplying by the relevant powers,
	\begin{align*}
		 & C_0I + C_1A + C_2A^2 + \dots + C_nA^n \\=\ &B_0A + (B_1A^2 - B_0A) + (B_2A^3 - B_1A^2) + \dots + (B_{n-1}A^n - B_{n-2}A^{n-1}) - B_{n-1}A^n \\=\ &0
	\end{align*}
\end{proof}

\subsection{Changing Bases in General}
Recall that given a linear map $T\colon V \to W$ where $V$ and $W$ are real or complex vector spaces, and choices of bases $\{ \bm e_i \}$ for $i = 1, \dots, n$ and $\{\bm f_a \}$ for $a = 1, \dots, m$, then the $m \times n$ matrix $A$ with respect to these bases is defined by
\[ T(\bm e_i) = \sum_a \bm f_a A_{ai} \]
So the entries in column $i$ of $A$ are the components of $T(\bm e_i)$ with respect to the basis $\{ \bm f_a \}$. This is chosen to ensure that the statement $\bm y = T(\bm x)$ is equivalent to the statement that $y_a = A_{ai}x_i$, where $\bm y = \sum_a y_a \bm f_a$ and $\bm x = \sum_i x_i \bm e_i$. This equivalence holds since
\[ T\left(\sum_i x_i \bm e_i \right) = \sum_i x_i T(\bm e_i) = \sum_i x_i \left( \sum_a \bm f_a A_{ai} \right) = \sum_a \underbrace{\left( \sum_i A_{ai} x_i \right)}_{y_a} \bm f_a \]
as required. For the same linear map $T$, there is a different matrix representation $A'$ with respect to different bases $\{ \bm e'_i \}$ and $\{ \bm f'_a \}$. To relate $A$ with $A'$, we need to understand how the new bases relate to the original bases. The change of base matrices $P$ ($n \times n$) and $Q$ ($m \times m$) are defined by
\[ \bm e_i' = \sum_j \bm e_j P_{ji};\quad \bm f_a' = \sum_b \bm f_b Q_{ba} \]
The entries in column $i$ of $P$ are the components of the new basis $\bm e_i'$ in terms of the old basis vectors $\{ \bm e_j \}$, and similarly for $Q$. Note, $P$ and $Q$ are invertible, and in the relation above we could exchange the roles of $\{ \bm e_i \}$ and $\{ \bm e'_i \}$ by replacing $P$ with $P^{-1}$, and similarly for $Q$.

\begin{proposition}[Change of base formula for a linear map]
	With the definitions above,
	\[ A' = Q^{-1}AP \]
\end{proposition}
\noindent First we will consider an example, then we will construct a proof. Let $n=2, m=3$, and
\begin{align*}
	T(\bm e_1) & = \bm f_1 + 2\bm f_2 - \bm f_3 = \sum_a \bm f_a A_{a1}  \\
	T(\bm e_2) & = -\bm f_1 + 2\bm f_2 + \bm f_3 = \sum_a \bm f_a A_{a2}
\end{align*}
Therefore,
\[ A = \begin{pmatrix}
		1 & -1 \\ 2 & 2 \\ -1 & 1
	\end{pmatrix} \]
Consider a new basis for $V$, given by
\begin{align*}
	\bm e'_1 & = \bm e_1 - \bm e_2 = \sum_i \bm e_i P_{i1} \\
	\bm e'_2 & = \bm e_1 + \bm e_2 = \sum_i \bm e_i P_{i2}
\end{align*}
\[ P = \begin{pmatrix}
		1 & 1 \\ -1 & 1
	\end{pmatrix} \]
Consider further a new basis for $W$, given by
\begin{align*}
	\bm f'_1 & = \bm f_1 - \bm f_3 = \sum_a \bm f_a Q_{a1} \\
	\bm f'_2 & = \bm f_2 = \sum_a \bm f_a Q_{a2}           \\
	\bm f'_3 & = \bm f_1 + \bm f_3 = \sum_a \bm f_a Q_{a3}
\end{align*}
\[ Q = \begin{pmatrix}
		1  & 0 & 1 \\
		0  & 1 & 0 \\
		-1 & 0 & 1
	\end{pmatrix} \]
From the change of base formula,
\begin{align*}
	A' & = Q^{-1}AP                                                                          \\
	   & = \begin{pmatrix}
		1/2 & 0 & -1/2 \\
		0   & 1 & 0    \\
		1/2 & 0 & 1/2
	\end{pmatrix}\begin{pmatrix}
		1 & -1 \\ 2 & 2 \\ -1 & 1
	\end{pmatrix}\begin{pmatrix}
		1 & 1 \\ -1 & 1
	\end{pmatrix} \\
	   & = \begin{pmatrix}
		2 & 0 \\ 0 & 4 \\ 0 & 0
	\end{pmatrix}
\end{align*}
Now checking this result directly,
\begin{align*}
	T(\bm e'_1) & = 2\bm f_1 - 2\bm f_3 = 2\bm f_1' \\
	T(\bm e'_2) & = 4\bm f_2 = 4\bm f_2'
\end{align*}
which matches the content of the matrix as required. Now, let us prove the proposition in general.
\begin{proof}
	\begin{align*}
		T(\bm e'_i) & = T\left( \sum_j \bm e_j P_{ji} \right)              \\
		            & = \sum_j T(\bm e_j) P_{ji}                           \\
		            & = \sum_j \left( \sum_a \bm f_a A_{aj} \right) P_{ji} \\
		            & = \sum_{ja} \bm f_a A_{aj} P_{ji}
	\end{align*}
	But on the other hand,
	\begin{align*}
		T(\bm e'_i) & = \sum_b \bm f'_b A'_{bi}                             \\
		            & = \sum_b \left( \sum_a \bm f_a Q_{ab} \right) A'_{bi} \\
		            & = \sum_{ab} \bm f_a Q_{ab} A'_{bi}
	\end{align*}
	which is a sum over the same set of basis vectors, so we may equate coefficients of $\bm f_a$.
	\begin{align*}
		\sum_j A_{aj} P_{ji} & = \sum_b Q_{ab} A'_{bi} \\
		(AP)_{ai}            & = (QA')_{ai}
	\end{align*}
	Therefore
	\[ AP = QA' \implies A' = Q^{-1}AP \]
	as required.
\end{proof}

\section{???}
\subsection{Changing Bases of Vector Components}
Here is another way to arrive at the formula $A' = Q^{-1}AP$. Consider changes in vector components
\begin{align*}
	\bm x        & = \sum_i x_i \bm e_i = \sum_j x_j' \bm e'_j       \\
	             & = \sum_i\left( \sum_j P_{ij} x'_j \right) \bm e_i \\
	\implies x_i & = P_{ij} x'_j
\end{align*}
We will write
\[ X = \begin{pmatrix}
		x_1 \\ \vdots \\ x_n
	\end{pmatrix};\quad X' = \begin{pmatrix}
		x'_1 \\ \vdots \\ x'_n
	\end{pmatrix} \]
Then $X = PX'$ or $X' = P^{-1}X$. Similarly,
\begin{align*}
	\bm y        & = \sum_a y_a \bm f_a = \sum_b y_b' \bm f'_b \\
	\implies y_a & = Q_{ab} y'_b
\end{align*}
Then $Y = QY'$ or $Y' = Q^{-1}Y$. So the matrices are defined to ensure that
\[ Y = AX;\quad Y' = A'X' \]
Therefore,
\[ QY' = APX' \implies Y' = (Q^{-1}AP)X' \implies A' = Q^{-1}AP \]

\subsection{Specialisations of Changes of Basis}
Now, let us consider some special cases (in increasing order of specialisation).
\begin{enumerate}[(i)]
	\item Let $V=W$ with $\bm e_i = \bm f_i$ and $\bm e'_i = \bm f'_i$. So $P=Q$ and the change of basis is
	      \[ A' = P^{-1}AP \]
	      Matrices representing the same linear map but with respect to different bases are similar. Conversely, if $A, A'$ are similar, then we can construct an invertible change of basis matrix $P$ which relates them, so they can be regarded as representing the same linear map. In an earlier section we noted that $\tr(A') = \tr(A)$, $\det(A') = \det(A)$ and $\chi_A(t) = \chi_{A'}(t)$. so these are intrinsic properties of the linear map, not just the particular matrix we choose to represent it.
	\item Let $V=W=\mathbb R^n$ or $\mathbb C^n$ where $\bm e_i$ is the standard basis, with respect to which, $T$ has matrix $A$. If there exists a basis of eigenvectors, $\bm e'_i = \bm v_i$ with $A\bm v_i = \lambda_i\bm v_i$. Then
	      \[ A' = P^{-1}AP = D = \begin{pmatrix}
			      \lambda_1 & 0         & \cdots & 0         \\
			      0         & \lambda_2 & \cdots & 0         \\
			      \vdots    & \vdots    & \ddots & \vdots    \\
			      0         & 0         & \cdots & \lambda_n
		      \end{pmatrix} \]
	      and
	      \[ \bm v_i = \sum_k \bm e_j P_{ji} \]
	      so the eigenvectors are the columns of $P$.
	\item Let $A$ be hermitian, i.e. $A^\dagger = A$, then we always have a basis of orthonormal eigenvectors $\bm e'_i = \bm u_i$. Then the relations in (ii) apply, and $P$ is unitary, $P^\dagger = P^{-1}$.
\end{enumerate}

\subsection{Jordan Normal Form}
Also known as the (Jordan) Canonical Form, this result classifies $n\times n$ complex matrices up to similarity.
\begin{proposition}
	Any $2\times 2$ complex matrix $A$ is similar to one of the following:
	\begin{enumerate}[(i)]
		\item $A' = \begin{pmatrix}
				      \lambda_1 & 0         \\
				      0         & \lambda_2
			      \end{pmatrix}$ with $\lambda_1 \neq \lambda_2$, so $\chi_A(t) = (t - \lambda_1)(t - \lambda_2)$.
		\item $A' = \begin{pmatrix}
				      \lambda & 0       \\
				      0       & \lambda
			      \end{pmatrix}$, so $\chi_A(t) = (t - \lambda)^2$.
		\item $A' = \begin{pmatrix}
				      \lambda & 1       \\
				      0       & \lambda
			      \end{pmatrix}$, so $\chi_A(t) = (t - \lambda)^2$ as in case (ii).
	\end{enumerate}
\end{proposition}
\begin{proof}
	$\chi_A(t)$ has two roots over $\mathbb C$.
	\begin{enumerate}[(i)]
		\item For distinct roots $\lambda_1, \lambda_2$, we have $M_{\lambda_1} = m_{\lambda_1} = M_{\lambda_2} = m_{\lambda_2} = 1$. So the eigenvectors $\bm v_1, \bm v_2$ provide a basis. Hence $A' = P^{-1}AP$ with the eigenvectors as the columns of $P$.
		\item For a repeated root $\lambda$ with $M_\lambda = m_\lambda = 2$, the same argument applies.
		\item For a repeated root $\lambda$ with $M_\lambda = 2$, $m_\lambda = 1$, we do not have a basis of eigenvectors so we cannot diagonalise the matrix. We only have one linearly independent eigenvector, which we will call $\bm v$. Let $\bm w$ be any other vector such that $\{ \bm v, \bm w \}$ are linearly indepdendent. Then
		      \begin{align*}
			      A\bm v & = \lambda \bm v              \\
			      A\bm w & = \alpha \bm v + \beta \bm w
		      \end{align*}
		      The matrix representing this linear map with respect to the basis vectors $\{ \bm v, \bm w \}$ is therefore
		      \[ \begin{pmatrix}
				      \lambda & \alpha \\
				      0       & \beta
			      \end{pmatrix} \]
		      Let us solve for some of these unknowns. We know that the characteristic polynomial of this matrix must be $(t - \lambda)^2$, so $\beta = \lambda$. Also, $\alpha \neq 0$, otherwise we have case (ii) above. So now we can set $\bm u = \alpha \bm v$, so
		      \begin{align*}
			      A(\alpha \bm v) & = \lambda (\alpha \bm v)     \\
			      A\bm w          & = \alpha \bm v + \beta \bm w
		      \end{align*}
		      So with respect to the basis $\{ \bm u, \bm w \}$ we get the matrix $A$ to be
		      \[ A' = \begin{pmatrix}
				      \lambda & 1       \\
				      0       & \lambda
			      \end{pmatrix} \]
	\end{enumerate}
\end{proof}
\begin{proof}[Alternative Proof]
	Here is an alternative appproach for case (iii). If $A$ has characteristic polynomial
	\[ \chi_A(t) = (t - \lambda)^2 \]
	but $A \neq \lambda I$, then there exists some vector $\bm w$ for which $\bm u = (A - \lambda I)\bm w \neq \bm 0$. So $(A - \lambda I)\bm u = (A - \lambda I)^2 \bm w  = \bm 0$ by the Cayley-Hamilton theorem. So
	\begin{align*}
		A\bm u & = \lambda \bm u         \\
		A\bm w & = \bm u + \lambda \bm w
	\end{align*}
	So with basis $\{ \bm u, \bm w \}$ we have the matrix
	\[ A' = \begin{pmatrix}
			\lambda & 1       \\
			0       & \lambda
		\end{pmatrix} \]
\end{proof}
Here is a concrete example using this alternative proof method.
\[ A = \begin{pmatrix}
		1 & 4 \\ -1 & 5
	\end{pmatrix} \implies \chi_A(t) = (t - 3)^2 \]
So
\[ A - 3I = \begin{pmatrix}
		-2 & 4 \\ -1 & 2
	\end{pmatrix} \]
We will choose $\bm w = \begin{pmatrix}
		1 \\ 0
	\end{pmatrix}$ and we find $\bm u = (A - 3I)\bm w = \begin{pmatrix}
		-2 \\ -1
	\end{pmatrix}$. $\bm w$ is not an eigenvector, as required for the construction. By the reasoning in the alternative argument above, $\bm u$ is an eigenvector by construction.
\begin{align*}
	A\bm u & = 3\bm u         \\
	A\bm w & = \bm u + 3\bm w
\end{align*}
So
\[ P = \begin{pmatrix}
		-2 & 1 \\ -1 & 0
	\end{pmatrix} \implies P^{-1} = \begin{pmatrix}
		0 & -1 \\ 1 & -2
	\end{pmatrix} \]
and we can check that
\[ P^{-1}AP = \begin{pmatrix}
		3 & 1 \\ 0 & 3
	\end{pmatrix} = A' \]

\subsection{Jordan Normal Forms in $n$ Dimensions}
To extend the arguments above to larger matrices, consider the $n\times n$ matrix
\[ N = \begin{pmatrix}
		0      & 1      & 0      & \cdots & 0      \\
		0      & 0      & 1      & \cdots & 0      \\
		0      & 0      & 0      & \cdots & 0      \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		0      & 0      & 0      & \cdots & 0
	\end{pmatrix} \]
When applied to the standard basis vectors in $\mathbb C^n$, the action of this matrix sends $\bm e_n \mapsto \bm e_{n-1} \mapsto \dots \mapsto \bm e_1 \mapsto \bm 0$. This is consistent with the property that $N^n = 0$. The kernel of this matrix has dimension 1. Now consider the matrix $J = \lambda I + N$, as follows:
\[ N = \begin{pmatrix}
		\lambda & 1       & 0       & \cdots & 0       \\
		0       & \lambda & 1       & \cdots & 0       \\
		0       & 0       & \lambda & \cdots & 0       \\
		\vdots  & \vdots  & \vdots  & \ddots & \vdots  \\
		0       & 0       & 0       & \cdots & \lambda
	\end{pmatrix} \]
This matrix has
\[ \chi_J(t) = (\lambda - t)^n \]
with $M_\lambda = n$ and $m_\lambda = 1$, since the kernel of $J - \lambda I = N$ has dimension 1 as before. The general result is as follows.
\begin{theorem}
	Any $n\times n$ complex matrix $A$ is similar to a matrix of the form
	\[ A' = \left( \begin{array}{c|c|c|c}
				J_{n_1}(\lambda_1) & 0                  & \cdots & 0                  \\\hline
				0                  & J_{n_2}(\lambda_2) & \cdots & 0                  \\\hline
				\vdots             & \vdots             & \ddots & \vdots             \\\hline
				0                  & 0                  & \cdots & J_{n_r}(\lambda_r)
			\end{array} \right) \]
	where each diagonal block is a Jordan block $J_{n_r}(\lambda_r)$ which is an $n_r \times n_r$ matrix $J$ with eigenvalue $\lambda_r$. $\lambda_1, \dots, \lambda_r$ are eigenvalues of $A$ and $A'$, and the same eigenvalue may appear in different blocks. Further, $n_1 + n_2 + \dots + n_r = n$ so we end up with an $n \times n$ matrix. $A$ is diagonalisable if and only if $A'$ consists entirely of $1 \times 1$ blocks. The expression above is the Jordan Normal Form.
\end{theorem}
The proof is non-examinable and depends on the Part IB courses Linear Algebra, and Groups, Rings and Modules, so is not included here.

\section{Conics and Quadrics}
\subsection{Quadrics in General}
A quadric in $\mathbb R^n$ is a hypersurface defined by an equation of the form
\[ Q(\bm x) = \bm x^\transpose A \bm x + \bm b^\transpose \bm x + c = 0 \]
for some nonzero, symmetric, real $n \times n$ matrix $A$, $b \in \mathbb R^n$, $c \in \mathbb R$. In components,
\[ Q(\bm x) = A_{ij}x_ix_j + b_ix_i + c = 0 \]
We will clasify solutions for $\bm x$ up to geometrical equivalence, so we will not distinguish between solutions here which are related by isometries in $\mathbb R^n$ (distance-preserving maps, i.e. translations and orthogonal transformations about the origin).

Note that $A$ is invertible if and only if it has no zero eigenvalues. In this case, we can complete the sequare in the equation $Q(\bm x) = 0$ by setting $\bm y = \bm x + \frac{1}{2}A^{-1} \bm b$. This is essentially a translation isometry, moving the origin to $\frac{1}{2}A^{-1} \bm b$.
\begin{align*}
	\bm y^\transpose A \bm y & = (\bm x + \frac{1}{2}A^{-1}\bm b)^\transpose A (\bm x + \frac{1}{2}A^{-1}\bm b)                         \\
	                         & = (\bm x^\transpose + \frac{1}{2}\bm b^\transpose(A^{-1})^\transpose) A (\bm x + \frac{1}{2}A^{-1}\bm b) \\
	                         & = \bm x^\transpose A \bm x + \bm b^\transpose \bm x + \frac{1}{4}\bm b^\transpose A^{-1}\bm b
\end{align*}
since $(A^\transpose)^{-1} = (A^{-1})^\transpose$. Then,
\[ Q(\bm x) = 0 \iff \mathcal F(\bm y) = k \]
with
\[ \mathcal F(\bm y) = \bm y^\transpose A \bm y \]
which is a quadratic form with respect to a new origin $\bm y = \bm 0$, and where $k = \frac{1}{4}\bm b^\transpose A^{-1}\bm b - c$. Now we can diagonalise $\mathcal F$ as in the above section, in particular, orthonormal eigenvectors give the principal axes, and the eigenvalues of $A$ and the value of $k$ determine the geometrical nature of the solution of the quadric. In $\mathbb R^3$, the geometrical possibilities are (as we saw before):
\begin{enumerate}[(i)]
	\item eigenvalues positive, $k$ positive gives an ellipsoid;
	\item eigenvalues different signs, $k$ nonzero gives a hyperboloid
\end{enumerate}
If $A$ has one or more zero eigenvalues, then the analysis we have just provided changes, since we can no longer construct such a $\bm y$ vector, since $A^{-1}$ does not exist. The simplest standard form of $Q$ may have both linear and quadratic terms.

\subsection{Conics as Quadrics}
Quadrics in $\mathbb R^2$ are curves called conics. Let us first consider the case where $\det A \neq 0$. By completing the square and diagonalising $A$, we get a standard form
\[ \lambda_1 {x'_1}^2 + \lambda_2 {x'_2}^2 = k \]
The variables $x'_i$ correspond to the principal axes and the new origin. We have the following cases.
\begin{itemize}
	\item ($\lambda_1, \lambda_2 > 0$) This is an ellipse for $k>0$, and a point for $k=0$. There are no solutions for $k<0$.
	\item ($\lambda_1 > 0, \lambda_2 < 0$) This gives a hyperbola for $k>0$, and a hyperbola in the other axis if $k<0$. If $k=0$, this is a pair of lines. For instance, ${x'_1}^2 - {x'_2}^2 = 0 \implies (x'_1 - x'_2)(x'_1 + x'_2) = 0$.
\end{itemize}
If $\det A = 0$, then there is exactly one zero eigenvalue since $A \neq 0$. Then:
\begin{itemize}
	\item ($\lambda_1 > 0, \lambda_2 = 0$) We will diagonalise $A$ in the original expression for the quadric. This gives
	      \[ \lambda_1 {x'_1}^2 + b'_1 x'_1 + b'_2 x'_2 + c = 0 \]
	      This is a new equation in the coordinate system defined by $A$'s principal axes. Completing the square here in the $x'_1$ term, we have
	      \[ \lambda_1 {x''_1}^2 + b'_2x'_2 + c' = 0 \]
	      where $x''_1 = x'_1 + \frac{1}{2\lambda_1}b'_1$, and $c' = c - \frac{{b'_1}^2}{4\lambda_1^2}$. If $b'_2 = 0$, then $x_2$ can take any value; and we get a pair of lines if $c'<0$, a single line if $c'=0$, and no solutions if $c'>0$. Otherwise, $b'_2 \neq 0$, and the equation becomes
	      \[ \lambda_1 {x''_1}^2 + b'_2x''_2 = 0 \]
	      where $x_2'' = x'_2 + \frac{1}{b_2'}c'$, and clearly this equation is a parabola.
\end{itemize}
All changes of coordinates correspond to translations (shifts of the origin) or orthogonal transformations, both of which preserve distance and angles.

\subsection{Standard Forms for Conics}
The general forms of conics can be written in terms of lengths $a, b$ (the semi-major and semi-minor axes), or equivalently a length scale $\ell$ and a dimensionless eccentricity constant $e$.
\begin{itemize}
	\item First, let us consider Cartesian coordinates. The formulas are:

	      \medskip\noindent\begin{tabular}{c|c|c|c}
		      conic     & formula                                 & eccentricity                       & foci       \\\hline
		      ellipse   & $\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1$ & $b^2=a^2(1-e^2)$, and $e<1$        & $x=\pm ae$ \\
		      parabola  & $y^2 = 4ax$                             & one quadratic term vanishes, $e=1$ & $x = +a$   \\
		      hyperbola & $\frac{x^2}{a^2} - \frac{y^2}{b^2} = 1$ & $b^2=a^2(e^2-1)$, and $e<1$        & $x=\pm ae$
	      \end{tabular}

	\item Polar coordinates are a convenient alternative to Cartesian coordinates. In this coordinate system, we set the origin to be at a focus. Then, the formulas are
	      \[ r = \frac{\ell}{1 + e\cos \theta} \]
	      \begin{itemize}
		      \item For the ellipse, $e<1$ and $\ell = a(1-e^2)$;
		      \item For the parabola, $e=1$ and $\ell = 2a$; and
		      \item For the hyperbola, $e>1$ and $\ell = a(e^2 - 1)$. There is only one branch for the hyperbola given by this polar form.
	      \end{itemize}
\end{itemize}

%TODO draw graphs for all of these curves in both coordinate systems

\subsection{Conics as Sections of a Cone}
The equation for a cone in $\mathbb R^3$ given by an apex $\bm c$, an axis $\nhat$, and an angle $\alpha < \frac{\pi}{2}$, is
\[ (\bm x - \bm c)\cdot\nhat = \abs{\bm x - \bm c}\cos \alpha \]
Less formally, the angle of $\bm x$ away from $\nhat$ must be $\alpha$. By squaring this equation, we can essentially define two cones which stretch out infinitely far and meet at the centre point $\bm c$.
\[ \left( (\bm x - \bm c)\cdot\nhat \right)^2 = \abs{\bm x - \bm c}^2\cos^2 \alpha \]
Let us choose a set of coordinate axes so that our equations end up slightly easier. Let $\bm c = c\bm e_3, \nhat = \cos\beta \bm e_1 - \sin\beta \bm e_3$. Then essentially the cone starts at $(0, 0, c)$ and points `downwards' in the $\bm e_1$--$\bm e_3$ plane. Then the conic section is the intersection of this cone with the $\bm e_1$--$\bm e_2$ plane, i.e. $x_3 = 0$.
\[ (x_1\cos\beta - c\sin\beta)^2 = (x_1^2 + x_2^2 + c^2)\cos^2\alpha \]
\[ \iff (\cos^2\alpha - \cos^2\beta)x_1^2 + (\cos^2\alpha)x_2^2 + 2x_1c\sin\beta\cos\beta = \text{const.} \]
Now we can compare the signs of the $x_1^2$ and $x_2^2$ terms. Clearly the $x_2^2$ term is always positive, so we consider the sign of the $x_1^2$ term.
\begin{itemize}
	\item If $\cos^2 \alpha > \cos^2\beta$ (i.e. $\alpha < \beta$), then we have an ellipse.
	\item If $\cos^2 \alpha = \cos^2\beta$ (i.e. $\alpha = \beta$), then we have a parabola.
	\item If $\cos^2 \alpha < \cos^2\beta$ (i.e. $\alpha > \beta$), then we have a hyperbola.
\end{itemize}

\section{Symmetries and Transformation Groups}
\subsection{Orthogonal Transformations and Rotations in $\mathbb R^n$}
We know that if a matrix $R$ is orthogonal, we have $R^\transpose R = I \iff (R\bm x) \cdot (R\bm y) = \bm x \cdot \bm y \iff$ the rows or columns are orthonormal. The set of $n \times n$ matrices $R$ forms the orthogonal group $O_n = O(n)$. If $R \in O(n)$ then $\det R = \pm 1$. $SO_n = SO(n)$ is the special orthogonal group, which is the subgroup of $O(n)$ defined by $\det R = 1$. If some matrix $R$ is an element of $O(n)$, then $R$ preserves the modulus of $n$-dimensional volume. If $R \in SO(n)$, then $R$ preserves not only the modulus but also the sign of such a volume.

$SO(n)$ consists precisely of all rotations in $\mathbb R^n$. $O(n) \setminus SO(n)$ consists of all reflections. For some specific $H \in O(n) \setminus SO(n)$, any element of $O(n)$ can be written as a product of $H$ with some element in $SO(n)$, i.e. $R$ or $RH$ with $R \in SO(n)$. For example, if $n$ is odd, we can choose $H = -I$.

Now, we can consider the transformation $x'_i = R_{ij} x_j$ under two distinct points of view.
\begin{itemize}
	\item (active) The rotation $R$ acts on the vector $\bm x$ and yields a new vector $\bm x'$. The $x'_i$ are components of the transformed vector in terms of the standard basis vectors.
	\item (passive) The $x'_i$ are components of the same vector $\bm x$ but with respect to new orthonormal basis vectors $\bm u_i$. In general, $\bm x = \sum_i x_i \bm e_i = \sum_i x'_i \bm u_i$ which is true where $\bm u_i = \sum_j R_{ij} \bm e_j = \sum_j \bm e_j P_{ji}$. So $P = R^{-1} = R^\transpose$ where $P$ is the change of basis matrix.
\end{itemize}

\subsection{2D Minkowski Space}
Consider a new `inner product' on $\mathbb R^2$ given by
\[ (\bm x, \bm y) = \bm x^\transpose J \bm y;\quad J = \begin{pmatrix}
		1 & 0 \\ 0 & -1
	\end{pmatrix} \]
\[ \therefore \left( \begin{pmatrix}
			x_0 \\ x_1
		\end{pmatrix}, \begin{pmatrix}
			y_0 \\ y_1
		\end{pmatrix} \right) = x_0 y_0 - x_1 y_1 \]
We start indexing these vectors from zero, not one. Here are some important properties.
\begin{itemize}
	\item This `inner product' is not positive definite. In fact, $(\bm x, \bm x) = x_0^2 - x_1^2$. (This is a  quadratic form for $\bm x$ with eigenvalues $\pm 1$.)
	\item It is bilinear and symmetric.
	\item Defining $\bm e_0 = \begin{pmatrix}
			      1 \\ 0
		      \end{pmatrix}$ and $\bm e_1 = \begin{pmatrix}
			      0 \\ 1
		      \end{pmatrix}$, they obey
	      \[ (\bm e_0, \bm e_0) = -(\bm e_1, \bm e_1) = 1;\quad (\bm e_0, \bm e_1) = 0 \]
	      This is similar to orthonormality, in this generalised sense.
\end{itemize}
This inner product is known as the Minkowski metric on $\mathbb R^2$. $\mathbb R^2$ with this metric is called Minkowski space.

\subsection{Lorentz Transformations}
Let us consider a matrix
\[ M = \begin{pmatrix}
		M_{00} & M_{01} \\
		M_{10} & M_{11}
	\end{pmatrix} \]
giving a map $\mathbb R^2 \to \mathbb R^2$; this preserves the Minkowski metric if and only if $(M\bm x, M\bm y) = (\bm x, \bm y)$ for any vectors $\bm x, \bm y$. Expanded, this condition is
\[ (M\bm x)^\transpose J(M \bm y) = \bm x^\transpose M^\transpose J M \bm y = \bm x^\transpose J \bm y \]
\[ \implies M^\transpose J M = J \]
The set of such matrices form a group. Also, $\det M = \pm 1$ for the same reason as before. Furthermore, $\abs{M_{00}}^2 \geq 1$, so either $M_{00} \geq 1$ or $M_{00} \leq -1$. The subgroup with $\det M = +1$ and $M_{00} \geq 1$ is known as the Lorentz group.

Let us find the general form of $M$, by using the fact that the columns $M \bm e_0$ and $M \bm e_i$ are orthonormal with respect to the Minkowski metric.
\[ (M \bm e_0, M \bm e_0) = M_{00}^2 - M_{10}^2 = (\bm e_0, \bm e_0) = 1\quad (\text{hence } \abs{M_{00}}^2 \geq 1) \]
Taking $M_{00} \geq 1$, we can write
\[ M\bm e_0 = \begin{pmatrix}
		\cosh \theta \\ \sinh \theta
	\end{pmatrix} \]
for some real value $\theta$. For the other column,
\[ (M \bm e_0, M \bm e_1) = 0;\; (M \bm e_1, M \bm e_1) = -1 \implies M \bm e_1 = \pm\begin{pmatrix}
		\sinh \theta \\
		\cosh \theta
	\end{pmatrix} \]
The sign is fixed to be positive by the condition that $\det M = +1$.
\[ M = \begin{pmatrix}
		\cosh \theta & \sinh \theta \\
		\sinh \theta & \cosh \theta
	\end{pmatrix} \]
The curves defined by $(\bm x, \bm x) = k$ where $k$ is a constant are hyperbolas. This is analogous to how the curves defined by $\bm x \cdot \bm x = k$ are circles. So applying $M$ to any vector on a given branch of a hyperbola, the resultant vector remains on the hyperbola.
%TODO graph
Note that these matrices obey the rule $M(\theta_1) M(\theta_2) = M(\theta_1 + \theta_2)$. This confirms that they form a group.

\subsection{Application to Special Relativity}
Let
\[ M(\theta) = \gamma(v) \begin{pmatrix}
		1 & v \\ v & 1
	\end{pmatrix};\quad v = \tanh \theta;\quad \gamma = (1 - v^2)^{-\frac{1}{2}} \]
Here, $v$ lies in the range $-1 < v < 1$. We will rename $x_0$ to be $t$, which is now our time coordinate. $x_1$ will just be written $x$, our one-dimensional space coordinate. Then,
\[ \bm x' = M\bm x \iff \begin{cases}
		t' & = \gamma \cdot (t + vx) \\
		x' & = \gamma \cdot (x + vt)
	\end{cases} \]
This is a Lorentz transformation, or `boost', relating the time and space coordinates for observers moving with relative velocity $v$ in Special Relativity, in units where the speed of light $c$ is taken to be 1. The $\gamma$ factor in the Lorentz transformation gives rise to time dilation and length contraction effects. The group property $M(\theta_3) = M(\theta_1)M(\theta_2)$ with $\theta_3 = \theta_1 + \theta_2$ corresponds to the velocities
\[ v_i = \tanh \theta_i \implies v_3 = \frac{v_1 + v_2}{1 + v_1 v_2} \]
This is consistent with the fact that all velocities are less than the speed of light, 1.
\end{document}