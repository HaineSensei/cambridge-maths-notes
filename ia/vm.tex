\documentclass{article}

\input{../util.tex}

\title{Vectors and Matrices}
\author{Cambridge University Mathematical Tripos: Part IA}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Complex Numbers}
\subsection{Definitions}
We construct the complex numbers from $\mathbb R$ by adding an element $i$ such that $i^2 = -1$. By definition, any complex number $z \in \mathbb C = x + iy$ where $x, y \in \mathbb R$. We use the notation $x = \Re z$ and $y = \Im z$ to query the components of a complex number. The complex numbers contains the set of real numbers, due to the fact that $x = x + i0$. We define the operations of addition and multiplication in familiar ways, which lets us state that $\mathbb C$ is a field.

We also define the complex conjugate $\overline{z}$ as negating the imaginary part of $z$. Trivially we can see facts such as $\overline{\left( \overline{z}\right) } = z$; $\overline{z + w} = \overline z + \overline w$ and $\overline{zw} = \overline z \cdot \overline w$.

\subsection{The Fundamental Theorem of Algebra}
The Fundamental Theorem of Algebra states that a polynomial of degree $n$ can be written as a product of $n$ linear factors:
\[ c_nz^n + \cdots + c_1z^1 + c_0z^0 = c_n(z-\alpha_1)(z-\alpha_2) \cdots (z-\alpha_n)\quad (\text{where } c_i, \alpha_i \in \mathbb C) \]

We can reformulate this statement as follows: a polynomial of degree $n$ has $n$ solutions $\alpha_i$, counting repeats. This theorem is not proved in this course.

\subsection{Properties of Modulus}
The modulus of complex numbers $z_1, z_2$ satisfies:
\begin{itemize}
    \item (composition) $\abs{z_1 z_2} = \abs{z_1} \abs{z_2}$, and
    \item (triangle inequality) $\abs{z_1 + z_2} \leq \abs{z_1} + \abs{z_2}$
\end{itemize}
\begin{proof}
    The composition property is trivial. To prove the triangle inequality, we square both sides and compare.
    \begin{align*}
        \text{LHS} & = \abs{z_1 + z_2}^2                                                 \\
                   & = (z_1 + z_2)\overline{(z_1 + z_2)}                                 \\
                   & = \abs{z_1}^2 + \overline{z_1}z_2 + z_1\overline{z_2} + \abs{z_2}^2 \\
        \text{RHS} & = \abs{z_1}^2 + 2 \abs{z_1}\abs{z_2} + \abs{z_2}^2
    \end{align*}
    Note that
    \begin{align*}
        \overline{z_1}z_2 + z_1\overline{z_2}                                           & \leq 2 \abs{z_1}\abs{z_2}     \\
        \iff \frac{1}{2}\left( \overline{z_1}z_2 + \overline{\overline{z_1}z_2} \right) & \leq \abs{z_1}\abs{z_2}       \\
        \iff \Re (\overline{z_1} z_2)                                                   & \leq \abs{\overline{z_1} z_2}
    \end{align*}
    which is true.
\end{proof}

We can alternatively use the map $z_2 \to z_2 - z_1$ to write the triangle inequality as
\begin{align*}
    \abs{z_2 - z_1}            & \geq \abs{z_2} - \abs{z_1}       \\
    \text{or } \abs{z_2 - z_1} & \geq \abs{z_1} - \abs{z_2}       \\
    \therefore \abs{z_2 - z_1} & \geq \abs{\abs{z_2} - \abs{z_1}} \\
\end{align*}

\subsection{De Moivre's Theorem}
De Moivre's Theorem states that
\[ (\cos \theta + i \sin \theta)^n = \cos n \theta + i \sin n \theta \quad(\forall n \in \mathbb Z) \]
We can prove this using induction for $n \geq 0$. To show the negative case, simply use the positive result and raise it to the power of $-1$.

\section{Complex Valued Functions}
\subsection{Complex Function Definitions}
For $z \in \mathbb C$, we can define:
\begin{align*}
    \exp z & = \sum_{n=0}^{\infty} \frac{1}{n!}z^n          \\
    \cos z & = \frac{1}{2} \left( e^{iz} + e^{-iz} \right)  \\
    \sin z & = \frac{1}{2i} \left( e^{iz} - e^{-iz} \right)
\end{align*}
By defining $\log z = w \st e^w = z$, we have a complex logarithm function. By expanding the definition, we get that $\log z = \log r + i\theta$ where $r = \abs{z}$ and $\theta = \arg{z}$. Note that because the argument of a complex number is multi-valued, so is the logarithm.

We can define exponentiation in the general case by defining $z^\alpha = e^{\alpha \log z}$. Depending on the choice of $\alpha$, we have three cases:
\begin{itemize}
    \item If $\alpha = p \in \mathbb Z$ then the result of $z^p$ is unambiguous because
          \[ z^p = e^{p \log z} = e^{p (\log r + i \theta + 2 \pi i n)} \]
          which has a factor of $e^{2 \pi i p n}$ which is 1.
    \item For a similar reason, a rational exponent has finitely many values.
    \item But in the general case, there are infinitely many values.
\end{itemize}
We can calculate results such as the square root of a complex number, which have two results as you might expect.

\begin{note}
    We can't use facts like $z^\alpha z^\beta = z^{\alpha + \beta}$ in the complex case because the left and right hand sides both have infinite sets of answers, which may not be the same.
\end{note}

\subsection{Transformations and Primitives}
We can represent a line passing through $x_0\in \mathbb C$ parallel to $w \in \mathbb C$ using the formula:
\[ z = z_0 + \lambda w\quad(\lambda \in \mathbb R) \]
We can eliminate the dependency on $\lambda$ by computing the conjugate of both sides:
\begin{align*}
    \overline{z}                  & = \overline{z_0} + \lambda \overline{w} \\
    \overline{w}z - w\overline{z} & = \overline{w}z_0 - w\overline{z_0}
\end{align*} % TODO prove this probably
We can also write the equation for a circle with centre $c \in \mathbb C$ and radius $\rho \in \mathbb R$:
\[ z = c + \rho e^{i\alpha} \]
or equivalently:
\[ \abs{z - c} = \abs{\rho e^{i\alpha}} = \rho \]
or by squaring both sides:
\[ \abs{z}^2 - c\overline{z} - \overline{c}z = \rho^2 - \abs{c}^2 \]

\section{Vectors in Three Dimensions}
We use the normal Euclidean notions of points, lines, planes, length, angles and so on. By choosing an (arbitrary) origin point $O$, we may write positions as position vectors with respect to that origin point.

\subsection{Vector Addition and Scalar Multiplication}
We define vector addition using the shape of a parallelogram with points $\vb 0, \vb a, \vb a + \vb b, \vb b$. We define scalar multiplication of a vector using the line $\overrightarrow{OA}$ and setting the length to be multiplied by the constant. Note that this vector space is an abelian group under addition.
\begin{definition}
    $\vb a$ and $\vb b$ are defined to be parallel if and only if $\vb a = \lambda \vb b$ or $\vb b = \lambda \vb a$ for some $\lambda \in \mathbb R$. This is denoted $a \parallel b$. Note that the vectors may be zero, in particular the zero vector is parallel to all vectors.
\end{definition}
\begin{definition}
    The span of a set of vectors is defined as $\vecspan \{\vb a, \vb b, \cdots, \vb c\} = \{ \alpha \vb a + \beta \vb b + \cdots + \gamma \vb c: \alpha, \beta, \gamma \in \mathbb R \}$. This is the line/plane/volume etc. containing the vectors. The span has an amount of dimensions at most equal to the amount of vectors in the input set. For example, the span of a set of two vectors may be a point, line or plane containing the vectors.
\end{definition}

\subsection{Scalar Product}
\begin{definition}
    Given two vectors $\vb a, \vb b$, let $\theta$ be the angle between the two vectors. Then, we define
    \[ \vb a \cdot \vb b = \abs{\vb a} \abs{\vb b} \cos \theta \]
    Note that if either of the vectors is zero, $\theta$ is undefined. However, the dot product is zero anyway here, so this is irrelevant.
\end{definition}
\begin{definition}
    Two vectors $\vb a$ and $\vb b$ are defined to be parallel (or orthogonal) if and only if $\vb a \cdot \vb b = 0$. This is denoted $\vb a \perp \vb b$. This is true in two cases:
    \begin{enumerate}
        \item $\cos \theta = 0 \iff \theta = \frac{\pi}{2} \mod \pi$, or
        \item $\vb a = 0$ or $\vb b = 0$.
    \end{enumerate}
    Therefore, the zero vector is perpendicular to all vectors.
\end{definition}
\begin{definition}
    We can decompose a vector $\vb b$ into components relative to $\vb a$:
    \[ \vb b = \vb b_\parallel + \vb b_\perp \]
    where $\vb b_\parallel$ is the component of $\vb b$ parallel to $\vb a$, and $\vb b_\perp$ is the component of $\vb b$ perpendicular to $\vb a$. In particular, we have that
    \[ \vb a \cdot \vb b = \vb a \cdot \vb b_\parallel \]
\end{definition}

\subsection{Vector Product}
\begin{definition}
    Given two vectors $\vb a, \vb b$, let $\theta$ be the angle between the two vectors measured with respect to an arbitrary normal $\vb{\hat n}$. Then, we define
    \[ \vb a \wedge \vb b = \vb a \times \vb b = \abs{\vb a} \abs{\vb b} \vb{\hat n} \sin \theta \]
    Note that by swapping the sign of $\vb{\hat n}$, $\theta$ changes to $2 \pi - \theta$, leaving the result unchanged. There are two degenerate cases:
    \begin{itemize}
        \item $\theta$ is undefined if $\vb a$ or $\vb b$ is the zero vector, but the result is zero anyway because we multiply by the magnitudes of both vectors.
        \item $\vb{\hat n}$ is undefined if $\vb a \parallel \vb b$, but here $\sin \theta = 0$ so the result is zero anyway.
    \end{itemize}
\end{definition}
We can provide several useful interpretations of the cross product:
\begin{itemize}
    \item The magnitude of $\vb a \times \vb b$ is the vector area of the parallelogram defined by the points $\vb 0, \vb a, \vb a + \vb b, \vb b$.
    \item By fixing a vector $\vb a$, we can consider the plane perpendicular to it. If $\vb x$ is another vector in the plane, $\vb x \mapsto \vb a \times \vb x$ rotates $\vb x$ by $\frac{\pi}{2}$ in the plane, scaling it by the magnitude of $\vb a$.
\end{itemize}
Note that by resolving a vector $\vb b$ perpendicular to another vector $\vb a$, we have that
\[ \vb a \times \vb b = \vb a \times \vb b_\perp \]
A final useful property of the cross product is that since the result is perpendicular to both input vectors, we have
\[ \vb a \cdot (\vb a \times \vb b) = \vb b \cdot(\vb a \times \vb b) = 0 \]

\section{Orthonormal Bases}
\subsection{Basis Vectors}
To represent vectors as some collection of numbers, we can choose some basis vectors $\vb e_1, \vb e_2, \vb e_3$ which are `orthonormal', i.e. they are unit vectors and pairwise orthogonal. Note that
\[ \vb e_i \cdot \vb e_j = \begin{cases}
        1 & \text{if } i = j \\
        0 & \text{otherwise}
    \end{cases} \]
The set $\{ \vb e_1, \vb e_2, \vb e_3 \}$ is called a basis because any vector can be written uniquely as a linear combination of the basis vectors. Because we have orthonormal basis vectors, we can reduce this to
\[ \vb a = \sum_i \vb a_i \vb e_i \implies \vb a_i = \vb e_i \cdot \vb a \]
By representing a vector as a linear combination of basis vectors, it is very easy to evaluate the scalar product algebraically. To calculate the vector product, we first need to define whether $\vb e_1 \times \vb e_2 = \vb e_3$ or $-\vb e_3$. By convention, we assume that the basis vectors are right-handed, i.e. $\vb e_1 \times \vb e_2 = \vb e_3$. Then we can calculate the formula for the cross product in terms of the vectors' components.

\subsection{Scalar Triple Product}
The scalar triple product is the scalar product of one vector with the cross product of two more.
\[ \vb a \cdot (\vb b \times \vb c) = \vb b \cdot (\vb c \times \vb a) = \vb c \cdot (\vb a \times \vb b) = [\vb a, \vb b, \vb c] \]
The result of the scalar triple product is the signed volume of the parallelepiped starting at the origin with axes $\vb a$, $\vb b$, $\vb c$. We can represent this triple product as the determinant of a matrix:
\[
    \vb a \cdot (\vb b \times \vb c) =
    \begin{vmatrix}
        \vb a_1 & \vb a_2 & \vb a_3 \\
        \vb b_1 & \vb b_2 & \vb b_3 \\
        \vb c_1 & \vb c_2 & \vb c_3
    \end{vmatrix}
\]
If the scalar triple product is greater than zero, then $\vb a, \vb b, \vb c$ is called a right handed set. If it is equal to zero, then the vectors are all coplanar: $\vb c \in \vecspan \{ \vb a, \vb b \}$.

\subsection{Vector Triple Product}
The vector triple product is the cross product of three vectors. Note that this is non-associative. The proof is covered in the subsequent lecture.
\[ \vb a \times (\vb b \times \vb c) = (\vb a \cdot \vb c) \vb b - (\vb a\cdot \vb b) \vb c \]
\[ (\vb a \times \vb b) \times \vb c = (\vb a \cdot \vb c) \vb b - (\vb b\cdot \vb c) \vb a \]

\subsection{Lines}
A line through $\vb a$ parallel to $\vb u$ is defined by
\[ \vb r = \vb a + \lambda \vb u \]
where $\lambda$ is some real parameter. We can eliminate lambda by using the cross product with $\vb u$. This will allow us to get a $\vb u \times \vb u$ term which will cancel to zero.
\[ \vb u \times \vb r = \vb u \times \vb a \]
Informally, this is saying that $\vb r$ and $\vb a$ have the same components perpendicular to $\vb u$. Note that we can also reverse this process. Consider the equation
\[ \vb u \times \vb r = \vb c \]
By using the dot product with $\vb u$ we can say
\[ \vb u \cdot (\vb u \times \vb r) = \vb u \cdot \vb c \]
If $\vb u \cdot \vb c \neq 0$ then the equation is inconsistent. Otherwise, we can suppose that maybe $\vb r = \vb u \times \vb c$ and use the formula for the vector product to get the left hand side to be $\vb u \times (\vb u \times \vb c) = -\abs{\vb u}^2 \vb c$. Therefore, by inspection, $\vb a = -\frac{1}{\abs{\vb u}^2}(\vb u \times \vb c)$ is a solution. Now, note that we can add any multiple of $\vb u$ to $\vb a$ and it remains a solution. So the general solution is $\vb r = \vb a + \lambda\vb u$.

\subsection{Planes}
The general point on a plane that passes through $\vb a$ and has directions $\vb u$ and $\vb v$ is
\[ \vb r = \vb a + \lambda \vb u + \mu \vb v \]
where $\vb u$ and $\vb v$ are not parallel, and $\lambda$ and $\mu$ are real parameters. We can do a dot product with $\vb n = (\vb u \times \vb v)$ to eliminate both parameters.
\[ \vb n \cdot \vb r = \kappa \]
where $\kappa = \vb n \cdot \vb a$. Note that $\abs{\kappa}/\abs{\vb n}$ is the perpendicular distance from the origin to the plane.

\subsection{Other Vector Equations}
The equation of a sphere is given by a quadratic vector equation in $\vb r$.
\[ \vb r^2 + \vb r \cdot \vb a = k \]
We can complete the square to give
\[ \left(\vb r + \frac 1 2 \vb a \right)^2 = \frac 1 4 \vb a^2 + k \]
which is clearly a sphere with centre $-\frac 1 2 \vb a$ and radius $\left( \frac 1 4 \vb a^2 + k \right)^{1/2}$.

Another example of a vector equation is
\[ \vb r + \vb a \times (\vb b \times \vb r) = \vb c \tag{1} \]
where $\vb a, \vb b, \vb c$ are fixed. We can dot with $\vb a$ to eliminate the second term:
\[ \vb a \cdot \vb r = \vb a \cdot \vb c \tag{2} \]
Note that using the dot product loses information --- this is simply a tool to make deductions; (2) does not contain the full information of (1). Combining (1) and (2), and using the formula for the vector triple product, we get
\begin{align*}
    \vb r + (\vb a \cdot \vb r) \vb b - (\vb a \cdot \vb b) \vb r          & = \vb c \tag{3} \\
    \implies \vb r + (\vb a \cdot \vb c) \vb b - (\vb a \cdot \vb b) \vb r & = \vb c
\end{align*}
This eliminates the dependency on $\vb r$ inside the dot product. Now, we can factorise, leaving
\[ (1 - \vb a \cdot \vb b) \vb r = \vb c - (\vb a \cdot \vb c) \vb b \tag{4} \]
If $1 - \vb a \cdot \vb b \neq 0$ then $\vb r$ has a single solution, a point. Otherwise, the right hand side must also be zero (otherwise the equation is inconsistent). Therefore, $\vb c - (\vb a \cdot \vb c)\vb b = \vb 0$. We can now combine this expression for $\vb c$ into (3), eliminating the $(1- \vb a \cdot \vb b)$ term, to get
\[ (\vb a \cdot \vb r - \vb a \cdot \vb c) \vb b = \vb 0 \]
This shows us that (given that $\vb b$ is non-zero) the solutions to the equation are given by (2), which is the equation of a plane.

\section{Index Notation and the Summation Convention}
\subsection{Kronecker $\delta$}
The Kronecker $\delta$ is defined by
\[ \delta_{ij} = \begin{cases}
        1 & \text{if } i = j    \\
        0 & \text{if } i \neq j
    \end{cases} \]
Then $\vb e_i \vb e_j = \delta_{ij}$. We can also use $\delta$ to rewrite indices: $\sum_i \delta_{ij} \vb a_i = \vb a_j$. So
\begin{align*}
    \vb a \cdot \vb b & = \left( \sum_i \vb a_i \vb e_i \right) \cdot \left( \sum_j \vb b_j \vb e_j \right) \\
                      & = \sum_{ij} \vb a_i \vb b_j (\vb e_i \cdot \vb e_j)                                 \\
                      & = \sum_{ij} \vb a_i \vb b_j \delta_{ij}                                             \\
                      & = \sum_i \vb a_i \vb b_i
\end{align*}

\subsection{Levi-Civita $\varepsilon$}
The Levi-Civita $\varepsilon$ is defined by
\[
    \varepsilon_{ijk} = \begin{cases}
        +1 & \text{if } ijk \text{ is an even permutation of } [1, 2, 3] \\
        -1 & \text{if } ijk \text{ is an odd permutation of } [1, 2, 3]  \\
        0  & \text{otherwise}
    \end{cases}
\]
Then
\begin{align*}
    \varepsilon_{123} = \varepsilon_{231} = \varepsilon_{312} & = +1 \\
    \varepsilon_{132} = \varepsilon_{321} = \varepsilon_{213} & = -1
\end{align*}
and all other permutations of $[1, 2, 3]$ yield 0. This shows that $\varepsilon$ is totally antisymmetric; exchanging any pair of indices changes the sign. We now have:
\begin{align*}
    \vb e_i \times \vb e_j & = \sum_k \varepsilon_{ijk} \vb e_k                                                   \\
    \intertext{And:}
    \vb a \times \vb b     & = \left( \sum_i \vb a_i \vb e_i \right) \times \left( \sum_j \vb b_j \vb e_j \right) \\
    \vb a \times \vb b     & = \sum_{ij} \vb a_i \vb b_j \left( \vb e_i \times \vb e_j \right)                    \\
    \vb a \times \vb b     & = \sum_{ijk} \vb a_i \vb b_j \varepsilon_{ijk} \vb e_k
\end{align*}
So the individual terms of the cross product can be written
\[ (\vb a \times \vb b)_k = \sum_{ij} \vb a_i \vb b_j \varepsilon_{ijk} \]

\subsection{Summation Convention}
We use the `summation convention' to abbreviate the many summation symbols used throughout linear algebra.
\begin{enumerate}
    \item An index which occurs exactly once in some term, denoted a `free index', must appear once in every term in that equation.
    \item An index which occurs exactly twice in a given term, denoted a `repeated/contracted/dummy index', is implicitly summed over.
    \item No index can occur more than twice in a given term.
\end{enumerate}

\subsection{$\varepsilon\varepsilon$ Identities}
The most general $\varepsilon\varepsilon$ identity is as follows:
\begin{align*}
    \varepsilon_{ijk} \varepsilon_{pqr}
     & = \delta_{ip}\delta_{jq}\delta_{kr} - \delta_{jp}\delta_{iq}\delta_{kr} \\
     & + \delta_{jp}\delta_{kq}\delta_{ir} - \delta_{kp}\delta_{jq}\delta_{ir} \\
     & = \delta_{kp}\delta_{iq}\delta_{jr} - \delta_{ip}\delta_{kq}\delta_{jr}
\end{align*}
This is, however, very verbose and not used often throughout the course. It is provable by noting the total antisymmetry in $i,j,k$ and $p,q,r$ on both sides of the equation implies that both sides agree up to a constant factor. We can check that this factor is 1 by substituting in values such as $i=p=1$, $j=q=2$ and $k=r=3$.

The next most generic form is a very useful identity.
\[ \varepsilon_{ijk}\varepsilon_{pqk} = \delta_{ip}\delta_{jq} - \delta_{iq}\delta_{jp} \]
This is essentially the first line of the above identity, noting that $k=r$. We can prove this is true by observing the antisymmetry, and that both sides vanish under $i=j$ or $p=q$. So it suffices to check two cases: $i=p, j=q$ and $i=q, j=p$.

We can now continue making more indices equal to each other to get even more specific identities:
\[ \varepsilon_{ijk}\varepsilon_{pjk} = 2\delta_{ip} \]
This is easy to prove by noting that $\delta_{jj} = \sum_j \delta_{jj} = 3$, and using the $\delta$ rewrite rule.

Finally, we have
\[ \varepsilon_{ijk}\varepsilon_{ijk} = 6 \]
No indices are free here, so the values of $i, j, k$ themselves are predetermined by the fact that we are in three-dimensional space.

\subsection{Vector Triple Product Identity}
Using the summation convention (as will now be implied for the remainder of the course), we can prove
\begin{align*}
    \left[ \vb a \times (\vb b \times \vb c) \right]_i
     & = \varepsilon_{ijk} \vb a_j (\vb b \times \vb c)_k                                                  \\
     & = \varepsilon_{ijk} \vb a_j \varepsilon_{pqk} \vb b_p \vb c_q                                       \\
     & = \varepsilon_{ijk}\varepsilon_{pqk} \vb a_j \vb b_p \vb c_q                                        \\
     & = (\delta_{ip}\delta_{jq})\vb a_j \vb b_p \vb c_q - (\delta_{iq}\delta_{jp})\vb a_j \vb b_p \vb c_q \\
     & = (\vb a \cdot \vb c) \vb b_i - (\vb a \cdot \vb b) \vb c_i
\end{align*}

\section{Vectors in $\mathbb R^n$}
\subsection{Definitions}
We define multidimensional real space as follows:
\[ \mathbb R^n = \{ \vb x = (x_1, x_2, \cdots, x_n) : x_i \in \mathbb R \} \]
We can define addition and scalar multiplication by mapping these operations over each term in the tuple. Therefore, we have a notion of linear combinations of vectors and hence a concept of parallel vectors. We can say, like before in $\mathbb R^3$, that $\vb x \parallel \vb y$ if and only if $\vb x = \lambda \vb y$ or $\vb y = \lambda \vb x$.

\subsection{Inner Product}
We define an operator analogous to the scalar product in $\mathbb R^3$. The inner product is defined as $x \cdot y = x_i y_i$. Directly from this definition, we can deduce some properties:
\begin{itemize}
    \item (symmetric) $\vb x \cdot \vb y = \vb y \cdot \vb x$
    \item (bilinear) $(\lambda \vb x + \lambda'\vb x')\cdot \vb y = \lambda \vb x\cdot \vb y + \lambda' \vb x' \cdot \vb y$
    \item (positive definite) $\vb x \cdot \vb x \geq 0$, and the equality holds if and only if $\vb x = \vb 0$.
\end{itemize}

\subsection{Norm}
We can define the norm of a vector (similar to the concept of length in three-dimension space), denoted $\abs {\vb x}$, by $\abs{\vb x}^2 = \vb x \cdot \vb x$. We can now define orthogonality as follows: $\vb x \perp \vb y \iff \vb x \cdot \vb y = 0$.

\subsection{Basis Vectors}
We define the standard basis vectors $\vb e_1, \vb e_2, \cdots \vb e_n$ by setting each element of the tuple $\vb e_i$ to zero apart from the $i$th element, which is set to one. Also, we redefine the Kronecker $\delta$ to be valid in higher-dimensional space. Note that under this definition, the standard basis vectors are orthonormal because $\vb e_i \cdot \vb e_j = \delta_{ij}$.

\subsection{Cauchy-Schwarz Inequality}
\begin{proposition}
    For vectors $\vb x, \vb y$ in $\mathbb R^n$, $\abs{\vb x \cdot \vb y} \leq \abs{\vb x} \abs{\vb y}$, where the equality is true if and only if $\vb x \parallel \vb y$.
\end{proposition}
\begin{proof}
    If $\vb y = \vb 0$, then the result is immediate. So suppose that $\vb y \neq 0$, then for some $\lambda \in \mathbb R$, we have
    \begin{align*}
        \abs{\vb x - \lambda \vb y}^2 & =
        (\vb x - \lambda \vb y) \cdot (\vb x - \lambda \vb y)                                                          \\
                                      & = \abs{\vb x}^2 - 2 \lambda \vb x \cdot \vb y + \lambda^2 \abs{\vb y}^2 \geq 0
    \end{align*}
    As this is a positive real quadratic in $\lambda$ that is always greater than zero, it has at most one real root. Therefore the discriminant is less than or equal to zero.
    \[ (-2 \vb x \cdot \vb y)^2 - 4 \abs{\vb x}^2\abs{\vb y}^2 \leq 0
        \implies \abs{\vb x \cdot \vb y} \leq \abs{\vb x}\abs{\vb y} \]
    where the equality only holds if $\vb x$ and $\vb y$ are parallel (i.e. when $\vb x - \lambda \vb y$ equals zero for some $\lambda$).
\end{proof}

\subsection{Triangle Inequality}
Following from the Cauchy-Schwarz inequality,
\begin{align*}
    \abs{\vb x + \vb y}^2
     & = \abs{\vb x}^2 + 2(\vb x \cdot \vb y) + \abs{\vb y}^2        \\
     & \leq \abs{\vb x}^2 + 2 \abs{\vb x}\abs{\vb y} + \abs{\vb y}^2 \\
     & = \left(\abs{\vb x} + \abs{\vb y}\right)^2
\end{align*}
where the equality holds under the same conditions as above.

\subsection{Levi-Civita $\varepsilon$ in $\mathbb R^n$}
Note that the Levi-Civita $\varepsilon$ has three indices in $\mathbb R^3$. We can extend this $\varepsilon$ to higher and lower dimensions by increasing or reducing the amount of indices. It does not make logical sense to use the same $\varepsilon$ without changing the amount of indices to define, for example, a vector product in four-dimensional space, since we would have unused indices. The expression $(\vb x \times \vb y)_k = \varepsilon_{ijk} \vb a_i \vb b_j$ works because there is one free index, $k$, on the right hand side, so we can use this to calculate the values of each element of the result.

We can, however, use this $\varepsilon$ to extend the notion of a scalar triple product to other dimensions, for example two-dimensional space, with $[\vb a, \vb b] := \varepsilon_{ij} \vb a_i \vb b_j$. This is the signed area of the parallelogram spanning $\vb a$ and $\vb b$.

\subsection{Vector Spaces}
Vector spaces are not studied axiomatically in this course, but the axioms are given here for completeness. A real (as in, $\mathbb R$) vector space $V$ is a set of objects with two operators $+: V \times V \to V$ and $\cdot: \mathbb R \times V \to V$ such that
\begin{itemize}
    \item $(V, +)$ is an abelian group
    \item $\lambda(v + w) = \lambda v + \lambda w$
    \item $(\lambda + \mu)v = \lambda v + \mu v$
    \item $\lambda(\mu v) = (\lambda \mu) v$
    \item $1v = v$ (to exclude trivial cases for example $\lambda v = 0$ for all $v$)
\end{itemize}

\section{Subspaces}
\subsection{Definitions}
A subspace of a real vector space $V$ is a subset $U \subseteq V$ that is a vector space. Equivalently, if all pairs of vectors $v, w \in U$ satisfy $\lambda v + \mu w \in U$. then $U$ is a subspace of $V$. Note that the span generated from a set of vectors is a subspace, as it is characterised by this equivalent definition. Also, note that the origin must be part of any subspace, because multiplying a vector by zero must yield the origin.

\subsection{Linear Dependence}
In some real vector space $V$, let $\vb v_1, \vb v_2 \cdots \vb v_r$ be vectors in $V$. Now consider the linear relation
\[ \lambda_1 \vb v_1 + \lambda_2 \vb v_2 + \cdots + \lambda_r \vb v_r = 0 \]
Then we call the set of vectors a linearly independent set if the only solution is where all $\lambda$ values are zero. Otherwise, it is a linearly dependent set.

\subsection{Inner Product Spaces}
An inner product is an extra structure that we can have on a real vector space $V$, which is often denoted by angle brackets or parentheses. It can also be characterised by axioms (specifically the ones in Section 6.2). Features like the norm of a vector, and theorems like the Cauchy-Schwarz inequality, follow from these axioms.

For example, let us consider the vector space
\[ V = \{ f: [0, 1] \to \mathbb R: f \text{ smooth}; f(0) = f(1) = 0 \} \]
We can define the inner product to be
\[ f \cdot g = \langle f, g \rangle = \int_0^1 f(x)g(x)\dd{x} \]
Then by the Cauchy-Schwarz inequality, we have
\begin{align*}
    \abs{\langle f, g \rangle}               & \leq \norm{f} \cdot \norm{g}                                    \\
    \therefore \abs{\int_0^1 f(x)g(x)\dd{x}} & \leq \sqrt{\int_0^1 f(x)^2 \dd{x}}\sqrt{\int_0^1 g(x)^2 \dd{x}}
\end{align*}

\begin{lemma}
    In any real inner product space $V$, if $\vb v_1 \cdots v_r \neq \vb 0$ are orthogonal, they are linearly independent.
\end{lemma}
\begin{proof}
    If $\sum_i \alpha_i \vb v_i = 0$, then
    \begin{align*}
        \left\langle \vb v_j, \sum_i \alpha_i \vb v_i \right\rangle     & = 0 \\
        \intertext{And because each vector that is not $\vb v_j$ is orthogonal to it, those terms cancel, leaving}
        \therefore \left\langle \vb v_j, \alpha_j \vb v_j \right\rangle & = 0 \\
        \alpha_j \left\langle \vb v_j, \vb v_j \right\rangle            & = 0 \\
        \alpha_j = 0
    \end{align*}
    So they are linearly independent.
\end{proof}

\subsection{Bases and Dimensions}
In a vector space $V$, a basis is a set $\mathcal B = \{ \vb e_1 \cdots \vb e_n \}$ such that
\begin{itemize}
    \item $\mathcal B$ spans $V$; and
    \item $\mathcal B$ is linearly independent, which implies that the coefficients on these basis vectors are unique for any vector in $V$, since it is impossible to write one vector in terms of the others
\end{itemize}

\begin{theorem}
    If $\{\vb e_1 \cdots \vb e_n \}$ and $\{ \vb f_1 \cdots \vb f_m \}$ are bases for a real vector space $V$, then $n=m$, which we call the dimension of $V$.
\end{theorem}
\begin{proof}
    This proof is non-examinable (without prompts). We can write each basis vector in terms of the others, since they all span the same vector space. Thus:
    \[ \vb f_a = \sum_i A_{ai} \vb e_i;\quad \vb e_i = \sum_a B_{ia} \vb f_a \]
    Note that indices $i,j$ span from 1 to $n$, while $a,b$ span from 1 to $m$. We can substitute one expression into the other, forming:
    \begin{align*}
        \vb f_a & = \sum_i A_{ai} \left( \sum_b B_{ib}\vb f_b \right)  \\
        \vb f_a & = \sum_b \left( \sum_i A_{ai} B_{ib} \right) \vb f_b
    \end{align*}
    Note that we have now written $\vb f_a$ as a linear combination of $\vb f_b$ for all valid $b$. But since they are linearly independent, the coefficient of $\vb f_b$ must be zero if $a \neq b$, and one of $a = b$. Therefore, we have
    \[ \delta_{ab} = \sum_i A_{ai} B_{ib} \]
    We can make a similar statement about $\vb e_i$:
    \[ \delta_{ij} = \sum_a B_{ia} A_{aj} = \sum_a A_{aj} B_{ia} \]
    Now, assigning $a=b$ and $i=j$, summing over both, and substituting into our two previous expressions for $\delta$, we have:
    \begin{alignat}{2}
        \sum_{ia} A_{ai} B_{ia} & = \sum_a \delta_{aa} &  & = \sum_i \delta_{ii} \\
                                & = m                  &  & = n
    \end{alignat}
\end{proof}

\section{Choosing Bases and $\mathbb C^n$}
\subsection{Choosing Bases}
Note that $\{ \vb 0 \}$ is a trivial subspace of all vector spaces, and it has dimension zero since it requires a linear combination of no vectors.

\begin{proposition}
    Let $V$ be a vector space with finite subsets $Y = \{ \vb w_1, \cdots, \vb w_m \}$ that spans $V$, and $X = \{ \vb u_1, \cdots, \vb u_k \}$ that is linearly independent. Let $n = \dim V$. Then:
    \begin{enumerate}[(i)]
        \item A basis can be found as a subset of $Y$ by discarding vectors in $Y$ as necessary, and that $n \leq m$.
        \item $X$ can be extended to a basis by adding in additional vectors from $Y$ as necessary, and that $k \leq n$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    This proof is non-examinable (without prompts).
    \begin{enumerate}[(i)]
        \item If $Y$ is linearly independent, then $Y$ is a basis and $m = n$. Otherwise, $Y$ is not linearly independent. So there exists some linear relation
              \[ \sum_{i=1}^{m} \lambda_i \vb w_i = \vb 0 \]
              where there is some $i$ such that $\lambda_i \neq 0$. Without loss of generality (because the order of elements in $Y$ does not matter) we will reorder $Y$ such that $\vb w_m \neq 0$. So we have
              \[ \vb w_m = \frac{-1}{\lambda_m} \sum_{i=1}^{m-1} \lambda_i \vb w_i \]
              So $\vecspan Y = \vecspan (Y \setminus \{ \vb w_m \})$. We can repeat this process of eliminating vectors from $Y$ until linear independence is achieved. We know that this process will end because $Y$ is a finite set. Clearly, in this case, $n < m$. So for all cases, $n \leq m$.

        \item If $X$ spans $V$, then $X$ is a basis and $k=n$. Else, there exists some $u_{k+1} \in V$ that is not in the span of $X$. Then, we will construct an arbitrary linear relation
              \[ \sum_{i=1}^{k+1} \mu_i \vb u_i = \vb 0 \]
              Note that this implies that $\mu_{k+1} = \vb 0$ because it is not in the span of $X$, and that $\mu_i = 0$ for all $i \leq k$ because the original $X$ was linearly independent. So we know that all the coefficients are zero, and therefore $X \cup \{ u_{k+1} \}$ is linearly independent.

              Note that we can always choose this $u_{k+1}$ to be an element of $Y$ because we just need to ensure that $u_{k+1} \notin \vecspan X$. Suppose we cannot choose such a vector in $Y$. Then $Y \subseteq \vecspan X \implies \vecspan Y \subseteq \vecspan X \implies \vecspan X = V$, which is clearly false because $X$ does not span $V$. This is a contradiction, so we can always choose such a vector from $Y$. We can repeat this process of taking vectors from $Y$ and adding them to $X$ until we have a basis. This process will always terminate in a finite amount of steps because we are taking new vectors from a finite set $Y$. Therefore $k \leq n$, as we are adding vectors (increasing $k$) until $k=n$.
    \end{enumerate}
\end{proof}

\subsection{Infinite Dimensions}
It is perfectly possible to have a vector space that has infinite dimensionality. However, they will be rarely touched upon in this course apart from specific examples, like the following example. Let $V = \{ f: [0, 1] \to \mathbb R: f \text{ smooth}, f(0) = f(1) = 0\}$. Then let $S_n(x) = \sqrt 2 \sin(n \pi x)$ where $n$ is a natural number $1, 2, \cdots$. Clearly, $S_n \in V$ for all $n$. The inner product of two of these $S$ functions is given by
\begin{align*}
    \langle S_n, S_m \rangle & = 2 \int_0^1 \sin(n \pi x) \sin(m \pi x) \dd{x} \\
                             & = \delta_{mn}
\end{align*}
So $S_n$ are orthonormal and therefore linearly independent. So we can continue adding more vectors until it becomes a basis. However, the set of all $S_n$ is already infinite --- so $V$ must have infinite dimensionality.

\subsection{Vectors in $\mathbb C^n$}
We define $\mathbb C^n$ by
\[ \mathbb C^n := \{ \vb z = (z_1, z_2, \cdots, z_n): \forall i, z_i \in \mathbb C \} \]
We define addition and scalar multiplication in obvious ways. Note that we have a choice over what the scalars are allowed to be. If we only allow scalars that are real numbers, $\mathbb C^n$ can be considered a real vector space with bases $(0, \cdots, 1, \cdots, 0)$ and $(0, \cdots, i, \cdots, 0)$ and dimension $2n$. Alternatively, if we let the scalars be any complex numbers, we don't need to have imaginary bases, thus giving us a complex vector space with bases $(0, \cdots, 1, \cdots, 0)$ and dimension $n$. We can say that $\mathbb C^n$ has dimension $2n$ over $\mathbb R$, and dimension $n$ over $\mathbb C$. From here on, unless stated otherwise, we treat $\mathbb C^n$ to be a complex vector space.

\subsection{Inner Product in $\mathbb C^n$}
We can define the inner product by
\[ \langle \vb z, \vb w \rangle := \sum_j \overline{z_j} w_j \]
The conjugate over the $z$ terms ensures that the inner product is positive definite. It has these properties, analogous to the properties of the inner product in the real vector space $\mathbb R^n$:
\begin{itemize}
    \item (Hermitian) $\langle \vb z, \vb w \rangle = \overline{\langle \vb w, \vb z \rangle}$
    \item (linear/antilinear) $\langle \vb z, \lambda \vb w + \lambda' \vb w' \rangle = \lambda \langle \vb z, \vb w \rangle + \lambda' \langle \vb z, \vb w' \rangle$ and $\langle \lambda \vb z + \lambda' \vb z', w \rangle = \overline{\lambda} \langle \vb z, \vb w \rangle + \overline{\lambda'} \langle \vb z', \vb w \rangle$
    \item (positive definite) $\langle \vb z, \vb z \rangle = \sum_j \abs{z_j}^2$ which is real and greater than or equal to zero, where the equality holds if and only if $\vb z = \vb 0$.
\end{itemize}
We can also define the norm of $\vb z$ to satisfy $\abs{\vb z} \geq 0$ and $\abs{\vb z}^2 = \langle \vb z, \vb z \rangle$. Note that the standard basis for $\mathbb C^n$ is orthonormal, since the inner product of any two basis vectors $\vb e_j$ and $\vb e_k$ is given by $\delta_{jk}$.

\subsection{Inner Product in Complex Plane}
Here is an example of the use of the complex inner product on $\mathbb C^1 = \mathbb C$. Note first that $\langle z, w \rangle = \overline z w$. Let $z = a_1 + ia_2$ and $w = b_1 + ib_2$ where $a_1, a_2, b_1, b_2 \in \mathbb R$. Then
\begin{align*}
    \langle z, w \rangle & = \overline z w                              \\
                         & = (a_1 b_1 + a_2 b_2) + i(a_1 b_2 - a_2 b_1) \\
                         & = (z \cdot w) + i[z, w]
\end{align*}
We can therefore use the inner product to compute two different scalar products at the same time.

\section{Linear Maps}
\subsection{Introduction}
A linear map (or linear transformation) is some operation $T: V \to W$ between vector spaces $V$ and $W$ preserving the core vector space structure (specifically, the linearity). It is defined such that
\[ T\left(\lambda \vb x + \mu \vb y\right) = \lambda T(\vb x) + \mu T(\vb y) \]
for all $\vb x, \vb y \in V$ where the scalars $\lambda$ and $\mu$ match up with the scalar field that $V$ and $W$ use (so this could be $\mathbb R$ or $\mathbb C$ in our examples). Much of the language used for linear maps between vector spaces is analogous to the language used for homomorphisms between groups.

Note that a linear map is completely determined by its action on a basis $\{ \vb e_1, \cdots, \vb e_n \}$ where $n = \dim V$, since
\[ T\left(\sum_i x_i \vb e_i \right) = \sum_i x_i T(\vb e_i) \]
We denote $\vb x' = T(\vb x) \in W$, and define $\vb x'$ as the image of $x$ under $T$. Further, we define
\[ \Im (T) = \{ \vb x' \in W : \vb x' =T(\vb x) \text{ for some } \vb x \in V \} \]
to be the image of $T$, and we define
\[ \ker (T) = \{ \vb x \in V : T(\vb x) = \vb 0 \} \]
to be the kernel of $T$.

\begin{lemma}
    $\ker T$ is a subspace of $V$, and $\Im T$ is a subspace of $W$.
\end{lemma}
\begin{proof}
    To verify that some subset is a subspace, it suffices to check that it is non-empty, and that it is closed under linear combinations.

    $\ker T$ is non-empty because $\vb 0 \in \ker T$. For $\vb x, \vb y \in \ker T$, we have $T(\lambda \vb x + \mu \vb y) = \lambda T(\vb x) + \mu T(\vb y) = \vb 0 \in \ker T$ as required.

    $\Im T$ is non-empty because $\vb 0 \in \Im T$. For $\vb x, \vb y \in V$, let $\vb x' = T(\vb x)$ and $\vb y' = T(\vb y)$, therefore $\vb x', \vb y' \in \Im T$. Now, $\lambda \vb x' + \mu \vb y' = T(\lambda \vb x + \mu \vb y)$ so it is closed under linear combinations as required.
\end{proof}
Here are some examples of images and kernels.
\begin{enumerate}[(i)]
    \item The zero linear map $\vb x \mapsto \vb 0$ has:
          \begin{align*}
              \Im T  & = \{ \vb 0 \} \\
              \ker T & = V
          \end{align*}
    \item The identity linear map $\vb x \mapsto \vb x$ has:
          \begin{align*}
              \Im T  & = V           \\
              \ker T & = \{ \vb 0 \}
          \end{align*}
    \item Let $T: \mathbb R^3 \to \mathbb R^3$, such that
          \begin{align*}
              x_1' & = 3x_1 - x_2 + 5x_3 \\
              x_2' & = -x_1 - 2x_3       \\
              x_3' & = 2x_1 + x_2 + 3x+3
          \end{align*}
          This map has
          \begin{align*}
              \Im T  & = \left\{ \lambda \begin{pmatrix} 3 \\ -1 \\ 2 \end{pmatrix} + \mu \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix} : \lambda, \mu \in \mathbb R \right\} \\
              \ker T & = \left\{ \lambda \begin{pmatrix} 2 \\ -1 \\ -1 \end{pmatrix} : \lambda \in \mathbb R \right\}
          \end{align*}
\end{enumerate}

\subsection{Rank and Nullity}
We define the rank of a linear map to be the dimension of its image, and the nullity of a linear map to be the dimension of its kernel.
\[ \rank T = \dim \Im T; \quad \nullity T = \dim \ker T \]
Note that therefore for $T: V \to W$, we have $\rank T \leq \dim W$ and $\ker T \leq \dim V$.
\begin{theorem}
    For some linear map $T: V \to W$,
    \[ \rank T + \nullity T = \dim V \]
\end{theorem}
\begin{proof}
    This proof is non-examinable (without prompts). Let $\vb e_1, \cdots, \vb e_k$ be a basis for $\ker T$, so $T(\vb e_i) = \vb 0$ for all valid $i$. We may extend this basis by adding more vectors $\vb e_i$ where $k < i \leq n$ until we have a basis for $V$, where $n=\dim V$. We claim that the set $\mathcal B = \{ T(\vb e_{k+1}), \cdots, T(\vb e_n) \}$ is a basis for $\Im T$. If this is true, then clearly the result follows because $k = \dim \ker T = \nullity T$ and $n-k = \dim \Im T = \rank T$.

    To prove the claim we need to show that $\mathcal B$ spans $\Im T$ and that it is a linearly independent set.
    \begin{itemize}
        \item $\mathcal B$ spans $\Im T$ because for any $\vb x = \sum_{i=1}^n x_i \vb e_i$, we have
              \[ T(\vb x) = \sum_{i=k+1}^n x_i T(\vb e_i) \in \vecspan \mathcal B \]
        \item $\mathcal B$ is linearly independent. Consider a general linear combination of basis vectors:
              \[ \sum_{i=k+1}^n \lambda_i T(\vb e_i) = 0 \implies T\left( \sum_{i=k+1}^n \lambda_i \vb e_i \right) = 0 \]
              so
              \[ \sum_{i=k+1}^n \lambda_i \vb e_i \in \ker T \]
              Because this is in the kernel, it may be written in terms of the basis vectors of the kernel. So, we have
              \[ \sum_{i=k+1}^n \lambda_i \vb e_i = \sum_{i=1}^k \mu_i \vb e_i \]
              This is a linear relation in terms of all basis vectors of $V$. So all coefficients are zero.
    \end{itemize}
\end{proof}

\subsection{Rotations}
Linear maps are often used to describe geometrical transformations, such as rotations, reflections, projections, dilations and shears. A convenient way to express these maps is by describing where the basis vectors are mapped to. In $\mathbb R^2$, we may describe a rotation anticlockwise around the origin by angle $\theta$ with
\begin{align*}
    \vb e_1 & \mapsto \cos \theta \vb e_1 + \sin \theta \vb e_2  \\
    \vb e_2 & \mapsto -\sin \theta \vb e_1 + \cos \theta \vb e_2
\end{align*}
In $\mathbb R^3$ we can construct a similar transformation for a rotation around the $\vb e_3$ axis with
\begin{align*}
    \vb e_1 & \mapsto \cos \theta \vb e_1 + \sin \theta \vb e_2  \\
    \vb e_2 & \mapsto -\sin \theta \vb e_1 + \cos \theta \vb e_2 \\
    \vb e_3 & \mapsto \vb e_3
\end{align*}
We can extend this to a general rotation in $\mathbb R^3$ about an axis given by a unit normal vector $\hat {\vb n}$. For any vector $\vb x \in \mathbb R^3$ we can resolve parallel and perpendicular to $\nhat$ as follows.
\[ \vb x = \vb x_\parallel + \vb x_\perp;\quad \vb x_\parallel = (\vb x \cdot \nhat) \nhat;\quad \vb x_\perp = x - (\vb x \cdot \nhat) \nhat \]
Note that $\nhat$ resembles the $\vb e_3$ axis here, and $\vb x_\perp$ resembles the $\vb e_1$ axis. So we can compute the equivalent of $\vb e_2$ using the cross product, $\nhat \times \vb x_\perp = \nhat \times \vb x$. Now we may define the map with
\begin{align*}
    \vb x_\parallel & \mapsto \vb x_\parallel                                               \\
    \vb x_\perp     & \mapsto (\cos \theta) \vb x_\perp + (\sin \theta)(\nhat \times \vb x)
\end{align*}
So all together, we have
\[ \vb x \mapsto (\cos \theta) \vb x + (1 - \cos \theta) (\nhat \cdot \vb x)\nhat + (\sin \theta)(\nhat \times \vb x) \]

\subsection{Reflections and Projections}
For a plane with normal $\nhat$, we define a projection to be
\begin{align*}
    \vb x_\parallel & \mapsto \vb 0                                           \\
    \vb x_\perp     & \mapsto \vb x_\perp                                     \\
    \vb x           & \mapsto \vb x_\perp = \vb x - (\vb x \cdot \nhat) \nhat
\end{align*}
and a reflection to be
\begin{align*}
    \vb x_\parallel & \mapsto -\vb x_\parallel                                                   \\
    \vb x_\perp     & \mapsto \vb x_\perp                                                        \\
    \vb x           & \mapsto \vb x_\perp - \vb x_\parallel = \vb x - 2(\vb x \cdot \nhat) \nhat
\end{align*}
The same expressions also apply in $\mathbb R^2$, where we replace the plane with a line.

\subsection{Dilations}
Given scale factors $\alpha, \beta, \gamma > 0$, we define a dilation along the axes by
\begin{align*}
    \vb e_1 & \mapsto \alpha \vb e_1 \\
    \vb e_2 & \mapsto \beta \vb e_2  \\
    \vb e_3 & \mapsto \gamma \vb e_3
\end{align*}

\subsection{Shears}
Let $\vb a, \vb b$ be orthogonal unit vectors in $\mathbb R^3$, i.e. $\abs{\vb a} = \abs{\vb b} = \vb 0$ and $\vb a \cdot \vb b = 0$, and we define a real parameter $\lambda$. A shear is defined as
\begin{align*}
    \vb x & \mapsto \vb x' = \vb x + \lambda \vb a (\vb x \cdot \vb b) \\
    \vb a & \mapsto \vb a                                              \\
    \vb b & \mapsto \vb b + \lambda \vb a
\end{align*}
This definition holds equivalently in $\mathbb R^2$.

\section{Matrices as Linear Maps}
\subsection{Definitions}
Consider a linear map $T: \mathbb R^n \to \mathbb R^m$, with standard bases $\{ \vb e_i \} \in \mathbb R^n$, $\{ \vb f_a \}, \in \mathbb R^m$, and with $T(\vb x) = \vb x'$.
Let further

\[ \vb x = \sum_i x_i \vb e_i = \begin{pmatrix}
        x_1 \\ x_2 \\ \vdots \\ x_n
    \end{pmatrix};\quad x' = \sum_a x_a' \vb f_a = \begin{pmatrix}
        x_1' \\ x_2' \\ \vdots \\ x_m'
    \end{pmatrix} \]

Linearity implies that $T$ is fixed by specifying
\[ T(\vb e_i) = \vb e_i' = \vb C_i \in \mathbb R^m \]
We take these $\vb C$ as columns of an $m \times n$ array or matrix $M$, with rows denoted as $\vb R_a \in \mathbb R^n$.

\[ \begin{pmatrix}
        \uparrow   &        & \uparrow   \\
        \vb C_1    & \cdots & \vb C_n    \\
        \downarrow &        & \downarrow
    \end{pmatrix} = M = \begin{pmatrix}
        \leftarrow & \vb R_1 & \rightarrow \\
                   & \vdots  &             \\
        \leftarrow & \vb R_m & \rightarrow
    \end{pmatrix} \]

$M$ has entries $M_{ai} \in \mathbb R$, where $a$ labels rows and $i$ labels columns, so
\[ (\vb C_i)_a = M_{ai} = (\vb R_a)_i \]
The action of $T$ is then given by the matrix $M$ multiplying the vector $\vb x$ in the following way:
\[ \vb x' = M \vb x \]
defined by
\[ x_a' = M_{ai}x_i \]
or explicitly:
\[
    \begin{pmatrix}
        x_1' \\ x_2' \\ \vdots \\ x_m'
    \end{pmatrix}
    =
    \begin{pmatrix}
        M_{11} & M_{12} & \cdots & M_{1n} \\
        M_{21} & M_{22} & \cdots & M_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        M_{m1} & M_{m2} & \cdots & M_{mn}
    \end{pmatrix}
    \begin{pmatrix}
        x_1 \\ x_2 \\ \vdots \\ x_n
    \end{pmatrix}
    =
    \begin{pmatrix}
        M_{11} x_1 + M_{12} x_2 + \cdots + M_{1n} x_n \\
        M_{21} x_1 + M_{22} x_2 + \cdots + M_{2n} x_n \\
        \vdots                                        \\
        M_{m1} x_1 + M_{m2} x_2 + \cdots + M_{mn} x_n
    \end{pmatrix}
\]
To check that the matrix multiplication above gives the action of $T$, we can plug in a generic value $\vb x$, and we get
\[ \vb x' = T\left(\sum_i x_i \vb e_i\right) = \sum_i x_i T(\vb e_i) = \sum_i x_i \vb C_i \]
and by taking component $a$ of the vector, we have
\[ x_a' = \sum_i x_i (\vb C_i)_a = \sum_i x_i M_{ai} \]
as required. Note also that
\[ x_a' = M_{ai}x_i = (\vb R_a)_i x_i = \vb R_a \cdot \vb x \]
We can now regard the properties of $T$ as properties of $M$ (suitably interpreted). For example:
\begin{itemize}
    \item $\Im(T) = \Im(M) = \vecspan \{ \vb C_1, \cdots, \vb C_n \}$. In words, the image of a matrix is the span of its columns.
    \item $\ker(T) = \ker(M) = \{ \vb x: \forall a, \vb R_a \cdot \vb x = 0 \}$. In some sense, the kernel of $M$ is the subspace perpendicular to all of its rows.
\end{itemize}

\subsection{Examples}
\begin{enumerate}[(i)]
    \item The zero map $\mathbb R^n \to \mathbb R^m$ corresponds to the zero matrix
          \[ M = 0 \text{ with } M_{ai} = 0 \]
    \item The identity map $\mathbb R^n \to \mathbb R^n$ corresponds to the identity (or unit) matrix
          \[ M = I \text{ with } I_{ij} = \delta_{ij} \]
    \item The map $\mathbb R^3 \to \mathbb R^3$ given by $\vb x' = T(\vb x) = M\vb x$ with
          \[ M = \begin{pmatrix}
                  3  & 1 & 5  \\
                  -1 & 0 & -2 \\
                  2  & 1 & 3
              \end{pmatrix} \]
          gives
          \[
              \begin{pmatrix}
                  x_1' \\ x_2' \\ x_3'
              \end{pmatrix}
              =
              \begin{pmatrix}
                  3x_1 + x_2 + 5x_3 \\
                  -x_1 - 2x_3       \\
                  2x_1 + x_2 + 3x_3
              \end{pmatrix}
          \]
          In this case, we may read off the column vectors $\vb C_a$ from the matrix. Note that since they form a linearly dependent set, we have
          \[ \Im(T) = \Im(M) = \vecspan \{ \vb C_1, \vb C_2, \vb C_3 \} = \vecspan \{ \vb C_1, \vb C_2 \} \]
          Here, $\vb R_2 \times \vb R_3 = \begin{pmatrix}
                  2 & -1 & -1
              \end{pmatrix} = \vb u$ is actually perpendicular to all rows as they form a linearly dependent set. So
          \[ \ker(T) = \ker(M) = \{ \lambda \vb u \} \]
    \item A rotation through $\theta$ in $\mathbb R^2$ is given by (building from the images of the basis vectors):
          \[ \begin{pmatrix}
                  \cos \theta & -\sin \theta \\
                  \sin \theta & \cos \theta
              \end{pmatrix} \]
    \item A dilation $\vb x' = M \vb x$ with scale factors $\alpha, \beta, \gamma$ along axes in $\mathbb R^3$ is given by
          \[ \begin{pmatrix}
                  \alpha & 0     & 0      \\
                  0      & \beta & 0      \\
                  0      & 0     & \gamma
              \end{pmatrix} \]
    \item A reflection in a plane perpendicular to a unit vector $\nhat$ is given by a matrix $H$ that must have the property that
          \begin{align*}
              \vb x' & = H \vb x = \vb x - 2(\vb x - \nhat) \nhat \\
              x_i'   & = x_i - 2x_jn_jn_i = H_{ij}x_j             \\
              \intertext{And by comparing coefficients of $x_j$, and using $\delta$ to rewrite $x_i$ using the $j$ index, we have}
              H_{ij} & = \delta_ij - 2n_in_j
          \end{align*}
          For example, with $\nhat = \frac{1}{\sqrt 3}\begin{pmatrix}
                  1 & 1 & 1
              \end{pmatrix}$, then $n_in_j = \frac{1}{3}$ for all $i, j$, so
          \[ H = \frac{1}{3}\begin{pmatrix}
                  1  & -2 & -2 \\
                  -2 & 1  & -2 \\
                  -2 & -2 & 1
              \end{pmatrix} \]
    \item A shear is defined by a matrix $S$ such that
          \[ \vb x' = S\vb x = \vb x + \lambda(\vb b \cdot \vb x)\vb a \]
          where $\vb a$, $\vb b$ are unit vectors with $\vb a \perp \vb b$, and where $\lambda$ is a real scale factor. Therefore:
          \begin{align*}
              x_i'              & = x_i + \lambda b_j x_j a_i = S_{ij}x_j \\
              \therefore S_{ij} & = \delta_{ij} + \lambda a_i b_j
          \end{align*}
          For example in $\mathbb R^2$ with $\vb a = \begin{pmatrix}
                  1 \\ 0
              \end{pmatrix}$ and $\vb b = \begin{pmatrix}
                  0 \\ 1
              \end{pmatrix}$, we have
          \[ S = \begin{pmatrix}
                  1 & \lambda \\ 0 & 1
              \end{pmatrix} \]
    \item A rotation matrix $R$ in $\mathbb R^3$ with axis $\nhat$ and angle $\theta$ must satisfy
          \begin{align*}
              \vb x'            & = R\vb x = (\cos \theta)\vb x + (1 - \cos \theta)(\nhat \cdot \vb x)\nhat + (\sin \theta)(\nhat \times \vb x) \\
              x_i'              & = (\cos \theta)x_i + (1 - \cos \theta)n_j x_j n_i - (\sin \theta) \varepsilon_{ijk}x_j n_k = R_{ij} x_j       \\
              \therefore R_{ij} & = \delta_{ij}(\cos \theta) - (1 - \cos \theta)n_in_j - (\sin \theta)\varepsilon_{ijk} n_k
          \end{align*}
\end{enumerate}

\subsection{Matrix of a General Linear Map}
Consider a linear map $T: V \to W$ between general real or complex vector spaces of dimension $n, m$ respectively. We will choose bases $\{ \vb e_i \}$ for $V$ and $\{ \vb f_a \}$ for $W$. The matrix representing the linear map $T$ with respect to these bases is an $m \times n$ array with entries $M_{ai} \in \mathbb R$ or $\mathbb C$ as appropriate, defined by
\[ T(\vb e_i) = \sum_a \vb f_a M_{ai} \]
Then
\[ \vb x' = T(\vb x) \iff x_a' = \sum_i M_{ai}x_i = M_{ai}x_i \]
where
\[ \vb x = \sum_i x_i \vb e_i;\quad \vb x' = \sum_a x_a \vb f_a \]
Note therefore that (in real vector spaces) given choices of bases $\{ \vb e_i \}$ and $\{ \vb f_a \}$, $V$ is identified with $\mathbb R_n$ in the sense that any vector has $n$ real components, and that $W$ is identified with $R_m$ analogously, and that therefore $T$ is identified with an $m\times n$ real matrix $M$. Note further that entries in column $i$ of $M$ are components of $T(\vb e_i)$ with respect to basis $\{ \vb f_a \}$.

\section{Matrix Algebra}
\subsection{Linear Combinations}
If $T: V \to W$ and $S: V \to W$, between real or complex vector spaces $V, W$ of dimension $n, m$ respectively, are linear, then
\[ \alpha T + \beta S: V \to W \]
is also a linear map, where
\[ (\alpha T + \beta S)(\vb x) = \alpha T(\vb x) + \beta S(\vb x) \]
for any $\vb x \in V$. So the set of linear maps is a vector space. If $M$ and $N$ are the $m\times N$ matrices for $T, S$ then $\alpha M + \beta N$ is the $m\times n$ matrix for the linear combination above, where
\[ (\alpha M + \beta N)_{ai} + \alpha M_{ai} + \beta N_{ai};\quad a = 1, \cdots, m;\quad i = 1, \cdots, n \]
with respect to the same bases.

\subsection{Matrix Multiplication}
If $A$ is an $m\times n$ matrix with entries $A_{ai}$, and $B$ is an $n \times p$ matrix with entries $B_{ir}$, then we define $AB$ to be an $m \times p$ matrix with entries
\[ (AB)_{ar} = A_{ai}B_{ir};\quad a = 1, \cdots, m;\quad i = 1, \cdots, n;\quad r = 1, \cdots, p \]
The product is not defined unless the amount of columns of $A$ matches the number of rows of $B$.

Matrix multiplication corresponds to composition of linear maps. Consider linear maps:
\begin{align*}
    S: \mathbb R^p \to \mathbb R^n                  & ;\; S(\vb x) = B \vb x,\, \vb x \in \mathbb R^p \\
    T: \mathbb R^n \to \mathbb R^m                  & ;\; T(\vb x) = A \vb x,\, \vb x \in \mathbb R^n \\
    \implies T \circ S: \mathbb R^p \to \mathbb R^m & ;\; (T\circ S)(\vb x) = (AB)x
\end{align*}
since
\[ \left[ (AB)\vb x \right]_a = (AB)_{ar}x_r \]
and
\[ A(B(\vb x)) = A_{ai} (B\vb x)_i = A_{ai} B_{ir} x_r = (AB)_{ar}x_r \]
as required. The definition of matrix multiplication ensures that these answers agree. Of course, this proof works for complex or general vector spaces.

\subsection{Properties of Matrix Product}
Whenever the products are defined, then for any scalars $\lambda$ and $\mu$:
\begin{itemize}
    \item $(\lambda M + \mu N)P = \lambda MP + \mu NP$
    \item $P(\lambda M + \mu N) = \lambda PM + \mu PN$
    \item $(MN)P = M(NP)$
    \item $IM = MI = M$ where $I_{ij} = \delta_{ij}$
\end{itemize}
We may view matrix multiplication in the following ways.
\begin{enumerate}[(i)]
    \item Regarding a vector $\vb x \in \mathbb R^n$ as a column vector (an $n \times 1$ matrix), then the matrix-vector and matrix-matrix multiplication rules agree.
    \item Consider the product $AB$ where $A$ is an $m \times n$ matrix and $B$ is an $n \times p$, with columns $\vb C_r(B) \in \mathbb R^n$ and columns $\vb C_r(AB) \in \mathbb R^m$, where $1 \leq r \leq p$. The columns are related by $\vb C_r(AB) = A \vb C_r(B)$. Less formally, eavh column in the right matrix is acted on by the left matrix as if it were a vector, then the resultant vectors are combined into the output matrix.
    \item In terms of rows and columns,
          \[ AB = \begin{pmatrix}
                             & \vdots     &             \\
                  \leftarrow & \vb R_n(A) & \rightarrow \\
                             & \vdots     &
              \end{pmatrix} \begin{pmatrix}
                         & \uparrow   &        \\
                  \cdots & \vb C_r(B) & \cdots \\
                         & \downarrow &
              \end{pmatrix} \]
          gives
          \begin{align*}
              (AB)_{ar} & = \left[ \vb R_a(A) \right]_i \left[ \vb C_r(B) \right]_i                                              \\
                        & = \vb R_a(A) \cdot \vb C_r(B) \text{ for real matrices, where the $\cdot$ is the dot product in $R^n$}
          \end{align*}
\end{enumerate}

\subsection{Matrix Inverses}
If $A$ is an $m \times n$ then $B$, an $n \times m$ matrix, is a left inverse of $A$ if $BA = I$ (the $n \times n$ identity matrix). $C$ is a right inverse of $A$ if $AC = I$ (the $m \times m$ identity matrix). If $m = n$ ($A$ is square), then one of these implies the other; there is no distinction between left and right inverses. We say that $B = C = A^{-1}$, \textit{the} inverse of the matrix $A$, such that $AA^{-1} = A^{-1}A = I$. Not every matrix has an inverse. If such an inverse exists, $A$ is called invertible, or non-singular.

Consider $\vb x, \vb x' \in \mathbb R^n$ or $\mathbb C^n$, and $M$ is an $n \times n$ matrix. If $M^{-1}$ exists, we can solve the equation $\vb x' = M \vb x$ for $\vb x$, given $\vb x'$, because we can apply the matrix inverse on the left. For example, where $n=2$, we have
\[ M = \begin{pmatrix}
        M_{11} & M_{12} \\
        M_{21} & M_{22}
    \end{pmatrix} \]
and
\begin{align*}
    x_1' & = M_{11}x_1 + M_{12}x_2 \\
    x_2' & = M_{21}x_1 + M_{22}x_2
\end{align*}
We can solve these simultaneous equations to construct the general matrix inverse.
\begin{align*}
    M_{22} x_1' - M_{12}x_2'  & = (\det M)x_1 \\
    -M_{21} x_1' + M_{11}x_2' & = (\det M)x_2
\end{align*}
where $\det M = M_{11} M_{22} - M_{12} M_{21}$, called the determinant of the matrix. Where the determinant is nonzero, the matrix inverse
\[ M^{-1} = \frac{1}{\det M}\begin{pmatrix}
        M_{22}  & -M_{12} \\
        -M_{21} & M_{11}
    \end{pmatrix} \]
exists. Note that
\begin{align*}
    \vb C_1     & = M \vb e_1 = \begin{pmatrix} M_{11} \\ M_{21} \end{pmatrix}                            \\
    \vb C_2     & = M \vb e_2 = \begin{pmatrix} M_{12} \\ M_{22} \end{pmatrix}                            \\
    \iff \det M & = [\vb C_1, \vb C_2] = [M\vb e_1, M\vb e_2] \text{ in } \mathbb R^2
\end{align*}
So the determinant gives the signed factor by which areas are scaled under the action of $M$. $\det M$ is nonzero if and only if $M\vb e_1$ and $M\vb e_2$ are linearly independent, which is true if and only if the image of $M$ has dimension 2, i.e. $M$ has maximal rank. For example, a shear
\[ S(\lambda) = \begin{pmatrix}
        1 & \lambda \\ 0 & 1
    \end{pmatrix} \]
has determinant 1, so areas are preserved. In particular, in this case,
\[ S^{-1}(\lambda) = \begin{pmatrix}
        1 & -\lambda \\ 0 & 1
    \end{pmatrix} = S(-\lambda) \]
As another example, we know that a matrix $R(\theta)$ for a rotation about a fixed axis $\nhat$ through angle $\theta$ has formula
\begin{align*}
    R(\theta)_{ij} R(-\theta)_{jk} & = (\delta_{ij}\cos \theta + (1 - \cos \theta) n_i n_j - \varepsilon_{ijp}n_p \sin \theta) \times (\delta_{jk}\cos \theta + (1 - \cos \theta) n_j n_k + \varepsilon_{jkq}n_q \sin \theta) \\
    \intertext{Expanding out, noting that $n_in_i = 1$ as $\nhat$ is a unit vector, and cancelling:}
                                   & = \delta_{ik} \cos^2 \theta + 2\cos \theta(1 - \cos \theta) n_in_k + (1 - \cos \theta)^2n_in_k - \varepsilon_{ijp}\varepsilon_{jkq} n_p n_q \sin^2 \theta                                \\
    \intertext{By using an $\varepsilon\varepsilon$ identity:}
                                   & = \delta_{ik}\cos^2\theta + (1 - \cos^2 \theta)n_in_k + \delta_{ik}n_pn_p \sin^2 \theta - (\sin^2 \theta)n_in_k                                                                          \\
                                   & = \delta_{ik}\cos^2\theta + \delta_{ik}n_pn_p \sin^2 \theta                                                                                                                              \\
                                   & = \delta_{ik}\cos^2\theta + \delta_{ik} \sin^2 \theta                                                                                                                                    \\
                                   & = \delta_{ik}
\end{align*}
as required.

\section{Real and Complex Transposes and Conjugates}
\subsection{Transpose}
If $M$ is an $m \times n$ (real or complex) matrix, the transpose $M^\transpose$ is an $n \times m$ matrix defined by
\[ (M^\transpose)_{ia} = M_{ai} \]
which essentially exchanges rows and columns. Here are some key properties.
\begin{itemize}
    \item $(\alpha A + \beta B))^\transpose = \alpha A^\transpose + \beta B^\transpose$ for $\alpha, \beta$ scalars, and $A, B$ both $m \times n$ matrices.
    \item $(AB)^\transpose = B^\transpose A^\transpose$, where $A$ is $m \times n$ and $B$ is $n \times p$. This is because
          \begin{align*}
              [(AB)^\transpose]_{ra} & = (AB)_{ar}                               \\
                                     & = A_{ai} B_{ir}                           \\
                                     & = (A^\transpose)_{ia} (B^\transpose)_{ri} \\
                                     & = (B^\transpose)_{ri} (A^\transpose)_{ia} \\
                                     & = (B^\transpose A^\transpose)_{ra}
          \end{align*}
    \item If $\vb x$ is a column vector (or an $n \times 1$ matrix), $\vb x^\transpose$ is the equivalent row vector (a $1 \times n$ matrix).
    \item The inner product in $\mathbb R^n$ can therefore be written $\vb x \cdot \vb y = \vb x^\transpose \vb y$. Note that this is not equivalent to $\vb x \vb y^\transpose$, which is known as the outer product, which results in a matrix not a scalar.
    \item If $M$ is $n \times n$ (square) then $M$ is:
          \begin{itemize}
              \item symmetric iff $M^\transpose = M$, or $M_{ij} = M_{ji}$
              \item antisymmetric iff $M^\transpose = -M$, or $M_{ij} = -M_{ji}$
          \end{itemize}
    \item Any $M$ which is square can be written as a sum of a symmetric and and an antisymmetric part
          \[ M = S + A\quad\text{where } S = \frac{1}{2}(M + M^\transpose);\quad A = \frac{1}{2}(M - M^\transpose) \]
          as $S$ is symmetric and $A$ is antisymmetric by construction.
    \item If $A$ is $3 \times 3$ and antisymmetric, then we can write
          \[ A_{ij} = \varepsilon_{ijk}a_k\text{ where } A = \begin{pmatrix}
                  0    & a_3  & -a_2 \\
                  -a_3 & 0    & a_1  \\
                  a_2  & -a_1 & 0
              \end{pmatrix} \]
          Then, we have
          \[
              (A \vb x)_i = \varepsilon_{ijk}a_k x_j = (\vb x \times \vb a)_i
          \]
\end{itemize}

\subsection{Hermitian Conjugate}
Let $M$ be an $m \times n$ matrix. Then the Hermitian conjugate (also known as the conjugate transpose) $M^\dagger$ is an $n \times m$ matrix defined by
\[
    (M^\dagger)_{ia} = \overline{M_{ai}}
\]
If $M$ is square, then $M$ is Hermitian if and only if $M^\dagger = M$, or alternatively $M_{ia} = \overline{M_{ai}}$; $M$ is anti-Hermitian if $M^\dagger = -M$, or alternatively $M_{ia} = -\overline{M_{ai}}$. Similarly to above, if $\vb z$ is a column vector in $\mathbb C^n$ (an $n \times 1$ matrix), then the complex inner product is given by $\vb z \cdot \vb w = \vb z^\dagger \vb w$.

\subsection{Trace}
For a complex $n \times n$ (square) matrix $M$, the trace of the matrix, denoted $\tr(M)$, is defined by
\[ \tr(M) = M_{ii} = M_{11} + M_{22} + \cdots + M_{nn} \]
It has a number of key properties.
\begin{itemize}
    \item $\tr(\alpha M + \beta N) = \alpha \tr M + \beta \tr N$ where $\alpha$ and $\beta$ are scalars, and $M$ and $N$ are $n \times n$ matrices.
    \item $\tr(MN) = \tr(NM)$ where $M$ is $m \times n$ and $N$ is $n \times m$. $MN$ and $NM$ need not have the same dimension, but their traces are identical. We can check this as follows: $\tr(MN) = (MN)_{aa} = M_{ai} N_{ia} = N_{ia} M_{ai} = (NM)_{ii} = \tr(NM)$.
    \item $\tr(M^\transpose) = \tr(M)$
    \item $\tr(I) = \delta_{ii} = n$ where $n$ is the dimensionality of the vector space.
    \item If $S$ is $n \times n$ and symmetric, let
          \begin{align*}
              T                             & = S - \frac{1}{n}\tr(S) I                \\
              \text{or } T_{ij}             & = S_{ij} - \frac{1}{n}\tr(S) \delta_{ij} \\
              \text{then } \tr (T) = T_{ii} & = S_{ii} = \frac{1}{n}\tr(S) \delta_{ii} \\
                                            & = \tr(S) - \frac{1}{n}\tr(S) = 0
          \end{align*}
          Then $S = T + \frac{1}{n}\tr(S)I$ where $T$ is traceless and the right hand term $\frac{1}{n}\tr(S)I$ is `pure trace'.
    \item If $A$ is $n \times n$ antisymmetric, $\tr(A) = A_{ii} = 0$.
\end{itemize}

\subsection{Orthogonal Matrices}
A real $n \times n$ matrix $U$ is orthogonal if and only if its transpose is its inverse.
\[ U^\transpose U = UU^\transpose = I \]
These conditions can be written
\[ U_{ki}U_{kj} = U_{ik}U_{jk} = \delta_{ij} \]
In words, the left hand side says that the columns of $U$ are orthonormal, and the middle part of the equation says that the rows of $U$ are orthonormal.
\[
    U^\transpose U = \begin{pmatrix}
                   & \vdots  &             \\
        \leftarrow & \vb C_i & \rightarrow \\
                   & \vdots  &
    \end{pmatrix}
    \begin{pmatrix}
               & \uparrow   &        \\
        \cdots & \vb C_j    & \cdots \\
               & \downarrow &
    \end{pmatrix}
    = \begin{pmatrix}
        1      & \cdots & 0      \\
        \vdots & \ddots & \vdots \\
        0      & \cdots & 1
    \end{pmatrix}
\]
For example, if $U = R(\theta)$ is a rotation through $\theta$ around an axis $\nhat$, then $U^\transpose = R(\theta)^\transpose = R(-\theta) = R(\theta)^{-1} = U^{-1}$. An equivalent definition for orthogonality is: $U$ is orthogonal if and only if it preserves the inner product on $\mathbb R^n$.
\[ (U\vb x)\cdot(U \vb y) = \vb x \cdot \vb y\quad \forall \vb x, \vb y \in \mathbb R^n \]
To check equivalence:
\begin{align*}
    (U\vb x)\cdot(U \vb y) & = (U\vb x)^\transpose (U\vb y)             \\
                           & = (\vb x^\transpose U^\transpose) (U\vb y) \\
                           & = \vb x^\transpose (U^\transpose U) \vb y  \\
                           & = \vb x^\transpose \vb y                   \\
                           & = \vb x \cdot \vb y
\end{align*}
which is true if and only if $U^\transpose U = I$. Note that in $\mathbb R^n$, the columns of $U$ are $U\vb e_i, \cdots, U\vb e_n$ so the inner product is preserved when $U$ acts on the standard basis vectors if and only if
\[ (U\vb e_i)\cdot(U\vb e_j) = \vb e_i \cdot \vb e_j = \delta_{ij} \]
i.e. the columns of $U$ are orthonormal.

Let us now try to find a general $2 \times 2$ orthogonal matrix. We begin by transforming the basis vectors. $\vb e_i = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$ must be transformed to a unit vector. Therefore, in the most general sense:
\[ U \begin{pmatrix}
        1 \\0
    \end{pmatrix} = \begin{pmatrix}
        \cos \theta & \sin \theta
    \end{pmatrix} \]
for some parameter $\theta$. Now, the other basis vector $\vb e_2$ must be orthogonal to it, and so it must be
\[
    U \begin{pmatrix}
        0 \\ 1
    \end{pmatrix} = \pm\begin{pmatrix}
        -\sin \theta \\
        \cos \theta
    \end{pmatrix}
\]
So we have two cases:
\[ U = R = \begin{pmatrix}
        \cos \theta & -\sin\theta \\ \sin \theta & \cos \theta
    \end{pmatrix};\quad U = H = \begin{pmatrix}
        \cos \theta & \sin \theta \\ \sin \theta & -\cos \theta
    \end{pmatrix} \]
where $R$ is a rotation by $\theta$ and $H$ is a reflection in $\mathbb R^2$, where
\[ \nhat = \begin{pmatrix}
        -\sin \frac{\theta}{2} \\ \cos \frac{\theta}{2}
    \end{pmatrix} \]
because
\[ H_{ij} = \delta_{ij} - 2n_in_j \therefore H = \begin{pmatrix}
        1 - 2 \sin^2 \frac{\theta}{2}              & 2\sin\frac{\theta}{2}\cos\frac{\theta}{2} \\
        2\sin\frac{\theta}{2} \cos\frac{\theta}{2} & 1-2\cos^2\frac{\theta}{2}
    \end{pmatrix} \]
which simplifies as required. Note that $\det R = +1$, but $\det H = -1$.

\subsection{Unitary Matrices}
A complex $n \times n$ matrix $U$ is called unitary if and only if
\[ U^\dagger U = U U^\dagger = I \]
Equivalently, $U$ is unitary if and only if it preserves the complex inner product on $\mathbb C_n$:
\[ \langle U \vb z, U \vb w \rangle = \langle \vb z, \vb w \rangle\quad \forall \vb z, \vb w \in \mathbb C^n \]
To check equivalence:
\begin{align*}
    \langle U \vb z, U \vb w \rangle & = (U \vb z)^\dagger (U \vb w)         \\
                                     & = (\vb z^\dagger U^\dagger) (U \vb w) \\
                                     & = \vb z^\dagger (U^\dagger U) \vb w   \\
                                     & = \vb z^\dagger \vb w
\end{align*}
which of course matches if and only if $U^\dagger U = I$.

\section{Adjugates and Alternating Forms}
\subsection{Inverses in Two Dimensions}
Consider a linear map $T\colon \mathbb R^n \to \mathbb R^n$. If $T$ is invertible (i.e. bijective), then $\ker T = \{ \vb 0 \}$ as $T$ is injective, and $\Im T = \mathbb R^n$ as $T$ is surjective. These conditions are actually equivalent due to the rank-nullity theorem. Conversely, if the conditions hold, then $T(\vb e_1), T(\vb e_2), \cdots, T(\vb e_n)$ must be a basis of the image, so we can just define $T^{-1}$ by defining its actions on the basis vectors $T(\vb e_1), T(\vb e_2) \cdots T(\vb e_n)$, specifically mapping them to the standard basis.

How can we test whether the conditions above hold for a matrix $M$ representing $T$, and how can we find $M^{-1}$ from $M$ explicitly? For any $n \times n$ matrix $M$ (not necessarily invertible), we will define the adjugate matrix $\adjugate M$ and the determinant $\det M$ such that
\[ \adjugate M M = (\det M) I \tag{$\ast$} \]
Then if $\det M \neq 0$, $M$ is invertible, where
\[ M^{-1} = \frac{1}{\det M}\adjugate M \]
From $n=2$, recall that ($\ast$) holds with
\[ M = \begin{pmatrix}
        M_{11} & M_{21} \\
        M_{12} & M_{22}
    \end{pmatrix};\quad \adjugate M = \begin{pmatrix}
        M_{22}  & -M_{21} \\
        -M_{12} & M_{11}
    \end{pmatrix};\quad \det M = [M\vb e_1, M\vb e_2] = \varepsilon_{ij}M_{i1}M_{j2} \]
The determinant in this case is the factor by which areas scale under $M$. $\det M \neq 0$ if and only if $M\vb e_1, M\vb e_2$ are linearly independent.

\subsection{Three Dimensions}
For $n=3$, we will define similarly
\[ \det M = [M\vb e_1, M\vb e_2, M\vb e_3] = \varepsilon_{ijk}M_{i1}M_{j2}M_{k3} \]
We define it like this because this is the factor by which volumes scale under $M$ in three dimensions. So
\[ \det M \neq 0 \iff \{ M \vb e_1, M \vb e_2, M \vb e_3 \} \text{ linearly independent, or } \Im M = \mathbb R^3 \]
Now we define $\adjugate M$ from $M$ using row/column notation.
\begin{align*}
    \vb R_1(\adjugate M) & = \vb C_2(M) \times \vb C_3(M) \\
    \vb R_2(\adjugate M) & = \vb C_3(M) \times \vb C_1(M) \\
    \vb R_3(\adjugate M) & = \vb C_1(M) \times \vb C_2(M)
\end{align*}
Note that therefore,
\[ (\adjugate M M)_{ij} = \vb R_i(\adjugate M) \cdot \vb C_j(M) = \underbrace{(\vb C_1(M) \times \vb C_2(M) \cdot \vb C_3(M))}_{\det M}\delta_{ij} \]
as claimed. For example, let us invert the following matrix.
\begin{align*}
    M                      & = \begin{pmatrix}
        1 & 3 & 0 \\ 0 & -1 & -2 \\ 4 & 1 & -1
    \end{pmatrix}                                                                  \\
    \vb C_2 \times \vb C_3 & = \begin{pmatrix} 3 \\ -1 \\ 1 \end{pmatrix} \times \begin{pmatrix} 0 \\ 2 \\ -1 \end{pmatrix} = \begin{pmatrix} -1 \\ 3 \\ 6 \end{pmatrix} \\
    \vb C_3 \times \vb C_1 & = \begin{pmatrix} 0 \\ 2 \\ -1 \end{pmatrix} \times \begin{pmatrix} 1 \\ 0 \\ 4 \end{pmatrix} = \begin{pmatrix} 8 \\ -1 \\ -2 \end{pmatrix} \\
    \vb C_1 \times \vb C_2 & = \begin{pmatrix} 1 \\ 0 \\ 4 \end{pmatrix} \times \begin{pmatrix} 3 \\ -1 \\ 1 \end{pmatrix} = \begin{pmatrix} 4 \\ 11 \\ -1 \end{pmatrix} \\
    \adjugate M            & = \begin{pmatrix}
        -1 & 3 & 6 \\ 8 & -1 & -2 \\ 4 & 11 & -1
    \end{pmatrix}                                                                  \\
    \det M                 & = \vb C_1 \cdot \vb C_2 \times \vb C_3 = 23                                                    \\
    \adjugate M M          & = 23 I
\end{align*}

\subsection{$\varepsilon$ in Higher Dimensions}
Recall (from IA Groups):
\begin{itemize}
    \item A permutation $\sigma$ on the set $\{ 1, 2, \cdots, n \}$ is a bijection from the set to itself, specified by an ordered list $\sigma(1), \sigma(2), \cdots, \sigma(n)$.
    \item Permutations form a group $S_n$, called the symmetric group of order $n!$
    \item A transposition $\tau = (p, q)$ where $p \neq q$ is a permutation that swaps $p$ and $q$.
    \item Any permutation is a product of of $k$ transpositions, where $k$ is unique modulo 2 for a given $\sigma$. In this course, we will write $\varepsilon(\sigma)$ to mean the sign (or signature) of the permutation, $(-1)^k$. $\sigma$ is even if the sign is 1, and odd if the sign is $-1$.
\end{itemize}
The alternating symbol $\varepsilon$ in $\mathbb R^n$ or $\mathbb C^n$ is an $n$-index object (tensor) defined by
\[ \varepsilon_{\underbrace{ij\cdots l}_{\mathclap{n \text{ indices}}}} = \begin{cases}
        +1 & \text{if } i, j \cdots, l \text{ is an even permutation of } 1, 2, \cdots, n \\
        -1 & \text{if } i, j \cdots, l \text{ is an odd permutation of } 1, 2, \cdots, n  \\
        0  & \text{otherwise, i.e. if any indices take the same value}
    \end{cases} \]
Thus if $\sigma$ is any permutation, then
\[ \varepsilon_{\sigma(1)\cdots\sigma(n)} = \varepsilon(\sigma) \]
So $\varepsilon_{ij\cdots l}$ is totally antisymmetric and changes sign whenever a pair of indices are exchanged.
\begin{definition}
    Given vectors $\vb v_1, \cdots \vb v_n \in \mathbb R^n$ or $\mathbb C^n$, the alternating form combines them to give the scalar
    \begin{align*}
        [\vb v_1, \vb v_2, \cdots, \vb v_n ] & = \varepsilon_{ij\cdots l} (\vb v_1)_i (\vb v_2)_j \cdots (\vb v_n)_l                                                            \\
                                             & = \sum_{\sigma \in S_n} \varepsilon(\sigma) \cdot (\vb v_1)_{\sigma(1)} \cdot (\vb v_2)_{\sigma(2)} \cdots (\vb v_n)_{\sigma(n)}
    \end{align*}
\end{definition}

\subsection{Properties}
\begin{enumerate}[(i)]
    \item The alternating form is multilinear.
          \[ [ \vb v_1, \cdots, \vb v_{p-1}, \alpha \vb u + \beta \vb w, \vb v_{p+1} \cdots, \vb v_n ] = \alpha [ \vb v_1, \cdots, \vb v_{p-1}, \vb u, \vb v_{p+1} \cdots, \vb v_n ] + \beta [ \vb v_1, \cdots, \vb v_{p-1}, \vb w, \vb v_{p+1} \cdots, \vb v_n ] \]
    \item It is totally antisymmetric. $[ \vb v_{\sigma(1)}, \vb v_{\sigma(2)}, \cdots, \vb v_{\sigma(n)} ] = \varepsilon(\sigma) [ \vb v_1, \cdots, \vb v_n ]$
    \item Standard basis vectors give a positive result: $[\vb e_i, \cdots, \vb e_n] = 1$.
\end{enumerate}
These three properties fix the alternating form completely, and they also imply
\begin{enumerate}[(i)]
    \setcounter{enumi}{3}
    \item If $\vb v_p = \vb v_q$ where $p \neq q$, then
          \[ [\vb v_1, \cdots, \vb v_p, \cdots, \vb v_q, \cdots, \vb v_n ] = 0 \]
    \item If $v_p$ can be written as a non-trivial linear combination of the other vectors, then
          \[ [\vb v_1, \cdots, \vb v_p, \cdots, \vb v_n ] = 0 \]
\end{enumerate}
Property (iv) follows from property (ii), where we swap $\vb v_p$ and $\vb v_q$. Property (v) follows from substituting the linear combination representation of $\vb v_p$ into the alternating form expression, the using properties (i) and (iv).

\section{Investigation into Alternating Forms}
\subsection{Checking Properties}
To justify (ii) above, it suffices to check a transposition $\tau = (p\ q)$ where (without loss of generality) $p < q$, then since transpositions generate all permutations the result follows.
\begin{align*}
     & [\vb v_1, \cdots, \vb v_{p-1}, \vb v_q, \vb v_{p+1}, \cdots, \vb v_{q-1}, \vb v_p, \vb v_{q+1}, \cdots, \vb v_n]                                                                                                                          \\
     & = \sum_\sigma \varepsilon(\sigma) (\vb v_1)_{\sigma(1)} \cdots (\vb v_{p-1})_{\sigma(p-1)}(\vb v_q)_{\sigma(p)}(\vb v_{p+1})_{\sigma(p+1)} \cdots (\vb v_{q-1})_{\sigma(q-1)}(\vb v_p)_{\sigma(q)}(\vb v_{q+1})_{\sigma(q+1)}             \\
     & = \sum_\sigma \varepsilon(\sigma) (\vb v_1)_{\sigma'(1)} \cdots (\vb v_{p-1})_{\sigma'(p-1)}(\vb v_q)_{\sigma'(q)}(\vb v_{p+1})_{\sigma'(p+1)} \cdots (\vb v_{q-1})_{\sigma'(q-1)}(\vb v_p)_{\sigma'(p)}(\vb v_{q+1})_{\sigma'(q+1)}      \\
    \intertext{where $\sigma' = \sigma\tau$}
     & = -\sum_{\sigma'} \varepsilon(\sigma') (\vb v_1)_{\sigma'(1)} \cdots (\vb v_{p-1})_{\sigma'(p-1)}(\vb v_p)_{\sigma'(p)}(\vb v_{p+1})_{\sigma'(p+1)} \cdots (\vb v_{q-1})_{\sigma'(q-1)}(\vb v_q)_{\sigma'(q)}(\vb v_{q+1})_{\sigma'(q+1)} \\
     & = -[\vb v_1, \cdots, \vb v_{p-1}, \vb v_p, \vb v_{p+1}, \cdots, \vb v_{q-1}, \vb v_q, \vb v_{q+1}, \cdots, \vb v_n]
\end{align*}
as required.

\begin{proposition}
    $[ \vb v_1, \vb v_2, \cdots, \vb v_n] \neq 0$ if and only if $\vb v_1, \vb v_2, \cdots, \vb v_n$ are linearly independent.
\end{proposition}
\begin{proof}
    To show the forward implication, let us suppose that they are not linearly independent and use property (v). Then we can express some $\vb v_p$ as a linear combination of the others. Then $[\vb v_1, \vb v_2, \cdots, \vb v_n] = 0$.

    To show the other direction, note that $\vb v_1, \vb v_2, \cdots, \vb v_3$ means that they span, and if they span then each of the standard basis vectors $\vb e_i$ can be written as a linear combination of the $\vb v$ vectors, i.e. $\vb e_i = U_{ai} \vb v_a$. Then
    \begin{align*}
        [\vb e_1, \vb e_2, \cdots, \vb e_n] & = [U_{a1}\vb v_a, U_{b2}\vb v_b, \cdots, U_{cn}\vb v_c]                                 \\
                                            & = U_{a1}U_{b2}\cdots U_{cn}[\vb v_a, \vb v_b, \cdots, \vb v_c]                          \\
                                            & = U_{a1}U_{b2}\cdots U_{cn} \varepsilon_{ab\cdots c}[\vb v_1, \vb v_2, \cdots, \vb v_n]
    \end{align*}
    By definition, the left hand side is $+1$, so $[\vb v_1, \vb v_2, \cdots, \vb v_n]$ is nonzero.
\end{proof}
As an example of these ideas, let
\[ \vb v_1 = \begin{pmatrix} i \\ 0 \\ 0 \\ 2 \end{pmatrix};\quad\vb v_2 = \begin{pmatrix} 0 \\ 0 \\ 5i \\ 0 \end{pmatrix};\quad\vb v_3 = \begin{pmatrix} 3 \\ 2i \\ 0 \\ 0 \end{pmatrix};\quad\vb v_4 = \begin{pmatrix} 0 \\ 0 \\ i \\ 1 \end{pmatrix};\quad \text{where }\vb v_j \in \mathbb C_4 \]
Then
\begin{align*}
    [\vb v_1, \vb v_2, \vb v_3, \vb v_4]
     & = 5i[\vb v_1, \vb e_3, \vb v_3, \vb v_4]                                      \\
     & = 5i[i\vb e_1 + 2\vb e_4, \vb e_3, 3\vb e_1 + 2i\vb e_2, -i\vb e_3 + \vb e_4] \\
    \intertext{By multilinearity, we can eliminate all $\vb e_3$ terms not in the second position because they will cancel with it, giving}
     & = 5i[i\vb e_1 + 2\vb e_4, \vb e_3, 3\vb e_1 + 2i\vb e_2, \vb e_4]             \\
    \intertext{And likewise with $\vb e_4$:}
     & = 5i[i\vb e_1, \vb e_3, 3\vb e_1 + 2i\vb e_2, \vb e_4]                        \\
    \intertext{And again with $\vb e_1$:}
     & = 5i[i\vb e_1, \vb e_3, 2i\vb e_2, \vb e_4]                                   \\
     & = 5i\cdot 2i \cdot i[\vb e_1, \vb e_3, \vb e_2, \vb e_4]                      \\
     & = 10i[\vb e_1, \vb e_2, \vb e_3, \vb e_4]                                     \\
     & = 10i
\end{align*}

\subsection{Defining the Determinant}
For an $n \times n$ matrix $M$ with columns $\vb C_a = M\vb e_a$, then the determinant $\det(M) = \abs{M} \in \mathbb R$ or $\mathbb C$ is given by any of the following equivalent definitions.
\begin{align*}
    \det M
     & = [\vb C_1, \vb C_2, \cdots, \vb C_n]                                                \\
     & = [M\vb e_1, M\vb e_2, \cdots, M\vb e_n]                                             \\
     & = \varepsilon_{ij\cdots l}M_{i1}M_{j2} \cdots M_{ln}                                 \\
     & = \sum_\sigma \varepsilon(\sigma) M_{\sigma(1)1}M_{\sigma(2)2} \cdots M_{\sigma(n)n}
\end{align*}
Here are some examples.
\begin{enumerate}[(i)]
    \item $n=2$
          \[ \det M = \sum_\sigma M_{\sigma(1)1}M_{\sigma(2)2} = \begin{vmatrix}
                  M_{11} & M_{21} \\ M_{12} & M_{22}
              \end{vmatrix} = M_{11}M_{22} - M_{12}M_{21} \]
    \item $M$ diagonal, i.e. $M_{ij} = 0$ for $i \neq j$
          \[ M = \begin{pmatrix}
                  M_{11} & 0      & \cdots & 0      \\
                  0      & M_{22} & \cdots & 0      \\
                  \vdots & \vdots & \ddots & \vdots \\
                  0      & 0      & \cdots & M_{nn}
              \end{pmatrix} \implies \det M = M_{11}M_{22}\cdots M_{nn} \]
    \item Let $M$ be $n\times n$, $A$ be $(n-1) \times (n-1)$, where
          \[ M = \left( \begin{array}{c|c}
                      A & 0 \\\hline
                      0 & 1
                  \end{array} \right) \]
          We call $M$ a matrix `in block form'. So $M_{ni} = M_{in} = 0$ if $i \neq n$. So we can restrict the permutation $\sigma$ to only transmuting the first $(n-1)$ terms, i.e. $\sigma(n) = n$. So $\det M = \det A$.
\end{enumerate}

\begin{proposition}
    If $\vb R_a$ are the rows of $M$, $\det M$ is given by
    \begin{align*}
        \det M
         & = [\vb R_1, \vb R_2, \cdots, \vb R_n]                                                \\
         & = \varepsilon_{ij\cdots l}M_{1i}M_{2j} \cdots M_{nl}                                 \\
         & = \sum_\sigma \varepsilon(\sigma) M_{1\sigma(1)}M_{2\sigma(2)} \cdots M_{n\sigma(n)}
    \end{align*}
    i.e. $\det M = \det M^\transpose$.
\end{proposition}
\begin{proof}
    Recall that $(\vb C_a)_i = M_{ia} = (\vb R_i)_a$. We need to show that one of these definitions is equivalent to one of the previous definitions, then all other equivalent definitions follow. We use the $\Sigma$ definition by considering the product $M_{1\sigma(1)}M_{2\sigma(2)} \cdots M_{n\sigma(n)}$. We may rewrite this product in a different order: $M_{\rho(1)1}M_{\rho(2)2} \cdots M_{\rho(n)n}$. Then $\rho = \sigma^{-1}$. But then $\varepsilon(\sigma) = \varepsilon(\rho)$, and a sum over $\sigma$ is equivalent to a sum over $\rho$.
\end{proof}

\subsection{Evaluating Determinants: Expanding by Rows or Columns}
For an $n \times n$ matrix $M$ with entries $M_{ia}$, we define the minor $M^{ia}$ to be the $(n-1)\times(n-1)$ determinant of the matrix obtained by deleting row $i$ and column $a$ from $M$.
\begin{proposition}
    The determinant of a generic $n \times n$ matrix $M$ is given by
    \begin{align*}
        \det M
         & = \sum_i (-1)^{i+a} M_{ia} M^{ia} \text{ for a fixed $a$} \\
         & = \sum_a (-1)^{i+a} M_{ia} M^{ia} \text{ for a fixed $i$}
    \end{align*}
\end{proposition}
This process is known as expanding by row $i$ or by column $a$. As an example, let us take the following $4 \times 4$ complex matrix
\[ M = \begin{pmatrix}
        i & 0  & 3  & 0  \\
        0 & 0  & 2i & 0  \\
        0 & 5i & 0  & -i \\
        2 & 0  & 0  & 1
    \end{pmatrix} \]
Then, the determinant is given by (expanding by row 3)
\begin{align*}
    \det M
     & = -5i\begin{vmatrix}
        i & 3  & 0 \\
        0 & 2i & 0 \\
        2 & 0  & 1
    \end{vmatrix} + i\begin{vmatrix}
        i & 0 & 3  \\
        0 & 0 & 2i \\
        2 & 0 & 0
    \end{vmatrix}                                                               \\
     & = -5i\left[i\begin{vmatrix}
            2i & 0 \\
            0  & 1
        \end{vmatrix} - 3 \begin{vmatrix}
            0 & 0 \\
            2 & 1
        \end{vmatrix}\right] + i\left[-2i\begin{vmatrix}
            i & 0 \\
            2 & 0
        \end{vmatrix}\right] \\
     & = -5i[i \cdot 2i - 3 \cdot 0] + i[-2i \cdot 0]                                                                                \\
     & = -5i[-2] + i[0]                                                                                                              \\
     & = 10i
\end{align*}

\section{Constructing General Determinants and Inverses}
\subsection{Simplifying Determinants: Row and Column Operations}
Consider the following consequences of the properties of the determinant:
\begin{itemize}
    \item (row and column scaling) If $\vb R_i \mapsto \lambda \vb R_i$ for a fixed $i$, or $\vb C_a \mapsto \lambda \vb C_a$, then $\det M \mapsto \lambda \det M$ by multilinearity. If we scale all rows or columns, then $M \mapsto \lambda M$, so $\det M \mapsto \lambda^n \det M$ where $M$ is an $n \times n$ matrix.
    \item (row and column operations) If $\vb R_i \mapsto \vb R_i + \lambda \vb R_j$ where $i \neq j$ (or the corresponding conversion with columns), then $\det M \mapsto \det M$.
    \item (row and column exchanges) If we swap $\vb R_i$ and $\vb R_j$ (or two columns), then $\det M \mapsto -\det M$.
\end{itemize}
For example, let us find the terminant of matrix $A$, where
\[ A = \begin{pmatrix}
        1 & 1 & a \\ a & 1 & 1 \\ 1 & a & 1
    \end{pmatrix};\quad a \in \mathbb C \]
Then:
\begin{align*}
    \det A                                                    & = \begin{vmatrix} 1 & 1 & a \\ a & 1 & 1 \\ 1 & a & 1 \end{vmatrix}                            \\
    \vb C_1 \mapsto \vb C_1 - \vb C_3:\quad \det A            & = \begin{vmatrix} 1-a & 1 & a \\ a-1 & 1 & 1 \\ 0 & a & 1 \end{vmatrix}                            \\
    \det A                                                    & = (1-a)\begin{vmatrix} 1 & 1 & a \\ -1 & 1 & 1 \\ 0 & a & 1 \end{vmatrix}                       \\
    \vb C_2 \mapsto \vb C_2 - \vb C_3:\quad \det A            & = (1-a)\begin{vmatrix} 1 & 1-a & a \\ -1 & 0 & 1 \\ 0 & a-1 & 1 \end{vmatrix}                       \\
    \det A                                                    & = (1-a)^2\begin{vmatrix} 1 & 1 & a \\ -1 & 0 & 1 \\ 0 & -1 & 1 \end{vmatrix}                     \\
    \vb R_1 \mapsto \vb R_1 + \vb R_2 + \vb R_3 :\quad \det A & = (1-a)^2\begin{vmatrix} 0 & 0 & a+2 \\ -1 & 0 & 1 \\ 0 & -1 & 1 \end{vmatrix}                     \\
    \det A                                                    & = (1-a)^2(a+2)\begin{vmatrix}-1&0\\0&-1\end{vmatrix} = (1-a)^2(a+2)
\end{align*}

\subsection{Multiplicative Property of Determinants}
\begin{theorem}
    For $n\times n$ matrices $M, N$, $\det (MN) = \det M \cdot \det N$.
\end{theorem}
\noindent We can prove this using the following elaboration on the definition of the determinant:
\begin{lemma}
    \[ \varepsilon_{i_1 i_2 \cdots i_n} M_{i_1 a_1} M_{i_2 a_2} \cdots M_{i_n a_n} = (\det M) \varepsilon_{a_1 a_2 \cdots a_n} \]
\end{lemma}
\begin{proof}
    The left hand side and right hand side are each totally antisymmetric (alternating) in $a_1, a_2, \cdots, a_n$, so they must be related by a constant of proportionality. To fix the constant, we can simply consider taking $a_i = i$ and the result follows.
\end{proof}
\noindent Now, we prove the above theorem.
\begin{proof}
    Using the lemma above:
    \begin{align*}
        \det MN & = \varepsilon_{i_1 i_2 \cdots i_n} (MN)_{i_1 1} (MN)_{i_2 2} \cdots (MN)_{i_n n}                                                    \\
                & = \varepsilon_{i_1 i_2 \cdots i_n} {M_{i_1 k_1} \atop N_{k_1 1}} {M_{i_2 k_2} \atop N_{k_2 2}} \cdots {M_{i_n k_n} \atop N_{k_n n}} \\
                & = (\det M) \varepsilon_{a_1 a_2 \cdots a_n} N_{k_1 1} N_{k_2 2} \cdots N_{k_n n}                                                    \\
                & = (\det M)(\det N)
    \end{align*}
    as required.
\end{proof}

\subsection{Consequences of Multiplicative Property}
\begin{enumerate}[(i)]
    \item $M^{-1}M = I \implies \det(M^{-1}) \det(M) = \det I = 1$. Therefore, $\det (M^{-1}) = (\det M)^{-1}$, so $\det M$ must be nonzero for $M$ to be invertible.
    \item For $R$ real and orthogonal, $R^\transpose R = I \implies \det(R^\transpose) \det(R) = 1$. But $\det (R^\transpose) = \det R$, so $(\det R)^2 = 1$, so $\det R = \pm 1$.
    \item For $U$ complex and unitary, $U^\dagger U = I \implies \det(U^\dagger) \det(U) = 1$. But since $U^\dagger = \overline{U^\transpose}$, we have $\overline{\det U} \det U = 1$, so $\abs{(\det U)^2} = 1$, so $\abs{\det U} = 1$.
\end{enumerate}

\subsection{Cofactors and Determinants}
Consider a column of some $n \times n$ matrix $M$, written in the form
\[ \vb C_a = \sum_i M_{ia} \vb e_i \]
\begin{align*}
    \implies \det M & = [ \vb C_1, \cdots, \vb C_a, \cdots, \vb C_n ]                                         \\
                    & = [ \vb C_1, \cdots, \vb C_{a-1}, \sum_i M_{ia} \vb e_i, \vb C_{a+1}, \cdots, \vb C_n ] \\
                    & = \sum_i M_{ia} \Delta_{ia}
\end{align*}
where
\begin{align*}
    \Delta_{ia} & = [ \vb C_1, \cdots, \vb C_{a-1}, \vb e_i, \vb C_{a+1}, \cdots, \vb C_n ]                                                                                          \\
                & = \begin{vmatrix}
        \mathhuge A                 & \begin{matrix}
            0 \\ \vdots \\ 0
        \end{matrix} & \mathhuge B                 \\
        \begin{matrix}
            0 & \cdots & 0
        \end{matrix} & 1                           & \begin{matrix}
            0 & \cdots & 0
        \end{matrix} \\
        \mathhuge C                 & \begin{matrix}
            0 \\ \vdots \\ 0
        \end{matrix} & \mathhuge D
    \end{vmatrix}
    \intertext{where the zero entries in the rows arise from antisymmetry, giving}
                & = \underbrace{(-1)^{n-a}}_{\text{amount of column transpositions}} \cdot \underbrace{(-1)^{n-i}}_{\text{amount of row transpositions}} \begin{vmatrix}
        \mathhuge A & \mathhuge B \\
        \mathhuge C & \mathhuge D
    \end{vmatrix} \\
                & = (-1)^{i+a}M^{ia}
\end{align*}
where $M^{ia}$ is the minor in this position; the determinant of the matrix with this particular row and column removed. We call $\Delta_{ia}$ the cofactor.
\[ \det M = \sum_i M_{ia} \Delta_{ia} = \sum_i(-1)^{i+a}M_{ia}M^{ia} \]
Similarly, by considering rows,
\[ \det M = \sum_a M_{ia} \Delta_{ia} = \sum_a(-1)^{i+a}M_{ia}M^{ia} \]

\subsection{Adjugates and Inverses}
Reasoning as above, consider $\vb C_b = \sum_i M_{ib} \vb e_i$. Then,
\[ [\vb C_1, \cdots, \vb C_{a-1}, \vb C_b, \vb C_{a+1}, \cdots, \vb C_n ] = \sum_i M_{ib} \Delta_{ia} \]
If $a=b$ then clearly this is $\det M$. Otherwise, $\vb C_b$ is equal to one of the other columns, so $\sum_i M_{ib} \Delta_{ia} = 0$.
\[ \sum_i M_{ib} \Delta_{ia} = (\det M)\delta_{ab} \]
Similarly,
\[ \sum_a M_{ja} \Delta_{ia} = (\det M)\delta_{ij} \]
Now, let $\Delta$ be the matrix of cofactors (i.e. entries $\Delta_{ia}$), and we define the adjugate $\adjugate M = \Delta^\transpose$. Then
\[ \Delta_{ia}M_{ib} = \adjugate M_{ai}M_{ib} = (\adjugate M M)_{ab} = (\det M)\delta_{ab} \]
Therefore,
\[ \adjugate M M = (\det M) I \]
We can reach this result similarly considering the other index. Hence, if $\det M \neq 0$ then $M^{-1} = \frac{1}{\det M}\adjugate M$.

\section{Systems of Linear Equations}
\subsection{Introduction and Nature of Solutions}
Consider a system of $n$ linear equations in $n$ unknowns $x_i$ written in matrix-vector form:
\[ A\vb x = \vb b,\quad \vb x, \vb b \in \mathbb R^n, \]
where $A$ is an $n \times n$ matrix. There are three possibilities:
\begin{enumerate}[(i)]
    \item $\det A \neq 0 \implies A^{-1}$ exists so there is a unique solution $\vb x = \vb A^{-1} \vb b$
    \item $\det A = 0$ and $b \notin \Im A$ means that there is no solution
    \item $\det A = 0$ and $b \in \Im A$ means that there are infinitely many solutions of the form
          \[ \vb x = \vb x_0 + \vb u \]
          where $\vb u \in \ker A$ and $\vb x_0$ is a particular solution
\end{enumerate}
A solution therefore exists if and only if $A\vb x_0 = \vb b$ for some $\vb x_0$, which is true if and only if $\vb b \in \Im A$. Then $\vb x$ is also a solution if and only if $\vb u = \vb x - \vb x_0$ satisfies
\[ A\vb u = \vb 0 \]
This equation is known as the equivalent homogeneous problem. Now, $\det A \neq 0 \iff \Im A = \mathbb R^n \iff \ker A = \{ \vb 0 \}$. So in case (i), there is always a unique solution for any $\vb b$. But $\det A = 0 \iff \rank(A) < n \iff \nullity A > 0$. Then either $b \notin \Im A$ as in case (ii), or $b \in \Im A$ as in case (iii).

If $\vb u_1, \dots, \vb u_k$ is a basis for $\ker A$, then the general solution to the homogeneous problem is some linear combination of these basis vectors, i.e.
\[ \vb u = \sum_{i=1}^k \lambda_i \vb u_i,\quad k = \nullity A \]
This is similar to the complementary function and particular integral technique used to solve linear differential equations.

For example, in $A\vb x = \vb b$, let
\[ A = \begin{pmatrix}
        1 & 1 & a \\ a & 1 & 1 \\ 1 & a & 1
    \end{pmatrix};\quad \vb b = \begin{pmatrix}
        1 \\ c \\ 1
    \end{pmatrix};\quad a, c \in \mathbb R \]
We have previously found that $\det A = (a-1)^2(a+2)$. So the cases are:
\begin{itemize}
    \item ($a \neq 1, a \neq -2$) $\det A \neq 0$ and $A^{-1}$ exists; we previously found this to be
          \[ A^{-1} = \frac{1}{(1-a)(2+a)}\begin{pmatrix}
                  1 & 1+a & 1 \\ 1 & 1 & -1-a \\ -1-a & 1 & 1
              \end{pmatrix} \]
          For these values of $a$, there is a unique solution for any $c$, demonstrating case (i) above:
          \[ \vb x = A^{-1} \vb b = \frac{1}{(1-a)(2+a)}\begin{pmatrix}
                  2-c-ca \\ c-a \\ c-a
              \end{pmatrix} \]
          Geometrically, this solution is simply a point.
    \item ($a = 1$) In this case, the matrix is simply
          \[ A = \begin{pmatrix}
                  1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1
              \end{pmatrix} \implies \Im A = \vecspan \left\{ \begin{pmatrix}
                  1 \\ 1 \\ 1
              \end{pmatrix} \right\} = \left\{ \lambda\begin{pmatrix}
                  1 \\ 1 \\ 1
              \end{pmatrix} \right\};\quad \ker A = \vecspan\left\{ \begin{pmatrix}
                  -1 \\ 1 \\ 0
              \end{pmatrix}, \begin{pmatrix}
                  -1 \\ 0 \\ 1
              \end{pmatrix} \right\} \]
          Note that $\vb b \in \Im A$ if and only if $c=1$, where a particular solution is
          \[ \vb x_0 = \begin{pmatrix}
                  1 \\ 0 \\ 0
              \end{pmatrix} \]
          So the general solution is given by
          \[ \vb x = \vb x_0 + \vb u = \begin{pmatrix}
                  1 - \lambda - \mu \\ \lambda \\ \mu
              \end{pmatrix} \]
          In summary, for $a=1$, $c=1$ we have case (iii). Geometrically this is a plane. For $a=1$, $c \neq 1$, we have case (ii) where there are no solutions.

    \item ($a=-2$) The matrix becomes
          \[ A = \begin{pmatrix}
                  1 & 1 & -2 \\ -2 & 1 & 1 \\ 1 & -2 & 1
              \end{pmatrix} \implies \Im A = \vecspan \left\{ \begin{pmatrix}
                  1 \\ -2 \\ 1
              \end{pmatrix}, \begin{pmatrix}
                  1 \\ 1 \\ -2
              \end{pmatrix} \right\};\quad \ker A = \left\{ \lambda\begin{pmatrix}
                  1 \\ 1 \\ 1
              \end{pmatrix} \right\} \]
          Now, $\vb b \in \Im A$ if and only if $c = -2$, the particular solution is
          \[ \vb x_0 = \begin{pmatrix}
                  1 \\ 0 \\ 0
              \end{pmatrix} \]
          The general solution is therefore
          \[ \vb x = \vb x_0 + \vb u = \begin{pmatrix}
                  1 + \lambda \\ \lambda \\ \lambda
              \end{pmatrix} \]
          In summary, for $a=-2$ and $c=-2$ we have case (iii). Geometrically this is a line. For $a=-2$, $c \neq -2$, we have case (ii) where there are no solutions.
\end{itemize}

\subsection{Geometrical Interpretation in $\mathbb R^3$}
Let $\vb R_1, \vb R_2, \vb R_3$ be the rows of the $3 \times 3$ matrix $A$. Then the rows represent the normals of planes. This is clear by expanding the matrix multiplication of the homogeneous form:
\begin{align*}
    A\vb u = \vb 0 \iff & \vb R_1 \cdot \vb u = 0 \\
                        & \vb R_2 \cdot \vb u = 0 \\
                        & \vb R_3 \cdot \vb u = 0
\end{align*}
So the solution of the homogeneous problem (i.e. finding the general solution) amounts to determining where the planes intersect.
\begin{itemize}
    \item ($\rank A = 3$) The rows are linearly independent, so the three planes' normals are linearly independent and the planes intersect at $\vb 0$ only.
    \item ($\rank A = 2$) The normals span a plane, so the planes intersect in a line.
    \item ($\rank A = 1$) The normals are parallel and therefore the planes coincide.
    \item ($\rank A = 0$) The normals are all zero, so any vector in $\mathbb R^3$ solves the equation.
\end{itemize}
Now, let us consider instead the original problem $A \vb x = \vb b$:
\begin{align*}
    A\vb b = \vb 0 \iff & \vb R_1 \cdot \vb u = b_1 \\
                        & \vb R_2 \cdot \vb u = b_2 \\
                        & \vb R_3 \cdot \vb u = b_3
\end{align*}
The planes still have normals $\vb R_i$ as before, but they do not necessarily pass through the origin.
\begin{itemize}
    \item ($\rank A = 3$) The planes' normals are linearly independent and the planes intersect at a point; this is the unique solution.
    \item ($\rank A < 3$) The existence of a solution depends on the value of $\vb b$.
          \begin{itemize}
              \item ($\rank A = 2$) The planes may intersect in a line as before, but they may instead form a sheaf (the planes pairwise intersect in lines but they do not as a triple), or two planes could be parallel and not intersect each other at all.
              \item ($\rank A = 1$) The normals are parallel, so the planes may coincide or they might be parallel. There is no solution unless all three planes coincide.
          \end{itemize}
\end{itemize}

\section{Eigenvalues and Eigenvectors}
\subsection{Definitions}
For a linear map $T\colon V \to V$, a vector $\vb v \in V$ with $\vb v \neq 0$ is called an eigenvector of $T$ with eigenvalue $\lambda$ if $T(\vb v) = \lambda \vb v$. If $V = \mathbb R^n$ or $\mathbb C^n$, and $T$ is given by an $n \times n$ matrix $A$, then
\[ A\vb v = \lambda v \iff (A - \lambda I)\vb v = \vb 0 \]
and for a given $\lambda$, this holds for some $\vb v \neq 0$ if and only if
\[ \det(A - \lambda I) = 0 \]
This is called the characteristic equation for $A$. So $\lambda$ is an eigenvalue if and only if it is a root of the characteristic polynomial
\[ \chi_A(t) = \det(A - tI) = \begin{vmatrix}
        A_{11} - t & A_{12}     & \cdots & A_{1n}     \\
        A_{21}     & A_{22} - t & \cdots & A_{2n}     \\
        \vdots     & \vdots     & \ddots & \vdots     \\
        A_{n1}     & A_{n2}     & \cdots & A_{nn} - t
    \end{vmatrix} \]
We can look for eigenvalues as roots of the characteristic polynomial or characteristic equation, and then determine the corresponding eigenvectors once we've deduced what the possibilities are.

\subsection{Examples of Eigenvalues and Eigenvectors}
\begin{enumerate}[(i)]
    \item $V = \mathbb C^2$:
          \[ A = \begin{pmatrix}
                  2 & i \\ -i & 2
              \end{pmatrix} \implies \det(A - \lambda I) = (2-\lambda)^2 - 1 = 0 \]
          So we have $(2 - \lambda)^2 = 1$ so $\lambda = 1$ or 3.
          \begin{itemize}
              \item ($\lambda = 1$)
                    \[ (A - I)\vb v = \begin{pmatrix}
                            1 & i \\ -i & 1
                        \end{pmatrix}\begin{pmatrix}
                            v_1 \\ v_2
                        \end{pmatrix} = \vb 0 \implies \vb v = \alpha\begin{pmatrix}
                            1 \\ i
                        \end{pmatrix} \]
                    for any $\alpha \neq 0$.
              \item ($\lambda = 3$)
                    \[ (A - 3I)\vb v = \begin{pmatrix}
                            -1 & i \\ -i & -1
                        \end{pmatrix}\begin{pmatrix}
                            v_1 \\ v_2
                        \end{pmatrix} = \vb 0 \implies \vb v = \beta\begin{pmatrix}
                            1 \\ -i
                        \end{pmatrix} \]
                    for any $\beta \neq 0$.
          \end{itemize}
    \item $V = \mathbb R^2$:
          \[ A = \begin{pmatrix}
                  1 & 1 \\ 0 & 1
              \end{pmatrix} \implies \det(A - \lambda I) = (1-\lambda)^2 = 0 \]
          So $\lambda = 1$ only, a repeated root.
          \[ (A - I)\vb v = \begin{pmatrix}
                  0 & 1 \\ 0 & 0
              \end{pmatrix}\begin{pmatrix}
                  v_1 \\ v_2
              \end{pmatrix} = \vb 0 \implies \vb v = \alpha\begin{pmatrix}
                  1 \\ 0
              \end{pmatrix} \]
          for any $\alpha \neq 0$. There is only one (linearly independent) eigenvector here.
    \item $V = \mathbb R^2$ or $\mathbb C^2$:
          \[ U = \begin{pmatrix}
                  \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta
              \end{pmatrix} \implies \chi_U(t) = \det(U - tI) = t^2 - 2t\cos\theta + 1 \]
          The eigenvalues $\lambda$ are $e^{\pm i \theta}$. The eigenvectors are
          \[ \vb v = \alpha \begin{pmatrix}
                  1 \\ \mp i
              \end{pmatrix};\quad \alpha \neq 0 \]
          So there are no real eigenvalues or eigenvectors except when $\theta = n \pi$.
    \item $V = \mathbb C^n$:
          \[ A = \begin{pmatrix}
                  \lambda_1 & 0         & \cdots & 0         \\
                  0         & \lambda_2 & \cdots & 0         \\
                  \vdots    & \vdots    & \ddots & \vdots    \\
                  0         & 0         & \cdots & \lambda_n
              \end{pmatrix} \implies \chi_A(t) = \det(A - tI) = (\lambda_1 - t)(\lambda_2 - t)(\lambda_3 - t)\dots(\lambda_n - t) \]
          So the eigenvalues are all the $\lambda_i$, and the eigenvectors are $\vb v = \alpha \vb e_i$ ($\alpha \neq 0$) for each $i$.
\end{enumerate}

\subsection{Deductions Involving $\chi_A(t)$}
For an $n \times n$ matrix $A$, the characteristic polynomial $\chi_A(t)$ has degree $n$:
\[ \chi_A(t) = \sum_{j = 0}^n c_j t^j = (-1)^n(t-\lambda_1)\dots(t-\lambda_n) \]
\begin{enumerate}[(i)]
    \item There exists at least one eigenvalue (solution to $\chi_A$), due to the fundamental theorem of algebra, or $n$ roots counted with multiplicity.
    \item $\tr(A) = A_{ii} = \sum_{i=1}^n \lambda_i$, the sum of the eigenvalues. Compare terms of degree $n-1$ in $t$, and from the determinant we get
          \[ (-t)^{n-1}A_{11} + (-t)^{n-1}A_{22} + \dots + (-t)^{n-1}A_{nn} \]
          The overall sign matches with the expansion of $(-1)^n(t-\lambda_1)(t-\lambda_2)\dots(t-\lambda_n)$.
    \item $\det(A) = \chi_A(0) = \prod_{i=1}^n \lambda_i$, the product of the eigenvalues.
    \item If $A$ is real, then the coefficients $c_i$ in the characteristic polynomial are real, so $\chi_A(\lambda) = 0 \iff \chi_A(\overline\lambda) = 0$. So the non-real roots occur in conjugate pairs if $A$ is real.
\end{enumerate}

\subsection{Eigenspaces and Multiplicities}
For an eigenvalue $\lambda$ of a matrix $A$, we define the eigenspace
\[ E_\lambda = \{ \vb v : A \vb v = \lambda \vb v \} = \ker (A - \lambda I) \]
All nonzero vectors in this space are eigenvectors. The geometric multiplicity is
\[ m_\lambda = \dim E_\lambda = \nullity (A - \lambda I) \]
equivalent to the number of linearly independent eigenvectors with the given eigenvalue $\lambda$. The algebraic multiplicity is
\[ M_\lambda = \text{the multiplicity of } \lambda \text{ as a root of } \chi_A(t) \]
i.e. $\chi_A(t) = (t - \lambda)^{M_t} f(t)$, where $f(\lambda) \neq 0$.

\begin{proposition}
    $M_\lambda \geq m_\lambda$ (and $m_\lambda \geq 1$ since $\lambda$ is an eigenvalue). The proof of this proposition is delayed until the next section where we will then have the tools to prove it.
\end{proposition}

\subsection{Examples of Eigenspaces}
\begin{enumerate}[(i)]
    \item
          \[ A = \begin{pmatrix}
                  -2 & 2 & -3 \\ 2 & 1 & -6 \\ -1 & -2 & 0
              \end{pmatrix} \implies \chi_A(t) = \det(A - tI) = (5-t)(t+3)^2 \]
          So $\lambda = 5, -3$. $M_5 = 1$, $M_{-3} = 2$. We will now find the eigenspaces.
          \begin{itemize}
              \item ($\lambda = 5$)
                    \[ E_5 = \left\{ \alpha\begin{pmatrix}
                            1 \\ 2 \\ -1
                        \end{pmatrix} \right\} \]
              \item ($\lambda = -3$)
                    \[ E_{-3} = \left\{ \alpha\begin{pmatrix}
                            -2 \\ 1 \\ 0
                        \end{pmatrix} + \beta\begin{pmatrix}
                            3 \\ 0 \\ 1
                        \end{pmatrix} \right\} \]
          \end{itemize}
          Note that to compute the eigenvectors, we just need to solve the equation $(A - \lambda I)\vb x = \vb 0$. In the case of $\lambda = -3$, for example, we then have
          \[ \begin{pmatrix}
                  1 & 2 & -3 \\ 2 & 4 & -6 \\ -1 & -2 & 3
              \end{pmatrix} \begin{pmatrix}
                  x_1 \\ x_2 \\ x_3
              \end{pmatrix} = \vb 0 \]
          We can use the first line of the matrix to get a linear combination for $x_1, x_2, x_3$, specifically $x_1 + 2x_2 = 3x_3 = 0$, so we can eliminate one of the variables (here, $x_1$) to get
          \[ \vb x = \begin{pmatrix}
                  -2x_2 + 3x_3 \\ x_2 \\ x_3
              \end{pmatrix} = \vb 0 \]
          Now, $\dim E_5 = m_5 = 1 = M_5$. Similarly, $\dim E_{-3} = m_{-3} = 2 = M_{-3}$.

    \item
          \[ A = \begin{pmatrix}
                  -3 & -1 & 1 \\ -1 & -3 & 1 \\ -2 & -2 & 0
              \end{pmatrix} \implies \chi_A(t) = \det(A - tI) = -(t + 2)^3 \]
          We have a root $\lambda = -2$ with $M_{-2} = 3$. To find the eigenspace, we will look for solutions of:
          \[ (A + 2I)\vb x = \begin{pmatrix}
                  -1 & -1 & 1 \\ -1 & -1 & 1 \\ -2 & -2 & 2
              \end{pmatrix} \begin{pmatrix}
                  x_1 \\ x_2 \\ x_3
              \end{pmatrix} = \vb 0 \implies \vb x = \begin{pmatrix}
                  -x_2 + x_3 \\ x_2 \\ x_3
              \end{pmatrix} \]
          So
          \[ E_{-2} = \left\{ \alpha\begin{pmatrix}
                  -1 \\ 1 \\ 0
              \end{pmatrix} + \beta\begin{pmatrix}
                  1 \\ 0 \\ 1
              \end{pmatrix} \right\} \]
          Further, $m_{-2} = 2 < 3 = M_{-2}$.

    \item A reflection in a plane through the origin with unit normal $\nhat$ satisfies
          \[ H\nhat = -\nhat;\quad \forall \vb u \perp \nhat, H \vb u = \vb u \]
          The eigenvalues are therefore $\pm 1$ and $E_{-1} = \{ \alpha \nhat \}$, and $E_1 = \{ \vb x: \vb x \cdot \nhat = 0 \}$. The multiplicities are given by $M_{-1} = m_{-1} = 1, M_1 = m_1 = 2$.

    \item A rotation about an axis $\nhat$ through angle $\theta$ in $\mathbb R^3$ satisfies
          \[ R\nhat = \nhat \]
          So the axis of rotation is the eigenvector with eigenvalue 1. There are no other real eigenvalues unless $\theta = n\pi$. The rotation restricted to the plane perpendicular to $\nhat$ has eigenvalues $e^{\pm i \theta}$ as shown above.
\end{enumerate}

\section{Diagonalisable Matrices}
\subsection{Linear Independence of Eigenvectors}
\begin{proposition}
    Let $\vb v_1, \vb v_2, \dots, \vb v_r$ be eigenvectors of an $n\times n$ matrix $A$ with eigenvalues $\lambda_1, \lambda_2,\dots,\lambda_r$. If the eigenvalues are distinct, then the eigenvectors are linearly independent.
\end{proposition}
\begin{proof}
    Note that if we take some linear combination $\vb w = \sum_{j=1}^r \alpha_j\vb v_j$, then $(A - \lambda I)\vb w = \sum_{j=1}^r \alpha_j(\lambda_j - \lambda)\vb v_j$. Here are two methods for getting this proof.
    \begin{enumerate}[(i)]
        \item Suppose the eigenvectors are linearly dependent, so there exist linear combinations $\vb w = \vb 0$ where some $\alpha$ are nonzero. Let $p$ be the amount of nonzero $\alpha$ values. So, $2 \leq p \leq r$. Now, pick such a $\vb w$ for which $p$ is least. Without loss of generality, let $\alpha_1$ be one of the nonzero coefficients. Then
              \[ (A - \lambda_1 I)\vb w = \sum_{j=2}^r \alpha_j(\lambda_j - \lambda_1)\vb v_j = \vb 0 \]
              This is a linear relation with $p-1$ nonzero coefficients \contradiction.
        \item Alternatively, given a linear relation $\vb w=\vb 0$,
              \[ \prod_{j \neq k} (A - \lambda_j I) \vb w = \alpha_k \prod_{j \neq k} (\lambda_k - \lambda_j) \vb v_k = \vb 0 \]
              for some fixed $k$. So $\alpha_k = 0$. So the eigenvectors are linearly independent as claimed.
    \end{enumerate}
\end{proof}
\begin{corollary}
    With conditions as in the proposition above, let $\mathcal B_{\lambda_i}$ be a basis for the eigenspace $E_{\lambda_i}$. Then $\mathcal B = \mathcal B_{\lambda_1} \cup \mathcal B_{\lambda_2} \cup \dots \cup \mathcal B_{\lambda_r}$ is linearly independent.
\end{corollary}
\begin{proof}
    Consider a general linear combination of all these vectors, it has the form
    \[ \vb w = \vb w_1 + \vb w_2 + \dots + \vb w_r \]
    where each $\vb w_i \in E_i$. Applying the same arguments as in the proposition, we find that
    \[ \vb w = 0 \implies \forall i\,\vb w_i = 0 \]
    So each $\vb w_i$ is the trivial linear combination of elements of $\mathcal B_{\lambda_i}$ and the result follows.
\end{proof}

\subsection{Diagonalisability and Similarity}
\begin{proposition}
    For an $n \times n$ matrix $A$ acting on $V = \mathbb R^n$ or $\mathbb C^n$, the following conditions are equivalent:
    \begin{enumerate}[(i)]
        \item there exists a basis of eigenvectors of $A$ for $V$, named $\vb v_1, \vb v_2, \dots, \vb v_n$ which $A\vb v_i = \lambda_i\vb v_i$ for each $i$; and
        \item there exists an $n \times n$ invertible matrix $P$ with the property that
              \[ P^{-1}AP = D = \begin{pmatrix}
                      \lambda_1 & 0         & \cdots & 0         \\
                      0         & \lambda_2 & \cdots & 0         \\
                      \vdots    & \vdots    & \ddots & \vdots    \\
                      0         & 0         & \cdots & \lambda_n
                  \end{pmatrix} \]
    \end{enumerate}
    If either of these conditions hold, then $A$ is diagonalisable.
\end{proposition}
\begin{proof}
    Note that for any matrix $P$, $AP$ has columns $A\vb C_i(P)$, and $PD$ has columns $\lambda_i \vb C_i(P)$. Then (i) and (ii) are related by choosing $\vb v_i = \vb C_i(P)$. Then $P^{-1}AP = D \iff AP = PD \iff A\vb v_i = \lambda_i\vb v_i$.

    In essence, given a basis of eigenvectors as in (i), the relation above defines $P$, and if the eigenvectors are linearly independent then $P$ is invertible. Conversely, given a matrix $P$ as in (ii), its columns are a basis of eigenvectors.
\end{proof}
Let's try some examples.
\begin{enumerate}[(i)]
    \item Let
          \[ A = \begin{pmatrix}
                  1 & 1 \\ 0 & 1
              \end{pmatrix} \implies E_1 = \left\{ \alpha\begin{pmatrix}
                  1 \\ 0
              \end{pmatrix} \right\} \]
          This is a single eigenvalue $\lambda = 1$ with one linearly independent eigenvector. So there is no basis of eigenvectors for $\mathbb R^2$ or $\mathbb C^2$, so $A$ is not diagonalisable.
    \item Let
          \[ U = \begin{pmatrix}
                  \cos \theta & -\sin \theta \\
                  \sin \theta & \cos \theta
              \end{pmatrix} \implies E_{e^{i\theta}} = \left\{ \alpha\begin{pmatrix}
                  1 \\ -i
              \end{pmatrix} \right\};\quad E_{e^{-i\theta}} = \left\{ \beta\begin{pmatrix}
                  1 \\ i
              \end{pmatrix} \right\} \]
          which are two linearly independent complex eigenvectors. So,
          \[ P = \begin{pmatrix}
                  1 & 1 \\ -i & i
              \end{pmatrix};\quad P^{-1} = \frac{1}{2}\begin{pmatrix}
                  1 & i \\ 1 & -i
              \end{pmatrix};\quad P^{-1}UP = \begin{pmatrix}
                  e^{i\theta} & 0 \\ 0 & e^{i\theta}
              \end{pmatrix} \]
          So $U$ is diagonalisable over $\mathbb C^2$ but not over $\mathbb R^2$.
\end{enumerate}

\subsection{Criteria for Diagonalisability}
\begin{proposition}
    Consider an $n \times n$ matrix $A$.
    \begin{enumerate}[(i)]
        \item $A$ is diagonalisable if it has $n$ distinct eigenvalues (sufficient condition).
        \item $A$ is diagonalisable if and only if for every eigenvalue $\lambda$, $M_\lambda = m_\lambda$ (necessary and sufficient condition).
    \end{enumerate}
\end{proposition}
\begin{proof}
    Use the proposition and corollary above.
    \begin{enumerate}[(i)]
        \item If we have $n$ distinct eigenvalues, then we have $n$ linearly independent eigenvectors. Hence they form a basis.
        \item If $\lambda_i$ are all the distinct eigenvalues, then $\mathcal B_{\lambda_1} \cup \dots \cup \mathcal B_{\lambda_r}$ are linearly independent. The number of elements in this new basis is $\sum_{i} m_{\lambda_i} = \sum_{i} M_{\lambda_i} = n$ which is the degree of the characteristic polynomial. So we have a basis.
    \end{enumerate}
    Note that case (i) is just a specialisation of case (ii) where both multiplicities are 1.
\end{proof}
Let us consider some examples.
\begin{enumerate}[(i)]
    \item Let
          \[ A = \begin{pmatrix}
                  -2 & 2 & -3 \\ 2 & 1 & -6 \\ -1 & -2 & 0
              \end{pmatrix} \implies \lambda = 5, -3;\quad M_5=m_5=1;\quad M_{-3}=m_{-3}=2 \]
          So $A$ is diagonalisable by case (ii) above, and moreover
          \[ P = \begin{pmatrix}
                  1  & -2 & 3 \\
                  2  & 1  & 0 \\
                  -1 & 0  & 1
              \end{pmatrix};\quad P^{-1} = \frac{1}{8}\begin{pmatrix}
                  1  & 2 & -3 \\
                  -2 & 4 & 6  \\
                  1  & 2 & 5
              \end{pmatrix} \implies P^{-1}AP = \begin{pmatrix}
                  5 & 0  & 0  \\
                  0 & -3 & 0  \\
                  0 & 0  & -3
              \end{pmatrix} \]
    \item Let
          \[ A = \begin{pmatrix}
                  -3 & -1 & 1 \\
                  -1 & -3 & 1 \\
                  -2 & 2  & 0
              \end{pmatrix} \implies \lambda = -2;\quad M_{-2}=3 > m_{-2} = 2 \]
          So $A$ is not diagonalisable. As a check, if it were diagonalisable, then there would be some matrix $P$ such that $P^{-1}AP = -2I \implies A = P(-2I)P^{-1} = -2I$ \contradiction.
\end{enumerate}

\subsection{Similarity}
Matrices $A$ and $B$ (both $n \times n$) are similar if $B = P^{-1}AP$ for some invertible $n\times n$ matrix $P$. This is an equivalence relation.
\begin{proposition}
    If $A$ and $B$ are similar, then
    \begin{enumerate}[(i)]
        \item $\tr B = \tr A$
        \item $\det B = \det A$
        \item $\chi_B = \chi_A$
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}[(i)]
        \item \begin{align*}
                  \tr B & = \tr (P^{-1}AP) \\&= \tr(APP^{-1}) \\&= \tr A
              \end{align*}
        \item \begin{align*}
                  \det B & = \det (P^{-1}AP) \\&= \det P^{-1} \det A \det P \\&= \det A
              \end{align*}
        \item \begin{align*}
                  \det(B - tI) & = \det(P^{-1}AP - tI) \\&= \det(P^{-1}AP - tP^{-1}P) \\&= \det(P^{-1}(A - tI)P) \\&= \det P^{-1} \det(A - tI) \det P \\&= \det(A - tI)
              \end{align*}
    \end{enumerate}
\end{proof}

\section{Hermitian and Symmetric Matrices}
\subsection{Real Eigenvalues and Orthogonal Eigenvectors}
Recall that an $n\times n$ matrix $A$ is hermitian if and only if $A^\dagger = \overline{A}^\transpose = A$, or $\overline{A_{ij}} = A_{ji}$. If $A$ is real, then it is hermitian if and only if it is symmetric. The complex inner product for $\vb v, \vb w \in \mathbb C^n$ is $\vb v^\dagger \vb w = \sum_i \overline{v_i}w_i$, and for $\vb v, \vb w \in \mathbb R^n$, this reduces to the dot product in $\mathbb R^n$, $\vb v^\transpose \vb w$.

Here is a key observation. If $A$ is hermitian, then
\[ (A\vb v)^\dagger \vb w = \vb v^\dagger (A \vb w) \]
\begin{theorem}
    For an $n \times n$ matrix $A$ that is hermitian:
    \begin{enumerate}[(i)]
        \item Every eigenvalue $\lambda$ is real;
        \item Eigenvectors $\vb v, \vb w$ with different eigenvalues $\lambda, \mu$ respectively, are orthogonal, i.e. $\vb v^\dagger \vb w = 0$; and
        \item If $A$ is real and symmetric, then for each eigenvalue $\lambda$ we can choose a real eigenvector, and part (ii) becomes $\vb v \cdot \vb w = 0$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}[(i)]
        \item Using the observation above with $\vb v = \vb w$ where $\vb v$ is any eigenvector with eigenvalue $\lambda$, we get
              \begin{align*}
                  \vb v^\dagger (A\vb v)        & = (A\vb v)^\dagger \vb v                   \\
                  \vb v^\dagger (\lambda\vb v)  & = (\lambda\vb v)^\dagger \vb v             \\
                  \lambda \vb v^\dagger (\vb v) & = \overline{\lambda} (\vb v)^\dagger \vb v \\
                  \intertext{As $\vb v$ is an eigenvector, it is nonzero, so $\vb v^\dagger \vb v \neq 0$, so}
                  \lambda                       & = \overline \lambda
              \end{align*}
        \item Using the same observation,
              \begin{align*}
                  \vb v^\dagger (A \vb w)   & = (A \vb v)^\dagger \vb w       \\
                  \vb v^\dagger (\mu \vb w) & = (\lambda \vb v)^\dagger \vb w \\
                  \mu \vb v^\dagger \vb w   & = \lambda bm v^\dagger \vb w
              \end{align*}
              Since $\lambda \neq \mu$, $\vb v^\dagger \vb w = 0$, so the eigenvectors are orthogonal.
        \item Given $A\vb v = \lambda \vb v$ with $\vb v \in \mathbb C^n$ but $A$ is real, let
              \[ \vb v = \vb u + i\vb u';\quad \vb u, \vb u' \in \mathbb R^n \]
              Since $\vb v$ is an eigenvector, and this is a linear equation, we have
              \[ A\vb u = \lambda \vb u;\quad A\vb u' = \lambda \vb u' \]
              So $\vb u$ and $\vb u'$ are eigenvectors. $\vb v \neq 0$ implies that at least one of $\vb u$ and $\vb u'$ are nonzero, so there is at least one real eigenvector with this eigenvalue.
    \end{enumerate}
\end{proof}
Case (ii) is a stronger claim for hermitian matrices than just showing that eigenvectors are linearly independent. Furthermore, previously we considered bases $\mathcal B_\lambda$ for each eigenspace $E_\lambda$, and it is now natural to choose bases $\mathcal B_\lambda$ to be orthonormal when we are considering hermitian matrices. Here are some examples.
\begin{enumerate}[(i)]
    \item Let
          \[ A = \begin{pmatrix}
                  2 & i \\ -i & 2
              \end{pmatrix};\quad A^\dagger = A;\quad \lambda = 1, 3;\quad\vb u_1 = \frac{1}{\sqrt{2}} \begin{pmatrix}
                  1 \\i
              \end{pmatrix};\quad\vb u_2 = \frac{1}{\sqrt{2}} \begin{pmatrix}
                  1 \\-i
              \end{pmatrix} \]
          We have chosen coefficients for the vectors $\vb u_1$ and $\vb u_2$ such that they are unit vectors. As shown above, they are then orthonormal. We know that having distinct eigenvalues means that a matrix is diagonalisable. So let us set
          \[ P =  \frac{1}{\sqrt{2}} \begin{pmatrix}
                  1 & 1 \\ i & -i
              \end{pmatrix} \implies P^{-1}AP = D = \begin{pmatrix}
                  1 & 0 \\ 0 & 3
              \end{pmatrix} \]
          Since the eigenvectors are orthonormal, so are the columns of $P$, so $P^{-1} = P^\dagger$ (i.e. $P$ is unitary).
    \item Let
          \[ A = \begin{pmatrix}
                  0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0
              \end{pmatrix} \]
          $A$ is real and symmetric, with eigenvalues $\lambda = -1, 2$ with $M_{-1} = 2$, $M_2 = 1$. Further,
          \[ E_{-1} = \vecspan \{ \vb w_1, \vb w_2 \};\quad \vb w_1 = \begin{pmatrix}
                  1 \\ -1 \\ 0
              \end{pmatrix};\quad \vb w_2 = \begin{pmatrix}
                  1 \\ 0 \\ -1
              \end{pmatrix} \]
          So $m_{-1} = 2$, and the matrix is diagonalisable. Let us choose an orthonormal basis for $E_{-1}$ by taking
          \[ \vb u_1 = \frac{1}{\abs{\vb w_1}}\vb w_1 = \frac{1}{\sqrt 2}\begin{pmatrix}
                  1 \\ -1 \\ 0
              \end{pmatrix} \]
          and we can consider
          \[ \vb w_2' = \vb w_2 - (\vb u_1 \cdot \vb w_2)\vb u_1 = \begin{pmatrix}
                  1/2 \\ 1/2 \\ -1
              \end{pmatrix} \]
          so that $\vb w_2'$ is orthogonal to $\vb u_1$ by construction. We can then normalise this vector to get
          \[ \vb u_2 = \frac{1}{\abs{\vb w_2'}}\vb w_2' = \frac{1}{\sqrt 6} \begin{pmatrix}
                  1 \\ 1 \\ -2
              \end{pmatrix} \]
          and therefore
          \[ \mathcal B_{-1} = \{ \vb u_1, \vb u_2 \} \]
          is an orthonormal basis. For $E_2$, let us choose $\mathcal B_2 = \{ \vb u_3 \}$ where
          \[ \vb u_3 = \frac{1}{\sqrt 3}\begin{pmatrix}
                  1 \\ 1 \\ 1
              \end{pmatrix} \]
          Together,
          \[ \mathcal B = \left\{ \frac{1}{\sqrt 2}\begin{pmatrix}
                  1 \\ -1 \\ 0
              \end{pmatrix}, \frac{1}{\sqrt 6} \begin{pmatrix}
                  1 \\ 1 \\ -2
              \end{pmatrix}, \frac{1}{\sqrt 3}\begin{pmatrix}
                  1 \\ 1 \\ 1
              \end{pmatrix} \right\} \]
          is an orthonormal basis for $\mathbb R^3$. Let $P$ be the matrix with columns $\vb u_1, \vb u_2, \vb u_3$, then $P^{-1}AP = D$ as required. Since we have chosen an orthonormal basis, $P$ is orthogonal, so $P^\transpose AP = D$.
\end{enumerate}

\subsection{Unitary and Orthogonal Diagonalisation}
\begin{theorem}
    Any $n\times n$ hermitian matrix $A$ is diagonalisable.
    \begin{enumerate}[(i)]
        \item There exists a basis of eigenvectors $\vb u_1, \dots, \vb u_n \in \mathbb C^n$ with $A\vb u_i = \lambda \vb u_i$; equivalently
        \item There exists an $n \times n$ invertible matrix $P$ with $P^{-1}AP = D$ where $D$ is the matrix with eigenvalues on the diagonal, where the columns of $P$ are the eigenvectors $\vb u_i$.
    \end{enumerate}
    In addition, the eigenvectors $\vb u_i$ can be chosen to be orthonormal, so
    \[ \vb u^\dagger_i \vb u_j = \delta_{ij} \]
    or equivalently, the matrix $P$ can be chosen to be unitary,
    \[ P^\dagger = P^{-1} \implies P^\dagger AP = D \]
    In the special case that the matrix $A$ is real, the eigenvectors can be chosen to be real, and so
    \[ \vb u^\transpose \vb u_j = \vb u_i \cdot \vb u_j = \delta_{ij} \]
    so $P$ is orthogonal, so
    \[ P^\transpose = P^{-1} \implies P^\transpose AP = D \]
\end{theorem}

\section{Quadratic Forms}
\subsection{Simple Example}
Consider a function $\mathcal F\colon \mathbb R^2 \to \mathbb R$ defined by
\[ \mathcal F(\vb x) = 2x_1^2 - 4x_1x_2 + 5x_2^2 \]
This can be simplified by writing
\[ \mathcal F(\vb x) = x_1'^2 + 6x_2'^2 \]
where
\[ x_1' = \frac{1}{\sqrt 5}(2x_1 + x_2);\quad x_2' = \frac{1}{\sqrt 5}(-x_1 + 2x_2) \]
This can be found by writing $\mathcal F(\vb x) = \vb x^\transpose A\vb x$ where
\[ A = \begin{pmatrix}
        2 & -2 \\ -2 & 5
    \end{pmatrix} \]
by inspection from the original equation, and then diagonalising $A$. We find the eigenvalues to be $\lambda = 1, 6$, with eigenvectors
\[ \frac{1}{\sqrt 5} \begin{pmatrix}
        2 \\ 1
    \end{pmatrix};\quad \frac{1}{\sqrt 5}\begin{pmatrix}
        -1 \\ 2
    \end{pmatrix} \]

\subsection{Diagonalising Quadratic Forms}
In general, a quadratic form is a function $\mathcal F\colon \mathbb R^n \to \mathbb R$ given by
\[ \mathcal F(\vb x) = \vb x^\transpose A \vb x \implies \mathcal F(\vb x)_{ij} = x_i A_{ij} x_j \]
where $A$ is a real symmetric $n \times n$ matrix. Any antisymmetric part of $A$ would not contribute to the result, so there is no loss of generality under this restriction. From the section above, we know we can write $P^\transpose A P = D$ where $D$ is a diagonal matrix containing the eigenvalues, and $P$ is constructed from the eigenvectors, with orthonormal columns $\vb u_i$. Setting $\vb x' = P^\transpose \vb x$, or equivalently $\vb x = P \vb x'$, we have
\begin{align*}
    \mathcal F(\vb x) & = \vb x^\transpose A \vb x                                              \\
                      & = (P \vb x')^\transpose A (P \vb x')                                    \\
                      & = (\vb x')^\transpose P^\transpose A P \vb x'                           \\
                      & = (\vb x')^\transpose D \vb x'                                          \\
                      & = \sum_i \lambda_i x_i'^2 = \lambda_1 x_1'^2 + \lambda_2 x_2'^2 + \dots
\end{align*}
We say that $\mathcal F$ has been diagonalised. Now, note that
\begin{align*}
    \vb x' & = x_1'\vb e_1 + \dots + x_n'\vb e_n \\
    \vb x  & = x_1\vb e_1 + \dots + x_n\vb e_n   \\
           & = x_1'\vb u_1 + \dots + x_n'\vb u_n
\end{align*}
where the $\vb e_i$ are the standard basis vectors, since
\[ \vb x_i' = \vb u_i \cdot \vb x \iff \vb x' = P^\transpose \vb x \]
Hence the $\vb x_i'$ can be regarded as coordinates with respect to a new set of axes defined by the orthonormal eigenvector basis, known as the principal axes of the quadratic form. They are related to the standard axes (given by basis vectors $\vb e_i$) by the orthogonal transformation $P$.

\subsection{Example in $\mathbb R^2$}
Consider $\mathcal F(\vb x) = \vb x^\transpose A \vb x$ with
\[ A = \begin{pmatrix}
        \alpha & \beta \\ \beta & \alpha
    \end{pmatrix} \]
The eigenvalues are $\lambda = \alpha + \beta, \alpha - \beta$ and
\[ \vb u_1 = \frac{1}{\sqrt 2}\begin{pmatrix}
        1 \\ 1
    \end{pmatrix};\quad \vb u_2 = \frac{1}{\sqrt 2}\begin{pmatrix}
        -1 \\ 1
    \end{pmatrix} \]
So in terms of the standard basis vectors,
\[ \mathcal F(\vb x) = \alpha x_1^2 + 2\beta x_1x_2 + \alpha x_2^2 \]
And in terms of our new basis vectors,
\[ \mathcal F(\vb x) = (\alpha + \beta) x_1'^2 + (\alpha - \beta) x_2'^2 \]
where
\begin{align*}
    \vb x_1' & = \vb u_1 \cdot \vb x = \frac{1}{\sqrt 2}(x_1 + x_2)  \\
    \vb x_2' & = \vb u_2 \cdot \vb x = \frac{1}{\sqrt 2}(-x_1 + x_2) \\
\end{align*}
Taking for example $\alpha = \frac{3}{2}, \beta = \frac{-1}{2}$, we have $\lambda_1 = 1, \lambda_2 = 2$. If we choose $\mathcal F = 1$, this represents an ellipse in our new coordinate system:
\[ x_1'^2 + 2x_2'^2 = 1 \]
If instead we chose $\alpha = \frac{-1}{2}, \beta = \frac{3}{2}$. We now have $\lambda_1 = 1, \lambda_2 = -2$. The locus at $\mathcal F = 1$ gives a hyperbola:
\[ x_1'^2 - 2x_2'^2 = 1 \]

\subsection{Example in $\mathbb R^3$}
In $\mathbb R^3$, note that if $\lambda_1, \lambda_2, \lambda_3$ are all strictly positive, then $\mathcal F = 1$ gives an ellipsoid. This is analogous to the $\mathbb R^2$ case above.

Let us consider an example. Earlier, we found that the eigenvalues of the matrix $A$ where
\[ A = \begin{pmatrix}
        0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0
    \end{pmatrix} \]
are $\lambda_1 = \lambda_2 = -1, \lambda_3 = 2$, where
\[ \vb u_1 = \frac{1}{\sqrt 2} \begin{pmatrix}
        1 \\ -1 \\ 0
    \end{pmatrix};\quad \vb u_2 = \frac{1}{\sqrt 6}\begin{pmatrix}
        1 \\ 1 \\ -2
    \end{pmatrix};\quad \vb u_3 = \frac{1}{\sqrt 3}\begin{pmatrix}
        1 \\ 1 \\ 1
    \end{pmatrix} \]
Then
\begin{align*}
    \mathcal F(\vb x) & = 2x_1x_2 + 2x_2x_3 + 2x_3x_1 \\
                      & = -x_1'^2 -x_2'^2 + 2x_3'^2
\end{align*}
Now, $\mathcal F = 1$ corresponds to
\[ 2x_3'^2 = 1 + x_1'^2 + x_2'^2 \]
So we can more clearly see that this is a hyperboloid of two sheets in $\mathbb R^3$ with rotational symmetry between the $x_1$ and $x_2$ axes. Further, $\mathcal F = -1$ corresponds to
\[ 1 + 2x_3'^2 = x_1'^2 + x_2'^2 \]
Here, this is a hyperboloid of one sheet since for any fixed $x_3$ coordinate, it defines a circle in the $x_1$ and $x_2$ axes.

\subsection{Hessian Matrix as a Quadratic Form}
Consider a smooth function $f\colon \mathbb R^n \to \mathbb R$ with a stationary point at $\vb x = \vb a$, i.e. $\frac{\partial f}{\partial x_i} = 0$ at $\vb x = \vb a$. By Taylor's theorem,
\[ f(\vb a + \vb h) + f(\vb a) + \mathcal F(\vb h) + O(\abs{\vb h}^3) \]
where $\mathcal F$ is a quadratic form with
\[ A_{ij} = \frac{1}{2}\frac{\partial^2 f}{\partial x_i\partial x_j} \]
all evaluated at $\vb x = \vb a$. Note that this $A$ is half of the Hessian matrix, and that the linear term vanishes since we are at a stationary point. Rewriting this $\vb h$ in terms of the eigenvectors of $A$ (the principal axes), we have
\[ \mathcal F = \lambda_1 h_1'^2 + \lambda_2 h_2'^2 + \dots + \lambda_n h_n'^2 \]
So clearly if $\lambda_i > 0$ for all $i$, then $f$ has a minimum at $\vb x = \vb a$. If $\lambda_i < 0$ for all $I$, then $f$ has a maximum at $\vb x = \vb a$. Otherwise, it has a saddle point. Note that it is often sufficient to consider the trace and determinant of $A$, since $\trace A = \lambda_1 + \lambda_2$ and $\det A = \lambda_1\lambda_2$.

\section{Cayley-Hamilton Theorem and Changing Bases}
\subsection{Matrix Polynomials}
If $A$ is an $n \times n$ complex matrix and
\[ p(t) = c_0 + c_1t + c_2^2 + \dots + c_kt^k \]
is a polynomial, then
\[ p(A) = c_0I + c_1A + c_2A^2 + \dots + c_kA^k \]
We can also define power series on matrices (subject to convergence). For example, the exponential series which always converges:
\[ \exp(A) = I + A + \frac{1}{2}A^2 + \dots + \frac{1}{r!}A^r + \dots \]
For a diagonal matrix, polynomials and power series can be computed easily since the power of a diagonal matrix just involves raising its diagonal elements to said power. Therefore,
\[ D = \begin{pmatrix}
        \lambda_1 & 0         & \cdots & 0         \\
        0         & \lambda_2 & \cdots & 0         \\
        \vdots    & \vdots    & \ddots & \vdots    \\
        0         & 0         & \cdots & \lambda_n
    \end{pmatrix} \implies p(D) = \begin{pmatrix}
        p(\lambda_1) & 0            & \cdots & 0            \\
        0            & p(\lambda_2) & \cdots & 0            \\
        \vdots       & \vdots       & \ddots & \vdots       \\
        0            & 0            & \cdots & p(\lambda_n)
    \end{pmatrix} \]
Therefore,
\[ \exp(D) = \begin{pmatrix}
        e^{\lambda_1} & 0             & \cdots & 0             \\
        0             & e^{\lambda_2} & \cdots & 0             \\
        \vdots        & \vdots        & \ddots & \vdots        \\
        0             & 0             & \cdots & e^{\lambda_n}
    \end{pmatrix} \]
If $B = P^{-1}AP$ (similar to $A$) where $P$ is an $n \times n$ invertible matrix, then
\[ B^r = P^{-1}A^rP \]
Therefore,
\[ p(B) = p(P^{-1}AP) = P^{-1}p(A)P \]
Of special interest is the characteristic polynomial,
\[ \chi_A(t) = \det(A - tI) = c_0 + c_1t + c_2t^2 + \dots + c_nt^n \]
where $c_0 = \det A$, and $c_n = (-1)^n$.
\begin{theorem}[Cayley-Hamilton Theorem]
    \[ \chi_A(A) = c_0I + c_1A + c_2A^2 + \dots + c_nA^n = 0 \]
    Less formally, a matrix satisfies its own characteristic equation.
\end{theorem}
\begin{remark}
    We can find an expression for the matrix inverse.
    \[ -c_0I = A(c_1 + c_2A + \dots + c_nA^{n-1}) \]
    If $c_0 = \det A \neq 0$, then
    \[ A^{-1} = \frac{-1}{c_0}(c_1 + c_2A + \dots + c_nA^{n-1}) \]
\end{remark}

\subsection{Proofs of Special Cases of Cayley-Hamilton Theorem}
\begin{proof}[Proof for a $2\times 2$ matrix]
    Let $A$ be a general $2\times 2$ matrix.
    \[ A = \begin{pmatrix}
            a & b \\ c & d
        \end{pmatrix} \implies \chi_A(t) = t^2 - (a+d)t + (ad-bc) \]
    We can check the theorem by substitution.
    \[ \chi_A(A) = A^2 - (a+d)A - (ad-bc)I \]
    This is shown on the last example sheet.
\end{proof}
\begin{proof}[Proof for diagonalisable $n \times n$ matrices]
    Consider $A$ with eigenvalues $\lambda_i$, and an invertible matrix $P$ such that $P^{-1}AP = D$, where $D$ is diagonal.
    \[ \chi_A(D) = \begin{pmatrix}
            \chi_A(\lambda_1) & 0                 & \cdots & 0                 \\
            0                 & \chi_A(\lambda_2) & \cdots & 0                 \\
            \vdots            & \vdots            & \ddots & \vdots            \\
            0                 & 0                 & \cdots & \chi_A(\lambda_n)
        \end{pmatrix} = 0 \]
    since the $\lambda_i$ are eigenvalues. Then
    \[ \chi_A(A) = \chi_A(PDP^{-1}) = P\chi_A(D)P^{-1} = 0 \]
\end{proof}

\subsection{Proof in General Case (non-examinable)}
\begin{proof}
    Let $M = A - tI$. Then $\det M = \det(A - tI) = \chi_A(t) = \sum_{r=0}c_rt^r$. We can construct the adjugate matrix.
    \[ \adjugate M = \sum_{r=0}^{n-1} B_rt^r \]
    Therefore,
    \begin{align*}
        \adjugate M M = (\det M) I & = \left(\sum_{r=0}^{n-1} B_rt^r\right)(A-tI)                                              \\
                                   & = B_0A + (B_1A - B_0)t + (B_2A - B_1)t^2 + \dots + (B_{n-1}A - B_{n-2})t^{n-1} - B_{n-1}t
    \end{align*}
    Now by comparing coefficients,
    \begin{align*}
        C_0I     & = B_0A               \\
        C_1I     & = B_1A - B_0         \\
        \vdots                          \\
        C_{n-1}I & = B_{n-1}A - B_{n-2} \\
        C_nI     & = -B_{n-1}
    \end{align*}
    Summing all of these coefficients, multiplying by the relevant powers,
    \begin{align*}
         & C_0I + C_1A + C_2A^2 + \dots + C_nA^n \\=\ &B_0A + (B_1A^2 - B_0A) + (B_2A^3 - B_1A^2) + \dots + (B_{n-1}A^n - B_{n-2}A^{n-1}) - B_{n-1}A^n \\=\ &0
    \end{align*}
\end{proof}

\subsection{Changing Bases in General}
Recall that given a linear map $T\colon V \to W$ where $V$ and $W$ are real or complex vector spaces, and choices of bases $\{ \vb e_i \}$ for $i = 1, \dots, n$ and $\{\vb f_a \}$ for $a = 1, \dots, m$, then the $m \times n$ matrix $A$ with respect to these bases is defined by
\[ T(\vb e_i) = \sum_a \vb f_a A_{ai} \]
So the entries in column $i$ of $A$ are the components of $T(\vb e_i)$ with respect to the basis $\{ \vb f_a \}$. This is chosen to ensure that the statement $\vb y = T(\vb x)$ is equivalent to the statement that $y_a = A_{ai}x_i$, where $\vb y = \sum_a y_a \vb f_a$ and $\vb x = \sum_i x_i \vb e_i$. This equivalence holds since
\[ T\left(\sum_i x_i \vb e_i \right) = \sum_i x_i T(\vb e_i) = \sum_i x_i \left( \sum_a \vb f_a A_{ai} \right) = \sum_a \underbrace{\left( \sum_i A_{ai} x_i \right)}_{y_a} \vb f_a \]
as required. For the same linear map $T$, there is a different matrix representation $A'$ with respect to different bases $\{ \vb e'_i \}$ and $\{ \vb f'_a \}$. To relate $A$ with $A'$, we need to understand how the new bases relate to the original bases. The change of base matrices $P$ ($n \times n$) and $Q$ ($m \times m$) are defined by
\[ \vb e_i' = \sum_j \vb e_j P_{ji};\quad \vb f_a' = \sum_b \vb f_b Q_{ba} \]
The entries in column $i$ of $P$ are the components of the new basis $\vb e_i'$ in terms of the old basis vectors $\{ \vb e_j \}$, and similarly for $Q$. Note, $P$ and $Q$ are invertible, and in the relation above we could exchange the roles of $\{ \vb e_i \}$ and $\{ \vb e'_i \}$ by replacing $P$ with $P^{-1}$, and similarly for $Q$.

\begin{proposition}[Change of base formula for a linear map]
    With the definitions above,
    \[ A' = Q^{-1}AP \]
\end{proposition}
\noindent First we will consider an example, then we will construct a proof. Let $n=2, m=3$, and
\begin{align*}
    T(\vb e_1) & = \vb f_1 + 2\vb f_2 - \vb f_3 = \sum_a \vb f_a A_{a1}  \\
    T(\vb e_2) & = -\vb f_1 + 2\vb f_2 + \vb f_3 = \sum_a \vb f_a A_{a2}
\end{align*}
Therefore,
\[ A = \begin{pmatrix}
        1 & -1 \\ 2 & 2 \\ -1 & 1
    \end{pmatrix} \]
Consider a new basis for $V$, given by
\begin{align*}
    \vb e'_1 & = \vb e_1 - \vb e_2 = \sum_i \vb e_i P_{i1} \\
    \vb e'_2 & = \vb e_1 + \vb e_2 = \sum_i \vb e_i P_{i2}
\end{align*}
\[ P = \begin{pmatrix}
        1 & 1 \\ -1 & 1
    \end{pmatrix} \]
Consider further a new basis for $W$, given by
\begin{align*}
    \vb f'_1 & = \vb f_1 - \vb f_3 = \sum_a \vb f_a Q_{a1} \\
    \vb f'_2 & = \vb f_2 = \sum_a \vb f_a Q_{a2}           \\
    \vb f'_3 & = \vb f_1 + \vb f_3 = \sum_a \vb f_a Q_{a3}
\end{align*}
\[ Q = \begin{pmatrix}
        1  & 0 & 1 \\
        0  & 1 & 0 \\
        -1 & 0 & 1
    \end{pmatrix} \]
From the change of base formula,
\begin{align*}
    A' & = Q^{-1}AP                                                                          \\
       & = \begin{pmatrix}
        1/2 & 0 & -1/2 \\
        0   & 1 & 0    \\
        1/2 & 0 & 1/2
    \end{pmatrix}\begin{pmatrix}
        1 & -1 \\ 2 & 2 \\ -1 & 1
    \end{pmatrix}\begin{pmatrix}
        1 & 1 \\ -1 & 1
    \end{pmatrix} \\
       & = \begin{pmatrix}
        2 & 0 \\ 0 & 4 \\ 0 & 0
    \end{pmatrix}
\end{align*}
Now checking this result directly,
\begin{align*}
    T(\vb e'_1) & = 2\vb f_1 - 2\vb f_3 = 2\vb f_1' \\
    T(\vb e'_2) & = 4\vb f_2 = 4\vb f_2'
\end{align*}
which matches the content of the matrix as required. Now, let us prove the proposition in general.
\begin{proof}
    \begin{align*}
        T(\vb e'_i) & = T\left( \sum_j \vb e_j P_{ji} \right)              \\
                    & = \sum_j T(\vb e_j) P_{ji}                           \\
                    & = \sum_j \left( \sum_a \vb f_a A_{aj} \right) P_{ji} \\
                    & = \sum_{ja} \vb f_a A_{aj} P_{ji}
    \end{align*}
    But on the other hand,
    \begin{align*}
        T(\vb e'_i) & = \sum_b \vb f'_b A'_{bi}                             \\
                    & = \sum_b \left( \sum_a \vb f_a Q_{ab} \right) A'_{bi} \\
                    & = \sum_{ab} \vb f_a Q_{ab} A'_{bi}
    \end{align*}
    which is a sum over the same set of basis vectors, so we may equate coefficients of $\vb f_a$.
    \begin{align*}
        \sum_j A_{aj} P_{ji} & = \sum_b Q_{ab} A'_{bi} \\
        (AP)_{ai}            & = (QA')_{ai}
    \end{align*}
    Therefore
    \[ AP = QA' \implies A' = Q^{-1}AP \]
    as required.
\end{proof}

\section{???}
\subsection{Changing Bases of Vector Components}
Here is another way to arrive at the formula $A' = Q^{-1}AP$. Consider changes in vector components
\begin{align*}
    \vb x        & = \sum_i x_i \vb e_i = \sum_j x_j' \vb e'_j       \\
                 & = \sum_i\left( \sum_j P_{ij} x'_j \right) \vb e_i \\
    \implies x_i & = P_{ij} x'_j
\end{align*}
We will write
\[ X = \begin{pmatrix}
        x_1 \\ \vdots \\ x_n
    \end{pmatrix};\quad X' = \begin{pmatrix}
        x'_1 \\ \vdots \\ x'_n
    \end{pmatrix} \]
Then $X = PX'$ or $X' = P^{-1}X$. Similarly,
\begin{align*}
    \vb y        & = \sum_a y_a \vb f_a = \sum_b y_b' \vb f'_b \\
    \implies y_a & = Q_{ab} y'_b
\end{align*}
Then $Y = QY'$ or $Y' = Q^{-1}Y$. So the matrices are defined to ensure that
\[ Y = AX;\quad Y' = A'X' \]
Therefore,
\[ QY' = APX' \implies Y' = (Q^{-1}AP)X' \implies A' = Q^{-1}AP \]

\subsection{Specialisations of Changes of Basis}
Now, let us consider some special cases (in increasing order of specialisation).
\begin{enumerate}[(i)]
    \item Let $V=W$ with $\vb e_i = \vb f_i$ and $\vb e'_i = \vb f'_i$. So $P=Q$ and the change of basis is
          \[ A' = P^{-1}AP \]
          Matrices representing the same linear map but with respect to different bases are similar. Conversely, if $A, A'$ are similar, then we can construct an invertible change of basis matrix $P$ which relates them, so they can be regarded as representing the same linear map. In an earlier section we noted that $\tr(A') = \tr(A)$, $\det(A') = \det(A)$ and $\chi_A(t) = \chi_{A'}(t)$. so these are intrinsic properties of the linear map, not just the particular matrix we choose to represent it.
    \item Let $V=W=\mathbb R^n$ or $\mathbb C^n$ where $\vb e_i$ is the standard basis, with respect to which, $T$ has matrix $A$. If there exists a basis of eigenvectors, $\vb e'_i = \vb v_i$ with $A\vb v_i = \lambda_i\vb v_i$. Then
          \[ A' = P^{-1}AP = D = \begin{pmatrix}
                  \lambda_1 & 0         & \cdots & 0         \\
                  0         & \lambda_2 & \cdots & 0         \\
                  \vdots    & \vdots    & \ddots & \vdots    \\
                  0         & 0         & \cdots & \lambda_n
              \end{pmatrix} \]
          and
          \[ \vb v_i = \sum_k \vb e_j P_{ji} \]
          so the eigenvectors are the columns of $P$.
    \item Let $A$ be hermitian, i.e. $A^\dagger = A$, then we always have a basis of orthonormal eigenvectors $\vb e'_i = \vb u_i$. Then the relations in (ii) apply, and $P$ is unitary, $P^\dagger = P^{-1}$.
\end{enumerate}

\subsection{Jordan Normal Form}
Also known as the (Jordan) Canonical Form, this result classifies $n\times n$ complex matrices up to similarity.
\begin{proposition}
    Any $2\times 2$ complex matrix $A$ is similar to one of the following:
    \begin{enumerate}[(i)]
        \item $A' = \begin{pmatrix}
                      \lambda_1 & 0         \\
                      0         & \lambda_2
                  \end{pmatrix}$ with $\lambda_1 \neq \lambda_2$, so $\chi_A(t) = (t - \lambda_1)(t - \lambda_2)$.
        \item $A' = \begin{pmatrix}
                      \lambda & 0       \\
                      0       & \lambda
                  \end{pmatrix}$, so $\chi_A(t) = (t - \lambda)^2$.
        \item $A' = \begin{pmatrix}
                      \lambda & 1       \\
                      0       & \lambda
                  \end{pmatrix}$, so $\chi_A(t) = (t - \lambda)^2$ as in case (ii).
    \end{enumerate}
\end{proposition}
\begin{proof}
    $\chi_A(t)$ has two roots over $\mathbb C$.
    \begin{enumerate}[(i)]
        \item For distinct roots $\lambda_1, \lambda_2$, we have $M_{\lambda_1} = m_{\lambda_1} = M_{\lambda_2} = m_{\lambda_2} = 1$. So the eigenvectors $\vb v_1, \vb v_2$ provide a basis. Hence $A' = P^{-1}AP$ with the eigenvectors as the columns of $P$.
        \item For a repeated root $\lambda$ with $M_\lambda = m_\lambda = 2$, the same argument applies.
        \item For a repeated root $\lambda$ with $M_\lambda = 2$, $m_\lambda = 1$, we do not have a basis of eigenvectors so we cannot diagonalise the matrix. We only have one linearly independent eigenvector, which we will call $\vb v$. Let $\vb w$ be any other vector such that $\{ \vb v, \vb w \}$ are linearly indepdendent. Then
              \begin{align*}
                  A\vb v & = \lambda \vb v              \\
                  A\vb w & = \alpha \vb v + \beta \vb w
              \end{align*}
              The matrix representing this linear map with respect to the basis vectors $\{ \vb v, \vb w \}$ is therefore
              \[ \begin{pmatrix}
                      \lambda & \alpha \\
                      0       & \beta
                  \end{pmatrix} \]
              Let us solve for some of these unknowns. We know that the characteristic polynomial of this matrix must be $(t - \lambda)^2$, so $\beta = \lambda$. Also, $\alpha \neq 0$, otherwise we have case (ii) above. So now we can set $\vb u = \alpha \vb v$, so
              \begin{align*}
                  A(\alpha \vb v) & = \lambda (\alpha \vb v)     \\
                  A\vb w          & = \alpha \vb v + \beta \vb w
              \end{align*}
              So with respect to the basis $\{ \vb u, \vb w \}$ we get the matrix $A$ to be
              \[ A' = \begin{pmatrix}
                      \lambda & 1       \\
                      0       & \lambda
                  \end{pmatrix} \]
    \end{enumerate}
\end{proof}
\begin{proof}[Alternative Proof]
    Here is an alternative appproach for case (iii). If $A$ has characteristic polynomial
    \[ \chi_A(t) = (t - \lambda)^2 \]
    but $A \neq \lambda I$, then there exists some vector $\vb w$ for which $\vb u = (A - \lambda I)\vb w \neq \vb 0$. So $(A - \lambda I)\vb u = (A - \lambda I)^2 \vb w  = \vb 0$ by the Cayley-Hamilton theorem. So
    \begin{align*}
        A\vb u & = \lambda \vb u         \\
        A\vb w & = \vb u + \lambda \vb w
    \end{align*}
    So with basis $\{ \vb u, \vb w \}$ we have the matrix
    \[ A' = \begin{pmatrix}
            \lambda & 1       \\
            0       & \lambda
        \end{pmatrix} \]
\end{proof}
Here is a concrete example using this alternative proof method.
\[ A = \begin{pmatrix}
        1 & 4 \\ -1 & 5
    \end{pmatrix} \implies \chi_A(t) = (t - 3)^2 \]
So
\[ A - 3I = \begin{pmatrix}
        -2 & 4 \\ -1 & 2
    \end{pmatrix} \]
We will choose $\vb w = \begin{pmatrix}
        1 \\ 0
    \end{pmatrix}$ and we find $\vb u = (A - 3I)\vb w = \begin{pmatrix}
        -2 \\ -1
    \end{pmatrix}$. $\vb w$ is not an eigenvector, as required for the construction. By the reasoning in the alternative argument above, $\vb u$ is an eigenvector by construction.
\begin{align*}
    A\vb u & = 3\vb u         \\
    A\vb w & = \vb u + 3\vb w
\end{align*}
So
\[ P = \begin{pmatrix}
        -2 & 1 \\ -1 & 0
    \end{pmatrix} \implies P^{-1} = \begin{pmatrix}
        0 & -1 \\ 1 & -2
    \end{pmatrix} \]
and we can check that
\[ P^{-1}AP = \begin{pmatrix}
        3 & 1 \\ 0 & 3
    \end{pmatrix} = A' \]

\subsection{Jordan Normal Forms in $n$ Dimensions}
To extend the arguments above to larger matrices, consider the $n\times n$ matrix
\[ N = \begin{pmatrix}
        0      & 1      & 0      & \cdots & 0      \\
        0      & 0      & 1      & \cdots & 0      \\
        0      & 0      & 0      & \cdots & 0      \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0      & 0      & 0      & \cdots & 0
    \end{pmatrix} \]
When applied to the standard basis vectors in $\mathbb C^n$, the action of this matrix sends $\vb e_n \mapsto \vb e_{n-1} \mapsto \dots \mapsto \vb e_1 \mapsto \vb 0$. This is consistent with the property that $N^n = 0$. The kernel of this matrix has dimension 1. Now consider the matrix $J = \lambda I + N$, as follows:
\[ N = \begin{pmatrix}
        \lambda & 1       & 0       & \cdots & 0       \\
        0       & \lambda & 1       & \cdots & 0       \\
        0       & 0       & \lambda & \cdots & 0       \\
        \vdots  & \vdots  & \vdots  & \ddots & \vdots  \\
        0       & 0       & 0       & \cdots & \lambda
    \end{pmatrix} \]
This matrix has
\[ \chi_J(t) = (\lambda - t)^n \]
with $M_\lambda = n$ and $m_\lambda = 1$, since the kernel of $J - \lambda I = N$ has dimension 1 as before. The general result is as follows.
\begin{theorem}
    Any $n\times n$ complex matrix $A$ is similar to a matrix of the form
    \[ A' = \left( \begin{array}{c|c|c|c}
                J_{n_1}(\lambda_1) & 0                  & \cdots & 0                  \\\hline
                0                  & J_{n_2}(\lambda_2) & \cdots & 0                  \\\hline
                \vdots             & \vdots             & \ddots & \vdots             \\\hline
                0                  & 0                  & \cdots & J_{n_r}(\lambda_r)
            \end{array} \right) \]
    where each diagonal block is a Jordan block $J_{n_r}(\lambda_r)$ which is an $n_r \times n_r$ matrix $J$ with eigenvalue $\lambda_r$. $\lambda_1, \dots, \lambda_r$ are eigenvalues of $A$ and $A'$, and the same eigenvalue may appear in different blocks. Further, $n_1 + n_2 + \dots + n_r = n$ so we end up with an $n \times n$ matrix. $A$ is diagonalisable if and only if $A'$ consists entirely of $1 \times 1$ blocks. The expression above is the Jordan Normal Form.
\end{theorem}
The proof is non-examinable and depends on the Part IB courses Linear Algebra, and Groups, Rings and Modules, so is not included here.

\section{Conics and Quadrics}
\subsection{Quadrics in General}
A quadric in $\mathbb R^n$ is a hypersurface defined by an equation of the form
\[ Q(\vb x) = \vb x^\transpose A \vb x + \vb b^\transpose \vb x + c = 0 \]
for some nonzero, symmetric, real $n \times n$ matrix $A$, $b \in \mathbb R^n$, $c \in \mathbb R$. In components,
\[ Q(\vb x) = A_{ij}x_ix_j + b_ix_i + c = 0 \]
We will clasify solutions for $\vb x$ up to geometrical equivalence, so we will not distinguish between solutions here which are related by isometries in $\mathbb R^n$ (distance-preserving maps, i.e. translations and orthogonal transformations about the origin).

Note that $A$ is invertible if and only if it has no zero eigenvalues. In this case, we can complete the sequare in the equation $Q(\vb x) = 0$ by setting $\vb y = \vb x + \frac{1}{2}A^{-1} \vb b$. This is essentially a translation isometry, moving the origin to $\frac{1}{2}A^{-1} \vb b$.
\begin{align*}
    \vb y^\transpose A \vb y & = (\vb x + \frac{1}{2}A^{-1}\vb b)^\transpose A (\vb x + \frac{1}{2}A^{-1}\vb b)                         \\
                             & = (\vb x^\transpose + \frac{1}{2}\vb b^\transpose(A^{-1})^\transpose) A (\vb x + \frac{1}{2}A^{-1}\vb b) \\
                             & = \vb x^\transpose A \vb x + \vb b^\transpose \vb x + \frac{1}{4}\vb b^\transpose A^{-1}\vb b
\end{align*}
since $(A^\transpose)^{-1} = (A^{-1})^\transpose$. Then,
\[ Q(\vb x) = 0 \iff \mathcal F(\vb y) = k \]
with
\[ \mathcal F(\vb y) = \vb y^\transpose A \vb y \]
which is a quadratic form with respect to a new origin $\vb y = \vb 0$, and where $k = \frac{1}{4}\vb b^\transpose A^{-1}\vb b - c$. Now we can diagonalise $\mathcal F$ as in the above section, in particular, orthonormal eigenvectors give the principal axes, and the eigenvalues of $A$ and the value of $k$ determine the geometrical nature of the solution of the quadric. In $\mathbb R^3$, the geometrical possibilities are (as we saw before):
\begin{enumerate}[(i)]
    \item eigenvalues positive, $k$ positive gives an ellipsoid;
    \item eigenvalues different signs, $k$ nonzero gives a hyperboloid
\end{enumerate}
If $A$ has one or more zero eigenvalues, then the analysis we have just provided changes, since we can no longer construct such a $\vb y$ vector, since $A^{-1}$ does not exist. The simplest standard form of $Q$ may have both linear and quadratic terms.

\subsection{Conics as Quadrics}
Quadrics in $\mathbb R^2$ are curves called conics. Let us first consider the case where $\det A \neq 0$. By completing the square and diagonalising $A$, we get a standard form
\[ \lambda_1 {x'_1}^2 + \lambda_2 {x'_2}^2 = k \]
The variables $x'_i$ correspond to the principal axes and the new origin. We have the following cases.
\begin{itemize}
    \item ($\lambda_1, \lambda_2 > 0$) This is an ellipse for $k>0$, and a point for $k=0$. There are no solutions for $k<0$.
    \item ($\lambda_1 > 0, \lambda_2 < 0$) This gives a hyperbola for $k>0$, and a hyperbola in the other axis if $k<0$. If $k=0$, this is a pair of lines. For instance, ${x'_1}^2 - {x'_2}^2 = 0 \implies (x'_1 - x'_2)(x'_1 + x'_2) = 0$.
\end{itemize}
If $\det A = 0$, then there is exactly one zero eigenvalue since $A \neq 0$. Then:
\begin{itemize}
    \item ($\lambda_1 > 0, \lambda_2 = 0$) We will diagonalise $A$ in the original expression for the quadric. This gives
          \[ \lambda_1 {x'_1}^2 + b'_1 x'_1 + b'_2 x'_2 + c = 0 \]
          This is a new equation in the coordinate system defined by $A$'s principal axes. Completing the square here in the $x'_1$ term, we have
          \[ \lambda_1 {x''_1}^2 + b'_2x'_2 + c' = 0 \]
          where $x''_1 = x'_1 + \frac{1}{2\lambda_1}b'_1$, and $c' = c - \frac{{b'_1}^2}{4\lambda_1^2}$. If $b'_2 = 0$, then $x_2$ can take any value; and we get a pair of lines if $c'<0$, a single line if $c'=0$, and no solutions if $c'>0$. Otherwise, $b'_2 \neq 0$, and the equation becomes
          \[ \lambda_1 {x''_1}^2 + b'_2x''_2 = 0 \]
          where $x_2'' = x'_2 + \frac{1}{b_2'}c'$, and clearly this equation is a parabola.
\end{itemize}
All changes of coordinates correspond to translations (shifts of the origin) or orthogonal transformations, both of which preserve distance and angles.

\subsection{Standard Forms for Conics}
The general forms of conics can be written in terms of lengths $a, b$ (the semi-major and semi-minor axes), or equivalently a length scale $\ell$ and a dimensionless eccentricity constant $e$.
\begin{itemize}
    \item First, let us consider Cartesian coordinates. The formulas are:

          \medskip\noindent\begin{tabular}{c|c|c|c}
              conic     & formula                                 & eccentricity                       & foci       \\\hline
              ellipse   & $\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1$ & $b^2=a^2(1-e^2)$, and $e<1$        & $x=\pm ae$ \\
              parabola  & $y^2 = 4ax$                             & one quadratic term vanishes, $e=1$ & $x = +a$   \\
              hyperbola & $\frac{x^2}{a^2} - \frac{y^2}{b^2} = 1$ & $b^2=a^2(e^2-1)$, and $e<1$        & $x=\pm ae$
          \end{tabular}

    \item Polar coordinates are a convenient alternative to Cartesian coordinates. In this coordinate system, we set the origin to be at a focus. Then, the formulas are
          \[ r = \frac{\ell}{1 + e\cos \theta} \]
          \begin{itemize}
              \item For the ellipse, $e<1$ and $\ell = a(1-e^2)$;
              \item For the parabola, $e=1$ and $\ell = 2a$; and
              \item For the hyperbola, $e>1$ and $\ell = a(e^2 - 1)$. There is only one branch for the hyperbola given by this polar form.
          \end{itemize}
\end{itemize}

%TODO draw graphs for all of these curves in both coordinate systems

\subsection{Conics as Sections of a Cone}
The equation for a cone in $\mathbb R^3$ given by an apex $\vb c$, an axis $\nhat$, and an angle $\alpha < \frac{\pi}{2}$, is
\[ (\vb x - \vb c)\cdot\nhat = \abs{\vb x - \vb c}\cos \alpha \]
Less formally, the angle of $\vb x$ away from $\nhat$ must be $\alpha$. By squaring this equation, we can essentially define two cones which stretch out infinitely far and meet at the centre point $\vb c$.
\[ \left( (\vb x - \vb c)\cdot\nhat \right)^2 = \abs{\vb x - \vb c}^2\cos^2 \alpha \]
Let us choose a set of coordinate axes so that our equations end up slightly easier. Let $\vb c = c\vb e_3, \nhat = \cos\beta \vb e_1 - \sin\beta \vb e_3$. Then essentially the cone starts at $(0, 0, c)$ and points `downwards' in the $\vb e_1$--$\vb e_3$ plane. Then the conic section is the intersection of this cone with the $\vb e_1$--$\vb e_2$ plane, i.e. $x_3 = 0$.
\[ (x_1\cos\beta - c\sin\beta)^2 = (x_1^2 + x_2^2 + c^2)\cos^2\alpha \]
\[ \iff (\cos^2\alpha - \cos^2\beta)x_1^2 + (\cos^2\alpha)x_2^2 + 2x_1c\sin\beta\cos\beta = \text{const.} \]
Now we can compare the signs of the $x_1^2$ and $x_2^2$ terms. Clearly the $x_2^2$ term is always positive, so we consider the sign of the $x_1^2$ term.
\begin{itemize}
    \item If $\cos^2 \alpha > \cos^2\beta$ (i.e. $\alpha < \beta$), then we have an ellipse.
    \item If $\cos^2 \alpha = \cos^2\beta$ (i.e. $\alpha = \beta$), then we have a parabola.
    \item If $\cos^2 \alpha < \cos^2\beta$ (i.e. $\alpha > \beta$), then we have a hyperbola.
\end{itemize}

\section{Symmetries and Transformation Groups}
\subsection{Orthogonal Transformations and Rotations in $\mathbb R^n$}
We know that if a matrix $R$ is orthogonal, we have $R^\transpose R = I \iff (R\vb x) \cdot (R\vb y) = \vb x \cdot \vb y \iff$ the rows or columns are orthonormal. The set of $n \times n$ matrices $R$ forms the orthogonal group $O_n = O(n)$. If $R \in O(n)$ then $\det R = \pm 1$. $SO_n = SO(n)$ is the special orthogonal group, which is the subgroup of $O(n)$ defined by $\det R = 1$. If some matrix $R$ is an element of $O(n)$, then $R$ preserves the modulus of $n$-dimensional volume. If $R \in SO(n)$, then $R$ preserves not only the modulus but also the sign of such a volume.

$SO(n)$ consists precisely of all rotations in $\mathbb R^n$. $O(n) \setminus SO(n)$ consists of all reflections. For some specific $H \in O(n) \setminus SO(n)$, any element of $O(n)$ can be written as a product of $H$ with some element in $SO(n)$, i.e. $R$ or $RH$ with $R \in SO(n)$. For example, if $n$ is odd, we can choose $H = -I$.

Now, we can consider the transformation $x'_i = R_{ij} x_j$ under two distinct points of view.
\begin{itemize}
    \item (active) The rotation $R$ acts on the vector $\vb x$ and yields a new vector $\vb x'$. The $x'_i$ are components of the transformed vector in terms of the standard basis vectors.
    \item (passive) The $x'_i$ are components of the same vector $\vb x$ but with respect to new orthonormal basis vectors $\vb u_i$. In general, $\vb x = \sum_i x_i \vb e_i = \sum_i x'_i \vb u_i$ which is true where $\vb u_i = \sum_j R_{ij} \vb e_j = \sum_j \vb e_j P_{ji}$. So $P = R^{-1} = R^\transpose$ where $P$ is the change of basis matrix.
\end{itemize}

\subsection{2D Minkowski Space}
Consider a new `inner product' on $\mathbb R^2$ given by
\[ (\vb x, \vb y) = \vb x^\transpose J \vb y;\quad J = \begin{pmatrix}
        1 & 0 \\ 0 & -1
    \end{pmatrix} \]
\[ \therefore \left( \begin{pmatrix}
            x_0 \\ x_1
        \end{pmatrix}, \begin{pmatrix}
            y_0 \\ y_1
        \end{pmatrix} \right) = x_0 y_0 - x_1 y_1 \]
We start indexing these vectors from zero, not one. Here are some important properties.
\begin{itemize}
    \item This `inner product' is not positive definite. In fact, $(\vb x, \vb x) = x_0^2 - x_1^2$. (This is a  quadratic form for $\vb x$ with eigenvalues $\pm 1$.)
    \item It is bilinear and symmetric.
    \item Defining $\vb e_0 = \begin{pmatrix}
                  1 \\ 0
              \end{pmatrix}$ and $\vb e_1 = \begin{pmatrix}
                  0 \\ 1
              \end{pmatrix}$, they obey
          \[ (\vb e_0, \vb e_0) = -(\vb e_1, \vb e_1) = 1;\quad (\vb e_0, \vb e_1) = 0 \]
          This is similar to orthonormality, in this generalised sense.
\end{itemize}
This inner product is known as the Minkowski metric on $\mathbb R^2$. $\mathbb R^2$ with this metric is called Minkowski space.

\subsection{Lorentz Transformations}
Let us consider a matrix
\[ M = \begin{pmatrix}
        M_{00} & M_{01} \\
        M_{10} & M_{11}
    \end{pmatrix} \]
giving a map $\mathbb R^2 \to \mathbb R^2$; this preserves the Minkowski metric if and only if $(M\vb x, M\vb y) = (\vb x, \vb y)$ for any vectors $\vb x, \vb y$. Expanded, this condition is
\[ (M\vb x)^\transpose J(M \vb y) = \vb x^\transpose M^\transpose J M \vb y = \vb x^\transpose J \vb y \]
\[ \implies M^\transpose J M = J \]
The set of such matrices form a group. Also, $\det M = \pm 1$ for the same reason as before. Furthermore, $\abs{M_{00}}^2 \geq 1$, so either $M_{00} \geq 1$ or $M_{00} \leq -1$. The subgroup with $\det M = +1$ and $M_{00} \geq 1$ is known as the Lorentz group.

Let us find the general form of $M$, by using the fact that the columns $M \vb e_0$ and $M \vb e_i$ are orthonormal with respect to the Minkowski metric.
\[ (M \vb e_0, M \vb e_0) = M_{00}^2 - M_{10}^2 = (\vb e_0, \vb e_0) = 1\quad (\text{hence } \abs{M_{00}}^2 \geq 1) \]
Taking $M_{00} \geq 1$, we can write
\[ M\vb e_0 = \begin{pmatrix}
        \cosh \theta \\ \sinh \theta
    \end{pmatrix} \]
for some real value $\theta$. For the other column,
\[ (M \vb e_0, M \vb e_1) = 0;\; (M \vb e_1, M \vb e_1) = -1 \implies M \vb e_1 = \pm\begin{pmatrix}
        \sinh \theta \\
        \cosh \theta
    \end{pmatrix} \]
The sign is fixed to be positive by the condition that $\det M = +1$.
\[ M = \begin{pmatrix}
        \cosh \theta & \sinh \theta \\
        \sinh \theta & \cosh \theta
    \end{pmatrix} \]
The curves defined by $(\vb x, \vb x) = k$ where $k$ is a constant are hyperbolas. This is analogous to how the curves defined by $\vb x \cdot \vb x = k$ are circles. So applying $M$ to any vector on a given branch of a hyperbola, the resultant vector remains on the hyperbola.
%TODO graph
Note that these matrices obey the rule $M(\theta_1) M(\theta_2) = M(\theta_1 + \theta_2)$. This confirms that they form a group.

\subsection{Application to Special Relativity}
Let
\[ M(\theta) = \gamma(v) \begin{pmatrix}
        1 & v \\ v & 1
    \end{pmatrix};\quad v = \tanh \theta;\quad \gamma = (1 - v^2)^{-\frac{1}{2}} \]
Here, $v$ lies in the range $-1 < v < 1$. We will rename $x_0$ to be $t$, which is now our time coordinate. $x_1$ will just be written $x$, our one-dimensional space coordinate. Then,
\[ \vb x' = M\vb x \iff \begin{cases}
        t' & = \gamma \cdot (t + vx) \\
        x' & = \gamma \cdot (x + vt)
    \end{cases} \]
This is a Lorentz transformation, or `boost', relating the time and space coordinates for observers moving with relative velocity $v$ in Special Relativity, in units where the speed of light $c$ is taken to be 1. The $\gamma$ factor in the Lorentz transformation gives rise to time dilation and length contraction effects. The group property $M(\theta_3) = M(\theta_1)M(\theta_2)$ with $\theta_3 = \theta_1 + \theta_2$ corresponds to the velocities
\[ v_i = \tanh \theta_i \implies v_3 = \frac{v_1 + v_2}{1 + v_1 v_2} \]
This is consistent with the fact that all velocities are less than the speed of light, 1.
\end{document}