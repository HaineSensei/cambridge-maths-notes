\subsection{Introduction}
A linear map (or linear transformation) is some operation \(T: V \to W\) between vector spaces \(V\) and \(W\) preserving the core vector space structure (specifically, the linearity).
It is defined such that
\[
	T\left(\lambda \vb x + \mu \vb y\right) = \lambda T(\vb x) + \mu T(\vb y)
\]
for all \(\vb x, \vb y \in V\) where the scalars \(\lambda\) and \(\mu\) match up with the scalar field that \(V\) and \(W\) use (so this could be \(\mathbb R\) or \(\mathbb C\) in our examples).
Much of the language used for linear maps between vector spaces is analogous to the language used for homomorphisms between groups.

Note that a linear map is completely determined by its action on a basis \(\{ \vb e_1, \cdots, \vb e_n \}\) where \(n = \dim V\), since
\[
	T\left(\sum_i x_i \vb e_i \right) = \sum_i x_i T(\vb e_i)
\]
We denote \(\vb x' = T(\vb x) \in W\), and define \(\vb x'\) as the image of \(x\) under \(T\).
Further, we define
\[
	\Im (T) = \{ \vb x' \in W : \vb x' =T(\vb x) \text{ for some } \vb x \in V \}
\]
to be the image of \(T\), and we define
\[
	\ker (T) = \{ \vb x \in V : T(\vb x) = \vb 0 \}
\]
to be the kernel of \(T\).

\begin{lemma}
	\(\ker T\) is a subspace of \(V\), and \(\Im T\) is a subspace of \(W\).
\end{lemma}
\begin{proof}
	To verify that some subset is a subspace, it suffices to check that it is non-empty, and that it is closed under linear combinations.

	\(\ker T\) is non-empty because \(\vb 0 \in \ker T\).
	For \(\vb x, \vb y \in \ker T\), we have \(T(\lambda \vb x + \mu \vb y) = \lambda T(\vb x) + \mu T(\vb y) = \vb 0 \in \ker T\) as required.

	\(\Im T\) is non-empty because \(\vb 0 \in \Im T\).
	For \(\vb x, \vb y \in V\), let \(\vb x' = T(\vb x)\) and \(\vb y' = T(\vb y)\), therefore \(\vb x', \vb y' \in \Im T\).
	Now, \(\lambda \vb x' + \mu \vb y' = T(\lambda \vb x + \mu \vb y)\) so it is closed under linear combinations as required.
\end{proof}
Here are some examples of images and kernels.
\begin{enumerate}[(i)]
	\item The zero linear map \(\vb x \mapsto \vb 0\) has:
	      \begin{align*}
		      \Im T  & = \{ \vb 0 \} \\
		      \ker T & = V
	      \end{align*}
	\item The identity linear map \(\vb x \mapsto \vb x\) has:
	      \begin{align*}
		      \Im T  & = V           \\
		      \ker T & = \{ \vb 0 \}
	      \end{align*}
	\item Let \(T: \mathbb R^3 \to \mathbb R^3\), such that
	      \begin{align*}
		      x_1' & = 3x_1 - x_2 + 5x_3 \\
		      x_2' & = -x_1 - 2x_3       \\
		      x_3' & = 2x_1 + x_2 + 3x+3
	      \end{align*}
	      This map has
	      \begin{align*}
		      \Im T  & = \left\{ \lambda \begin{pmatrix} 3 \\ -1 \\ 2 \end{pmatrix} + \mu \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix} : \lambda, \mu \in \mathbb R \right\} \\
		      \ker T & = \left\{ \lambda \begin{pmatrix} 2 \\ -1 \\ -1 \end{pmatrix} : \lambda \in \mathbb R \right\}
	      \end{align*}
\end{enumerate}

\subsection{Rank and nullity}
We define the rank of a linear map to be the dimension of its image, and the nullity of a linear map to be the dimension of its kernel.
\[
	\rank T = \dim \Im T; \quad \nullity T = \dim \ker T
\]
Note that therefore for \(T: V \to W\), we have \(\rank T \leq \dim W\) and \(\ker T \leq \dim V\).
\begin{theorem}
	For some linear map \(T: V \to W\),
	\[
		\rank T + \nullity T = \dim V
	\]
\end{theorem}
\begin{proof}
	This proof is non-examinable (without prompts).
	Let \(\vb e_1, \cdots, \vb e_k\) be a basis for \(\ker T\), so \(T(\vb e_i) = \vb 0\) for all valid \(i\).
	We may extend this basis by adding more vectors \(\vb e_i\) where \(k < i \leq n\) until we have a basis for \(V\), where \(n=\dim V\).
	We claim that the set \(\mathcal B = \{ T(\vb e_{k+1}), \cdots, T(\vb e_n) \}\) is a basis for \(\Im T\).
	If this is true, then clearly the result follows because \(k = \dim \ker T = \nullity T\) and \(n-k = \dim \Im T = \rank T\).

	To prove the claim we need to show that \(\mathcal B\) spans \(\Im T\) and that it is a linearly independent set.
	\begin{itemize}
		\item \(\mathcal B\) spans \(\Im T\) because for any \(\vb x = \sum_{i=1}^n x_i \vb e_i\), we have
		      \[
			      T(\vb x) = \sum_{i=k+1}^n x_i T(\vb e_i) \in \vecspan \mathcal B
		      \]
		\item \(\mathcal B\) is linearly independent.
		      Consider a general linear combination of basis vectors:
		      \[
			      \sum_{i=k+1}^n \lambda_i T(\vb e_i) = 0 \implies T\left( \sum_{i=k+1}^n \lambda_i \vb e_i \right) = 0
		      \]
		      so
		      \[
			      \sum_{i=k+1}^n \lambda_i \vb e_i \in \ker T
		      \]
		      Because this is in the kernel, it may be written in terms of the basis vectors of the kernel.
		      So, we have
		      \[
			      \sum_{i=k+1}^n \lambda_i \vb e_i = \sum_{i=1}^k \mu_i \vb e_i
		      \]
		      This is a linear relation in terms of all basis vectors of \(V\).
		      So all coefficients are zero.
	\end{itemize}
\end{proof}

\subsection{Rotations}
Linear maps are often used to describe geometrical transformations, such as rotations, reflections, projections, dilations and shears.
A convenient way to express these maps is by describing where the basis vectors are mapped to.
In \(\mathbb R^2\), we may describe a rotation anticlockwise around the origin by angle \(\theta\) with
\begin{align*}
	\vb e_1 & \mapsto \cos \theta \vb e_1 + \sin \theta \vb e_2  \\
	\vb e_2 & \mapsto -\sin \theta \vb e_1 + \cos \theta \vb e_2
\end{align*}
In \(\mathbb R^3\) we can construct a similar transformation for a rotation around the \(\vb e_3\) axis with
\begin{align*}
	\vb e_1 & \mapsto \cos \theta \vb e_1 + \sin \theta \vb e_2  \\
	\vb e_2 & \mapsto -\sin \theta \vb e_1 + \cos \theta \vb e_2 \\
	\vb e_3 & \mapsto \vb e_3
\end{align*}
We can extend this to a general rotation in \(\mathbb R^3\) about an axis given by a unit normal vector \(\hat {\vb n}\).
For any vector \(\vb x \in \mathbb R^3\) we can resolve parallel and perpendicular to \(\nhat\) as follows.
\[
	\vb x = \vb x_\parallel + \vb x_\perp;\quad \vb x_\parallel = (\vb x \cdot \nhat) \nhat;\quad \vb x_\perp = \vb x - (\vb x \cdot \nhat) \nhat
\]
Note that \(\nhat\) resembles the \(\vb e_3\) axis here, and \(\vb x_\perp\) resembles the \(\vb e_1\) axis.
So we can compute the equivalent of \(\vb e_2\) using the cross product, \(\nhat \times \vb x_\perp = \nhat \times \vb x\).
Now we may define the map with
\begin{align*}
	\vb x_\parallel & \mapsto \vb x_\parallel                                               \\
	\vb x_\perp     & \mapsto (\cos \theta) \vb x_\perp + (\sin \theta)(\nhat \times \vb x)
\end{align*}
So all together, we have
\[
	\vb x \mapsto (\cos \theta) \vb x + (1 - \cos \theta) (\nhat \cdot \vb x)\nhat + (\sin \theta)(\nhat \times \vb x)
\]

\subsection{Reflections and projections}
For a plane with normal \(\nhat\), we define a projection to be
\begin{align*}
	\vb x_\parallel & \mapsto \vb 0                                           \\
	\vb x_\perp     & \mapsto \vb x_\perp                                     \\
	\vb x           & \mapsto \vb x_\perp = \vb x - (\vb x \cdot \nhat) \nhat
\end{align*}
and a reflection to be
\begin{align*}
	\vb x_\parallel & \mapsto -\vb x_\parallel                                                   \\
	\vb x_\perp     & \mapsto \vb x_\perp                                                        \\
	\vb x           & \mapsto \vb x_\perp - \vb x_\parallel = \vb x - 2(\vb x \cdot \nhat) \nhat
\end{align*}
The same expressions also apply in \(\mathbb R^2\), where we replace the plane with a line.

\subsection{Dilations}
Given scale factors \(\alpha, \beta, \gamma > 0\), we define a dilation along the axes by
\begin{align*}
	\vb e_1 & \mapsto \alpha \vb e_1 \\
	\vb e_2 & \mapsto \beta \vb e_2  \\
	\vb e_3 & \mapsto \gamma \vb e_3
\end{align*}

\subsection{Shears}
Let \(\vb a, \vb b\) be orthogonal unit vectors in \(\mathbb R^3\), i.e.\ \(\abs{\vb a} = \abs{\vb b} = \vb 0\) and \(\vb a \cdot \vb b = 0\), and we define a real parameter \(\lambda\).
A shear is defined as
\begin{align*}
	\vb x & \mapsto \vb x' = \vb x + \lambda \vb a (\vb x \cdot \vb b) \\
	\vb a & \mapsto \vb a                                              \\
	\vb b & \mapsto \vb b + \lambda \vb a
\end{align*}
This definition holds equivalently in \(\mathbb R^2\).
