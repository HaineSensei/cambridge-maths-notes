\subsection{Introduction}
Vector spaces are not studied axiomatically in this course, but the axioms are given here for completeness.
A real (as in, \(\mathbb R\)) vector space \(V\) is a set of objects with two operators \(+: V \times V \to V\) and \(\cdot: \mathbb R \times V \to V\) such that
\begin{itemize}
	\item \((V, +)\) is an abelian group
	\item \(\lambda(v + w) = \lambda v + \lambda w\)
	\item \((\lambda + \mu)v = \lambda v + \mu v\)
	\item \(\lambda(\mu v) = (\lambda \mu) v\)
	\item \(1v = v\) (to exclude trivial cases for example \(\lambda v = 0\) for all \(v\))
\end{itemize}

\subsection{Subspaces}
A subspace of a real vector space \(V\) is a subset \(U \subseteq V\) that is a vector space.
Equivalently, if all pairs of vectors \(v, w \in U\) satisfy \(\lambda v + \mu w \in U\), then \(U\) is a subspace of \(V\).
Note that the span generated from a set of vectors is a subspace, as it is characterised by this equivalent definition.
Also, note that the origin must be part of any subspace, because multiplying a vector by zero must yield the origin.

\subsection{Linear dependence}
In some real vector space \(V\), let \(\vb v_1, \vb v_2 \cdots \vb v_r\) be vectors in \(V\).
Now consider the linear relation
\[
	\lambda_1 \vb v_1 + \lambda_2 \vb v_2 + \cdots + \lambda_r \vb v_r = 0
\]
Then we call the set of vectors a linearly independent set if the only solution is where all \(\lambda\) values are zero.
Otherwise, it is a linearly dependent set.

\subsection{Inner product spaces}
An inner product is an extra structure that we can have on a real vector space \(V\), which is often denoted by angle brackets or parentheses.
It can also be characterised by axioms (specifically the ones in Section 6.2).
Features like the norm of a vector, and theorems like the Cauchy-Schwarz inequality, follow from these axioms.

For example, let us consider the vector space
\[
	V = \{ f: [0, 1] \to \mathbb R: f \text{ smooth}; f(0) = f(1) = 0 \}
\]
We can define the inner product to be
\[
	f \cdot g = \langle f, g \rangle = \int_0^1 f(x)g(x)\dd{x}
\]
Then by the Cauchy-Schwarz inequality, we have
\begin{align*}
	\abs{\langle f, g \rangle}               & \leq \norm{f} \cdot \norm{g}                                    \\
	\therefore \abs{\int_0^1 f(x)g(x)\dd{x}} & \leq \sqrt{\int_0^1 f(x)^2 \dd{x}}\sqrt{\int_0^1 g(x)^2 \dd{x}}
\end{align*}

\begin{lemma}
	In any real inner product space \(V\), if \(\vb v_1 \cdots v_r \neq \vb 0\) are orthogonal, they are linearly independent.
\end{lemma}
\begin{proof}
	If \(\sum_i \alpha_i \vb v_i = 0\), then
	\begin{align*}
		\left\langle \vb v_j, \sum_i \alpha_i \vb v_i \right\rangle     & = 0 \\
		\intertext{And because each vector that is not \(\vb v_j\) is orthogonal to it, those terms cancel, leaving}
		\therefore \left\langle \vb v_j, \alpha_j \vb v_j \right\rangle & = 0 \\
		\alpha_j \left\langle \vb v_j, \vb v_j \right\rangle            & = 0 \\
		\alpha_j = 0
	\end{align*}
	So they are linearly independent.
\end{proof}

\subsection{Bases and dimensions}
In a vector space \(V\), a basis is a set \(\mathcal B = \{ \vb e_1 \cdots \vb e_n \}\) such that
\begin{itemize}
	\item \(\mathcal B\) spans \(V\); and
	\item \(\mathcal B\) is linearly independent, which implies that the coefficients on these basis vectors are unique for any vector in \(V\), since it is impossible to write one vector in terms of the others
\end{itemize}

\begin{theorem}
	If \(\{\vb e_1 \cdots \vb e_n \}\) and \(\{ \vb f_1 \cdots \vb f_m \}\) are bases for a real vector space \(V\), then \(n=m\), which we call the dimension of \(V\).
\end{theorem}
\begin{proof}
	This proof is non-examinable (without prompts).
	We can write each basis vector in terms of the others, since they all span the same vector space.
	Thus:
	\[
		\vb f_a = \sum_i A_{ai} \vb e_i;\quad \vb e_i = \sum_a B_{ia} \vb f_a
	\]
	Note that indices \(i,j\) span from 1 to \(n\), while \(a,b\) span from 1 to \(m\).
	We can substitute one expression into the other, forming:
	\begin{align*}
		\vb f_a & = \sum_i A_{ai} \left( \sum_b B_{ib}\vb f_b \right)  \\
		\vb f_a & = \sum_b \left( \sum_i A_{ai} B_{ib} \right) \vb f_b
	\end{align*}
	Note that we have now written \(\vb f_a\) as a linear combination of \(\vb f_b\) for all valid \(b\).
	But since they are linearly independent, the coefficient of \(\vb f_b\) must be zero if \(a \neq b\), and one of \(a = b\).
	Therefore, we have
	\[
		\delta_{ab} = \sum_i A_{ai} B_{ib}
	\]
	We can make a similar statement about \(\vb e_i\):
	\[
		\delta_{ij} = \sum_a B_{ia} A_{aj} = \sum_a A_{aj} B_{ia}
	\]
	Now, assigning \(a=b\) and \(i=j\), summing over both, and substituting into our two previous expressions for \(\delta\), we have:
	\begin{alignat*}{2}
		\sum_{ia} A_{ai} B_{ia} & = \sum_a \delta_{aa} &  & = \sum_i \delta_{ii} \\
		                        & = m                  &  & = n
	\end{alignat*}
\end{proof}

\subsection{Choosing bases}
Note that \(\{ \vb 0 \}\) is a trivial subspace of all vector spaces, and it has dimension zero since it requires a linear combination of no vectors.

\begin{proposition}
	Let \(V\) be a vector space with finite subsets \(Y = \{ \vb w_1, \cdots, \vb w_m \}\) that spans \(V\), and \(X = \{ \vb u_1, \cdots, \vb u_k \}\) that is linearly independent.
	Let \(n = \dim V\).
	Then:
	\begin{enumerate}[(i)]
		\item A basis can be found as a subset of \(Y\) by discarding vectors in \(Y\) as necessary, and that \(n \leq m\).
		\item \(X\) can be extended to a basis by adding in additional vectors from \(Y\) as necessary, and that \(k \leq n\).
	\end{enumerate}
\end{proposition}
\begin{proof}
	This proof is non-examinable (without prompts).
	\begin{enumerate}[(i)]
		\item If \(Y\) is linearly independent, then \(Y\) is a basis and \(m = n\).
		      Otherwise, \(Y\) is not linearly independent.
		      So there exists some linear relation
		      \[
			      \sum_{i=1}^{m} \lambda_i \vb w_i = \vb 0
		      \]
		      where there is some \(i\) such that \(\lambda_i \neq 0\).
		      Without loss of generality (because the order of elements in \(Y\) does not matter) we will reorder \(Y\) such that \(\vb w_m \neq 0\).
		      So we have
		      \[
			      \vb w_m = \frac{-1}{\lambda_m} \sum_{i=1}^{m-1} \lambda_i \vb w_i
		      \]
		      So \(\vecspan Y = \vecspan (Y \setminus \{ \vb w_m \})\).
		      We can repeat this process of eliminating vectors from \(Y\) until linear independence is achieved.
		      We know that this process will end because \(Y\) is a finite set.
		      Clearly, in this case, \(n < m\).
		      So for all cases, \(n \leq m\).

		\item If \(X\) spans \(V\), then \(X\) is a basis and \(k=n\).
		      Else, there exists some \(u_{k+1} \in V\) that is not in the span of \(X\).
		      Then, we will construct an arbitrary linear relation
		      \[
			      \sum_{i=1}^{k+1} \mu_i \vb u_i = \vb 0
		      \]
		      Note that this implies that \(\mu_{k+1} = \vb 0\) because it is not in the span of \(X\), and that \(\mu_i = 0\) for all \(i \leq k\) because the original \(X\) was linearly independent.
		      So we know that all the coefficients are zero, and therefore \(X \cup \{ u_{k+1} \}\) is linearly independent.

		      Note that we can always choose this \(u_{k+1}\) to be an element of \(Y\) because we just need to ensure that \(u_{k+1} \notin \vecspan X\).
		      Suppose we cannot choose such a vector in \(Y\).
		      Then \(Y \subseteq \vecspan X \implies \vecspan Y \subseteq \vecspan X \implies \vecspan X = V\), which is clearly false because \(X\) does not span \(V\).
		      This is a contradiction, so we can always choose such a vector from \(Y\).
		      We can repeat this process of taking vectors from \(Y\) and adding them to \(X\) until we have a basis.
		      This process will always terminate in a finite amount of steps because we are taking new vectors from a finite set \(Y\).
		      Therefore \(k \leq n\), as we are adding vectors (increasing \(k\)) until \(k=n\).
	\end{enumerate}
\end{proof}

\subsection{Infinite dimensions}
It is perfectly possible to have a vector space that has infinite dimensionality.
However, they will be rarely touched upon in this course apart from specific examples, like the following example.
Let \(V = \{ f: [0, 1] \to \mathbb R: f \text{ smooth}, f(0) = f(1) = 0\}\).
Then let \(S_n(x) = \sqrt 2 \sin(n \pi x)\) where \(n\) is a natural number \(1, 2, \cdots\).
Clearly, \(S_n \in V\) for all \(n\).
The inner product of two of these \(S\) functions is given by
\begin{align*}
	\langle S_n, S_m \rangle & = 2 \int_0^1 \sin(n \pi x) \sin(m \pi x) \dd{x} \\
	                         & = \delta_{mn}
\end{align*}
So \(S_n\) are orthonormal and therefore linearly independent.
So we can continue adding more vectors until it becomes a basis.
However, the set of all \(S_n\) is already infinite --- so \(V\) must have infinite dimensionality.
