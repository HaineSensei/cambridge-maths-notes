\subsection{Definitions}
For a linear map \(T\colon V \to V\), a vector \(\vb v \in V\) with \(\vb v \neq 0\) is called an eigenvector of \(T\) with eigenvalue \(\lambda\) if \(T(\vb v) = \lambda \vb v\).
If \(V = \mathbb R^n\) or \(\mathbb C^n\), and \(T\) is given by an \(n \times n\) matrix \(A\), then
\[
	A\vb v = \lambda v \iff (A - \lambda I)\vb v = \vb 0
\]
and for a given \(\lambda\), this holds for some \(\vb v \neq 0\) if and only if
\[
	\det(A - \lambda I) = 0
\]
This is called the characteristic equation for \(A\).
So \(\lambda\) is an eigenvalue if and only if it is a root of the characteristic polynomial
\[
	\chi_A(t) = \det(A - tI) = \begin{vmatrix}
		A_{11} - t & A_{12}     & \cdots & A_{1n}     \\
		A_{21}     & A_{22} - t & \cdots & A_{2n}     \\
		\vdots     & \vdots     & \ddots & \vdots     \\
		A_{n1}     & A_{n2}     & \cdots & A_{nn} - t
	\end{vmatrix}
\]
We can look for eigenvalues as roots of the characteristic polynomial or characteristic equation, and then determine the corresponding eigenvectors once we've deduced what the possibilities are.

\subsection{Examples of eigenvalues and eigenvectors}
\begin{enumerate}
	\item \(V = \mathbb C^2\):
	      \[
		      A = \begin{pmatrix}
			      2 & i \\ -i & 2
		      \end{pmatrix} \implies \det(A - \lambda I) = (2-\lambda)^2 - 1 = 0
	      \]
	      So we have \((2 - \lambda)^2 = 1\) so \(\lambda = 1\) or 3.
	      \begin{itemize}
		      \item (\(\lambda = 1\))
		            \[
			            (A - I)\vb v = \begin{pmatrix}
				            1 & i \\ -i & 1
			            \end{pmatrix}\begin{pmatrix}
				            v_1 \\ v_2
			            \end{pmatrix} = \vb 0 \implies \vb v = \alpha\begin{pmatrix}
				            1 \\ i
			            \end{pmatrix}
		            \]
		            for any \(\alpha \neq 0\).
		      \item (\(\lambda = 3\))
		            \[
			            (A - 3I)\vb v = \begin{pmatrix}
				            -1 & i \\ -i & -1
			            \end{pmatrix}\begin{pmatrix}
				            v_1 \\ v_2
			            \end{pmatrix} = \vb 0 \implies \vb v = \beta\begin{pmatrix}
				            1 \\ -i
			            \end{pmatrix}
		            \]
		            for any \(\beta \neq 0\).
	      \end{itemize}
	\item \(V = \mathbb R^2\):
	      \[
		      A = \begin{pmatrix}
			      1 & 1 \\ 0 & 1
		      \end{pmatrix} \implies \det(A - \lambda I) = (1-\lambda)^2 = 0
	      \]
	      So \(\lambda = 1\) only, a repeated root.
	      \[
		      (A - I)\vb v = \begin{pmatrix}
			      0 & 1 \\ 0 & 0
		      \end{pmatrix}\begin{pmatrix}
			      v_1 \\ v_2
		      \end{pmatrix} = \vb 0 \implies \vb v = \alpha\begin{pmatrix}
			      1 \\ 0
		      \end{pmatrix}
	      \]
	      for any \(\alpha \neq 0\).
	      There is only one (linearly independent) eigenvector here.
	\item \(V = \mathbb R^2\) or \(\mathbb C^2\):
	      \[
		      U = \begin{pmatrix}
			      \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta
		      \end{pmatrix} \implies \chi_U(t) = \det(U - tI) = t^2 - 2t\cos\theta + 1
	      \]
	      The eigenvalues \(\lambda\) are \(e^{\pm i \theta}\).
	      The eigenvectors are
	      \[
		      \vb v = \alpha \begin{pmatrix}
			      1 \\ \mp i
		      \end{pmatrix};\quad \alpha \neq 0
	      \]
	      So there are no real eigenvalues or eigenvectors except when \(\theta = n \pi\).
	\item \(V = \mathbb C^n\):
	      \[
		      A = \begin{pmatrix}
			      \lambda_1 & 0         & \cdots & 0         \\
			      0         & \lambda_2 & \cdots & 0         \\
			      \vdots    & \vdots    & \ddots & \vdots    \\
			      0         & 0         & \cdots & \lambda_n
		      \end{pmatrix} \implies \chi_A(t) = \det(A - tI) = (\lambda_1 - t)(\lambda_2 - t)(\lambda_3 - t)\dots(\lambda_n - t)
	      \]
	      So the eigenvalues are all the \(\lambda_i\), and the eigenvectors are \(\vb v = \alpha \vb e_i\) (\(\alpha \neq 0\)) for each \(i\).
\end{enumerate}

\subsection{Deductions involving the characteristic polynomial}
For an \(n \times n\) matrix \(A\), the characteristic polynomial \(\chi_A(t)\) has degree \(n\):
\[
	\chi_A(t) = \sum_{j = 0}^n c_j t^j = (-1)^n(t-\lambda_1)\dots(t-\lambda_n)
\]
\begin{enumerate}
	\item There exists at least one eigenvalue (solution to \(\chi_A\)), due to the fundamental theorem of algebra, or \(n\) roots counted with multiplicity.
	\item \(\tr(A) = A_{ii} = \sum_{i=1}^n \lambda_i\), the sum of the eigenvalues.
	      Compare terms of degree \(n-1\) in \(t\), and from the determinant we get
	      \[
		      (-t)^{n-1}A_{11} + (-t)^{n-1}A_{22} + \dots + (-t)^{n-1}A_{nn}
	      \]
	      The overall sign matches with the expansion of \((-1)^n(t-\lambda_1)(t-\lambda_2)\dots(t-\lambda_n)\).
	\item \(\det(A) = \chi_A(0) = \prod_{i=1}^n \lambda_i\), the product of the eigenvalues.
	\item If \(A\) is real, then the coefficients \(c_i\) in the characteristic polynomial are real, so \(\chi_A(\lambda) = 0 \iff \chi_A(\overline\lambda) = 0\).
	      So the non-real roots occur in conjugate pairs if \(A\) is real.
\end{enumerate}

\subsection{Eigenspaces and multiplicities}
For an eigenvalue \(\lambda\) of a matrix \(A\), we define the eigenspace
\[
	E_\lambda = \{ \vb v : A \vb v = \lambda \vb v \} = \ker (A - \lambda I)
\]
All nonzero vectors in this space are eigenvectors.
The geometric multiplicity is
\[
	m_\lambda = \dim E_\lambda = \nullity (A - \lambda I)
\]
equivalent to the number of linearly independent eigenvectors with the given eigenvalue \(\lambda\).
The algebraic multiplicity is
\[
	M_\lambda = \text{the multiplicity of } \lambda \text{ as a root of } \chi_A(t)
\]
i.e.\ \(\chi_A(t) = (t - \lambda)^{M_t} f(t)\), where \(f(\lambda) \neq 0\).

\begin{proposition}
	\(M_\lambda \geq m_\lambda\) (and \(m_\lambda \geq 1\) since \(\lambda\) is an eigenvalue).
	The proof of this proposition is delayed until the next section where we will then have the tools to prove it.
\end{proposition}

\subsection{Examples of eigenspaces}
\begin{enumerate}
	\item
	      \[
		      A = \begin{pmatrix}
			      -2 & 2 & -3 \\ 2 & 1 & -6 \\ -1 & -2 & 0
		      \end{pmatrix} \implies \chi_A(t) = \det(A - tI) = (5-t)(t+3)^2
	      \]
	      So \(\lambda = 5, -3\).
	      \(M_5 = 1\), \(M_{-3} = 2\).
	      We will now find the eigenspaces.
	      \begin{itemize}
		      \item (\(\lambda = 5\))
		            \[
			            E_5 = \left\{ \alpha\begin{pmatrix}
				            1 \\ 2 \\ -1
			            \end{pmatrix} \right\}
		            \]
		      \item (\(\lambda = -3\))
		            \[
			            E_{-3} = \left\{ \alpha\begin{pmatrix}
				            -2 \\ 1 \\ 0
			            \end{pmatrix} + \beta\begin{pmatrix}
				            3 \\ 0 \\ 1
			            \end{pmatrix} \right\}
		            \]
	      \end{itemize}
	      Note that to compute the eigenvectors, we just need to solve the equation \((A - \lambda I)\vb x = \vb 0\).
	      In the case of \(\lambda = -3\), for example, we then have
	      \[
		      \begin{pmatrix}
			      1 & 2 & -3 \\ 2 & 4 & -6 \\ -1 & -2 & 3
		      \end{pmatrix} \begin{pmatrix}
			      x_1 \\ x_2 \\ x_3
		      \end{pmatrix} = \vb 0
	      \]
	      We can use the first line of the matrix to get a linear combination for \(x_1, x_2, x_3\), specifically \(x_1 + 2x_2 = 3x_3 = 0\), so we can eliminate one of the variables (here, \(x_1\)) to get
	      \[
		      \vb x = \begin{pmatrix}
			      -2x_2 + 3x_3 \\ x_2 \\ x_3
		      \end{pmatrix} = \vb 0
	      \]
	      Now, \(\dim E_5 = m_5 = 1 = M_5\).
	      Similarly, \(\dim E_{-3} = m_{-3} = 2 = M_{-3}\).

	\item
	      \[
		      A = \begin{pmatrix}
			      -3 & -1 & 1 \\ -1 & -3 & 1 \\ -2 & -2 & 0
		      \end{pmatrix} \implies \chi_A(t) = \det(A - tI) = -(t + 2)^3
	      \]
	      We have a root \(\lambda = -2\) with \(M_{-2} = 3\).
	      To find the eigenspace, we will look for solutions of:
	      \[
		      (A + 2I)\vb x = \begin{pmatrix}
			      -1 & -1 & 1 \\ -1 & -1 & 1 \\ -2 & -2 & 2
		      \end{pmatrix} \begin{pmatrix}
			      x_1 \\ x_2 \\ x_3
		      \end{pmatrix} = \vb 0 \implies \vb x = \begin{pmatrix}
			      -x_2 + x_3 \\ x_2 \\ x_3
		      \end{pmatrix}
	      \]
	      So
	      \[
		      E_{-2} = \left\{ \alpha\begin{pmatrix}
			      -1 \\ 1 \\ 0
		      \end{pmatrix} + \beta\begin{pmatrix}
			      1 \\ 0 \\ 1
		      \end{pmatrix} \right\}
	      \]
	      Further, \(m_{-2} = 2 < 3 = M_{-2}\).

	\item A reflection in a plane through the origin with unit normal \(\nhat\) satisfies
	      \[
		      H\nhat = -\nhat;\quad \forall \vb u \perp \nhat, H \vb u = \vb u
	      \]
	      The eigenvalues are therefore \(\pm 1\) and \(E_{-1} = \{ \alpha \nhat \}\), and \(E_1 = \{ \vb x: \vb x \cdot \nhat = 0 \}\).
	      The multiplicities are given by \(M_{-1} = m_{-1} = 1, M_1 = m_1 = 2\).

	\item A rotation about an axis \(\nhat\) through angle \(\theta\) in \(\mathbb R^3\) satisfies
	      \[
		      R\nhat = \nhat
	      \]
	      So the axis of rotation is the eigenvector with eigenvalue 1.
	      There are no other real eigenvalues unless \(\theta = n\pi\).
	      The rotation restricted to the plane perpendicular to \(\nhat\) has eigenvalues \(e^{\pm i \theta}\) as shown above.
\end{enumerate}

\subsection{Linear independence of eigenvectors}
\begin{proposition}
	Let \(\vb v_1, \vb v_2, \dots, \vb v_r\) be eigenvectors of an \(n\times n\) matrix \(A\) with eigenvalues \(\lambda_1, \lambda_2,\dots,\lambda_r\).
	If the eigenvalues are distinct, then the eigenvectors are linearly independent.
\end{proposition}
\begin{proof}
	Note that if we take some linear combination \(\vb w = \sum_{j=1}^r \alpha_j\vb v_j\), then \((A - \lambda I)\vb w = \sum_{j=1}^r \alpha_j(\lambda_j - \lambda)\vb v_j\).
	Here are two methods for getting this proof.
	\begin{enumerate}
		\item Suppose the eigenvectors are linearly dependent, so there exist linear combinations \(\vb w = \vb 0\) where some \(\alpha\) are nonzero.
		      Let \(p\) be the amount of nonzero \(\alpha\) values.
		      So, \(2 \leq p \leq r\).
		      Now, pick such a \(\vb w\) for which \(p\) is least.
		      Without loss of generality, let \(\alpha_1\) be one of the nonzero coefficients.
		      Then
		      \[
			      (A - \lambda_1 I)\vb w = \sum_{j=2}^r \alpha_j(\lambda_j - \lambda_1)\vb v_j = \vb 0
		      \]
		      This is a linear relation with \(p-1\) nonzero coefficients \contradiction.
		\item Alternatively, given a linear relation \(\vb w=\vb 0\),
		      \[
			      \prod_{j \neq k} (A - \lambda_j I) \vb w = \alpha_k \prod_{j \neq k} (\lambda_k - \lambda_j) \vb v_k = \vb 0
		      \]
		      for some fixed \(k\).
		      So \(\alpha_k = 0\).
		      So the eigenvectors are linearly independent as claimed.
	\end{enumerate}
\end{proof}
\begin{corollary}
	With conditions as in the proposition above, let \(\mathcal B_{\lambda_i}\) be a basis for the eigenspace \(E_{\lambda_i}\).
	Then \(\mathcal B = \mathcal B_{\lambda_1} \cup \mathcal B_{\lambda_2} \cup \dots \cup \mathcal B_{\lambda_r}\) is linearly independent.
\end{corollary}
\begin{proof}
	Consider a general linear combination of all these vectors, it has the form
	\[
		\vb w = \vb w_1 + \vb w_2 + \dots + \vb w_r
	\]
	where each \(\vb w_i \in E_i\).
	Applying the same arguments as in the proposition, we find that
	\[
		\vb w = 0 \implies \forall i\,\vb w_i = 0
	\]
	So each \(\vb w_i\) is the trivial linear combination of elements of \(\mathcal B_{\lambda_i}\) and the result follows.
\end{proof}
