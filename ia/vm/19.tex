\subsection{Real Eigenvalues and Orthogonal Eigenvectors}
Recall that an \(n\times n\) matrix \(A\) is hermitian if and only if \(A^\dagger = \overline{A}^\transpose = A\), or \(\overline{A_{ij}} = A_{ji}\).
If \(A\) is real, then it is hermitian if and only if it is symmetric.
The complex inner product for \(\vb v, \vb w \in \mathbb C^n\) is \(\vb v^\dagger \vb w = \sum_i \overline{v_i}w_i\), and for \(\vb v, \vb w \in \mathbb R^n\), this reduces to the dot product in \(\mathbb R^n\), \(\vb v^\transpose \vb w\).

Here is a key observation.
If \(A\) is hermitian, then
\[
	(A\vb v)^\dagger \vb w = \vb v^\dagger (A \vb w)
\]
\begin{theorem}
	For an \(n \times n\) matrix \(A\) that is hermitian:
	\begin{enumerate}[(i)]
		\item Every eigenvalue \(\lambda\) is real;
		\item Eigenvectors \(\vb v, \vb w\) with different eigenvalues \(\lambda, \mu\) respectively, are orthogonal, i.e.\ \(\vb v^\dagger \vb w = 0\); and
		\item If \(A\) is real and symmetric, then for each eigenvalue \(\lambda\) we can choose a real eigenvector, and part (ii) becomes \(\vb v \cdot \vb w = 0\).
	\end{enumerate}
\end{theorem}
\begin{proof}
	\begin{enumerate}[(i)]
		\item Using the observation above with \(\vb v = \vb w\) where \(\vb v\) is any eigenvector with eigenvalue \(\lambda\), we get
		      \begin{align*}
			      \vb v^\dagger (A\vb v)        & = (A\vb v)^\dagger \vb v                   \\
			      \vb v^\dagger (\lambda\vb v)  & = (\lambda\vb v)^\dagger \vb v             \\
			      \lambda \vb v^\dagger (\vb v) & = \overline{\lambda} (\vb v)^\dagger \vb v \\
			      \intertext{As \(\vb v\) is an eigenvector, it is nonzero, so \(\vb v^\dagger \vb v \neq 0\), so}
			      \lambda                       & = \overline \lambda
		      \end{align*}
		\item Using the same observation,
		      \begin{align*}
			      \vb v^\dagger (A \vb w)   & = (A \vb v)^\dagger \vb w       \\
			      \vb v^\dagger (\mu \vb w) & = (\lambda \vb v)^\dagger \vb w \\
			      \mu \vb v^\dagger \vb w   & = \lambda \bm v^\dagger \vb w
		      \end{align*}
		      Since \(\lambda \neq \mu\), \(\vb v^\dagger \vb w = 0\), so the eigenvectors are orthogonal.
		\item Given \(A\vb v = \lambda \vb v\) with \(\vb v \in \mathbb C^n\) but \(A\) is real, let
		      \[
			      \vb v = \vb u + i\vb u';\quad \vb u, \vb u' \in \mathbb R^n
		      \]
		      Since \(\vb v\) is an eigenvector, and this is a linear equation, we have
		      \[
			      A\vb u = \lambda \vb u;\quad A\vb u' = \lambda \vb u'
		      \]
		      So \(\vb u\) and \(\vb u'\) are eigenvectors.
		      \(\vb v \neq 0\) implies that at least one of \(\vb u\) and \(\vb u'\) are nonzero, so there is at least one real eigenvector with this eigenvalue.
	\end{enumerate}
\end{proof}
Case (ii) is a stronger claim for hermitian matrices than just showing that eigenvectors are linearly independent.
Furthermore, previously we considered bases \(\mathcal B_\lambda\) for each eigenspace \(E_\lambda\), and it is now natural to choose bases \(\mathcal B_\lambda\) to be orthonormal when we are considering hermitian matrices.
Here are some examples.
\begin{enumerate}[(i)]
	\item Let
	      \[
		      A = \begin{pmatrix}
			      2 & i \\ -i & 2
		      \end{pmatrix};\quad A^\dagger = A;\quad \lambda = 1, 3;\quad\vb u_1 = \frac{1}{\sqrt{2}} \begin{pmatrix}
			      1 \\i
		      \end{pmatrix};\quad\vb u_2 = \frac{1}{\sqrt{2}} \begin{pmatrix}
			      1 \\-i
		      \end{pmatrix}
	      \]
	      We have chosen coefficients for the vectors \(\vb u_1\) and \(\vb u_2\) such that they are unit vectors.
	      As shown above, they are then orthonormal.
	      We know that having distinct eigenvalues means that a matrix is diagonalisable.
	      So let us set
	      \[
		      P =  \frac{1}{\sqrt{2}} \begin{pmatrix}
			      1 & 1 \\ i & -i
		      \end{pmatrix} \implies P^{-1}AP = D = \begin{pmatrix}
			      1 & 0 \\ 0 & 3
		      \end{pmatrix}
	      \]
	      Since the eigenvectors are orthonormal, so are the columns of \(P\), so \(P^{-1} = P^\dagger\) (i.e.\ \(P\) is unitary).
	\item Let
	      \[
		      A = \begin{pmatrix}
			      0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0
		      \end{pmatrix}
	      \]
	      \(A\) is real and symmetric, with eigenvalues \(\lambda = -1, 2\) with \(M_{-1} = 2\), \(M_2 = 1\).
	      Further,
	      \[
		      E_{-1} = \vecspan \{ \vb w_1, \vb w_2 \};\quad \vb w_1 = \begin{pmatrix}
			      1 \\ -1 \\ 0
		      \end{pmatrix};\quad \vb w_2 = \begin{pmatrix}
			      1 \\ 0 \\ -1
		      \end{pmatrix}
	      \]
	      So \(m_{-1} = 2\), and the matrix is diagonalisable.
	      Let us choose an orthonormal basis for \(E_{-1}\) by taking
	      \[
		      \vb u_1 = \frac{1}{\abs{\vb w_1}}\vb w_1 = \frac{1}{\sqrt 2}\begin{pmatrix}
			      1 \\ -1 \\ 0
		      \end{pmatrix}
	      \]
	      and we can consider
	      \[
		      \vb w_2' = \vb w_2 - (\vb u_1 \cdot \vb w_2)\vb u_1 = \begin{pmatrix}
			      1/2 \\ 1/2 \\ -1
		      \end{pmatrix}
	      \]
	      so that \(\vb w_2'\) is orthogonal to \(\vb u_1\) by construction.
	      We can then normalise this vector to get
	      \[
		      \vb u_2 = \frac{1}{\abs{\vb w_2'}}\vb w_2' = \frac{1}{\sqrt 6} \begin{pmatrix}
			      1 \\ 1 \\ -2
		      \end{pmatrix}
	      \]
	      and therefore
	      \[
		      \mathcal B_{-1} = \{ \vb u_1, \vb u_2 \}
	      \]
	      is an orthonormal basis.
	      For \(E_2\), let us choose \(\mathcal B_2 = \{ \vb u_3 \}\) where
	      \[
		      \vb u_3 = \frac{1}{\sqrt 3}\begin{pmatrix}
			      1 \\ 1 \\ 1
		      \end{pmatrix}
	      \]
	      Together,
	      \[
		      \mathcal B = \left\{ \frac{1}{\sqrt 2}\begin{pmatrix}
			      1 \\ -1 \\ 0
		      \end{pmatrix}, \frac{1}{\sqrt 6} \begin{pmatrix}
			      1 \\ 1 \\ -2
		      \end{pmatrix}, \frac{1}{\sqrt 3}\begin{pmatrix}
			      1 \\ 1 \\ 1
		      \end{pmatrix} \right\}
	      \]
	      is an orthonormal basis for \(\mathbb R^3\).
	      Let \(P\) be the matrix with columns \(\vb u_1, \vb u_2, \vb u_3\), then \(P^{-1}AP = D\) as required.
	      Since we have chosen an orthonormal basis, \(P\) is orthogonal, so \(P^\transpose AP = D\).
\end{enumerate}

\subsection{Unitary and Orthogonal Diagonalisation}
\begin{theorem}
	Any \(n\times n\) hermitian matrix \(A\) is diagonalisable.
	\begin{enumerate}[(i)]
		\item There exists a basis of eigenvectors \(\vb u_1, \dots, \vb u_n \in \mathbb C^n\) with \(A\vb u_i = \lambda \vb u_i\); equivalently
		\item There exists an \(n \times n\) invertible matrix \(P\) with \(P^{-1}AP = D\) where \(D\) is the matrix with eigenvalues on the diagonal, where the columns of \(P\) are the eigenvectors \(\vb u_i\).
	\end{enumerate}
	In addition, the eigenvectors \(\vb u_i\) can be chosen to be orthonormal, so
	\[
		\vb u^\dagger_i \vb u_j = \delta_{ij}
	\]
	or equivalently, the matrix \(P\) can be chosen to be unitary,
	\[
		P^\dagger = P^{-1} \implies P^\dagger AP = D
	\]
	In the special case that the matrix \(A\) is real, the eigenvectors can be chosen to be real, and so
	\[
		\vb u^\transpose \vb u_j = \vb u_i \cdot \vb u_j = \delta_{ij}
	\]
	so \(P\) is orthogonal, so
	\[
		P^\transpose = P^{-1} \implies P^\transpose AP = D
	\]
\end{theorem}
