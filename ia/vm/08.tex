\subsection{Choosing Bases}
Note that $\{ \vb 0 \}$ is a trivial subspace of all vector spaces, and it has dimension zero since it requires a linear combination of no vectors.

\begin{proposition}
	Let $V$ be a vector space with finite subsets $Y = \{ \vb w_1, \cdots, \vb w_m \}$ that spans $V$, and $X = \{ \vb u_1, \cdots, \vb u_k \}$ that is linearly independent. Let $n = \dim V$. Then:
	\begin{enumerate}[(i)]
		\item A basis can be found as a subset of $Y$ by discarding vectors in $Y$ as necessary, and that $n \leq m$.
		\item $X$ can be extended to a basis by adding in additional vectors from $Y$ as necessary, and that $k \leq n$.
	\end{enumerate}
\end{proposition}
\begin{proof}
	This proof is non-examinable (without prompts).
	\begin{enumerate}[(i)]
		\item If $Y$ is linearly independent, then $Y$ is a basis and $m = n$. Otherwise, $Y$ is not linearly independent. So there exists some linear relation
		      \[ \sum_{i=1}^{m} \lambda_i \vb w_i = \vb 0 \]
		      where there is some $i$ such that $\lambda_i \neq 0$. Without loss of generality (because the order of elements in $Y$ does not matter) we will reorder $Y$ such that $\vb w_m \neq 0$. So we have
		      \[ \vb w_m = \frac{-1}{\lambda_m} \sum_{i=1}^{m-1} \lambda_i \vb w_i \]
		      So $\vecspan Y = \vecspan (Y \setminus \{ \vb w_m \})$. We can repeat this process of eliminating vectors from $Y$ until linear independence is achieved. We know that this process will end because $Y$ is a finite set. Clearly, in this case, $n < m$. So for all cases, $n \leq m$.

		\item If $X$ spans $V$, then $X$ is a basis and $k=n$. Else, there exists some $u_{k+1} \in V$ that is not in the span of $X$. Then, we will construct an arbitrary linear relation
		      \[ \sum_{i=1}^{k+1} \mu_i \vb u_i = \vb 0 \]
		      Note that this implies that $\mu_{k+1} = \vb 0$ because it is not in the span of $X$, and that $\mu_i = 0$ for all $i \leq k$ because the original $X$ was linearly independent. So we know that all the coefficients are zero, and therefore $X \cup \{ u_{k+1} \}$ is linearly independent.

		      Note that we can always choose this $u_{k+1}$ to be an element of $Y$ because we just need to ensure that $u_{k+1} \notin \vecspan X$. Suppose we cannot choose such a vector in $Y$. Then $Y \subseteq \vecspan X \implies \vecspan Y \subseteq \vecspan X \implies \vecspan X = V$, which is clearly false because $X$ does not span $V$. This is a contradiction, so we can always choose such a vector from $Y$. We can repeat this process of taking vectors from $Y$ and adding them to $X$ until we have a basis. This process will always terminate in a finite amount of steps because we are taking new vectors from a finite set $Y$. Therefore $k \leq n$, as we are adding vectors (increasing $k$) until $k=n$.
	\end{enumerate}
\end{proof}

\subsection{Infinite Dimensions}
It is perfectly possible to have a vector space that has infinite dimensionality. However, they will be rarely touched upon in this course apart from specific examples, like the following example. Let $V = \{ f: [0, 1] \to \mathbb R: f \text{ smooth}, f(0) = f(1) = 0\}$. Then let $S_n(x) = \sqrt 2 \sin(n \pi x)$ where $n$ is a natural number $1, 2, \cdots$. Clearly, $S_n \in V$ for all $n$. The inner product of two of these $S$ functions is given by
\begin{align*}
	\langle S_n, S_m \rangle & = 2 \int_0^1 \sin(n \pi x) \sin(m \pi x) \dd{x} \\
	                         & = \delta_{mn}
\end{align*}
So $S_n$ are orthonormal and therefore linearly independent. So we can continue adding more vectors until it becomes a basis. However, the set of all $S_n$ is already infinite --- so $V$ must have infinite dimensionality.

\subsection{Vectors in $\mathbb C^n$}
We define $\mathbb C^n$ by
\[ \mathbb C^n := \{ \vb z = (z_1, z_2, \cdots, z_n): \forall i, z_i \in \mathbb C \} \]
We define addition and scalar multiplication in obvious ways. Note that we have a choice over what the scalars are allowed to be. If we only allow scalars that are real numbers, $\mathbb C^n$ can be considered a real vector space with bases $(0, \cdots, 1, \cdots, 0)$ and $(0, \cdots, i, \cdots, 0)$ and dimension $2n$. Alternatively, if we let the scalars be any complex numbers, we don't need to have imaginary bases, thus giving us a complex vector space with bases $(0, \cdots, 1, \cdots, 0)$ and dimension $n$. We can say that $\mathbb C^n$ has dimension $2n$ over $\mathbb R$, and dimension $n$ over $\mathbb C$. From here on, unless stated otherwise, we treat $\mathbb C^n$ to be a complex vector space.

\subsection{Inner Product in $\mathbb C^n$}
We can define the inner product by
\[ \langle \vb z, \vb w \rangle := \sum_j \overline{z_j} w_j \]
The conjugate over the $z$ terms ensures that the inner product is positive definite. It has these properties, analogous to the properties of the inner product in the real vector space $\mathbb R^n$:
\begin{itemize}
	\item (Hermitian) $\langle \vb z, \vb w \rangle = \overline{\langle \vb w, \vb z \rangle}$
	\item (linear/antilinear) $\langle \vb z, \lambda \vb w + \lambda' \vb w' \rangle = \lambda \langle \vb z, \vb w \rangle + \lambda' \langle \vb z, \vb w' \rangle$ and $\langle \lambda \vb z + \lambda' \vb z', w \rangle = \overline{\lambda} \langle \vb z, \vb w \rangle + \overline{\lambda'} \langle \vb z', \vb w \rangle$
	\item (positive definite) $\langle \vb z, \vb z \rangle = \sum_j \abs{z_j}^2$ which is real and greater than or equal to zero, where the equality holds if and only if $\vb z = \vb 0$.
\end{itemize}
We can also define the norm of $\vb z$ to satisfy $\abs{\vb z} \geq 0$ and $\abs{\vb z}^2 = \langle \vb z, \vb z \rangle$. Note that the standard basis for $\mathbb C^n$ is orthonormal, since the inner product of any two basis vectors $\vb e_j$ and $\vb e_k$ is given by $\delta_{jk}$.

\subsection{Inner Product in Complex Plane}
Here is an example of the use of the complex inner product on $\mathbb C^1 = \mathbb C$. Note first that $\langle z, w \rangle = \overline z w$. Let $z = a_1 + ia_2$ and $w = b_1 + ib_2$ where $a_1, a_2, b_1, b_2 \in \mathbb R$. Then
\begin{align*}
	\langle z, w \rangle & = \overline z w                              \\
	                     & = (a_1 b_1 + a_2 b_2) + i(a_1 b_2 - a_2 b_1) \\
	                     & = (z \cdot w) + i[z, w]
\end{align*}
We can therefore use the inner product to compute two different scalar products at the same time.
