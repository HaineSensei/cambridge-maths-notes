\subsection{Multidimensional real space}
We define multidimensional real space as follows:
\[
	\mathbb R^n = \{ \vb x = (x_1, x_2, \cdots, x_n) : x_i \in \mathbb R \}
\]
We can define addition and scalar multiplication by mapping these operations over each term in the tuple.
Therefore, we have a notion of linear combinations of vectors and hence a concept of parallel vectors.
We can say, like before in \(\mathbb R^3\), that \(\vb x \parallel \vb y\) if and only if \(\vb x = \lambda \vb y\) or \(\vb y = \lambda \vb x\).

We define an operator analogous to the scalar product in \(\mathbb R^3\).
The inner product is defined as \(x \cdot y = x_i y_i\).
Directly from this definition, we can deduce some properties:
\begin{itemize}
	\item (symmetric) \(\vb x \cdot \vb y = \vb y \cdot \vb x\)
	\item (bilinear) \((\lambda \vb x + \lambda'\vb x')\cdot \vb y = \lambda \vb x\cdot \vb y + \lambda' \vb x' \cdot \vb y\)
	\item (positive definite) \(\vb x \cdot \vb x \geq 0\), and the equality holds if and only if \(\vb x = \vb 0\).
\end{itemize}

We can define the norm of a vector (similar to the concept of length in three-dimension space), denoted \(\abs {\vb x}\), by \(\abs{\vb x}^2 = \vb x \cdot \vb x\).
We can now define orthogonality as follows: \(\vb x \perp \vb y \iff \vb x \cdot \vb y = 0\).

We define the standard basis vectors \(\vb e_1, \vb e_2, \dots, \vb e_n\) by setting each element of the tuple \(\vb e_i\) to zero apart from the \(i\)th element, which is set to one.
Also, we redefine the Kronecker \(\delta\) to be valid in higher-dimensional space.
Note that under this definition, the standard basis vectors are orthonormal because \(\vb e_i \cdot \vb e_j = \delta_{ij}\).

\subsection{Cauchy--Schwarz inequality}
\begin{proposition}
	For vectors \(\vb x, \vb y\) in \(\mathbb R^n\), \(\abs{\vb x \cdot \vb y} \leq \abs{\vb x} \abs{\vb y}\), where the equality is true if and only if \(\vb x \parallel \vb y\).
\end{proposition}
\begin{proof}
	If \(\vb y = \vb 0\), then the result is immediate.
	So suppose that \(\vb y \neq 0\), then for some \(\lambda \in \mathbb R\), we have
	\begin{align*}
		\abs{\vb x - \lambda \vb y}^2 & =
		(\vb x - \lambda \vb y) \cdot (\vb x - \lambda \vb y)                                                          \\
		                              & = \abs{\vb x}^2 - 2 \lambda \vb x \cdot \vb y + \lambda^2 \abs{\vb y}^2 \geq 0
	\end{align*}
	As this is a positive real quadratic in \(\lambda\) that is always greater than zero, it has at most one real root.
	Therefore the discriminant is less than or equal to zero.
	\[
		(-2 \vb x \cdot \vb y)^2 - 4 \abs{\vb x}^2\abs{\vb y}^2 \leq 0
		\implies \abs{\vb x \cdot \vb y} \leq \abs{\vb x}\abs{\vb y}
	\]
	where the equality only holds if \(\vb x\) and \(\vb y\) are parallel (i.e.\ when \(\vb x - \lambda \vb y\) equals zero for some \(\lambda\)).
\end{proof}

\subsection{Triangle inequality}
Following from the Cauchy--Schwarz inequality,
\begin{align*}
	\abs{\vb x + \vb y}^2
	 & = \abs{\vb x}^2 + 2(\vb x \cdot \vb y) + \abs{\vb y}^2        \\
	 & \leq \abs{\vb x}^2 + 2 \abs{\vb x}\abs{\vb y} + \abs{\vb y}^2 \\
	 & = \left(\abs{\vb x} + \abs{\vb y}\right)^2
\end{align*}
where the equality holds under the same conditions as above.

\subsection{Levi-Civita \texorpdfstring{\( \varepsilon \)}{ùúÄ} in higher dimensions}
Note that the Levi-Civita \(\varepsilon\) has three indices in \(\mathbb R^3\).
We can extend this \(\varepsilon\) to higher and lower dimensions by increasing or reducing the amount of indices.
It does not make logical sense to use the same \(\varepsilon\) without changing the amount of indices to define, for example, a vector product in four-dimensional space, since we would have unused indices.
The expression \((\vb x \times \vb y)_k = \varepsilon_{ijk} \vb a_i \vb b_j\) works because there is one free index, \(k\), on the right hand side, so we can use this to calculate the values of each element of the result.

We can, however, use this \(\varepsilon\) to extend the notion of a scalar triple product to other dimensions, for example two-dimensional space, with \([\vb a, \vb b] \coloneq \varepsilon_{ij} \vb a_i \vb b_j\).
This is the signed area of the parallelogram spanning \(\vb a\) and \(\vb b\).

\subsection{General real vector spaces}
Vector spaces are not studied axiomatically in this course, but the axioms are given here for completeness.
A real (as in, \(\mathbb R\)) vector space \(V\) is a set of objects with two operators \(+: V \times V \to V\) and \(\cdot: \mathbb R \times V \to V\) such that
\begin{itemize}
	\item \((V, +)\) is an abelian group
	\item \(\lambda(v + w) = \lambda v + \lambda w\)
	\item \((\lambda + \mu)v = \lambda v + \mu v\)
	\item \(\lambda(\mu v) = (\lambda \mu) v\)
	\item \(1v = v\) (to exclude trivial cases for example \(\lambda v = 0\) for all \(v\))
\end{itemize}

A subspace of a real vector space \(V\) is a subset \(U \subseteq V\) that is a vector space.
Equivalently, if all pairs of vectors \(v, w \in U\) satisfy \(\lambda v + \mu w \in U\), then \(U\) is a subspace of \(V\).
Note that the span generated from a set of vectors is a subspace, as it is characterised by this equivalent definition.
Also, note that the origin must be part of any subspace, because multiplying a vector by zero must yield the origin.

In some real vector space \(V\), let \(\vb v_1, \vb v_2 \cdots \vb v_r\) be vectors in \(V\).
Now consider the linear relation
\[
	\lambda_1 \vb v_1 + \lambda_2 \vb v_2 + \cdots + \lambda_r \vb v_r = 0
\]
Then we call the set of vectors a linearly independent set if the only solution is where all \(\lambda\) values are zero.
Otherwise, it is a linearly dependent set.

\subsection{Inner product spaces}
An inner product is an extra structure that we can have on a real vector space \(V\), which is often denoted by angle brackets or parentheses.
It can also be characterised by axioms (specifically the ones in Section 6.2).
Features like the norm of a vector, and theorems like the Cauchy--Schwarz inequality, follow from these axioms.

For example, let us consider the vector space
\[
	V = \{ f: [0, 1] \to \mathbb R: f \text{ smooth}; f(0) = f(1) = 0 \}
\]
We can define the inner product to be
\[
	f \cdot g = \langle f, g \rangle = \int_0^1 f(x)g(x)\dd{x}
\]
Then by the Cauchy--Schwarz inequality, we have
\begin{align*}
	\abs{\langle f, g \rangle}               & \leq \norm{f} \cdot \norm{g}                                    \\
	\therefore\ \abs{\int_0^1 f(x)g(x)\dd{x}} & \leq \sqrt{\int_0^1 f(x)^2 \dd{x}}\sqrt{\int_0^1 g(x)^2 \dd{x}}
\end{align*}

\begin{lemma}
	In any real inner product space \(V\), if \(\vb v_1 \cdots v_r \neq \vb 0\) are orthogonal, they are linearly independent.
\end{lemma}
\begin{proof}
	If \(\sum_i \alpha_i \vb v_i = 0\), then
	\begin{align*}
		\left\langle \vb v_j, \sum_i \alpha_i \vb v_i \right\rangle     & = 0 \\
		\intertext{And because each vector that is not \(\vb v_j\) is orthogonal to it, those terms cancel, leaving}
		\therefore\ \left\langle \vb v_j, \alpha_j \vb v_j \right\rangle & = 0 \\
		\alpha_j \left\langle \vb v_j, \vb v_j \right\rangle            & = 0 \\
		\alpha_j = 0
	\end{align*}
	So they are linearly independent.
\end{proof}

\subsection{Bases and dimensions}
In a vector space \(V\), a basis is a set \(\mathcal B = \{ \vb e_1 \cdots \vb e_n \}\) such that
\begin{itemize}
	\item \(\mathcal B\) spans \(V\); and
	\item \(\mathcal B\) is linearly independent, which implies that the coefficients on these basis vectors are unique for any vector in \(V\), since it is impossible to write one vector in terms of the others
\end{itemize}

\begin{theorem}
	If \(\{\vb e_1 \cdots \vb e_n \}\) and \(\{ \vb f_1 \cdots \vb f_m \}\) are bases for a real vector space \(V\), then \(n=m\), which we call the dimension of \(V\).
\end{theorem}
\begin{proof}
	This proof is non-examinable (without prompts).
	We can write each basis vector in terms of the others, since they all span the same vector space.
	Thus:
	\[
		\vb f_a = \sum_i A_{ai} \vb e_i;\quad \vb e_i = \sum_a B_{ia} \vb f_a
	\]
	Note that indices \(i,j\) span from 1 to \(n\), while \(a,b\) span from 1 to \(m\).
	We can substitute one expression into the other, forming:
	\begin{align*}
		\vb f_a & = \sum_i A_{ai} \left( \sum_b B_{ib}\vb f_b \right)  \\
		\vb f_a & = \sum_b \left( \sum_i A_{ai} B_{ib} \right) \vb f_b
	\end{align*}
	Note that we have now written \(\vb f_a\) as a linear combination of \(\vb f_b\) for all valid \(b\).
	But since they are linearly independent, the coefficient of \(\vb f_b\) must be zero if \(a \neq b\), and one of \(a = b\).
	Therefore, we have
	\[
		\delta_{ab} = \sum_i A_{ai} B_{ib}
	\]
	We can make a similar statement about \(\vb e_i\):
	\[
		\delta_{ij} = \sum_a B_{ia} A_{aj} = \sum_a A_{aj} B_{ia}
	\]
	Now, assigning \(a=b\) and \(i=j\), summing over both, and substituting into our two previous expressions for \(\delta\), we have:
	\begin{alignat*}{2}
		\sum_{ia} A_{ai} B_{ia} & = \sum_a \delta_{aa} &  & = \sum_i \delta_{ii} \\
		                        & = m                  &  & = n
	\end{alignat*}
\end{proof}

Note that \(\{ \vb 0 \}\) is a trivial subspace of all vector spaces, and it has dimension zero since it requires a linear combination of no vectors.

\begin{proposition}
	Let \(V\) be a vector space with finite subsets \(Y = \{ \vb w_1, \cdots, \vb w_m \}\) that spans \(V\), and \(X = \{ \vb u_1, \cdots, \vb u_k \}\) that is linearly independent.
	Let \(n = \dim V\).
	Then:
	\begin{enumerate}
		\item A basis can be found as a subset of \(Y\) by discarding vectors in \(Y\) as necessary, and that \(n \leq m\).
		\item \(X\) can be extended to a basis by adding in additional vectors from \(Y\) as necessary, and that \(k \leq n\).
	\end{enumerate}
\end{proposition}
\begin{proof}
	This proof is non-examinable (without prompts).
	\begin{enumerate}
		\item If \(Y\) is linearly independent, then \(Y\) is a basis and \(m = n\).
		      Otherwise, \(Y\) is not linearly independent.
		      So there exists some linear relation
		      \[
			      \sum_{i=1}^{m} \lambda_i \vb w_i = \vb 0
		      \]
		      where there is some \(i\) such that \(\lambda_i \neq 0\).
		      Without loss of generality (because the order of elements in \(Y\) does not matter) we will reorder \(Y\) such that \(\vb w_m \neq 0\).
		      So we have
		      \[
			      \vb w_m = \frac{-1}{\lambda_m} \sum_{i=1}^{m-1} \lambda_i \vb w_i
		      \]
		      So \(\vecspan Y = \vecspan (Y \setminus \{ \vb w_m \})\).
		      We can repeat this process of eliminating vectors from \(Y\) until linear independence is achieved.
		      We know that this process will end because \(Y\) is a finite set.
		      Clearly, in this case, \(n < m\).
		      So for all cases, \(n \leq m\).

		\item If \(X\) spans \(V\), then \(X\) is a basis and \(k=n\).
		      Else, there exists some \(u_{k+1} \in V\) that is not in the span of \(X\).
		      Then, we will construct an arbitrary linear relation
		      \[
			      \sum_{i=1}^{k+1} \mu_i \vb u_i = \vb 0
		      \]
		      Note that this implies that \(\mu_{k+1} = \vb 0\) because it is not in the span of \(X\), and that \(\mu_i = 0\) for all \(i \leq k\) because the original \(X\) was linearly independent.
		      So we know that all the coefficients are zero, and therefore \(X \cup \{ u_{k+1} \}\) is linearly independent.

		      Note that we can always choose this \(u_{k+1}\) to be an element of \(Y\) because we just need to ensure that \(u_{k+1} \notin \vecspan X\).
		      Suppose we cannot choose such a vector in \(Y\).
		      Then \(Y \subseteq \vecspan X \implies \vecspan Y \subseteq \vecspan X \implies \vecspan X = V\), which is clearly false because \(X\) does not span \(V\).
		      This is a contradiction, so we can always choose such a vector from \(Y\).
		      We can repeat this process of taking vectors from \(Y\) and adding them to \(X\) until we have a basis.
		      This process will always terminate in a finite amount of steps because we are taking new vectors from a finite set \(Y\).
		      Therefore \(k \leq n\), as we are adding vectors (increasing \(k\)) until \(k=n\).
	\end{enumerate}
\end{proof}

It is perfectly possible to have a vector space that has infinite dimensionality.
However, they will be rarely touched upon in this course apart from specific examples, like the following example.
Let \(V = \{ f: [0, 1] \to \mathbb R: f \text{ smooth}, f(0) = f(1) = 0\}\).
Then let \(S_n(x) = \sqrt 2 \sin(n \pi x)\) where \(n\) is a natural number \(1, 2, \cdots\).
Clearly, \(S_n \in V\) for all \(n\).
The inner product of two of these \(S\) functions is given by
\begin{align*}
	\langle S_n, S_m \rangle & = 2 \int_0^1 \sin(n \pi x) \sin(m \pi x) \dd{x} \\
	                         & = \delta_{mn}
\end{align*}
So \(S_n\) are orthonormal and therefore linearly independent.
So we can continue adding more vectors until it becomes a basis.
However, the set of all \(S_n\) is already infinite---so \(V\) must have infinite dimensionality.

\subsection{Multidimensional complex space}
We define \(\mathbb C^n\) by
\[
	\mathbb C^n \coloneq \{ \vb z = (z_1, z_2, \cdots, z_n): \forall i, z_i \in \mathbb C \}
\]
We define addition and scalar multiplication in obvious ways.
Note that we have a choice over what the scalars are allowed to be.
If we only allow scalars that are real numbers, \(\mathbb C^n\) can be considered a real vector space with bases \((0, \cdots, 1, \cdots, 0)\) and \((0, \cdots, i, \cdots, 0)\) and dimension \(2n\).
Alternatively, if we let the scalars be any complex numbers, we don't need to have imaginary bases, thus giving us a complex vector space with bases \((0, \cdots, 1, \cdots, 0)\) and dimension \(n\).
We can say that \(\mathbb C^n\) has dimension \(2n\) over \(\mathbb R\), and dimension \(n\) over \(\mathbb C\).
From here on, unless stated otherwise, we treat \(\mathbb C^n\) to be a complex vector space.

We can define the inner product by
\[
	\langle \vb z, \vb w \rangle \coloneq \sum_j \overline{z_j} w_j
\]
The conjugate over the \(z\) terms ensures that the inner product is positive definite.
It has these properties, analogous to the properties of the inner product in the real vector space \(\mathbb R^n\):
\begin{itemize}
	\item (Hermitian) \(\langle \vb z, \vb w \rangle = \overline{\langle \vb w, \vb z \rangle}\)
	\item (linear/antilinear) \(\langle \vb z, \lambda \vb w + \lambda' \vb w' \rangle = \lambda \langle \vb z, \vb w \rangle + \lambda' \langle \vb z, \vb w' \rangle\) and \(\langle \lambda \vb z + \lambda' \vb z', w \rangle = \overline{\lambda} \langle \vb z, \vb w \rangle + \overline{\lambda'} \langle \vb z', \vb w \rangle\)
	\item (positive definite) \(\langle \vb z, \vb z \rangle = \sum_j \abs{z_j}^2\) which is real and greater than or equal to zero, where the equality holds if and only if \(\vb z = \vb 0\).
\end{itemize}
We can also define the norm of \(\vb z\) to satisfy \(\abs{\vb z} \geq 0\) and \(\abs{\vb z}^2 = \langle \vb z, \vb z \rangle\).
Note that the standard basis for \(\mathbb C^n\) is orthonormal, since the inner product of any two basis vectors \(\vb e_j\) and \(\vb e_k\) is given by \(\delta_{jk}\).

Here is an example of the use of the complex inner product on \(\mathbb C^1 = \mathbb C\).
Note first that \(\langle z, w \rangle = \overline z w\).
Let \(z = a_1 + ia_2\) and \(w = b_1 + ib_2\) where \(a_1, a_2, b_1, b_2 \in \mathbb R\).
Then
\begin{align*}
	\langle z, w \rangle & = \overline z w                              \\
	                     & = (a_1 b_1 + a_2 b_2) + i(a_1 b_2 - a_2 b_1) \\
	                     & = (z \cdot w) + i[z, w]
\end{align*}
We can therefore use the inner product to compute two different scalar products at the same time.
