\subsection{Checking Properties}
To justify (ii) above, it suffices to check a transposition \(\tau = (p\ q)\) where (without loss of generality) \(p < q\), then since transpositions generate all permutations the result follows.
\begin{align*}
	 & [\vb v_1, \cdots, \vb v_{p-1}, \vb v_q, \vb v_{p+1}, \cdots, \vb v_{q-1}, \vb v_p, \vb v_{q+1}, \cdots, \vb v_n]                                                                                                                          \\
	 & = \sum_\sigma \varepsilon(\sigma) (\vb v_1)_{\sigma(1)} \cdots (\vb v_{p-1})_{\sigma(p-1)}(\vb v_q)_{\sigma(p)}(\vb v_{p+1})_{\sigma(p+1)} \cdots (\vb v_{q-1})_{\sigma(q-1)}(\vb v_p)_{\sigma(q)}(\vb v_{q+1})_{\sigma(q+1)}             \\
	 & = \sum_\sigma \varepsilon(\sigma) (\vb v_1)_{\sigma'(1)} \cdots (\vb v_{p-1})_{\sigma'(p-1)}(\vb v_q)_{\sigma'(q)}(\vb v_{p+1})_{\sigma'(p+1)} \cdots (\vb v_{q-1})_{\sigma'(q-1)}(\vb v_p)_{\sigma'(p)}(\vb v_{q+1})_{\sigma'(q+1)}      \\
	\intertext{where \(\sigma' = \sigma\tau\)}
	 & = -\sum_{\sigma'} \varepsilon(\sigma') (\vb v_1)_{\sigma'(1)} \cdots (\vb v_{p-1})_{\sigma'(p-1)}(\vb v_p)_{\sigma'(p)}(\vb v_{p+1})_{\sigma'(p+1)} \cdots (\vb v_{q-1})_{\sigma'(q-1)}(\vb v_q)_{\sigma'(q)}(\vb v_{q+1})_{\sigma'(q+1)} \\
	 & = -[\vb v_1, \cdots, \vb v_{p-1}, \vb v_p, \vb v_{p+1}, \cdots, \vb v_{q-1}, \vb v_q, \vb v_{q+1}, \cdots, \vb v_n]
\end{align*}
as required.

\begin{proposition}
	\([ \vb v_1, \vb v_2, \cdots, \vb v_n] \neq 0\) if and only if \(\vb v_1, \vb v_2, \cdots, \vb v_n\) are linearly independent.
\end{proposition}
\begin{proof}
	To show the forward implication, let us suppose that they are not linearly independent and use property (v). Then we can express some \(\vb v_p\) as a linear combination of the others. Then \([\vb v_1, \vb v_2, \cdots, \vb v_n] = 0\).

	To show the other direction, note that \(\vb v_1, \vb v_2, \cdots, \vb v_3\) means that they span, and if they span then each of the standard basis vectors \(\vb e_i\) can be written as a linear combination of the \(\vb v\) vectors, i.e. \(\vb e_i = U_{ai} \vb v_a\). Then
	\begin{align*}
		[\vb e_1, \vb e_2, \cdots, \vb e_n] & = [U_{a1}\vb v_a, U_{b2}\vb v_b, \cdots, U_{cn}\vb v_c]                                 \\
		                                    & = U_{a1}U_{b2}\cdots U_{cn}[\vb v_a, \vb v_b, \cdots, \vb v_c]                          \\
		                                    & = U_{a1}U_{b2}\cdots U_{cn} \varepsilon_{ab\cdots c}[\vb v_1, \vb v_2, \cdots, \vb v_n]
	\end{align*}
	By definition, the left hand side is \(+1\), so \([\vb v_1, \vb v_2, \cdots, \vb v_n]\) is nonzero.
\end{proof}
As an example of these ideas, let
\[ \vb v_1 = \begin{pmatrix} i \\ 0 \\ 0 \\ 2 \end{pmatrix};\quad\vb v_2 = \begin{pmatrix} 0 \\ 0 \\ 5i \\ 0 \end{pmatrix};\quad\vb v_3 = \begin{pmatrix} 3 \\ 2i \\ 0 \\ 0 \end{pmatrix};\quad\vb v_4 = \begin{pmatrix} 0 \\ 0 \\ i \\ 1 \end{pmatrix};\quad \text{where }\vb v_j \in \mathbb C_4 \]
Then
\begin{align*}
	[\vb v_1, \vb v_2, \vb v_3, \vb v_4]
	 & = 5i[\vb v_1, \vb e_3, \vb v_3, \vb v_4]                                      \\
	 & = 5i[i\vb e_1 + 2\vb e_4, \vb e_3, 3\vb e_1 + 2i\vb e_2, -i\vb e_3 + \vb e_4] \\
	\intertext{By multilinearity, we can eliminate all \(\vb e_3\) terms not in the second position because they will cancel with it, giving}
	 & = 5i[i\vb e_1 + 2\vb e_4, \vb e_3, 3\vb e_1 + 2i\vb e_2, \vb e_4]             \\
	\intertext{And likewise with \(\vb e_4\):}
	 & = 5i[i\vb e_1, \vb e_3, 3\vb e_1 + 2i\vb e_2, \vb e_4]                        \\
	\intertext{And again with \(\vb e_1\):}
	 & = 5i[i\vb e_1, \vb e_3, 2i\vb e_2, \vb e_4]                                   \\
	 & = 5i\cdot 2i \cdot i[\vb e_1, \vb e_3, \vb e_2, \vb e_4]                      \\
	 & = 10i[\vb e_1, \vb e_2, \vb e_3, \vb e_4]                                     \\
	 & = 10i
\end{align*}

\subsection{Defining the Determinant}
For an \(n \times n\) matrix \(M\) with columns \(\vb C_a = M\vb e_a\), then the determinant \(\det(M) = \abs{M} \in \mathbb R\) or \(\mathbb C\) is given by any of the following equivalent definitions.
\begin{align*}
	\det M
	 & = [\vb C_1, \vb C_2, \cdots, \vb C_n]                                                \\
	 & = [M\vb e_1, M\vb e_2, \cdots, M\vb e_n]                                             \\
	 & = \varepsilon_{ij\cdots l}M_{i1}M_{j2} \cdots M_{ln}                                 \\
	 & = \sum_\sigma \varepsilon(\sigma) M_{\sigma(1)1}M_{\sigma(2)2} \cdots M_{\sigma(n)n}
\end{align*}
Here are some examples.
\begin{enumerate}[(i)]
	\item \(n=2\)
	      \[ \det M = \sum_\sigma M_{\sigma(1)1}M_{\sigma(2)2} = \begin{vmatrix}
			      M_{11} & M_{21} \\ M_{12} & M_{22}
		      \end{vmatrix} = M_{11}M_{22} - M_{12}M_{21} \]
	\item \(M\) diagonal, i.e. \(M_{ij} = 0\) for \(i \neq j\)
	      \[ M = \begin{pmatrix}
			      M_{11} & 0      & \cdots & 0      \\
			      0      & M_{22} & \cdots & 0      \\
			      \vdots & \vdots & \ddots & \vdots \\
			      0      & 0      & \cdots & M_{nn}
		      \end{pmatrix} \implies \det M = M_{11}M_{22}\cdots M_{nn} \]
	\item Let \(M\) be \(n\times n\), \(A\) be \((n-1) \times (n-1)\), where
	      \[ M = \left( \begin{array}{c|c}
				      A & 0 \\\hline
				      0 & 1
			      \end{array} \right) \]
	      We call \(M\) a matrix `in block form'. So \(M_{ni} = M_{in} = 0\) if \(i \neq n\). So we can restrict the permutation \(\sigma\) to only transmuting the first \((n-1)\) terms, i.e. \(\sigma(n) = n\). So \(\det M = \det A\).
\end{enumerate}

\begin{proposition}
	If \(\vb R_a\) are the rows of \(M\), \(\det M\) is given by
	\begin{align*}
		\det M
		 & = [\vb R_1, \vb R_2, \cdots, \vb R_n]                                                \\
		 & = \varepsilon_{ij\cdots l}M_{1i}M_{2j} \cdots M_{nl}                                 \\
		 & = \sum_\sigma \varepsilon(\sigma) M_{1\sigma(1)}M_{2\sigma(2)} \cdots M_{n\sigma(n)}
	\end{align*}
	i.e. \(\det M = \det M^\transpose\).
\end{proposition}
\begin{proof}
	Recall that \((\vb C_a)_i = M_{ia} = (\vb R_i)_a\). We need to show that one of these definitions is equivalent to one of the previous definitions, then all other equivalent definitions follow. We use the \(\Sigma\) definition by considering the product \(M_{1\sigma(1)}M_{2\sigma(2)} \cdots M_{n\sigma(n)}\). We may rewrite this product in a different order: \(M_{\rho(1)1}M_{\rho(2)2} \cdots M_{\rho(n)n}\). Then \(\rho = \sigma^{-1}\). But then \(\varepsilon(\sigma) = \varepsilon(\rho)\), and a sum over \(\sigma\) is equivalent to a sum over \(\rho\).
\end{proof}

\subsection{Evaluating Determinants: Expanding by Rows or Columns}
For an \(n \times n\) matrix \(M\) with entries \(M_{ia}\), we define the minor \(M^{ia}\) to be the \((n-1)\times(n-1)\) determinant of the matrix obtained by deleting row \(i\) and column \(a\) from \(M\).
\begin{proposition}
	The determinant of a generic \(n \times n\) matrix \(M\) is given by
	\begin{align*}
		\det M
		 & = \sum_i (-1)^{i+a} M_{ia} M^{ia} \text{ for a fixed \(a\)} \\
		 & = \sum_a (-1)^{i+a} M_{ia} M^{ia} \text{ for a fixed \(i\)}
	\end{align*}
\end{proposition}
This process is known as expanding by row \(i\) or by column \(a\). As an example, let us take the following \(4 \times 4\) complex matrix
\[ M = \begin{pmatrix}
		i & 0  & 3  & 0  \\
		0 & 0  & 2i & 0  \\
		0 & 5i & 0  & -i \\
		2 & 0  & 0  & 1
	\end{pmatrix} \]
Then, the determinant is given by (expanding by row 3)
\begin{align*}
	\det M
	 & = -5i\begin{vmatrix}
		i & 3  & 0 \\
		0 & 2i & 0 \\
		2 & 0  & 1
	\end{vmatrix} + i\begin{vmatrix}
		i & 0 & 3  \\
		0 & 0 & 2i \\
		2 & 0 & 0
	\end{vmatrix}                                                              \\
	 & = -5i\left[i\begin{vmatrix}
			2i & 0 \\
			0  & 1
		\end{vmatrix} - 3 \begin{vmatrix}
			0 & 0 \\
			2 & 1
		\end{vmatrix}\right] + i\left[-2i\begin{vmatrix}
			i & 0 \\
			2 & 0
		\end{vmatrix}\right] \\
	 & = -5i[i \cdot 2i - 3 \cdot 0] + i[-2i \cdot 0]                                                                             \\
	 & = -5i[-2] + i[0]                                                                                                           \\
	 & = 10i
\end{align*}
