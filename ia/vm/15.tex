\subsection{Simplifying Determinants: Row and Column Operations}
Consider the following consequences of the properties of the determinant:
\begin{itemize}
	\item (row and column scaling) If $\vb R_i \mapsto \lambda \vb R_i$ for a fixed $i$, or $\vb C_a \mapsto \lambda \vb C_a$, then $\det M \mapsto \lambda \det M$ by multilinearity. If we scale all rows or columns, then $M \mapsto \lambda M$, so $\det M \mapsto \lambda^n \det M$ where $M$ is an $n \times n$ matrix.
	\item (row and column operations) If $\vb R_i \mapsto \vb R_i + \lambda \vb R_j$ where $i \neq j$ (or the corresponding conversion with columns), then $\det M \mapsto \det M$.
	\item (row and column exchanges) If we swap $\vb R_i$ and $\vb R_j$ (or two columns), then $\det M \mapsto -\det M$.
\end{itemize}
For example, let us find the terminant of matrix $A$, where
\[ A = \begin{pmatrix}
		1 & 1 & a \\ a & 1 & 1 \\ 1 & a & 1
	\end{pmatrix};\quad a \in \mathbb C \]
Then:
\begin{align*}
	\det A                                                    & = \begin{vmatrix} 1 & 1 & a \\ a & 1 & 1 \\ 1 & a & 1 \end{vmatrix}                            \\
	\vb C_1 \mapsto \vb C_1 - \vb C_3:\quad \det A            & = \begin{vmatrix} 1-a & 1 & a \\ a-1 & 1 & 1 \\ 0 & a & 1 \end{vmatrix}                            \\
	\det A                                                    & = (1-a)\begin{vmatrix} 1 & 1 & a \\ -1 & 1 & 1 \\ 0 & a & 1 \end{vmatrix}                       \\
	\vb C_2 \mapsto \vb C_2 - \vb C_3:\quad \det A            & = (1-a)\begin{vmatrix} 1 & 1-a & a \\ -1 & 0 & 1 \\ 0 & a-1 & 1 \end{vmatrix}                       \\
	\det A                                                    & = (1-a)^2\begin{vmatrix} 1 & 1 & a \\ -1 & 0 & 1 \\ 0 & -1 & 1 \end{vmatrix}                     \\
	\vb R_1 \mapsto \vb R_1 + \vb R_2 + \vb R_3 :\quad \det A & = (1-a)^2\begin{vmatrix} 0 & 0 & a+2 \\ -1 & 0 & 1 \\ 0 & -1 & 1 \end{vmatrix}                     \\
	\det A                                                    & = (1-a)^2(a+2)\begin{vmatrix}-1&0\\0&-1\end{vmatrix} = (1-a)^2(a+2)
\end{align*}

\subsection{Multiplicative Property of Determinants}
\begin{theorem}
	For $n\times n$ matrices $M, N$, $\det (MN) = \det M \cdot \det N$.
\end{theorem}
\noindent We can prove this using the following elaboration on the definition of the determinant:
\begin{lemma}
	\[ \varepsilon_{i_1 i_2 \cdots i_n} M_{i_1 a_1} M_{i_2 a_2} \cdots M_{i_n a_n} = (\det M) \varepsilon_{a_1 a_2 \cdots a_n} \]
\end{lemma}
\begin{proof}
	The left hand side and right hand side are each totally antisymmetric (alternating) in $a_1, a_2, \cdots, a_n$, so they must be related by a constant of proportionality. To fix the constant, we can simply consider taking $a_i = i$ and the result follows.
\end{proof}
\noindent Now, we prove the above theorem.
\begin{proof}
	Using the lemma above:
	\begin{align*}
		\det MN & = \varepsilon_{i_1 i_2 \cdots i_n} (MN)_{i_1 1} (MN)_{i_2 2} \cdots (MN)_{i_n n}                                                    \\
		        & = \varepsilon_{i_1 i_2 \cdots i_n} {M_{i_1 k_1} \atop N_{k_1 1}} {M_{i_2 k_2} \atop N_{k_2 2}} \cdots {M_{i_n k_n} \atop N_{k_n n}} \\
		        & = (\det M) \varepsilon_{a_1 a_2 \cdots a_n} N_{k_1 1} N_{k_2 2} \cdots N_{k_n n}                                                    \\
		        & = (\det M)(\det N)
	\end{align*}
	as required.
\end{proof}

\subsection{Consequences of Multiplicative Property}
\begin{enumerate}[(i)]
	\item $M^{-1}M = I \implies \det(M^{-1}) \det(M) = \det I = 1$. Therefore, $\det (M^{-1}) = (\det M)^{-1}$, so $\det M$ must be nonzero for $M$ to be invertible.
	\item For $R$ real and orthogonal, $R^\transpose R = I \implies \det(R^\transpose) \det(R) = 1$. But $\det (R^\transpose) = \det R$, so $(\det R)^2 = 1$, so $\det R = \pm 1$.
	\item For $U$ complex and unitary, $U^\dagger U = I \implies \det(U^\dagger) \det(U) = 1$. But since $U^\dagger = \overline{U^\transpose}$, we have $\overline{\det U} \det U = 1$, so $\abs{(\det U)^2} = 1$, so $\abs{\det U} = 1$.
\end{enumerate}

\subsection{Cofactors and Determinants}
Consider a column of some $n \times n$ matrix $M$, written in the form
\[ \vb C_a = \sum_i M_{ia} \vb e_i \]
\begin{align*}
	\implies \det M & = [ \vb C_1, \cdots, \vb C_a, \cdots, \vb C_n ]                                         \\
	                & = [ \vb C_1, \cdots, \vb C_{a-1}, \sum_i M_{ia} \vb e_i, \vb C_{a+1}, \cdots, \vb C_n ] \\
	                & = \sum_i M_{ia} \Delta_{ia}
\end{align*}
where
\begin{align*}
	\Delta_{ia} & = [ \vb C_1, \cdots, \vb C_{a-1}, \vb e_i, \vb C_{a+1}, \cdots, \vb C_n ]                                                                                          \\
	            & = \begin{vmatrix}
		\mathhuge A                 & \begin{matrix}
			0 \\ \vdots \\ 0
		\end{matrix} & \mathhuge B                 \\
		\begin{matrix}
			0 & \cdots & 0
		\end{matrix} & 1                           & \begin{matrix}
			0 & \cdots & 0
		\end{matrix} \\
		\mathhuge C                 & \begin{matrix}
			0 \\ \vdots \\ 0
		\end{matrix} & \mathhuge D
	\end{vmatrix}
	\intertext{where the zero entries in the rows arise from antisymmetry, giving}
	            & = \underbrace{(-1)^{n-a}}_{\text{amount of column transpositions}} \cdot \underbrace{(-1)^{n-i}}_{\text{amount of row transpositions}} \begin{vmatrix}
		\mathhuge A & \mathhuge B \\
		\mathhuge C & \mathhuge D
	\end{vmatrix} \\
	            & = (-1)^{i+a}M^{ia}
\end{align*}
where $M^{ia}$ is the minor in this position; the determinant of the matrix with this particular row and column removed. We call $\Delta_{ia}$ the cofactor.
\[ \det M = \sum_i M_{ia} \Delta_{ia} = \sum_i(-1)^{i+a}M_{ia}M^{ia} \]
Similarly, by considering rows,
\[ \det M = \sum_a M_{ia} \Delta_{ia} = \sum_a(-1)^{i+a}M_{ia}M^{ia} \]

\subsection{Adjugates and Inverses}
Reasoning as above, consider $\vb C_b = \sum_i M_{ib} \vb e_i$. Then,
\[ [\vb C_1, \cdots, \vb C_{a-1}, \vb C_b, \vb C_{a+1}, \cdots, \vb C_n ] = \sum_i M_{ib} \Delta_{ia} \]
If $a=b$ then clearly this is $\det M$. Otherwise, $\vb C_b$ is equal to one of the other columns, so $\sum_i M_{ib} \Delta_{ia} = 0$.
\[ \sum_i M_{ib} \Delta_{ia} = (\det M)\delta_{ab} \]
Similarly,
\[ \sum_a M_{ja} \Delta_{ia} = (\det M)\delta_{ij} \]
Now, let $\Delta$ be the matrix of cofactors (i.e. entries $\Delta_{ia}$), and we define the adjugate $\adjugate M = \Delta^\transpose$. Then
\[ \Delta_{ia}M_{ib} = \adjugate M_{ai}M_{ib} = (\adjugate M M)_{ab} = (\det M)\delta_{ab} \]
Therefore,
\[ \adjugate M M = (\det M) I \]
We can reach this result similarly considering the other index. Hence, if $\det M \neq 0$ then $M^{-1} = \frac{1}{\det M}\adjugate M$.
