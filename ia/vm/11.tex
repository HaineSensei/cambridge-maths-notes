\subsection{Linear Combinations}
If \(T: V \to W\) and \(S: V \to W\), between real or complex vector spaces \(V, W\) of dimension \(n, m\) respectively, are linear, then
\[
	\alpha T + \beta S: V \to W
\]
is also a linear map, where
\[
	(\alpha T + \beta S)(\vb x) = \alpha T(\vb x) + \beta S(\vb x)
\]
for any \(\vb x \in V\).
So the set of linear maps is a vector space.
If \(M\) and \(N\) are the \(m\times N\) matrices for \(T, S\) then \(\alpha M + \beta N\) is the \(m\times n\) matrix for the linear combination above, where
\[
	(\alpha M + \beta N)_{ai} + \alpha M_{ai} + \beta N_{ai};\quad a = 1, \cdots, m;\quad i = 1, \cdots, n
\]
with respect to the same bases.

\subsection{Matrix Multiplication}
If \(A\) is an \(m\times n\) matrix with entries \(A_{ai}\), and \(B\) is an \(n \times p\) matrix with entries \(B_{ir}\), then we define \(AB\) to be an \(m \times p\) matrix with entries
\[
	(AB)_{ar} = A_{ai}B_{ir};\quad a = 1, \cdots, m;\quad i = 1, \cdots, n;\quad r = 1, \cdots, p
\]
The product is not defined unless the amount of columns of \(A\) matches the number of rows of \(B\).

Matrix multiplication corresponds to composition of linear maps.
Consider linear maps:
\begin{align*}
	S: \mathbb R^p \to \mathbb R^n                  & ;\; S(\vb x) = B \vb x,\, \vb x \in \mathbb R^p \\
	T: \mathbb R^n \to \mathbb R^m                  & ;\; T(\vb x) = A \vb x,\, \vb x \in \mathbb R^n \\
	\implies T \circ S: \mathbb R^p \to \mathbb R^m & ;\; (T\circ S)(\vb x) = (AB)x
\end{align*}
since
\[
	\left[ (AB)\vb x \right]_a = (AB)_{ar}x_r
\]
and
\[
	A(B(\vb x)) = A_{ai} (B\vb x)_i = A_{ai} B_{ir} x_r = (AB)_{ar}x_r
\]
as required.
The definition of matrix multiplication ensures that these answers agree.
Of course, this proof works for complex or general vector spaces.

\subsection{Properties of Matrix Product}
Whenever the products are defined, then for any scalars \(\lambda\) and \(\mu\):
\begin{itemize}
	\item \((\lambda M + \mu N)P = \lambda MP + \mu NP\)
	\item \(P(\lambda M + \mu N) = \lambda PM + \mu PN\)
	\item \((MN)P = M(NP)\)
	\item \(IM = MI = M\) where \(I_{ij} = \delta_{ij}\)
\end{itemize}
We may view matrix multiplication in the following ways.
\begin{enumerate}[(i)]
	\item Regarding a vector \(\vb x \in \mathbb R^n\) as a column vector (an \(n \times 1\) matrix), then the matrix-vector and matrix-matrix multiplication rules agree.
	\item Consider the product \(AB\) where \(A\) is an \(m \times n\) matrix and \(B\) is an \(n \times p\), with columns \(\vb C_r(B) \in \mathbb R^n\) and columns \(\vb C_r(AB) \in \mathbb R^m\), where \(1 \leq r \leq p\).
	      The columns are related by \(\vb C_r(AB) = A \vb C_r(B)\).
	      Less formally, eavh column in the right matrix is acted on by the left matrix as if it were a vector, then the resultant vectors are combined into the output matrix.
	\item In terms of rows and columns,
	      \[
		      AB = \begin{pmatrix}
			                 & \vdots     &             \\
			      \leftarrow & \vb R_n(A) & \rightarrow \\
			                 & \vdots     &
		      \end{pmatrix} \begin{pmatrix}
			             & \uparrow   &        \\
			      \cdots & \vb C_r(B) & \cdots \\
			             & \downarrow &
		      \end{pmatrix}
	      \]
	      gives
	      \begin{align*}
		      (AB)_{ar} & = \left[ \vb R_a(A) \right]_i \left[ \vb C_r(B) \right]_i                                                  \\
		                & = \vb R_a(A) \cdot \vb C_r(B) \text{ for real matrices, where the \(\cdot\) is the dot product in \(R^n\)}
	      \end{align*}
\end{enumerate}

\subsection{Matrix Inverses}
If \(A\) is an \(m \times n\) then \(B\), an \(n \times m\) matrix, is a left inverse of \(A\) if \(BA = I\) (the \(n \times n\) identity matrix).
\(C\) is a right inverse of \(A\) if \(AC = I\) (the \(m \times m\) identity matrix).
If \(m = n\) (\(A\) is square), then one of these implies the other; there is no distinction between left and right inverses.
We say that \(B = C = A^{-1}\), \textit{the} inverse of the matrix \(A\), such that \(AA^{-1} = A^{-1}A = I\).
Not every matrix has an inverse.
If such an inverse exists, \(A\) is called invertible, or non-singular.

Consider \(\vb x, \vb x' \in \mathbb R^n\) or \(\mathbb C^n\), and \(M\) is an \(n \times n\) matrix.
If \(M^{-1}\) exists, we can solve the equation \(\vb x' = M \vb x\) for \(\vb x\), given \(\vb x'\), because we can apply the matrix inverse on the left.
For example, where \(n=2\), we have
\[
	M = \begin{pmatrix}
		M_{11} & M_{12} \\
		M_{21} & M_{22}
	\end{pmatrix}
\]
and
\begin{align*}
	x_1' & = M_{11}x_1 + M_{12}x_2 \\
	x_2' & = M_{21}x_1 + M_{22}x_2
\end{align*}
We can solve these simultaneous equations to construct the general matrix inverse.
\begin{align*}
	M_{22} x_1' - M_{12}x_2'  & = (\det M)x_1 \\
	-M_{21} x_1' + M_{11}x_2' & = (\det M)x_2
\end{align*}
where \(\det M = M_{11} M_{22} - M_{12} M_{21}\), called the determinant of the matrix.
Where the determinant is nonzero, the matrix inverse
\[
	M^{-1} = \frac{1}{\det M}\begin{pmatrix}
		M_{22}  & -M_{12} \\
		-M_{21} & M_{11}
	\end{pmatrix}
\]
exists.
Note that
\begin{align*}
	\vb C_1     & = M \vb e_1 = \begin{pmatrix} M_{11} \\ M_{21} \end{pmatrix}                            \\
	\vb C_2     & = M \vb e_2 = \begin{pmatrix} M_{12} \\ M_{22} \end{pmatrix}                            \\
	\iff \det M & = [\vb C_1, \vb C_2] = [M\vb e_1, M\vb e_2] \text{ in } \mathbb R^2
\end{align*}
So the determinant gives the signed factor by which areas are scaled under the action of \(M\).
\(\det M\) is nonzero if and only if \(M\vb e_1\) and \(M\vb e_2\) are linearly independent, which is true if and only if the image of \(M\) has dimension 2, i.e.\ \(M\) has maximal rank.
For example, a shear
\[
	S(\lambda) = \begin{pmatrix}
		1 & \lambda \\ 0 & 1
	\end{pmatrix}
\]
has determinant 1, so areas are preserved.
In particular, in this case,
\[
	S^{-1}(\lambda) = \begin{pmatrix}
		1 & -\lambda \\ 0 & 1
	\end{pmatrix} = S(-\lambda)
\]
As another example, we know that a matrix \(R(\theta)\) for a rotation about a fixed axis \(\nhat\) through angle \(\theta\) has formula
\begin{align*}
	R(\theta)_{ij} R(-\theta)_{jk} & = (\delta_{ij}\cos \theta + (1 - \cos \theta) n_i n_j - \varepsilon_{ijp}n_p \sin \theta) \times (\delta_{jk}\cos \theta + (1 - \cos \theta) n_j n_k + \varepsilon_{jkq}n_q \sin \theta) \\
	\intertext{Expanding out, noting that \(n_i n_i = 1\) as \(\nhat\) is a unit vector, and cancelling:}
	                               & = \delta_{ik} \cos^2 \theta + 2\cos \theta(1 - \cos \theta) n_i n_k + (1 - \cos \theta)^2n_i n_k - \varepsilon_{ijp}\varepsilon_{jkq} n_p n_q \sin^2 \theta                              \\
	\intertext{By using an \(\varepsilon\varepsilon\) identity:}
	                               & = \delta_{ik}\cos^2\theta + (1 - \cos^2 \theta)n_i n_k + \delta_{ik}n_p n_p \sin^2 \theta - (\sin^2 \theta)n_i n_k                                                                       \\
	                               & = \delta_{ik}\cos^2\theta + \delta_{ik}n_p n_p \sin^2 \theta                                                                                                                             \\
	                               & = \delta_{ik}\cos^2\theta + \delta_{ik} \sin^2 \theta                                                                                                                                    \\
	                               & = \delta_{ik}
\end{align*}
as required.
